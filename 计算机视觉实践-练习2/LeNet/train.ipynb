{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import argparse\n",
    "from src.config import mnist_cfg as cfg\n",
    "from src.dataset import create_dataset\n",
    "from src.lenet import LeNet5\n",
    "import mindspore.nn as nn\n",
    "from mindspore import context\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n",
    "from mindspore.train import Model\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.common import set_seed\n",
    "\n",
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(5792:6012,MainProcess):2024-05-02-13:56:11.481.178 [mindspore\\dataset\\core\\validator_helpers.py:806] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(5792:6012,MainProcess):2024-05-02-13:56:11.482.179 [mindspore\\dataset\\core\\validator_helpers.py:806] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(5792:6012,MainProcess):2024-05-02-13:56:11.483.177 [mindspore\\dataset\\core\\validator_helpers.py:806] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(5792:6012,MainProcess):2024-05-02-13:56:11.483.177 [mindspore\\dataset\\core\\validator_helpers.py:806] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(5792:6012,MainProcess):2024-05-02-13:56:11.484.687 [mindspore\\dataset\\core\\validator_helpers.py:806] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Starting Training ==============\n",
      "epoch: 1 step: 1, loss is 2.302579402923584\n",
      "epoch: 1 step: 2, loss is 2.3028666973114014\n",
      "epoch: 1 step: 3, loss is 2.3025550842285156\n",
      "epoch: 1 step: 4, loss is 2.3025496006011963\n",
      "epoch: 1 step: 5, loss is 2.3027262687683105\n",
      "epoch: 1 step: 6, loss is 2.3019509315490723\n",
      "epoch: 1 step: 7, loss is 2.3015546798706055\n",
      "epoch: 1 step: 8, loss is 2.303046941757202\n",
      "epoch: 1 step: 9, loss is 2.304112434387207\n",
      "epoch: 1 step: 10, loss is 2.3022210597991943\n",
      "epoch: 1 step: 11, loss is 2.3011741638183594\n",
      "epoch: 1 step: 12, loss is 2.3022422790527344\n",
      "epoch: 1 step: 13, loss is 2.299929618835449\n",
      "epoch: 1 step: 14, loss is 2.302668809890747\n",
      "epoch: 1 step: 15, loss is 2.298689603805542\n",
      "epoch: 1 step: 16, loss is 2.3024749755859375\n",
      "epoch: 1 step: 17, loss is 2.3005356788635254\n",
      "epoch: 1 step: 18, loss is 2.302804708480835\n",
      "epoch: 1 step: 19, loss is 2.297750473022461\n",
      "epoch: 1 step: 20, loss is 2.3072292804718018\n",
      "epoch: 1 step: 21, loss is 2.297698497772217\n",
      "epoch: 1 step: 22, loss is 2.302112102508545\n",
      "epoch: 1 step: 23, loss is 2.3054490089416504\n",
      "epoch: 1 step: 24, loss is 2.3008246421813965\n",
      "epoch: 1 step: 25, loss is 2.3035426139831543\n",
      "epoch: 1 step: 26, loss is 2.2953755855560303\n",
      "epoch: 1 step: 27, loss is 2.3010776042938232\n",
      "epoch: 1 step: 28, loss is 2.302558183670044\n",
      "epoch: 1 step: 29, loss is 2.305751323699951\n",
      "epoch: 1 step: 30, loss is 2.294111490249634\n",
      "epoch: 1 step: 31, loss is 2.295314073562622\n",
      "epoch: 1 step: 32, loss is 2.2987608909606934\n",
      "epoch: 1 step: 33, loss is 2.3175771236419678\n",
      "epoch: 1 step: 34, loss is 2.295365333557129\n",
      "epoch: 1 step: 35, loss is 2.2823293209075928\n",
      "epoch: 1 step: 36, loss is 2.2993762493133545\n",
      "epoch: 1 step: 37, loss is 2.2948601245880127\n",
      "epoch: 1 step: 38, loss is 2.2998287677764893\n",
      "epoch: 1 step: 39, loss is 2.2967889308929443\n",
      "epoch: 1 step: 40, loss is 2.3120999336242676\n",
      "epoch: 1 step: 41, loss is 2.2990169525146484\n",
      "epoch: 1 step: 42, loss is 2.2976341247558594\n",
      "epoch: 1 step: 43, loss is 2.305043935775757\n",
      "epoch: 1 step: 44, loss is 2.3090109825134277\n",
      "epoch: 1 step: 45, loss is 2.3019092082977295\n",
      "epoch: 1 step: 46, loss is 2.3090834617614746\n",
      "epoch: 1 step: 47, loss is 2.308558464050293\n",
      "epoch: 1 step: 48, loss is 2.301846981048584\n",
      "epoch: 1 step: 49, loss is 2.3089380264282227\n",
      "epoch: 1 step: 50, loss is 2.307163953781128\n",
      "epoch: 1 step: 51, loss is 2.300257921218872\n",
      "epoch: 1 step: 52, loss is 2.308443307876587\n",
      "epoch: 1 step: 53, loss is 2.306683301925659\n",
      "epoch: 1 step: 54, loss is 2.321748733520508\n",
      "epoch: 1 step: 55, loss is 2.2914175987243652\n",
      "epoch: 1 step: 56, loss is 2.297477960586548\n",
      "epoch: 1 step: 57, loss is 2.313457727432251\n",
      "epoch: 1 step: 58, loss is 2.2925496101379395\n",
      "epoch: 1 step: 59, loss is 2.3092432022094727\n",
      "epoch: 1 step: 60, loss is 2.316958427429199\n",
      "epoch: 1 step: 61, loss is 2.300848960876465\n",
      "epoch: 1 step: 62, loss is 2.3027210235595703\n",
      "epoch: 1 step: 63, loss is 2.306481122970581\n",
      "epoch: 1 step: 64, loss is 2.304760456085205\n",
      "epoch: 1 step: 65, loss is 2.294001340866089\n",
      "epoch: 1 step: 66, loss is 2.2999143600463867\n",
      "epoch: 1 step: 67, loss is 2.295755624771118\n",
      "epoch: 1 step: 68, loss is 2.3075218200683594\n",
      "epoch: 1 step: 69, loss is 2.3031692504882812\n",
      "epoch: 1 step: 70, loss is 2.3039257526397705\n",
      "epoch: 1 step: 71, loss is 2.299245595932007\n",
      "epoch: 1 step: 72, loss is 2.302201271057129\n",
      "epoch: 1 step: 73, loss is 2.3046233654022217\n",
      "epoch: 1 step: 74, loss is 2.291719675064087\n",
      "epoch: 1 step: 75, loss is 2.306959867477417\n",
      "epoch: 1 step: 76, loss is 2.299839496612549\n",
      "epoch: 1 step: 77, loss is 2.310338020324707\n",
      "epoch: 1 step: 78, loss is 2.3019206523895264\n",
      "epoch: 1 step: 79, loss is 2.307598829269409\n",
      "epoch: 1 step: 80, loss is 2.305398941040039\n",
      "epoch: 1 step: 81, loss is 2.2968480587005615\n",
      "epoch: 1 step: 82, loss is 2.2995123863220215\n",
      "epoch: 1 step: 83, loss is 2.3055224418640137\n",
      "epoch: 1 step: 84, loss is 2.305598020553589\n",
      "epoch: 1 step: 85, loss is 2.295340061187744\n",
      "epoch: 1 step: 86, loss is 2.2952880859375\n",
      "epoch: 1 step: 87, loss is 2.297071695327759\n",
      "epoch: 1 step: 88, loss is 2.3001179695129395\n",
      "epoch: 1 step: 89, loss is 2.3036935329437256\n",
      "epoch: 1 step: 90, loss is 2.3032732009887695\n",
      "epoch: 1 step: 91, loss is 2.307086706161499\n",
      "epoch: 1 step: 92, loss is 2.303300142288208\n",
      "epoch: 1 step: 93, loss is 2.302050828933716\n",
      "epoch: 1 step: 94, loss is 2.3007466793060303\n",
      "epoch: 1 step: 95, loss is 2.303765296936035\n",
      "epoch: 1 step: 96, loss is 2.2922401428222656\n",
      "epoch: 1 step: 97, loss is 2.3023505210876465\n",
      "epoch: 1 step: 98, loss is 2.2979109287261963\n",
      "epoch: 1 step: 99, loss is 2.3100149631500244\n",
      "epoch: 1 step: 100, loss is 2.289846420288086\n",
      "epoch: 1 step: 101, loss is 2.303846836090088\n",
      "epoch: 1 step: 102, loss is 2.3016622066497803\n",
      "epoch: 1 step: 103, loss is 2.300605297088623\n",
      "epoch: 1 step: 104, loss is 2.309904098510742\n",
      "epoch: 1 step: 105, loss is 2.308803081512451\n",
      "epoch: 1 step: 106, loss is 2.300459861755371\n",
      "epoch: 1 step: 107, loss is 2.2943007946014404\n",
      "epoch: 1 step: 108, loss is 2.2871992588043213\n",
      "epoch: 1 step: 109, loss is 2.291633367538452\n",
      "epoch: 1 step: 110, loss is 2.3031086921691895\n",
      "epoch: 1 step: 111, loss is 2.3089418411254883\n",
      "epoch: 1 step: 112, loss is 2.3130719661712646\n",
      "epoch: 1 step: 113, loss is 2.2948362827301025\n",
      "epoch: 1 step: 114, loss is 2.3054118156433105\n",
      "epoch: 1 step: 115, loss is 2.2938122749328613\n",
      "epoch: 1 step: 116, loss is 2.2902681827545166\n",
      "epoch: 1 step: 117, loss is 2.306440591812134\n",
      "epoch: 1 step: 118, loss is 2.310457706451416\n",
      "epoch: 1 step: 119, loss is 2.3054306507110596\n",
      "epoch: 1 step: 120, loss is 2.3078033924102783\n",
      "epoch: 1 step: 121, loss is 2.3113698959350586\n",
      "epoch: 1 step: 122, loss is 2.307339906692505\n",
      "epoch: 1 step: 123, loss is 2.2993037700653076\n",
      "epoch: 1 step: 124, loss is 2.2874836921691895\n",
      "epoch: 1 step: 125, loss is 2.2955315113067627\n",
      "epoch: 1 step: 126, loss is 2.3015644550323486\n",
      "epoch: 1 step: 127, loss is 2.301201105117798\n",
      "epoch: 1 step: 128, loss is 2.314370632171631\n",
      "epoch: 1 step: 129, loss is 2.315009593963623\n",
      "epoch: 1 step: 130, loss is 2.3057804107666016\n",
      "epoch: 1 step: 131, loss is 2.2991368770599365\n",
      "epoch: 1 step: 132, loss is 2.293092966079712\n",
      "epoch: 1 step: 133, loss is 2.310338258743286\n",
      "epoch: 1 step: 134, loss is 2.3054075241088867\n",
      "epoch: 1 step: 135, loss is 2.3114590644836426\n",
      "epoch: 1 step: 136, loss is 2.296584129333496\n",
      "epoch: 1 step: 137, loss is 2.3104817867279053\n",
      "epoch: 1 step: 138, loss is 2.308377265930176\n",
      "epoch: 1 step: 139, loss is 2.305114984512329\n",
      "epoch: 1 step: 140, loss is 2.2885470390319824\n",
      "epoch: 1 step: 141, loss is 2.2826173305511475\n",
      "epoch: 1 step: 142, loss is 2.2918128967285156\n",
      "epoch: 1 step: 143, loss is 2.305823564529419\n",
      "epoch: 1 step: 144, loss is 2.3139474391937256\n",
      "epoch: 1 step: 145, loss is 2.2920339107513428\n",
      "epoch: 1 step: 146, loss is 2.3015403747558594\n",
      "epoch: 1 step: 147, loss is 2.2949600219726562\n",
      "epoch: 1 step: 148, loss is 2.3026413917541504\n",
      "epoch: 1 step: 149, loss is 2.306864023208618\n",
      "epoch: 1 step: 150, loss is 2.3034510612487793\n",
      "epoch: 1 step: 151, loss is 2.316392183303833\n",
      "epoch: 1 step: 152, loss is 2.2898879051208496\n",
      "epoch: 1 step: 153, loss is 2.308810234069824\n",
      "epoch: 1 step: 154, loss is 2.296926975250244\n",
      "epoch: 1 step: 155, loss is 2.2952253818511963\n",
      "epoch: 1 step: 156, loss is 2.303220748901367\n",
      "epoch: 1 step: 157, loss is 2.3010594844818115\n",
      "epoch: 1 step: 158, loss is 2.302745819091797\n",
      "epoch: 1 step: 159, loss is 2.290422201156616\n",
      "epoch: 1 step: 160, loss is 2.300124406814575\n",
      "epoch: 1 step: 161, loss is 2.2971930503845215\n",
      "epoch: 1 step: 162, loss is 2.2945618629455566\n",
      "epoch: 1 step: 163, loss is 2.289635419845581\n",
      "epoch: 1 step: 164, loss is 2.3048360347747803\n",
      "epoch: 1 step: 165, loss is 2.2957231998443604\n",
      "epoch: 1 step: 166, loss is 2.302586555480957\n",
      "epoch: 1 step: 167, loss is 2.2974889278411865\n",
      "epoch: 1 step: 168, loss is 2.304302453994751\n",
      "epoch: 1 step: 169, loss is 2.3030643463134766\n",
      "epoch: 1 step: 170, loss is 2.305422306060791\n",
      "epoch: 1 step: 171, loss is 2.2998032569885254\n",
      "epoch: 1 step: 172, loss is 2.3067049980163574\n",
      "epoch: 1 step: 173, loss is 2.2783303260803223\n",
      "epoch: 1 step: 174, loss is 2.3074333667755127\n",
      "epoch: 1 step: 175, loss is 2.2812767028808594\n",
      "epoch: 1 step: 176, loss is 2.3161404132843018\n",
      "epoch: 1 step: 177, loss is 2.313746690750122\n",
      "epoch: 1 step: 178, loss is 2.294997453689575\n",
      "epoch: 1 step: 179, loss is 2.2963221073150635\n",
      "epoch: 1 step: 180, loss is 2.2985219955444336\n",
      "epoch: 1 step: 181, loss is 2.301056146621704\n",
      "epoch: 1 step: 182, loss is 2.3154513835906982\n",
      "epoch: 1 step: 183, loss is 2.29213285446167\n",
      "epoch: 1 step: 184, loss is 2.310748815536499\n",
      "epoch: 1 step: 185, loss is 2.311601400375366\n",
      "epoch: 1 step: 186, loss is 2.2890613079071045\n",
      "epoch: 1 step: 187, loss is 2.2900032997131348\n",
      "epoch: 1 step: 188, loss is 2.291548252105713\n",
      "epoch: 1 step: 189, loss is 2.300065040588379\n",
      "epoch: 1 step: 190, loss is 2.3046035766601562\n",
      "epoch: 1 step: 191, loss is 2.279724359512329\n",
      "epoch: 1 step: 192, loss is 2.304182529449463\n",
      "epoch: 1 step: 193, loss is 2.291490316390991\n",
      "epoch: 1 step: 194, loss is 2.2815306186676025\n",
      "epoch: 1 step: 195, loss is 2.3023877143859863\n",
      "epoch: 1 step: 196, loss is 2.320355176925659\n",
      "epoch: 1 step: 197, loss is 2.3035454750061035\n",
      "epoch: 1 step: 198, loss is 2.289217948913574\n",
      "epoch: 1 step: 199, loss is 2.2817165851593018\n",
      "epoch: 1 step: 200, loss is 2.2782669067382812\n",
      "epoch: 1 step: 201, loss is 2.2931830883026123\n",
      "epoch: 1 step: 202, loss is 2.3003265857696533\n",
      "epoch: 1 step: 203, loss is 2.2920374870300293\n",
      "epoch: 1 step: 204, loss is 2.307249069213867\n",
      "epoch: 1 step: 205, loss is 2.3078436851501465\n",
      "epoch: 1 step: 206, loss is 2.307403802871704\n",
      "epoch: 1 step: 207, loss is 2.3242552280426025\n",
      "epoch: 1 step: 208, loss is 2.286637306213379\n",
      "epoch: 1 step: 209, loss is 2.3204331398010254\n",
      "epoch: 1 step: 210, loss is 2.2781052589416504\n",
      "epoch: 1 step: 211, loss is 2.30001163482666\n",
      "epoch: 1 step: 212, loss is 2.337494134902954\n",
      "epoch: 1 step: 213, loss is 2.3112266063690186\n",
      "epoch: 1 step: 214, loss is 2.297013521194458\n",
      "epoch: 1 step: 215, loss is 2.298182725906372\n",
      "epoch: 1 step: 216, loss is 2.310241222381592\n",
      "epoch: 1 step: 217, loss is 2.284698247909546\n",
      "epoch: 1 step: 218, loss is 2.3155877590179443\n",
      "epoch: 1 step: 219, loss is 2.3086166381835938\n",
      "epoch: 1 step: 220, loss is 2.2957842350006104\n",
      "epoch: 1 step: 221, loss is 2.301175594329834\n",
      "epoch: 1 step: 222, loss is 2.2999281883239746\n",
      "epoch: 1 step: 223, loss is 2.292433500289917\n",
      "epoch: 1 step: 224, loss is 2.3044273853302\n",
      "epoch: 1 step: 225, loss is 2.2871241569519043\n",
      "epoch: 1 step: 226, loss is 2.288245439529419\n",
      "epoch: 1 step: 227, loss is 2.316077470779419\n",
      "epoch: 1 step: 228, loss is 2.305758476257324\n",
      "epoch: 1 step: 229, loss is 2.2796709537506104\n",
      "epoch: 1 step: 230, loss is 2.286647319793701\n",
      "epoch: 1 step: 231, loss is 2.3126394748687744\n",
      "epoch: 1 step: 232, loss is 2.3016207218170166\n",
      "epoch: 1 step: 233, loss is 2.2999513149261475\n",
      "epoch: 1 step: 234, loss is 2.2992351055145264\n",
      "epoch: 1 step: 235, loss is 2.298643112182617\n",
      "epoch: 1 step: 236, loss is 2.3200113773345947\n",
      "epoch: 1 step: 237, loss is 2.2669012546539307\n",
      "epoch: 1 step: 238, loss is 2.2956066131591797\n",
      "epoch: 1 step: 239, loss is 2.312350273132324\n",
      "epoch: 1 step: 240, loss is 2.279961347579956\n",
      "epoch: 1 step: 241, loss is 2.2908172607421875\n",
      "epoch: 1 step: 242, loss is 2.3208179473876953\n",
      "epoch: 1 step: 243, loss is 2.299891948699951\n",
      "epoch: 1 step: 244, loss is 2.299862861633301\n",
      "epoch: 1 step: 245, loss is 2.3123092651367188\n",
      "epoch: 1 step: 246, loss is 2.285080671310425\n",
      "epoch: 1 step: 247, loss is 2.311192512512207\n",
      "epoch: 1 step: 248, loss is 2.29471492767334\n",
      "epoch: 1 step: 249, loss is 2.300440788269043\n",
      "epoch: 1 step: 250, loss is 2.2924375534057617\n",
      "epoch: 1 step: 251, loss is 2.2935352325439453\n",
      "epoch: 1 step: 252, loss is 2.297484874725342\n",
      "epoch: 1 step: 253, loss is 2.291038751602173\n",
      "epoch: 1 step: 254, loss is 2.312938928604126\n",
      "epoch: 1 step: 255, loss is 2.2970447540283203\n",
      "epoch: 1 step: 256, loss is 2.289745807647705\n",
      "epoch: 1 step: 257, loss is 2.2926290035247803\n",
      "epoch: 1 step: 258, loss is 2.3064873218536377\n",
      "epoch: 1 step: 259, loss is 2.307055950164795\n",
      "epoch: 1 step: 260, loss is 2.310030698776245\n",
      "epoch: 1 step: 261, loss is 2.3177249431610107\n",
      "epoch: 1 step: 262, loss is 2.309544324874878\n",
      "epoch: 1 step: 263, loss is 2.3221778869628906\n",
      "epoch: 1 step: 264, loss is 2.313959836959839\n",
      "epoch: 1 step: 265, loss is 2.313474416732788\n",
      "epoch: 1 step: 266, loss is 2.2861969470977783\n",
      "epoch: 1 step: 267, loss is 2.3115668296813965\n",
      "epoch: 1 step: 268, loss is 2.290858268737793\n",
      "epoch: 1 step: 269, loss is 2.309422731399536\n",
      "epoch: 1 step: 270, loss is 2.2739570140838623\n",
      "epoch: 1 step: 271, loss is 2.311851978302002\n",
      "epoch: 1 step: 272, loss is 2.3091375827789307\n",
      "epoch: 1 step: 273, loss is 2.307731866836548\n",
      "epoch: 1 step: 274, loss is 2.294053077697754\n",
      "epoch: 1 step: 275, loss is 2.2973690032958984\n",
      "epoch: 1 step: 276, loss is 2.3040244579315186\n",
      "epoch: 1 step: 277, loss is 2.296048641204834\n",
      "epoch: 1 step: 278, loss is 2.289074659347534\n",
      "epoch: 1 step: 279, loss is 2.3042726516723633\n",
      "epoch: 1 step: 280, loss is 2.3009753227233887\n",
      "epoch: 1 step: 281, loss is 2.285672426223755\n",
      "epoch: 1 step: 282, loss is 2.2812962532043457\n",
      "epoch: 1 step: 283, loss is 2.2983264923095703\n",
      "epoch: 1 step: 284, loss is 2.30604887008667\n",
      "epoch: 1 step: 285, loss is 2.3214874267578125\n",
      "epoch: 1 step: 286, loss is 2.295437812805176\n",
      "epoch: 1 step: 287, loss is 2.3069047927856445\n",
      "epoch: 1 step: 288, loss is 2.295339584350586\n",
      "epoch: 1 step: 289, loss is 2.311746120452881\n",
      "epoch: 1 step: 290, loss is 2.287613868713379\n",
      "epoch: 1 step: 291, loss is 2.314288377761841\n",
      "epoch: 1 step: 292, loss is 2.304858446121216\n",
      "epoch: 1 step: 293, loss is 2.3099567890167236\n",
      "epoch: 1 step: 294, loss is 2.3043901920318604\n",
      "epoch: 1 step: 295, loss is 2.307424545288086\n",
      "epoch: 1 step: 296, loss is 2.3159232139587402\n",
      "epoch: 1 step: 297, loss is 2.299935817718506\n",
      "epoch: 1 step: 298, loss is 2.322411060333252\n",
      "epoch: 1 step: 299, loss is 2.279130220413208\n",
      "epoch: 1 step: 300, loss is 2.304043769836426\n",
      "epoch: 1 step: 301, loss is 2.2986817359924316\n",
      "epoch: 1 step: 302, loss is 2.3158018589019775\n",
      "epoch: 1 step: 303, loss is 2.3180058002471924\n",
      "epoch: 1 step: 304, loss is 2.295625925064087\n",
      "epoch: 1 step: 305, loss is 2.321929454803467\n",
      "epoch: 1 step: 306, loss is 2.309382200241089\n",
      "epoch: 1 step: 307, loss is 2.303504705429077\n",
      "epoch: 1 step: 308, loss is 2.3058829307556152\n",
      "epoch: 1 step: 309, loss is 2.289900779724121\n",
      "epoch: 1 step: 310, loss is 2.2960715293884277\n",
      "epoch: 1 step: 311, loss is 2.3108432292938232\n",
      "epoch: 1 step: 312, loss is 2.309705972671509\n",
      "epoch: 1 step: 313, loss is 2.283146858215332\n",
      "epoch: 1 step: 314, loss is 2.321242332458496\n",
      "epoch: 1 step: 315, loss is 2.3195271492004395\n",
      "epoch: 1 step: 316, loss is 2.307786464691162\n",
      "epoch: 1 step: 317, loss is 2.3012073040008545\n",
      "epoch: 1 step: 318, loss is 2.286123275756836\n",
      "epoch: 1 step: 319, loss is 2.3084492683410645\n",
      "epoch: 1 step: 320, loss is 2.30780291557312\n",
      "epoch: 1 step: 321, loss is 2.3146345615386963\n",
      "epoch: 1 step: 322, loss is 2.3059756755828857\n",
      "epoch: 1 step: 323, loss is 2.286508560180664\n",
      "epoch: 1 step: 324, loss is 2.3153810501098633\n",
      "epoch: 1 step: 325, loss is 2.2948081493377686\n",
      "epoch: 1 step: 326, loss is 2.3057358264923096\n",
      "epoch: 1 step: 327, loss is 2.291992425918579\n",
      "epoch: 1 step: 328, loss is 2.317211151123047\n",
      "epoch: 1 step: 329, loss is 2.3062798976898193\n",
      "epoch: 1 step: 330, loss is 2.2954225540161133\n",
      "epoch: 1 step: 331, loss is 2.3159408569335938\n",
      "epoch: 1 step: 332, loss is 2.298710346221924\n",
      "epoch: 1 step: 333, loss is 2.2975807189941406\n",
      "epoch: 1 step: 334, loss is 2.2932424545288086\n",
      "epoch: 1 step: 335, loss is 2.295719623565674\n",
      "epoch: 1 step: 336, loss is 2.2982254028320312\n",
      "epoch: 1 step: 337, loss is 2.307521104812622\n",
      "epoch: 1 step: 338, loss is 2.2894651889801025\n",
      "epoch: 1 step: 339, loss is 2.289332866668701\n",
      "epoch: 1 step: 340, loss is 2.305213451385498\n",
      "epoch: 1 step: 341, loss is 2.3088796138763428\n",
      "epoch: 1 step: 342, loss is 2.2905917167663574\n",
      "epoch: 1 step: 343, loss is 2.303197145462036\n",
      "epoch: 1 step: 344, loss is 2.289167881011963\n",
      "epoch: 1 step: 345, loss is 2.3076677322387695\n",
      "epoch: 1 step: 346, loss is 2.292266607284546\n",
      "epoch: 1 step: 347, loss is 2.303375720977783\n",
      "epoch: 1 step: 348, loss is 2.300523281097412\n",
      "epoch: 1 step: 349, loss is 2.29616379737854\n",
      "epoch: 1 step: 350, loss is 2.273591995239258\n",
      "epoch: 1 step: 351, loss is 2.315859794616699\n",
      "epoch: 1 step: 352, loss is 2.2898805141448975\n",
      "epoch: 1 step: 353, loss is 2.3096818923950195\n",
      "epoch: 1 step: 354, loss is 2.294948101043701\n",
      "epoch: 1 step: 355, loss is 2.309230327606201\n",
      "epoch: 1 step: 356, loss is 2.3011133670806885\n",
      "epoch: 1 step: 357, loss is 2.3152191638946533\n",
      "epoch: 1 step: 358, loss is 2.29609751701355\n",
      "epoch: 1 step: 359, loss is 2.304668664932251\n",
      "epoch: 1 step: 360, loss is 2.3037843704223633\n",
      "epoch: 1 step: 361, loss is 2.3034191131591797\n",
      "epoch: 1 step: 362, loss is 2.302668809890747\n",
      "epoch: 1 step: 363, loss is 2.297210693359375\n",
      "epoch: 1 step: 364, loss is 2.3123390674591064\n",
      "epoch: 1 step: 365, loss is 2.3076138496398926\n",
      "epoch: 1 step: 366, loss is 2.2994916439056396\n",
      "epoch: 1 step: 367, loss is 2.29468035697937\n",
      "epoch: 1 step: 368, loss is 2.3198680877685547\n",
      "epoch: 1 step: 369, loss is 2.3057236671447754\n",
      "epoch: 1 step: 370, loss is 2.304126501083374\n",
      "epoch: 1 step: 371, loss is 2.3024368286132812\n",
      "epoch: 1 step: 372, loss is 2.3072071075439453\n",
      "epoch: 1 step: 373, loss is 2.3122260570526123\n",
      "epoch: 1 step: 374, loss is 2.3029143810272217\n",
      "epoch: 1 step: 375, loss is 2.3076958656311035\n",
      "epoch: 1 step: 376, loss is 2.2886221408843994\n",
      "epoch: 1 step: 377, loss is 2.2953155040740967\n",
      "epoch: 1 step: 378, loss is 2.3013834953308105\n",
      "epoch: 1 step: 379, loss is 2.2975950241088867\n",
      "epoch: 1 step: 380, loss is 2.3095831871032715\n",
      "epoch: 1 step: 381, loss is 2.3185222148895264\n",
      "epoch: 1 step: 382, loss is 2.302868366241455\n",
      "epoch: 1 step: 383, loss is 2.2909116744995117\n",
      "epoch: 1 step: 384, loss is 2.316603660583496\n",
      "epoch: 1 step: 385, loss is 2.30790376663208\n",
      "epoch: 1 step: 386, loss is 2.2997548580169678\n",
      "epoch: 1 step: 387, loss is 2.282423973083496\n",
      "epoch: 1 step: 388, loss is 2.3086540699005127\n",
      "epoch: 1 step: 389, loss is 2.303326368331909\n",
      "epoch: 1 step: 390, loss is 2.2916319370269775\n",
      "epoch: 1 step: 391, loss is 2.3047003746032715\n",
      "epoch: 1 step: 392, loss is 2.309011936187744\n",
      "epoch: 1 step: 393, loss is 2.273710250854492\n",
      "epoch: 1 step: 394, loss is 2.296607255935669\n",
      "epoch: 1 step: 395, loss is 2.338270664215088\n",
      "epoch: 1 step: 396, loss is 2.3248209953308105\n",
      "epoch: 1 step: 397, loss is 2.3017635345458984\n",
      "epoch: 1 step: 398, loss is 2.29596209526062\n",
      "epoch: 1 step: 399, loss is 2.2990307807922363\n",
      "epoch: 1 step: 400, loss is 2.3150181770324707\n",
      "epoch: 1 step: 401, loss is 2.3251278400421143\n",
      "epoch: 1 step: 402, loss is 2.303367853164673\n",
      "epoch: 1 step: 403, loss is 2.304661750793457\n",
      "epoch: 1 step: 404, loss is 2.314150333404541\n",
      "epoch: 1 step: 405, loss is 2.3100645542144775\n",
      "epoch: 1 step: 406, loss is 2.3116815090179443\n",
      "epoch: 1 step: 407, loss is 2.2969954013824463\n",
      "epoch: 1 step: 408, loss is 2.3019251823425293\n",
      "epoch: 1 step: 409, loss is 2.295480966567993\n",
      "epoch: 1 step: 410, loss is 2.2848238945007324\n",
      "epoch: 1 step: 411, loss is 2.2985124588012695\n",
      "epoch: 1 step: 412, loss is 2.303502321243286\n",
      "epoch: 1 step: 413, loss is 2.293883800506592\n",
      "epoch: 1 step: 414, loss is 2.297771453857422\n",
      "epoch: 1 step: 415, loss is 2.2895612716674805\n",
      "epoch: 1 step: 416, loss is 2.2994799613952637\n",
      "epoch: 1 step: 417, loss is 2.305198907852173\n",
      "epoch: 1 step: 418, loss is 2.307023763656616\n",
      "epoch: 1 step: 419, loss is 2.295647382736206\n",
      "epoch: 1 step: 420, loss is 2.2915866374969482\n",
      "epoch: 1 step: 421, loss is 2.2867419719696045\n",
      "epoch: 1 step: 422, loss is 2.305583953857422\n",
      "epoch: 1 step: 423, loss is 2.3123655319213867\n",
      "epoch: 1 step: 424, loss is 2.287783622741699\n",
      "epoch: 1 step: 425, loss is 2.2941136360168457\n",
      "epoch: 1 step: 426, loss is 2.3023788928985596\n",
      "epoch: 1 step: 427, loss is 2.289180040359497\n",
      "epoch: 1 step: 428, loss is 2.296250343322754\n",
      "epoch: 1 step: 429, loss is 2.2946128845214844\n",
      "epoch: 1 step: 430, loss is 2.287318229675293\n",
      "epoch: 1 step: 431, loss is 2.28109073638916\n",
      "epoch: 1 step: 432, loss is 2.3152570724487305\n",
      "epoch: 1 step: 433, loss is 2.3065037727355957\n",
      "epoch: 1 step: 434, loss is 2.288045883178711\n",
      "epoch: 1 step: 435, loss is 2.3109686374664307\n",
      "epoch: 1 step: 436, loss is 2.304227113723755\n",
      "epoch: 1 step: 437, loss is 2.309894561767578\n",
      "epoch: 1 step: 438, loss is 2.29459547996521\n",
      "epoch: 1 step: 439, loss is 2.3204078674316406\n",
      "epoch: 1 step: 440, loss is 2.3044469356536865\n",
      "epoch: 1 step: 441, loss is 2.29719877243042\n",
      "epoch: 1 step: 442, loss is 2.306114912033081\n",
      "epoch: 1 step: 443, loss is 2.315702438354492\n",
      "epoch: 1 step: 444, loss is 2.2917845249176025\n",
      "epoch: 1 step: 445, loss is 2.3088321685791016\n",
      "epoch: 1 step: 446, loss is 2.3117690086364746\n",
      "epoch: 1 step: 447, loss is 2.303954839706421\n",
      "epoch: 1 step: 448, loss is 2.292762517929077\n",
      "epoch: 1 step: 449, loss is 2.282681941986084\n",
      "epoch: 1 step: 450, loss is 2.276886463165283\n",
      "epoch: 1 step: 451, loss is 2.323031187057495\n",
      "epoch: 1 step: 452, loss is 2.3147988319396973\n",
      "epoch: 1 step: 453, loss is 2.3028550148010254\n",
      "epoch: 1 step: 454, loss is 2.342586040496826\n",
      "epoch: 1 step: 455, loss is 2.304945468902588\n",
      "epoch: 1 step: 456, loss is 2.295588254928589\n",
      "epoch: 1 step: 457, loss is 2.2917966842651367\n",
      "epoch: 1 step: 458, loss is 2.2990355491638184\n",
      "epoch: 1 step: 459, loss is 2.3196401596069336\n",
      "epoch: 1 step: 460, loss is 2.2978122234344482\n",
      "epoch: 1 step: 461, loss is 2.305232048034668\n",
      "epoch: 1 step: 462, loss is 2.3020362854003906\n",
      "epoch: 1 step: 463, loss is 2.294177770614624\n",
      "epoch: 1 step: 464, loss is 2.3184783458709717\n",
      "epoch: 1 step: 465, loss is 2.3108460903167725\n",
      "epoch: 1 step: 466, loss is 2.30385422706604\n",
      "epoch: 1 step: 467, loss is 2.305173635482788\n",
      "epoch: 1 step: 468, loss is 2.3142220973968506\n",
      "epoch: 1 step: 469, loss is 2.298219680786133\n",
      "epoch: 1 step: 470, loss is 2.3076610565185547\n",
      "epoch: 1 step: 471, loss is 2.318706750869751\n",
      "epoch: 1 step: 472, loss is 2.2973439693450928\n",
      "epoch: 1 step: 473, loss is 2.313563108444214\n",
      "epoch: 1 step: 474, loss is 2.2934746742248535\n",
      "epoch: 1 step: 475, loss is 2.30334734916687\n",
      "epoch: 1 step: 476, loss is 2.297654151916504\n",
      "epoch: 1 step: 477, loss is 2.29494571685791\n",
      "epoch: 1 step: 478, loss is 2.3062262535095215\n",
      "epoch: 1 step: 479, loss is 2.301253080368042\n",
      "epoch: 1 step: 480, loss is 2.3146331310272217\n",
      "epoch: 1 step: 481, loss is 2.2959680557250977\n",
      "epoch: 1 step: 482, loss is 2.2972729206085205\n",
      "epoch: 1 step: 483, loss is 2.3177804946899414\n",
      "epoch: 1 step: 484, loss is 2.2976856231689453\n",
      "epoch: 1 step: 485, loss is 2.2917468547821045\n",
      "epoch: 1 step: 486, loss is 2.3199422359466553\n",
      "epoch: 1 step: 487, loss is 2.300891876220703\n",
      "epoch: 1 step: 488, loss is 2.305943489074707\n",
      "epoch: 1 step: 489, loss is 2.2984471321105957\n",
      "epoch: 1 step: 490, loss is 2.300318479537964\n",
      "epoch: 1 step: 491, loss is 2.315687894821167\n",
      "epoch: 1 step: 492, loss is 2.2977662086486816\n",
      "epoch: 1 step: 493, loss is 2.292283773422241\n",
      "epoch: 1 step: 494, loss is 2.3010520935058594\n",
      "epoch: 1 step: 495, loss is 2.2984328269958496\n",
      "epoch: 1 step: 496, loss is 2.293300151824951\n",
      "epoch: 1 step: 497, loss is 2.321695327758789\n",
      "epoch: 1 step: 498, loss is 2.2819366455078125\n",
      "epoch: 1 step: 499, loss is 2.3026387691497803\n",
      "epoch: 1 step: 500, loss is 2.302443504333496\n",
      "epoch: 1 step: 501, loss is 2.2848901748657227\n",
      "epoch: 1 step: 502, loss is 2.3096563816070557\n",
      "epoch: 1 step: 503, loss is 2.301706552505493\n",
      "epoch: 1 step: 504, loss is 2.2960011959075928\n",
      "epoch: 1 step: 505, loss is 2.2888081073760986\n",
      "epoch: 1 step: 506, loss is 2.279062032699585\n",
      "epoch: 1 step: 507, loss is 2.310168981552124\n",
      "epoch: 1 step: 508, loss is 2.3030638694763184\n",
      "epoch: 1 step: 509, loss is 2.306488513946533\n",
      "epoch: 1 step: 510, loss is 2.2870798110961914\n",
      "epoch: 1 step: 511, loss is 2.2775864601135254\n",
      "epoch: 1 step: 512, loss is 2.2873101234436035\n",
      "epoch: 1 step: 513, loss is 2.3017666339874268\n",
      "epoch: 1 step: 514, loss is 2.2777273654937744\n",
      "epoch: 1 step: 515, loss is 2.3111486434936523\n",
      "epoch: 1 step: 516, loss is 2.297100305557251\n",
      "epoch: 1 step: 517, loss is 2.3109495639801025\n",
      "epoch: 1 step: 518, loss is 2.2948005199432373\n",
      "epoch: 1 step: 519, loss is 2.312305212020874\n",
      "epoch: 1 step: 520, loss is 2.3046398162841797\n",
      "epoch: 1 step: 521, loss is 2.283518075942993\n",
      "epoch: 1 step: 522, loss is 2.291713237762451\n",
      "epoch: 1 step: 523, loss is 2.3000330924987793\n",
      "epoch: 1 step: 524, loss is 2.303530693054199\n",
      "epoch: 1 step: 525, loss is 2.310882091522217\n",
      "epoch: 1 step: 526, loss is 2.2847349643707275\n",
      "epoch: 1 step: 527, loss is 2.305506706237793\n",
      "epoch: 1 step: 528, loss is 2.3161678314208984\n",
      "epoch: 1 step: 529, loss is 2.2925007343292236\n",
      "epoch: 1 step: 530, loss is 2.307678699493408\n",
      "epoch: 1 step: 531, loss is 2.314086437225342\n",
      "epoch: 1 step: 532, loss is 2.316070079803467\n",
      "epoch: 1 step: 533, loss is 2.2755277156829834\n",
      "epoch: 1 step: 534, loss is 2.3174030780792236\n",
      "epoch: 1 step: 535, loss is 2.3143396377563477\n",
      "epoch: 1 step: 536, loss is 2.313910722732544\n",
      "epoch: 1 step: 537, loss is 2.3248305320739746\n",
      "epoch: 1 step: 538, loss is 2.2899293899536133\n",
      "epoch: 1 step: 539, loss is 2.3159358501434326\n",
      "epoch: 1 step: 540, loss is 2.3255486488342285\n",
      "epoch: 1 step: 541, loss is 2.286715030670166\n",
      "epoch: 1 step: 542, loss is 2.315542697906494\n",
      "epoch: 1 step: 543, loss is 2.309825897216797\n",
      "epoch: 1 step: 544, loss is 2.3062703609466553\n",
      "epoch: 1 step: 545, loss is 2.285881280899048\n",
      "epoch: 1 step: 546, loss is 2.306649923324585\n",
      "epoch: 1 step: 547, loss is 2.323868751525879\n",
      "epoch: 1 step: 548, loss is 2.2937347888946533\n",
      "epoch: 1 step: 549, loss is 2.2906432151794434\n",
      "epoch: 1 step: 550, loss is 2.29906964302063\n",
      "epoch: 1 step: 551, loss is 2.2788143157958984\n",
      "epoch: 1 step: 552, loss is 2.3054497241973877\n",
      "epoch: 1 step: 553, loss is 2.3060333728790283\n",
      "epoch: 1 step: 554, loss is 2.2730867862701416\n",
      "epoch: 1 step: 555, loss is 2.300886631011963\n",
      "epoch: 1 step: 556, loss is 2.305469274520874\n",
      "epoch: 1 step: 557, loss is 2.3090643882751465\n",
      "epoch: 1 step: 558, loss is 2.2949821949005127\n",
      "epoch: 1 step: 559, loss is 2.3206260204315186\n",
      "epoch: 1 step: 560, loss is 2.326374053955078\n",
      "epoch: 1 step: 561, loss is 2.310312509536743\n",
      "epoch: 1 step: 562, loss is 2.308323383331299\n",
      "epoch: 1 step: 563, loss is 2.2965803146362305\n",
      "epoch: 1 step: 564, loss is 2.296483278274536\n",
      "epoch: 1 step: 565, loss is 2.3065831661224365\n",
      "epoch: 1 step: 566, loss is 2.3221518993377686\n",
      "epoch: 1 step: 567, loss is 2.310725688934326\n",
      "epoch: 1 step: 568, loss is 2.296816825866699\n",
      "epoch: 1 step: 569, loss is 2.3031022548675537\n",
      "epoch: 1 step: 570, loss is 2.28706431388855\n",
      "epoch: 1 step: 571, loss is 2.297071695327759\n",
      "epoch: 1 step: 572, loss is 2.3122870922088623\n",
      "epoch: 1 step: 573, loss is 2.2797374725341797\n",
      "epoch: 1 step: 574, loss is 2.283820152282715\n",
      "epoch: 1 step: 575, loss is 2.304842233657837\n",
      "epoch: 1 step: 576, loss is 2.307612657546997\n",
      "epoch: 1 step: 577, loss is 2.2855374813079834\n",
      "epoch: 1 step: 578, loss is 2.2952167987823486\n",
      "epoch: 1 step: 579, loss is 2.3149430751800537\n",
      "epoch: 1 step: 580, loss is 2.288972854614258\n",
      "epoch: 1 step: 581, loss is 2.300305128097534\n",
      "epoch: 1 step: 582, loss is 2.293478012084961\n",
      "epoch: 1 step: 583, loss is 2.3099710941314697\n",
      "epoch: 1 step: 584, loss is 2.2999746799468994\n",
      "epoch: 1 step: 585, loss is 2.291139841079712\n",
      "epoch: 1 step: 586, loss is 2.28621506690979\n",
      "epoch: 1 step: 587, loss is 2.3093345165252686\n",
      "epoch: 1 step: 588, loss is 2.303694486618042\n",
      "epoch: 1 step: 589, loss is 2.2986903190612793\n",
      "epoch: 1 step: 590, loss is 2.318434953689575\n",
      "epoch: 1 step: 591, loss is 2.285355567932129\n",
      "epoch: 1 step: 592, loss is 2.2829370498657227\n",
      "epoch: 1 step: 593, loss is 2.306786298751831\n",
      "epoch: 1 step: 594, loss is 2.3108320236206055\n",
      "epoch: 1 step: 595, loss is 2.301906108856201\n",
      "epoch: 1 step: 596, loss is 2.3201138973236084\n",
      "epoch: 1 step: 597, loss is 2.2922911643981934\n",
      "epoch: 1 step: 598, loss is 2.276776075363159\n",
      "epoch: 1 step: 599, loss is 2.313598155975342\n",
      "epoch: 1 step: 600, loss is 2.3183889389038086\n",
      "epoch: 1 step: 601, loss is 2.304837465286255\n",
      "epoch: 1 step: 602, loss is 2.278735876083374\n",
      "epoch: 1 step: 603, loss is 2.322498083114624\n",
      "epoch: 1 step: 604, loss is 2.2954399585723877\n",
      "epoch: 1 step: 605, loss is 2.28875470161438\n",
      "epoch: 1 step: 606, loss is 2.286572217941284\n",
      "epoch: 1 step: 607, loss is 2.2950572967529297\n",
      "epoch: 1 step: 608, loss is 2.306232452392578\n",
      "epoch: 1 step: 609, loss is 2.312976837158203\n",
      "epoch: 1 step: 610, loss is 2.2892332077026367\n",
      "epoch: 1 step: 611, loss is 2.3030455112457275\n",
      "epoch: 1 step: 612, loss is 2.312309503555298\n",
      "epoch: 1 step: 613, loss is 2.315250873565674\n",
      "epoch: 1 step: 614, loss is 2.296623706817627\n",
      "epoch: 1 step: 615, loss is 2.3261568546295166\n",
      "epoch: 1 step: 616, loss is 2.295542001724243\n",
      "epoch: 1 step: 617, loss is 2.3116800785064697\n",
      "epoch: 1 step: 618, loss is 2.281446695327759\n",
      "epoch: 1 step: 619, loss is 2.2767422199249268\n",
      "epoch: 1 step: 620, loss is 2.3007428646087646\n",
      "epoch: 1 step: 621, loss is 2.3146958351135254\n",
      "epoch: 1 step: 622, loss is 2.2971441745758057\n",
      "epoch: 1 step: 623, loss is 2.3030664920806885\n",
      "epoch: 1 step: 624, loss is 2.294893741607666\n",
      "epoch: 1 step: 625, loss is 2.2879269123077393\n",
      "epoch: 1 step: 626, loss is 2.313908100128174\n",
      "epoch: 1 step: 627, loss is 2.3084020614624023\n",
      "epoch: 1 step: 628, loss is 2.2836008071899414\n",
      "epoch: 1 step: 629, loss is 2.311707019805908\n",
      "epoch: 1 step: 630, loss is 2.302483320236206\n",
      "epoch: 1 step: 631, loss is 2.2725582122802734\n",
      "epoch: 1 step: 632, loss is 2.3056604862213135\n",
      "epoch: 1 step: 633, loss is 2.2899327278137207\n",
      "epoch: 1 step: 634, loss is 2.2947235107421875\n",
      "epoch: 1 step: 635, loss is 2.2777626514434814\n",
      "epoch: 1 step: 636, loss is 2.3386378288269043\n",
      "epoch: 1 step: 637, loss is 2.282984733581543\n",
      "epoch: 1 step: 638, loss is 2.3044869899749756\n",
      "epoch: 1 step: 639, loss is 2.3166794776916504\n",
      "epoch: 1 step: 640, loss is 2.3035244941711426\n",
      "epoch: 1 step: 641, loss is 2.2975480556488037\n",
      "epoch: 1 step: 642, loss is 2.3039352893829346\n",
      "epoch: 1 step: 643, loss is 2.3049070835113525\n",
      "epoch: 1 step: 644, loss is 2.3056700229644775\n",
      "epoch: 1 step: 645, loss is 2.3109278678894043\n",
      "epoch: 1 step: 646, loss is 2.301410436630249\n",
      "epoch: 1 step: 647, loss is 2.30786395072937\n",
      "epoch: 1 step: 648, loss is 2.3173909187316895\n",
      "epoch: 1 step: 649, loss is 2.290400505065918\n",
      "epoch: 1 step: 650, loss is 2.306854009628296\n",
      "epoch: 1 step: 651, loss is 2.3125438690185547\n",
      "epoch: 1 step: 652, loss is 2.319545030593872\n",
      "epoch: 1 step: 653, loss is 2.3113160133361816\n",
      "epoch: 1 step: 654, loss is 2.316528081893921\n",
      "epoch: 1 step: 655, loss is 2.315134286880493\n",
      "epoch: 1 step: 656, loss is 2.311642646789551\n",
      "epoch: 1 step: 657, loss is 2.2975692749023438\n",
      "epoch: 1 step: 658, loss is 2.2898507118225098\n",
      "epoch: 1 step: 659, loss is 2.3229386806488037\n",
      "epoch: 1 step: 660, loss is 2.3081653118133545\n",
      "epoch: 1 step: 661, loss is 2.309900999069214\n",
      "epoch: 1 step: 662, loss is 2.2939352989196777\n",
      "epoch: 1 step: 663, loss is 2.289418935775757\n",
      "epoch: 1 step: 664, loss is 2.3220927715301514\n",
      "epoch: 1 step: 665, loss is 2.293382406234741\n",
      "epoch: 1 step: 666, loss is 2.3162312507629395\n",
      "epoch: 1 step: 667, loss is 2.322542667388916\n",
      "epoch: 1 step: 668, loss is 2.3027122020721436\n",
      "epoch: 1 step: 669, loss is 2.2824909687042236\n",
      "epoch: 1 step: 670, loss is 2.29371976852417\n",
      "epoch: 1 step: 671, loss is 2.3125104904174805\n",
      "epoch: 1 step: 672, loss is 2.305655002593994\n",
      "epoch: 1 step: 673, loss is 2.302541971206665\n",
      "epoch: 1 step: 674, loss is 2.299427032470703\n",
      "epoch: 1 step: 675, loss is 2.2906076908111572\n",
      "epoch: 1 step: 676, loss is 2.291095018386841\n",
      "epoch: 1 step: 677, loss is 2.305046558380127\n",
      "epoch: 1 step: 678, loss is 2.2966861724853516\n",
      "epoch: 1 step: 679, loss is 2.29484486579895\n",
      "epoch: 1 step: 680, loss is 2.3046181201934814\n",
      "epoch: 1 step: 681, loss is 2.302912712097168\n",
      "epoch: 1 step: 682, loss is 2.2864296436309814\n",
      "epoch: 1 step: 683, loss is 2.3088438510894775\n",
      "epoch: 1 step: 684, loss is 2.3099489212036133\n",
      "epoch: 1 step: 685, loss is 2.2947614192962646\n",
      "epoch: 1 step: 686, loss is 2.300431966781616\n",
      "epoch: 1 step: 687, loss is 2.3063292503356934\n",
      "epoch: 1 step: 688, loss is 2.326019763946533\n",
      "epoch: 1 step: 689, loss is 2.298272132873535\n",
      "epoch: 1 step: 690, loss is 2.3152639865875244\n",
      "epoch: 1 step: 691, loss is 2.311490297317505\n",
      "epoch: 1 step: 692, loss is 2.3014817237854004\n",
      "epoch: 1 step: 693, loss is 2.3009891510009766\n",
      "epoch: 1 step: 694, loss is 2.2975502014160156\n",
      "epoch: 1 step: 695, loss is 2.3158841133117676\n",
      "epoch: 1 step: 696, loss is 2.3024230003356934\n",
      "epoch: 1 step: 697, loss is 2.32177734375\n",
      "epoch: 1 step: 698, loss is 2.3062593936920166\n",
      "epoch: 1 step: 699, loss is 2.2912323474884033\n",
      "epoch: 1 step: 700, loss is 2.310363531112671\n",
      "epoch: 1 step: 701, loss is 2.3031435012817383\n",
      "epoch: 1 step: 702, loss is 2.311896562576294\n",
      "epoch: 1 step: 703, loss is 2.310152530670166\n",
      "epoch: 1 step: 704, loss is 2.3073136806488037\n",
      "epoch: 1 step: 705, loss is 2.2959372997283936\n",
      "epoch: 1 step: 706, loss is 2.2890963554382324\n",
      "epoch: 1 step: 707, loss is 2.3011863231658936\n",
      "epoch: 1 step: 708, loss is 2.3030154705047607\n",
      "epoch: 1 step: 709, loss is 2.3090991973876953\n",
      "epoch: 1 step: 710, loss is 2.301332950592041\n",
      "epoch: 1 step: 711, loss is 2.2902169227600098\n",
      "epoch: 1 step: 712, loss is 2.2934017181396484\n",
      "epoch: 1 step: 713, loss is 2.3004965782165527\n",
      "epoch: 1 step: 714, loss is 2.300292491912842\n",
      "epoch: 1 step: 715, loss is 2.289797067642212\n",
      "epoch: 1 step: 716, loss is 2.3049771785736084\n",
      "epoch: 1 step: 717, loss is 2.290534257888794\n",
      "epoch: 1 step: 718, loss is 2.3007380962371826\n",
      "epoch: 1 step: 719, loss is 2.296564817428589\n",
      "epoch: 1 step: 720, loss is 2.294362783432007\n",
      "epoch: 1 step: 721, loss is 2.2923128604888916\n",
      "epoch: 1 step: 722, loss is 2.2924022674560547\n",
      "epoch: 1 step: 723, loss is 2.3054864406585693\n",
      "epoch: 1 step: 724, loss is 2.2975611686706543\n",
      "epoch: 1 step: 725, loss is 2.2957980632781982\n",
      "epoch: 1 step: 726, loss is 2.308032512664795\n",
      "epoch: 1 step: 727, loss is 2.2910220623016357\n",
      "epoch: 1 step: 728, loss is 2.306379556655884\n",
      "epoch: 1 step: 729, loss is 2.314239740371704\n",
      "epoch: 1 step: 730, loss is 2.2913103103637695\n",
      "epoch: 1 step: 731, loss is 2.3062305450439453\n",
      "epoch: 1 step: 732, loss is 2.296215772628784\n",
      "epoch: 1 step: 733, loss is 2.2904016971588135\n",
      "epoch: 1 step: 734, loss is 2.3083980083465576\n",
      "epoch: 1 step: 735, loss is 2.295806884765625\n",
      "epoch: 1 step: 736, loss is 2.2952487468719482\n",
      "epoch: 1 step: 737, loss is 2.301161050796509\n",
      "epoch: 1 step: 738, loss is 2.299590587615967\n",
      "epoch: 1 step: 739, loss is 2.2881040573120117\n",
      "epoch: 1 step: 740, loss is 2.301675796508789\n",
      "epoch: 1 step: 741, loss is 2.312739849090576\n",
      "epoch: 1 step: 742, loss is 2.2920498847961426\n",
      "epoch: 1 step: 743, loss is 2.28764271736145\n",
      "epoch: 1 step: 744, loss is 2.2988712787628174\n",
      "epoch: 1 step: 745, loss is 2.3078813552856445\n",
      "epoch: 1 step: 746, loss is 2.294239044189453\n",
      "epoch: 1 step: 747, loss is 2.311039686203003\n",
      "epoch: 1 step: 748, loss is 2.313917398452759\n",
      "epoch: 1 step: 749, loss is 2.311007022857666\n",
      "epoch: 1 step: 750, loss is 2.3063740730285645\n",
      "epoch: 1 step: 751, loss is 2.296196937561035\n",
      "epoch: 1 step: 752, loss is 2.3037023544311523\n",
      "epoch: 1 step: 753, loss is 2.310640335083008\n",
      "epoch: 1 step: 754, loss is 2.3149266242980957\n",
      "epoch: 1 step: 755, loss is 2.2943177223205566\n",
      "epoch: 1 step: 756, loss is 2.2912821769714355\n",
      "epoch: 1 step: 757, loss is 2.2924089431762695\n",
      "epoch: 1 step: 758, loss is 2.3233542442321777\n",
      "epoch: 1 step: 759, loss is 2.276524066925049\n",
      "epoch: 1 step: 760, loss is 2.2929558753967285\n",
      "epoch: 1 step: 761, loss is 2.299511671066284\n",
      "epoch: 1 step: 762, loss is 2.321451187133789\n",
      "epoch: 1 step: 763, loss is 2.2932815551757812\n",
      "epoch: 1 step: 764, loss is 2.3056769371032715\n",
      "epoch: 1 step: 765, loss is 2.2928996086120605\n",
      "epoch: 1 step: 766, loss is 2.297563314437866\n",
      "epoch: 1 step: 767, loss is 2.308270215988159\n",
      "epoch: 1 step: 768, loss is 2.293377161026001\n",
      "epoch: 1 step: 769, loss is 2.290487766265869\n",
      "epoch: 1 step: 770, loss is 2.3064634799957275\n",
      "epoch: 1 step: 771, loss is 2.3069205284118652\n",
      "epoch: 1 step: 772, loss is 2.3025622367858887\n",
      "epoch: 1 step: 773, loss is 2.2918148040771484\n",
      "epoch: 1 step: 774, loss is 2.3022663593292236\n",
      "epoch: 1 step: 775, loss is 2.3048295974731445\n",
      "epoch: 1 step: 776, loss is 2.29903507232666\n",
      "epoch: 1 step: 777, loss is 2.2832224369049072\n",
      "epoch: 1 step: 778, loss is 2.2917044162750244\n",
      "epoch: 1 step: 779, loss is 2.2938735485076904\n",
      "epoch: 1 step: 780, loss is 2.2881598472595215\n",
      "epoch: 1 step: 781, loss is 2.3073816299438477\n",
      "epoch: 1 step: 782, loss is 2.3087189197540283\n",
      "epoch: 1 step: 783, loss is 2.303382396697998\n",
      "epoch: 1 step: 784, loss is 2.3113865852355957\n",
      "epoch: 1 step: 785, loss is 2.286205530166626\n",
      "epoch: 1 step: 786, loss is 2.294255256652832\n",
      "epoch: 1 step: 787, loss is 2.3069496154785156\n",
      "epoch: 1 step: 788, loss is 2.313295602798462\n",
      "epoch: 1 step: 789, loss is 2.2971580028533936\n",
      "epoch: 1 step: 790, loss is 2.27894926071167\n",
      "epoch: 1 step: 791, loss is 2.306093454360962\n",
      "epoch: 1 step: 792, loss is 2.272627592086792\n",
      "epoch: 1 step: 793, loss is 2.2937941551208496\n",
      "epoch: 1 step: 794, loss is 2.2957212924957275\n",
      "epoch: 1 step: 795, loss is 2.303429365158081\n",
      "epoch: 1 step: 796, loss is 2.2970221042633057\n",
      "epoch: 1 step: 797, loss is 2.2914323806762695\n",
      "epoch: 1 step: 798, loss is 2.291189432144165\n",
      "epoch: 1 step: 799, loss is 2.298421621322632\n",
      "epoch: 1 step: 800, loss is 2.288717269897461\n",
      "epoch: 1 step: 801, loss is 2.303135633468628\n",
      "epoch: 1 step: 802, loss is 2.304535150527954\n",
      "epoch: 1 step: 803, loss is 2.2816925048828125\n",
      "epoch: 1 step: 804, loss is 2.311227798461914\n",
      "epoch: 1 step: 805, loss is 2.3161840438842773\n",
      "epoch: 1 step: 806, loss is 2.31781005859375\n",
      "epoch: 1 step: 807, loss is 2.2810113430023193\n",
      "epoch: 1 step: 808, loss is 2.3292407989501953\n",
      "epoch: 1 step: 809, loss is 2.300370216369629\n",
      "epoch: 1 step: 810, loss is 2.328275442123413\n",
      "epoch: 1 step: 811, loss is 2.2976999282836914\n",
      "epoch: 1 step: 812, loss is 2.297470808029175\n",
      "epoch: 1 step: 813, loss is 2.301079750061035\n",
      "epoch: 1 step: 814, loss is 2.2937722206115723\n",
      "epoch: 1 step: 815, loss is 2.3121724128723145\n",
      "epoch: 1 step: 816, loss is 2.3162119388580322\n",
      "epoch: 1 step: 817, loss is 2.309032440185547\n",
      "epoch: 1 step: 818, loss is 2.3084349632263184\n",
      "epoch: 1 step: 819, loss is 2.2955822944641113\n",
      "epoch: 1 step: 820, loss is 2.2879478931427\n",
      "epoch: 1 step: 821, loss is 2.3120741844177246\n",
      "epoch: 1 step: 822, loss is 2.297795534133911\n",
      "epoch: 1 step: 823, loss is 2.2984402179718018\n",
      "epoch: 1 step: 824, loss is 2.315753221511841\n",
      "epoch: 1 step: 825, loss is 2.2928624153137207\n",
      "epoch: 1 step: 826, loss is 2.2883241176605225\n",
      "epoch: 1 step: 827, loss is 2.3024511337280273\n",
      "epoch: 1 step: 828, loss is 2.2974443435668945\n",
      "epoch: 1 step: 829, loss is 2.2798519134521484\n",
      "epoch: 1 step: 830, loss is 2.308098554611206\n",
      "epoch: 1 step: 831, loss is 2.300405263900757\n",
      "epoch: 1 step: 832, loss is 2.306021213531494\n",
      "epoch: 1 step: 833, loss is 2.3093948364257812\n",
      "epoch: 1 step: 834, loss is 2.3156871795654297\n",
      "epoch: 1 step: 835, loss is 2.3112528324127197\n",
      "epoch: 1 step: 836, loss is 2.296079158782959\n",
      "epoch: 1 step: 837, loss is 2.300909996032715\n",
      "epoch: 1 step: 838, loss is 2.2938332557678223\n",
      "epoch: 1 step: 839, loss is 2.288968563079834\n",
      "epoch: 1 step: 840, loss is 2.297640085220337\n",
      "epoch: 1 step: 841, loss is 2.2888777256011963\n",
      "epoch: 1 step: 842, loss is 2.3016395568847656\n",
      "epoch: 1 step: 843, loss is 2.2951912879943848\n",
      "epoch: 1 step: 844, loss is 2.293027639389038\n",
      "epoch: 1 step: 845, loss is 2.3110060691833496\n",
      "epoch: 1 step: 846, loss is 2.2957763671875\n",
      "epoch: 1 step: 847, loss is 2.2976393699645996\n",
      "epoch: 1 step: 848, loss is 2.3024463653564453\n",
      "epoch: 1 step: 849, loss is 2.280266284942627\n",
      "epoch: 1 step: 850, loss is 2.2830002307891846\n",
      "epoch: 1 step: 851, loss is 2.270862340927124\n",
      "epoch: 1 step: 852, loss is 2.2919867038726807\n",
      "epoch: 1 step: 853, loss is 2.3039536476135254\n",
      "epoch: 1 step: 854, loss is 2.2974164485931396\n",
      "epoch: 1 step: 855, loss is 2.316800832748413\n",
      "epoch: 1 step: 856, loss is 2.2965517044067383\n",
      "epoch: 1 step: 857, loss is 2.2975544929504395\n",
      "epoch: 1 step: 858, loss is 2.302239179611206\n",
      "epoch: 1 step: 859, loss is 2.3017585277557373\n",
      "epoch: 1 step: 860, loss is 2.2915282249450684\n",
      "epoch: 1 step: 861, loss is 2.2829666137695312\n",
      "epoch: 1 step: 862, loss is 2.2911646366119385\n",
      "epoch: 1 step: 863, loss is 2.2963881492614746\n",
      "epoch: 1 step: 864, loss is 2.283424139022827\n",
      "epoch: 1 step: 865, loss is 2.312962293624878\n",
      "epoch: 1 step: 866, loss is 2.283374071121216\n",
      "epoch: 1 step: 867, loss is 2.286360740661621\n",
      "epoch: 1 step: 868, loss is 2.2807042598724365\n",
      "epoch: 1 step: 869, loss is 2.3014066219329834\n",
      "epoch: 1 step: 870, loss is 2.2822859287261963\n",
      "epoch: 1 step: 871, loss is 2.2796218395233154\n",
      "epoch: 1 step: 872, loss is 2.291531801223755\n",
      "epoch: 1 step: 873, loss is 2.2729642391204834\n",
      "epoch: 1 step: 874, loss is 2.2947161197662354\n",
      "epoch: 1 step: 875, loss is 2.290989875793457\n",
      "epoch: 1 step: 876, loss is 2.271482467651367\n",
      "epoch: 1 step: 877, loss is 2.278348445892334\n",
      "epoch: 1 step: 878, loss is 2.2810747623443604\n",
      "epoch: 1 step: 879, loss is 2.2888288497924805\n",
      "epoch: 1 step: 880, loss is 2.275986909866333\n",
      "epoch: 1 step: 881, loss is 2.285848379135132\n",
      "epoch: 1 step: 882, loss is 2.2872893810272217\n",
      "epoch: 1 step: 883, loss is 2.302031993865967\n",
      "epoch: 1 step: 884, loss is 2.298739433288574\n",
      "epoch: 1 step: 885, loss is 2.2855019569396973\n",
      "epoch: 1 step: 886, loss is 2.292698621749878\n",
      "epoch: 1 step: 887, loss is 2.3021697998046875\n",
      "epoch: 1 step: 888, loss is 2.2959964275360107\n",
      "epoch: 1 step: 889, loss is 2.277618885040283\n",
      "epoch: 1 step: 890, loss is 2.2805964946746826\n",
      "epoch: 1 step: 891, loss is 2.2851364612579346\n",
      "epoch: 1 step: 892, loss is 2.2783498764038086\n",
      "epoch: 1 step: 893, loss is 2.2772481441497803\n",
      "epoch: 1 step: 894, loss is 2.2872729301452637\n",
      "epoch: 1 step: 895, loss is 2.2887117862701416\n",
      "epoch: 1 step: 896, loss is 2.275848627090454\n",
      "epoch: 1 step: 897, loss is 2.2816245555877686\n",
      "epoch: 1 step: 898, loss is 2.255465269088745\n",
      "epoch: 1 step: 899, loss is 2.265787124633789\n",
      "epoch: 1 step: 900, loss is 2.2806508541107178\n",
      "epoch: 1 step: 901, loss is 2.2804055213928223\n",
      "epoch: 1 step: 902, loss is 2.261058807373047\n",
      "epoch: 1 step: 903, loss is 2.264434576034546\n",
      "epoch: 1 step: 904, loss is 2.256934881210327\n",
      "epoch: 1 step: 905, loss is 2.2471120357513428\n",
      "epoch: 1 step: 906, loss is 2.2655680179595947\n",
      "epoch: 1 step: 907, loss is 2.248307228088379\n",
      "epoch: 1 step: 908, loss is 2.2249233722686768\n",
      "epoch: 1 step: 909, loss is 2.2309024333953857\n",
      "epoch: 1 step: 910, loss is 2.238738775253296\n",
      "epoch: 1 step: 911, loss is 2.2682383060455322\n",
      "epoch: 1 step: 912, loss is 2.225433588027954\n",
      "epoch: 1 step: 913, loss is 2.248420238494873\n",
      "epoch: 1 step: 914, loss is 2.194124460220337\n",
      "epoch: 1 step: 915, loss is 2.1808199882507324\n",
      "epoch: 1 step: 916, loss is 2.1938698291778564\n",
      "epoch: 1 step: 917, loss is 2.221149444580078\n",
      "epoch: 1 step: 918, loss is 2.1604766845703125\n",
      "epoch: 1 step: 919, loss is 2.1853926181793213\n",
      "epoch: 1 step: 920, loss is 2.1390347480773926\n",
      "epoch: 1 step: 921, loss is 2.1461663246154785\n",
      "epoch: 1 step: 922, loss is 2.129829168319702\n",
      "epoch: 1 step: 923, loss is 2.133599281311035\n",
      "epoch: 1 step: 924, loss is 2.0721161365509033\n",
      "epoch: 1 step: 925, loss is 2.0834929943084717\n",
      "epoch: 1 step: 926, loss is 2.005222797393799\n",
      "epoch: 1 step: 927, loss is 1.9539082050323486\n",
      "epoch: 1 step: 928, loss is 2.0156211853027344\n",
      "epoch: 1 step: 929, loss is 1.9691789150238037\n",
      "epoch: 1 step: 930, loss is 2.0899500846862793\n",
      "epoch: 1 step: 931, loss is 1.95010244846344\n",
      "epoch: 1 step: 932, loss is 1.5761641263961792\n",
      "epoch: 1 step: 933, loss is 1.8147481679916382\n",
      "epoch: 1 step: 934, loss is 1.7896599769592285\n",
      "epoch: 1 step: 935, loss is 1.656423807144165\n",
      "epoch: 1 step: 936, loss is 1.702958106994629\n",
      "epoch: 1 step: 937, loss is 1.379073977470398\n",
      "epoch: 1 step: 938, loss is 1.4693665504455566\n",
      "epoch: 1 step: 939, loss is 1.361245036125183\n",
      "epoch: 1 step: 940, loss is 1.6648162603378296\n",
      "epoch: 1 step: 941, loss is 1.237286925315857\n",
      "epoch: 1 step: 942, loss is 1.663799524307251\n",
      "epoch: 1 step: 943, loss is 1.7378478050231934\n",
      "epoch: 1 step: 944, loss is 1.1225444078445435\n",
      "epoch: 1 step: 945, loss is 1.6428300142288208\n",
      "epoch: 1 step: 946, loss is 1.4581820964813232\n",
      "epoch: 1 step: 947, loss is 1.1415690183639526\n",
      "epoch: 1 step: 948, loss is 1.333799958229065\n",
      "epoch: 1 step: 949, loss is 1.1269806623458862\n",
      "epoch: 1 step: 950, loss is 1.6875784397125244\n",
      "epoch: 1 step: 951, loss is 1.029086709022522\n",
      "epoch: 1 step: 952, loss is 1.1470365524291992\n",
      "epoch: 1 step: 953, loss is 1.2662787437438965\n",
      "epoch: 1 step: 954, loss is 1.0179535150527954\n",
      "epoch: 1 step: 955, loss is 1.0737963914871216\n",
      "epoch: 1 step: 956, loss is 0.8563392758369446\n",
      "epoch: 1 step: 957, loss is 1.2300606966018677\n",
      "epoch: 1 step: 958, loss is 1.507491946220398\n",
      "epoch: 1 step: 959, loss is 1.0328477621078491\n",
      "epoch: 1 step: 960, loss is 1.1397656202316284\n",
      "epoch: 1 step: 961, loss is 0.9447904229164124\n",
      "epoch: 1 step: 962, loss is 0.7427192330360413\n",
      "epoch: 1 step: 963, loss is 0.8387701511383057\n",
      "epoch: 1 step: 964, loss is 0.8758036494255066\n",
      "epoch: 1 step: 965, loss is 1.0098261833190918\n",
      "epoch: 1 step: 966, loss is 0.8919342160224915\n",
      "epoch: 1 step: 967, loss is 1.037410855293274\n",
      "epoch: 1 step: 968, loss is 1.0461429357528687\n",
      "epoch: 1 step: 969, loss is 0.7040785551071167\n",
      "epoch: 1 step: 970, loss is 0.9114795327186584\n",
      "epoch: 1 step: 971, loss is 0.8267472386360168\n",
      "epoch: 1 step: 972, loss is 0.669169008731842\n",
      "epoch: 1 step: 973, loss is 1.2539631128311157\n",
      "epoch: 1 step: 974, loss is 0.5935027599334717\n",
      "epoch: 1 step: 975, loss is 1.1995389461517334\n",
      "epoch: 1 step: 976, loss is 0.7686319947242737\n",
      "epoch: 1 step: 977, loss is 0.8787959218025208\n",
      "epoch: 1 step: 978, loss is 1.3377376794815063\n",
      "epoch: 1 step: 979, loss is 1.034323811531067\n",
      "epoch: 1 step: 980, loss is 0.7325877547264099\n",
      "epoch: 1 step: 981, loss is 1.174561619758606\n",
      "epoch: 1 step: 982, loss is 0.951939046382904\n",
      "epoch: 1 step: 983, loss is 0.6134892702102661\n",
      "epoch: 1 step: 984, loss is 0.9403367042541504\n",
      "epoch: 1 step: 985, loss is 1.0839470624923706\n",
      "epoch: 1 step: 986, loss is 0.90503990650177\n",
      "epoch: 1 step: 987, loss is 0.7829368710517883\n",
      "epoch: 1 step: 988, loss is 0.7610803842544556\n",
      "epoch: 1 step: 989, loss is 0.825605571269989\n",
      "epoch: 1 step: 990, loss is 0.685120165348053\n",
      "epoch: 1 step: 991, loss is 0.7271418571472168\n",
      "epoch: 1 step: 992, loss is 0.6437318921089172\n",
      "epoch: 1 step: 993, loss is 0.466335266828537\n",
      "epoch: 1 step: 994, loss is 0.8799643516540527\n",
      "epoch: 1 step: 995, loss is 0.5646783113479614\n",
      "epoch: 1 step: 996, loss is 1.0487159490585327\n",
      "epoch: 1 step: 997, loss is 0.6550192832946777\n",
      "epoch: 1 step: 998, loss is 1.1005935668945312\n",
      "epoch: 1 step: 999, loss is 1.054429054260254\n",
      "epoch: 1 step: 1000, loss is 0.6845507621765137\n",
      "epoch: 1 step: 1001, loss is 0.40548834204673767\n",
      "epoch: 1 step: 1002, loss is 1.097749948501587\n",
      "epoch: 1 step: 1003, loss is 0.5069798231124878\n",
      "epoch: 1 step: 1004, loss is 0.48152658343315125\n",
      "epoch: 1 step: 1005, loss is 0.5662490725517273\n",
      "epoch: 1 step: 1006, loss is 0.6009695529937744\n",
      "epoch: 1 step: 1007, loss is 0.4296981692314148\n",
      "epoch: 1 step: 1008, loss is 0.5079535841941833\n",
      "epoch: 1 step: 1009, loss is 0.5708003044128418\n",
      "epoch: 1 step: 1010, loss is 0.4164031445980072\n",
      "epoch: 1 step: 1011, loss is 0.484032541513443\n",
      "epoch: 1 step: 1012, loss is 0.488521009683609\n",
      "epoch: 1 step: 1013, loss is 0.6136280298233032\n",
      "epoch: 1 step: 1014, loss is 0.4942072927951813\n",
      "epoch: 1 step: 1015, loss is 0.6468915343284607\n",
      "epoch: 1 step: 1016, loss is 0.274608850479126\n",
      "epoch: 1 step: 1017, loss is 0.6124765872955322\n",
      "epoch: 1 step: 1018, loss is 0.5467497110366821\n",
      "epoch: 1 step: 1019, loss is 0.5526319742202759\n",
      "epoch: 1 step: 1020, loss is 0.2803373336791992\n",
      "epoch: 1 step: 1021, loss is 0.2684497833251953\n",
      "epoch: 1 step: 1022, loss is 0.3204725980758667\n",
      "epoch: 1 step: 1023, loss is 0.5724083185195923\n",
      "epoch: 1 step: 1024, loss is 0.3722018599510193\n",
      "epoch: 1 step: 1025, loss is 0.37865781784057617\n",
      "epoch: 1 step: 1026, loss is 0.9395046830177307\n",
      "epoch: 1 step: 1027, loss is 0.9265225529670715\n",
      "epoch: 1 step: 1028, loss is 0.30788910388946533\n",
      "epoch: 1 step: 1029, loss is 0.9645997285842896\n",
      "epoch: 1 step: 1030, loss is 0.8909984230995178\n",
      "epoch: 1 step: 1031, loss is 0.4992889165878296\n",
      "epoch: 1 step: 1032, loss is 0.5557862520217896\n",
      "epoch: 1 step: 1033, loss is 0.4884413480758667\n",
      "epoch: 1 step: 1034, loss is 0.3502993583679199\n",
      "epoch: 1 step: 1035, loss is 0.607937753200531\n",
      "epoch: 1 step: 1036, loss is 0.466718465089798\n",
      "epoch: 1 step: 1037, loss is 0.42952367663383484\n",
      "epoch: 1 step: 1038, loss is 0.5461744070053101\n",
      "epoch: 1 step: 1039, loss is 0.5581411719322205\n",
      "epoch: 1 step: 1040, loss is 0.6298016309738159\n",
      "epoch: 1 step: 1041, loss is 0.3311608135700226\n",
      "epoch: 1 step: 1042, loss is 0.4534160792827606\n",
      "epoch: 1 step: 1043, loss is 0.9069610238075256\n",
      "epoch: 1 step: 1044, loss is 0.32591354846954346\n",
      "epoch: 1 step: 1045, loss is 0.6368402242660522\n",
      "epoch: 1 step: 1046, loss is 0.2730872333049774\n",
      "epoch: 1 step: 1047, loss is 0.51865553855896\n",
      "epoch: 1 step: 1048, loss is 0.6337862610816956\n",
      "epoch: 1 step: 1049, loss is 0.6681771278381348\n",
      "epoch: 1 step: 1050, loss is 0.33893296122550964\n",
      "epoch: 1 step: 1051, loss is 0.2759459316730499\n",
      "epoch: 1 step: 1052, loss is 0.28771889209747314\n",
      "epoch: 1 step: 1053, loss is 0.17878076434135437\n",
      "epoch: 1 step: 1054, loss is 0.5025424957275391\n",
      "epoch: 1 step: 1055, loss is 0.24176397919654846\n",
      "epoch: 1 step: 1056, loss is 0.3955577313899994\n",
      "epoch: 1 step: 1057, loss is 0.32666364312171936\n",
      "epoch: 1 step: 1058, loss is 0.3995429277420044\n",
      "epoch: 1 step: 1059, loss is 0.2658996880054474\n",
      "epoch: 1 step: 1060, loss is 0.20969176292419434\n",
      "epoch: 1 step: 1061, loss is 0.29739680886268616\n",
      "epoch: 1 step: 1062, loss is 0.24187304079532623\n",
      "epoch: 1 step: 1063, loss is 0.2966342568397522\n",
      "epoch: 1 step: 1064, loss is 0.12023725360631943\n",
      "epoch: 1 step: 1065, loss is 0.24929270148277283\n",
      "epoch: 1 step: 1066, loss is 0.5789846777915955\n",
      "epoch: 1 step: 1067, loss is 0.5538017749786377\n",
      "epoch: 1 step: 1068, loss is 0.2160387933254242\n",
      "epoch: 1 step: 1069, loss is 0.43352845311164856\n",
      "epoch: 1 step: 1070, loss is 0.41986390948295593\n",
      "epoch: 1 step: 1071, loss is 0.529150128364563\n",
      "epoch: 1 step: 1072, loss is 0.3708418011665344\n",
      "epoch: 1 step: 1073, loss is 0.7624305486679077\n",
      "epoch: 1 step: 1074, loss is 0.3688623905181885\n",
      "epoch: 1 step: 1075, loss is 0.6239590048789978\n",
      "epoch: 1 step: 1076, loss is 0.6859739422798157\n",
      "epoch: 1 step: 1077, loss is 0.685438871383667\n",
      "epoch: 1 step: 1078, loss is 0.5231823921203613\n",
      "epoch: 1 step: 1079, loss is 0.590824544429779\n",
      "epoch: 1 step: 1080, loss is 0.34543377161026\n",
      "epoch: 1 step: 1081, loss is 0.5868366360664368\n",
      "epoch: 1 step: 1082, loss is 0.49762555956840515\n",
      "epoch: 1 step: 1083, loss is 0.34920260310173035\n",
      "epoch: 1 step: 1084, loss is 0.3313119411468506\n",
      "epoch: 1 step: 1085, loss is 0.2035790979862213\n",
      "epoch: 1 step: 1086, loss is 0.3923657536506653\n",
      "epoch: 1 step: 1087, loss is 0.2329590916633606\n",
      "epoch: 1 step: 1088, loss is 0.31022879481315613\n",
      "epoch: 1 step: 1089, loss is 0.31763124465942383\n",
      "epoch: 1 step: 1090, loss is 0.22041885554790497\n",
      "epoch: 1 step: 1091, loss is 0.12209489196538925\n",
      "epoch: 1 step: 1092, loss is 0.3809824585914612\n",
      "epoch: 1 step: 1093, loss is 0.3614351153373718\n",
      "epoch: 1 step: 1094, loss is 0.3192589282989502\n",
      "epoch: 1 step: 1095, loss is 0.5503981113433838\n",
      "epoch: 1 step: 1096, loss is 0.3065953850746155\n",
      "epoch: 1 step: 1097, loss is 0.3483740985393524\n",
      "epoch: 1 step: 1098, loss is 0.40078458189964294\n",
      "epoch: 1 step: 1099, loss is 0.5321537852287292\n",
      "epoch: 1 step: 1100, loss is 0.3662390410900116\n",
      "epoch: 1 step: 1101, loss is 0.16739347577095032\n",
      "epoch: 1 step: 1102, loss is 0.26727625727653503\n",
      "epoch: 1 step: 1103, loss is 0.1476505994796753\n",
      "epoch: 1 step: 1104, loss is 0.4304444193840027\n",
      "epoch: 1 step: 1105, loss is 0.3186621367931366\n",
      "epoch: 1 step: 1106, loss is 0.5613579154014587\n",
      "epoch: 1 step: 1107, loss is 0.26388686895370483\n",
      "epoch: 1 step: 1108, loss is 0.409429669380188\n",
      "epoch: 1 step: 1109, loss is 0.3555975556373596\n",
      "epoch: 1 step: 1110, loss is 0.2091120183467865\n",
      "epoch: 1 step: 1111, loss is 0.3570941686630249\n",
      "epoch: 1 step: 1112, loss is 0.2600107192993164\n",
      "epoch: 1 step: 1113, loss is 0.28551942110061646\n",
      "epoch: 1 step: 1114, loss is 0.26461276412010193\n",
      "epoch: 1 step: 1115, loss is 0.3430548310279846\n",
      "epoch: 1 step: 1116, loss is 0.3582293689250946\n",
      "epoch: 1 step: 1117, loss is 0.20498208701610565\n",
      "epoch: 1 step: 1118, loss is 0.43387383222579956\n",
      "epoch: 1 step: 1119, loss is 0.3408638834953308\n",
      "epoch: 1 step: 1120, loss is 0.0955919548869133\n",
      "epoch: 1 step: 1121, loss is 0.10911750793457031\n",
      "epoch: 1 step: 1122, loss is 0.08001876622438431\n",
      "epoch: 1 step: 1123, loss is 0.14459021389484406\n",
      "epoch: 1 step: 1124, loss is 0.06305281817913055\n",
      "epoch: 1 step: 1125, loss is 0.7122925519943237\n",
      "epoch: 1 step: 1126, loss is 0.26210546493530273\n",
      "epoch: 1 step: 1127, loss is 0.14922602474689484\n",
      "epoch: 1 step: 1128, loss is 0.19080635905265808\n",
      "epoch: 1 step: 1129, loss is 0.3795204162597656\n",
      "epoch: 1 step: 1130, loss is 0.5879911780357361\n",
      "epoch: 1 step: 1131, loss is 0.2170046865940094\n",
      "epoch: 1 step: 1132, loss is 0.26546379923820496\n",
      "epoch: 1 step: 1133, loss is 0.4729699194431305\n",
      "epoch: 1 step: 1134, loss is 0.6902127265930176\n",
      "epoch: 1 step: 1135, loss is 0.09112786501646042\n",
      "epoch: 1 step: 1136, loss is 0.1949530392885208\n",
      "epoch: 1 step: 1137, loss is 0.23270317912101746\n",
      "epoch: 1 step: 1138, loss is 0.07449500262737274\n",
      "epoch: 1 step: 1139, loss is 0.32830262184143066\n",
      "epoch: 1 step: 1140, loss is 0.20077267289161682\n",
      "epoch: 1 step: 1141, loss is 0.23151610791683197\n",
      "epoch: 1 step: 1142, loss is 0.20490889251232147\n",
      "epoch: 1 step: 1143, loss is 0.27439603209495544\n",
      "epoch: 1 step: 1144, loss is 0.4528551399707794\n",
      "epoch: 1 step: 1145, loss is 0.298267126083374\n",
      "epoch: 1 step: 1146, loss is 0.23914015293121338\n",
      "epoch: 1 step: 1147, loss is 0.4608679711818695\n",
      "epoch: 1 step: 1148, loss is 0.39219120144844055\n",
      "epoch: 1 step: 1149, loss is 0.5795741081237793\n",
      "epoch: 1 step: 1150, loss is 0.6436858773231506\n",
      "epoch: 1 step: 1151, loss is 0.06192297860980034\n",
      "epoch: 1 step: 1152, loss is 0.5013735294342041\n",
      "epoch: 1 step: 1153, loss is 0.31176623702049255\n",
      "epoch: 1 step: 1154, loss is 0.6402616500854492\n",
      "epoch: 1 step: 1155, loss is 0.34552624821662903\n",
      "epoch: 1 step: 1156, loss is 0.527168869972229\n",
      "epoch: 1 step: 1157, loss is 0.4285027086734772\n",
      "epoch: 1 step: 1158, loss is 0.34859699010849\n",
      "epoch: 1 step: 1159, loss is 0.12680205702781677\n",
      "epoch: 1 step: 1160, loss is 0.47891485691070557\n",
      "epoch: 1 step: 1161, loss is 0.20534703135490417\n",
      "epoch: 1 step: 1162, loss is 0.2823750674724579\n",
      "epoch: 1 step: 1163, loss is 0.37709546089172363\n",
      "epoch: 1 step: 1164, loss is 0.17067435383796692\n",
      "epoch: 1 step: 1165, loss is 0.4054059386253357\n",
      "epoch: 1 step: 1166, loss is 0.29691630601882935\n",
      "epoch: 1 step: 1167, loss is 0.45803302526474\n",
      "epoch: 1 step: 1168, loss is 0.21853864192962646\n",
      "epoch: 1 step: 1169, loss is 0.14209139347076416\n",
      "epoch: 1 step: 1170, loss is 0.2904694080352783\n",
      "epoch: 1 step: 1171, loss is 0.05132515728473663\n",
      "epoch: 1 step: 1172, loss is 0.08204344660043716\n",
      "epoch: 1 step: 1173, loss is 0.2331724315881729\n",
      "epoch: 1 step: 1174, loss is 0.4058857262134552\n",
      "epoch: 1 step: 1175, loss is 0.6125215888023376\n",
      "epoch: 1 step: 1176, loss is 0.17336560785770416\n",
      "epoch: 1 step: 1177, loss is 0.714749276638031\n",
      "epoch: 1 step: 1178, loss is 0.45772993564605713\n",
      "epoch: 1 step: 1179, loss is 0.2791234850883484\n",
      "epoch: 1 step: 1180, loss is 0.4649248421192169\n",
      "epoch: 1 step: 1181, loss is 0.0889892652630806\n",
      "epoch: 1 step: 1182, loss is 0.2042378932237625\n",
      "epoch: 1 step: 1183, loss is 0.16477663815021515\n",
      "epoch: 1 step: 1184, loss is 0.12305204570293427\n",
      "epoch: 1 step: 1185, loss is 0.3603436052799225\n",
      "epoch: 1 step: 1186, loss is 0.10906976461410522\n",
      "epoch: 1 step: 1187, loss is 0.3613741099834442\n",
      "epoch: 1 step: 1188, loss is 0.09004680812358856\n",
      "epoch: 1 step: 1189, loss is 0.08323454111814499\n",
      "epoch: 1 step: 1190, loss is 0.2140360176563263\n",
      "epoch: 1 step: 1191, loss is 0.12230799347162247\n",
      "epoch: 1 step: 1192, loss is 0.41545823216438293\n",
      "epoch: 1 step: 1193, loss is 0.20878086984157562\n",
      "epoch: 1 step: 1194, loss is 0.24449867010116577\n",
      "epoch: 1 step: 1195, loss is 0.272127628326416\n",
      "epoch: 1 step: 1196, loss is 0.1387590616941452\n",
      "epoch: 1 step: 1197, loss is 0.20470665395259857\n",
      "epoch: 1 step: 1198, loss is 0.146531343460083\n",
      "epoch: 1 step: 1199, loss is 0.09293700754642487\n",
      "epoch: 1 step: 1200, loss is 0.3875780999660492\n",
      "epoch: 1 step: 1201, loss is 0.21057669818401337\n",
      "epoch: 1 step: 1202, loss is 0.2503669261932373\n",
      "epoch: 1 step: 1203, loss is 0.08986391127109528\n",
      "epoch: 1 step: 1204, loss is 0.22496047616004944\n",
      "epoch: 1 step: 1205, loss is 0.06483057886362076\n",
      "epoch: 1 step: 1206, loss is 0.17352962493896484\n",
      "epoch: 1 step: 1207, loss is 0.4220174551010132\n",
      "epoch: 1 step: 1208, loss is 0.328229159116745\n",
      "epoch: 1 step: 1209, loss is 0.1714438647031784\n",
      "epoch: 1 step: 1210, loss is 0.0997779369354248\n",
      "epoch: 1 step: 1211, loss is 0.12245597690343857\n",
      "epoch: 1 step: 1212, loss is 0.30307579040527344\n",
      "epoch: 1 step: 1213, loss is 0.26872262358665466\n",
      "epoch: 1 step: 1214, loss is 0.13223770260810852\n",
      "epoch: 1 step: 1215, loss is 0.47742584347724915\n",
      "epoch: 1 step: 1216, loss is 0.01989700086414814\n",
      "epoch: 1 step: 1217, loss is 0.30567488074302673\n",
      "epoch: 1 step: 1218, loss is 0.24970877170562744\n",
      "epoch: 1 step: 1219, loss is 0.4451304078102112\n",
      "epoch: 1 step: 1220, loss is 0.5172604918479919\n",
      "epoch: 1 step: 1221, loss is 0.07472759485244751\n",
      "epoch: 1 step: 1222, loss is 0.18227802217006683\n",
      "epoch: 1 step: 1223, loss is 0.4557555615901947\n",
      "epoch: 1 step: 1224, loss is 0.06636904180049896\n",
      "epoch: 1 step: 1225, loss is 0.15651991963386536\n",
      "epoch: 1 step: 1226, loss is 0.16845978796482086\n",
      "epoch: 1 step: 1227, loss is 0.2447313815355301\n",
      "epoch: 1 step: 1228, loss is 0.04535422846674919\n",
      "epoch: 1 step: 1229, loss is 0.27036717534065247\n",
      "epoch: 1 step: 1230, loss is 0.09716083109378815\n",
      "epoch: 1 step: 1231, loss is 0.46921849250793457\n",
      "epoch: 1 step: 1232, loss is 0.17210716009140015\n",
      "epoch: 1 step: 1233, loss is 0.5460785031318665\n",
      "epoch: 1 step: 1234, loss is 0.19848594069480896\n",
      "epoch: 1 step: 1235, loss is 0.1134515032172203\n",
      "epoch: 1 step: 1236, loss is 0.11461100727319717\n",
      "epoch: 1 step: 1237, loss is 0.4312978684902191\n",
      "epoch: 1 step: 1238, loss is 0.13685183227062225\n",
      "epoch: 1 step: 1239, loss is 0.19079551100730896\n",
      "epoch: 1 step: 1240, loss is 0.2791700065135956\n",
      "epoch: 1 step: 1241, loss is 0.24791063368320465\n",
      "epoch: 1 step: 1242, loss is 0.29263368248939514\n",
      "epoch: 1 step: 1243, loss is 0.07762376964092255\n",
      "epoch: 1 step: 1244, loss is 0.05793589726090431\n",
      "epoch: 1 step: 1245, loss is 0.07111743092536926\n",
      "epoch: 1 step: 1246, loss is 0.49183446168899536\n",
      "epoch: 1 step: 1247, loss is 0.17675018310546875\n",
      "epoch: 1 step: 1248, loss is 0.3036019206047058\n",
      "epoch: 1 step: 1249, loss is 0.0936744287610054\n",
      "epoch: 1 step: 1250, loss is 0.16034811735153198\n",
      "epoch: 1 step: 1251, loss is 0.680470883846283\n",
      "epoch: 1 step: 1252, loss is 0.10718484967947006\n",
      "epoch: 1 step: 1253, loss is 0.3033682405948639\n",
      "epoch: 1 step: 1254, loss is 0.26785436272621155\n",
      "epoch: 1 step: 1255, loss is 0.22350382804870605\n",
      "epoch: 1 step: 1256, loss is 0.11959291994571686\n",
      "epoch: 1 step: 1257, loss is 0.3646031320095062\n",
      "epoch: 1 step: 1258, loss is 0.32638823986053467\n",
      "epoch: 1 step: 1259, loss is 0.49535712599754333\n",
      "epoch: 1 step: 1260, loss is 0.2435014396905899\n",
      "epoch: 1 step: 1261, loss is 0.12622791528701782\n",
      "epoch: 1 step: 1262, loss is 0.1443106234073639\n",
      "epoch: 1 step: 1263, loss is 0.11063679307699203\n",
      "epoch: 1 step: 1264, loss is 0.15533402562141418\n",
      "epoch: 1 step: 1265, loss is 0.3797164857387543\n",
      "epoch: 1 step: 1266, loss is 0.3462280333042145\n",
      "epoch: 1 step: 1267, loss is 0.16803298890590668\n",
      "epoch: 1 step: 1268, loss is 0.2876715064048767\n",
      "epoch: 1 step: 1269, loss is 0.2870260179042816\n",
      "epoch: 1 step: 1270, loss is 0.1642232984304428\n",
      "epoch: 1 step: 1271, loss is 0.2683914303779602\n",
      "epoch: 1 step: 1272, loss is 0.22588342428207397\n",
      "epoch: 1 step: 1273, loss is 0.18800902366638184\n",
      "epoch: 1 step: 1274, loss is 0.16790413856506348\n",
      "epoch: 1 step: 1275, loss is 0.18661938607692719\n",
      "epoch: 1 step: 1276, loss is 0.06461671739816666\n",
      "epoch: 1 step: 1277, loss is 0.21850404143333435\n",
      "epoch: 1 step: 1278, loss is 0.18917395174503326\n",
      "epoch: 1 step: 1279, loss is 0.07373504340648651\n",
      "epoch: 1 step: 1280, loss is 0.2573983371257782\n",
      "epoch: 1 step: 1281, loss is 0.03699169680476189\n",
      "epoch: 1 step: 1282, loss is 0.6659300923347473\n",
      "epoch: 1 step: 1283, loss is 0.08406031131744385\n",
      "epoch: 1 step: 1284, loss is 0.2157764583826065\n",
      "epoch: 1 step: 1285, loss is 0.17943771183490753\n",
      "epoch: 1 step: 1286, loss is 0.28215694427490234\n",
      "epoch: 1 step: 1287, loss is 0.4133220911026001\n",
      "epoch: 1 step: 1288, loss is 0.3736681342124939\n",
      "epoch: 1 step: 1289, loss is 0.09430456906557083\n",
      "epoch: 1 step: 1290, loss is 0.250230073928833\n",
      "epoch: 1 step: 1291, loss is 0.4790951609611511\n",
      "epoch: 1 step: 1292, loss is 0.30953216552734375\n",
      "epoch: 1 step: 1293, loss is 0.17434155941009521\n",
      "epoch: 1 step: 1294, loss is 0.1278807371854782\n",
      "epoch: 1 step: 1295, loss is 0.6048623323440552\n",
      "epoch: 1 step: 1296, loss is 0.11553039401769638\n",
      "epoch: 1 step: 1297, loss is 0.4149501621723175\n",
      "epoch: 1 step: 1298, loss is 0.47126680612564087\n",
      "epoch: 1 step: 1299, loss is 0.13561691343784332\n",
      "epoch: 1 step: 1300, loss is 0.4789542555809021\n",
      "epoch: 1 step: 1301, loss is 0.1951192319393158\n",
      "epoch: 1 step: 1302, loss is 0.2510966658592224\n",
      "epoch: 1 step: 1303, loss is 0.3070640563964844\n",
      "epoch: 1 step: 1304, loss is 0.17242637276649475\n",
      "epoch: 1 step: 1305, loss is 0.09199810028076172\n",
      "epoch: 1 step: 1306, loss is 0.24299763143062592\n",
      "epoch: 1 step: 1307, loss is 0.1702805608510971\n",
      "epoch: 1 step: 1308, loss is 0.291571706533432\n",
      "epoch: 1 step: 1309, loss is 0.40264371037483215\n",
      "epoch: 1 step: 1310, loss is 0.6213483214378357\n",
      "epoch: 1 step: 1311, loss is 0.14850623905658722\n",
      "epoch: 1 step: 1312, loss is 0.05728571116924286\n",
      "epoch: 1 step: 1313, loss is 0.25391504168510437\n",
      "epoch: 1 step: 1314, loss is 0.15124264359474182\n",
      "epoch: 1 step: 1315, loss is 0.0408216267824173\n",
      "epoch: 1 step: 1316, loss is 0.10556488484144211\n",
      "epoch: 1 step: 1317, loss is 0.26429861783981323\n",
      "epoch: 1 step: 1318, loss is 0.1268504112958908\n",
      "epoch: 1 step: 1319, loss is 0.29888054728507996\n",
      "epoch: 1 step: 1320, loss is 0.09591205418109894\n",
      "epoch: 1 step: 1321, loss is 0.2659498453140259\n",
      "epoch: 1 step: 1322, loss is 0.056400932371616364\n",
      "epoch: 1 step: 1323, loss is 0.2365967035293579\n",
      "epoch: 1 step: 1324, loss is 0.06003941595554352\n",
      "epoch: 1 step: 1325, loss is 0.09100382775068283\n",
      "epoch: 1 step: 1326, loss is 0.21663454174995422\n",
      "epoch: 1 step: 1327, loss is 0.21416525542736053\n",
      "epoch: 1 step: 1328, loss is 0.2492828220129013\n",
      "epoch: 1 step: 1329, loss is 0.21550892293453217\n",
      "epoch: 1 step: 1330, loss is 0.3748912513256073\n",
      "epoch: 1 step: 1331, loss is 0.16968171298503876\n",
      "epoch: 1 step: 1332, loss is 0.1275026947259903\n",
      "epoch: 1 step: 1333, loss is 0.08193882554769516\n",
      "epoch: 1 step: 1334, loss is 0.18774913251399994\n",
      "epoch: 1 step: 1335, loss is 0.1343533843755722\n",
      "epoch: 1 step: 1336, loss is 0.18957039713859558\n",
      "epoch: 1 step: 1337, loss is 0.1839672476053238\n",
      "epoch: 1 step: 1338, loss is 0.30808988213539124\n",
      "epoch: 1 step: 1339, loss is 0.1708735078573227\n",
      "epoch: 1 step: 1340, loss is 0.27640706300735474\n",
      "epoch: 1 step: 1341, loss is 0.15890607237815857\n",
      "epoch: 1 step: 1342, loss is 0.33532392978668213\n",
      "epoch: 1 step: 1343, loss is 0.21552206575870514\n",
      "epoch: 1 step: 1344, loss is 0.1721060872077942\n",
      "epoch: 1 step: 1345, loss is 0.15843072533607483\n",
      "epoch: 1 step: 1346, loss is 0.10167337208986282\n",
      "epoch: 1 step: 1347, loss is 0.39551207423210144\n",
      "epoch: 1 step: 1348, loss is 0.1419859379529953\n",
      "epoch: 1 step: 1349, loss is 0.5012435913085938\n",
      "epoch: 1 step: 1350, loss is 0.07369248569011688\n",
      "epoch: 1 step: 1351, loss is 0.1053423136472702\n",
      "epoch: 1 step: 1352, loss is 0.12110323458909988\n",
      "epoch: 1 step: 1353, loss is 0.2598683834075928\n",
      "epoch: 1 step: 1354, loss is 0.14545758068561554\n",
      "epoch: 1 step: 1355, loss is 0.18825817108154297\n",
      "epoch: 1 step: 1356, loss is 0.06195978820323944\n",
      "epoch: 1 step: 1357, loss is 0.09941843897104263\n",
      "epoch: 1 step: 1358, loss is 0.30634236335754395\n",
      "epoch: 1 step: 1359, loss is 0.06458844244480133\n",
      "epoch: 1 step: 1360, loss is 0.1495167464017868\n",
      "epoch: 1 step: 1361, loss is 0.24363958835601807\n",
      "epoch: 1 step: 1362, loss is 0.26374056935310364\n",
      "epoch: 1 step: 1363, loss is 0.06905345618724823\n",
      "epoch: 1 step: 1364, loss is 0.09326149523258209\n",
      "epoch: 1 step: 1365, loss is 0.40664607286453247\n",
      "epoch: 1 step: 1366, loss is 0.13641846179962158\n",
      "epoch: 1 step: 1367, loss is 0.11275117099285126\n",
      "epoch: 1 step: 1368, loss is 0.09244048595428467\n",
      "epoch: 1 step: 1369, loss is 0.20642077922821045\n",
      "epoch: 1 step: 1370, loss is 0.4525302052497864\n",
      "epoch: 1 step: 1371, loss is 0.08562440425157547\n",
      "epoch: 1 step: 1372, loss is 0.15724210441112518\n",
      "epoch: 1 step: 1373, loss is 0.03584853559732437\n",
      "epoch: 1 step: 1374, loss is 0.05618906393647194\n",
      "epoch: 1 step: 1375, loss is 0.13692374527454376\n",
      "epoch: 1 step: 1376, loss is 0.042090315371751785\n",
      "epoch: 1 step: 1377, loss is 0.0702790766954422\n",
      "epoch: 1 step: 1378, loss is 0.11570035666227341\n",
      "epoch: 1 step: 1379, loss is 0.21376940608024597\n",
      "epoch: 1 step: 1380, loss is 0.23964300751686096\n",
      "epoch: 1 step: 1381, loss is 0.21418918669223785\n",
      "epoch: 1 step: 1382, loss is 0.10039869695901871\n",
      "epoch: 1 step: 1383, loss is 0.048254355788230896\n",
      "epoch: 1 step: 1384, loss is 0.15348903834819794\n",
      "epoch: 1 step: 1385, loss is 0.2702728807926178\n",
      "epoch: 1 step: 1386, loss is 0.15683278441429138\n",
      "epoch: 1 step: 1387, loss is 0.46624210476875305\n",
      "epoch: 1 step: 1388, loss is 0.0764341950416565\n",
      "epoch: 1 step: 1389, loss is 0.08551585674285889\n",
      "epoch: 1 step: 1390, loss is 0.1420518159866333\n",
      "epoch: 1 step: 1391, loss is 0.29329603910446167\n",
      "epoch: 1 step: 1392, loss is 0.3478849232196808\n",
      "epoch: 1 step: 1393, loss is 0.1097656786441803\n",
      "epoch: 1 step: 1394, loss is 0.0641339048743248\n",
      "epoch: 1 step: 1395, loss is 0.21151185035705566\n",
      "epoch: 1 step: 1396, loss is 0.16875505447387695\n",
      "epoch: 1 step: 1397, loss is 0.2578947842121124\n",
      "epoch: 1 step: 1398, loss is 0.11789567023515701\n",
      "epoch: 1 step: 1399, loss is 0.15570864081382751\n",
      "epoch: 1 step: 1400, loss is 0.20605263113975525\n",
      "epoch: 1 step: 1401, loss is 0.11810961365699768\n",
      "epoch: 1 step: 1402, loss is 0.2412303388118744\n",
      "epoch: 1 step: 1403, loss is 0.10561401396989822\n",
      "epoch: 1 step: 1404, loss is 0.01897120289504528\n",
      "epoch: 1 step: 1405, loss is 0.2687673568725586\n",
      "epoch: 1 step: 1406, loss is 0.09062395244836807\n",
      "epoch: 1 step: 1407, loss is 0.09470687061548233\n",
      "epoch: 1 step: 1408, loss is 0.16377317905426025\n",
      "epoch: 1 step: 1409, loss is 0.18195974826812744\n",
      "epoch: 1 step: 1410, loss is 0.07691722363233566\n",
      "epoch: 1 step: 1411, loss is 0.3797459602355957\n",
      "epoch: 1 step: 1412, loss is 0.21258237957954407\n",
      "epoch: 1 step: 1413, loss is 0.15060581266880035\n",
      "epoch: 1 step: 1414, loss is 0.1104467511177063\n",
      "epoch: 1 step: 1415, loss is 0.21449542045593262\n",
      "epoch: 1 step: 1416, loss is 0.06604558229446411\n",
      "epoch: 1 step: 1417, loss is 0.18610000610351562\n",
      "epoch: 1 step: 1418, loss is 0.3223789930343628\n",
      "epoch: 1 step: 1419, loss is 0.17848443984985352\n",
      "epoch: 1 step: 1420, loss is 0.33461451530456543\n",
      "epoch: 1 step: 1421, loss is 0.38364940881729126\n",
      "epoch: 1 step: 1422, loss is 0.06649203598499298\n",
      "epoch: 1 step: 1423, loss is 0.19307924807071686\n",
      "epoch: 1 step: 1424, loss is 0.11663635075092316\n",
      "epoch: 1 step: 1425, loss is 0.17451611161231995\n",
      "epoch: 1 step: 1426, loss is 0.10487417131662369\n",
      "epoch: 1 step: 1427, loss is 0.26451945304870605\n",
      "epoch: 1 step: 1428, loss is 0.09184599667787552\n",
      "epoch: 1 step: 1429, loss is 0.0723286047577858\n",
      "epoch: 1 step: 1430, loss is 0.17970611155033112\n",
      "epoch: 1 step: 1431, loss is 0.06724700331687927\n",
      "epoch: 1 step: 1432, loss is 0.2599930167198181\n",
      "epoch: 1 step: 1433, loss is 0.24125903844833374\n",
      "epoch: 1 step: 1434, loss is 0.3005340099334717\n",
      "epoch: 1 step: 1435, loss is 0.11179234832525253\n",
      "epoch: 1 step: 1436, loss is 0.33423709869384766\n",
      "epoch: 1 step: 1437, loss is 0.06018108129501343\n",
      "epoch: 1 step: 1438, loss is 0.14055515825748444\n",
      "epoch: 1 step: 1439, loss is 0.12027005851268768\n",
      "epoch: 1 step: 1440, loss is 0.07547543197870255\n",
      "epoch: 1 step: 1441, loss is 0.22801798582077026\n",
      "epoch: 1 step: 1442, loss is 0.11143554747104645\n",
      "epoch: 1 step: 1443, loss is 0.27310529351234436\n",
      "epoch: 1 step: 1444, loss is 0.04404859617352486\n",
      "epoch: 1 step: 1445, loss is 0.04737788438796997\n",
      "epoch: 1 step: 1446, loss is 0.0737837702035904\n",
      "epoch: 1 step: 1447, loss is 0.03975808992981911\n",
      "epoch: 1 step: 1448, loss is 0.03375566750764847\n",
      "epoch: 1 step: 1449, loss is 0.08417248725891113\n",
      "epoch: 1 step: 1450, loss is 0.10227177292108536\n",
      "epoch: 1 step: 1451, loss is 0.46748724579811096\n",
      "epoch: 1 step: 1452, loss is 0.8081598281860352\n",
      "epoch: 1 step: 1453, loss is 0.3715260624885559\n",
      "epoch: 1 step: 1454, loss is 0.12330035120248795\n",
      "epoch: 1 step: 1455, loss is 0.04654059186577797\n",
      "epoch: 1 step: 1456, loss is 0.09497017413377762\n",
      "epoch: 1 step: 1457, loss is 0.11833135038614273\n",
      "epoch: 1 step: 1458, loss is 0.14781205356121063\n",
      "epoch: 1 step: 1459, loss is 0.11731687188148499\n",
      "epoch: 1 step: 1460, loss is 0.11573963612318039\n",
      "epoch: 1 step: 1461, loss is 0.12619911134243011\n",
      "epoch: 1 step: 1462, loss is 0.19379357993602753\n",
      "epoch: 1 step: 1463, loss is 0.10099553316831589\n",
      "epoch: 1 step: 1464, loss is 0.0750383585691452\n",
      "epoch: 1 step: 1465, loss is 0.6666474342346191\n",
      "epoch: 1 step: 1466, loss is 0.11883487552404404\n",
      "epoch: 1 step: 1467, loss is 0.31564566493034363\n",
      "epoch: 1 step: 1468, loss is 0.1576034426689148\n",
      "epoch: 1 step: 1469, loss is 0.06174721568822861\n",
      "epoch: 1 step: 1470, loss is 0.1917632818222046\n",
      "epoch: 1 step: 1471, loss is 0.10380220413208008\n",
      "epoch: 1 step: 1472, loss is 0.513262152671814\n",
      "epoch: 1 step: 1473, loss is 0.048197560012340546\n",
      "epoch: 1 step: 1474, loss is 0.06961464136838913\n",
      "epoch: 1 step: 1475, loss is 0.16879187524318695\n",
      "epoch: 1 step: 1476, loss is 0.17129050195217133\n",
      "epoch: 1 step: 1477, loss is 0.1025857999920845\n",
      "epoch: 1 step: 1478, loss is 0.03731188178062439\n",
      "epoch: 1 step: 1479, loss is 0.13878102600574493\n",
      "epoch: 1 step: 1480, loss is 0.027643417939543724\n",
      "epoch: 1 step: 1481, loss is 0.07091366499662399\n",
      "epoch: 1 step: 1482, loss is 0.1626179814338684\n",
      "epoch: 1 step: 1483, loss is 0.14833544194698334\n",
      "epoch: 1 step: 1484, loss is 0.1924232691526413\n",
      "epoch: 1 step: 1485, loss is 0.028370559215545654\n",
      "epoch: 1 step: 1486, loss is 0.09599138051271439\n",
      "epoch: 1 step: 1487, loss is 0.21127989888191223\n",
      "epoch: 1 step: 1488, loss is 0.08929464966058731\n",
      "epoch: 1 step: 1489, loss is 0.31790152192115784\n",
      "epoch: 1 step: 1490, loss is 0.2678087055683136\n",
      "epoch: 1 step: 1491, loss is 0.37675347924232483\n",
      "epoch: 1 step: 1492, loss is 0.10338933765888214\n",
      "epoch: 1 step: 1493, loss is 0.1521192491054535\n",
      "epoch: 1 step: 1494, loss is 0.13537921011447906\n",
      "epoch: 1 step: 1495, loss is 0.04635132476687431\n",
      "epoch: 1 step: 1496, loss is 0.471600204706192\n",
      "epoch: 1 step: 1497, loss is 0.35647499561309814\n",
      "epoch: 1 step: 1498, loss is 0.2576853036880493\n",
      "epoch: 1 step: 1499, loss is 0.05155755206942558\n",
      "epoch: 1 step: 1500, loss is 0.10861602425575256\n",
      "epoch: 1 step: 1501, loss is 0.19583962857723236\n",
      "epoch: 1 step: 1502, loss is 0.02210175432264805\n",
      "epoch: 1 step: 1503, loss is 0.35332047939300537\n",
      "epoch: 1 step: 1504, loss is 0.17433024942874908\n",
      "epoch: 1 step: 1505, loss is 0.07135963439941406\n",
      "epoch: 1 step: 1506, loss is 0.08988974243402481\n",
      "epoch: 1 step: 1507, loss is 0.04006499424576759\n",
      "epoch: 1 step: 1508, loss is 0.24222902953624725\n",
      "epoch: 1 step: 1509, loss is 0.10502376407384872\n",
      "epoch: 1 step: 1510, loss is 0.06865545362234116\n",
      "epoch: 1 step: 1511, loss is 0.19822555780410767\n",
      "epoch: 1 step: 1512, loss is 0.14907072484493256\n",
      "epoch: 1 step: 1513, loss is 0.08938527852296829\n",
      "epoch: 1 step: 1514, loss is 0.08085218071937561\n",
      "epoch: 1 step: 1515, loss is 0.04322866350412369\n",
      "epoch: 1 step: 1516, loss is 0.06867127120494843\n",
      "epoch: 1 step: 1517, loss is 0.07008161395788193\n",
      "epoch: 1 step: 1518, loss is 0.04581477865576744\n",
      "epoch: 1 step: 1519, loss is 0.18822869658470154\n",
      "epoch: 1 step: 1520, loss is 0.15342718362808228\n",
      "epoch: 1 step: 1521, loss is 0.26276248693466187\n",
      "epoch: 1 step: 1522, loss is 0.11944510042667389\n",
      "epoch: 1 step: 1523, loss is 0.09197283536195755\n",
      "epoch: 1 step: 1524, loss is 0.18978992104530334\n",
      "epoch: 1 step: 1525, loss is 0.08262500166893005\n",
      "epoch: 1 step: 1526, loss is 0.03080315887928009\n",
      "epoch: 1 step: 1527, loss is 0.06536875665187836\n",
      "epoch: 1 step: 1528, loss is 0.1447039395570755\n",
      "epoch: 1 step: 1529, loss is 0.02438964694738388\n",
      "epoch: 1 step: 1530, loss is 0.07659585773944855\n",
      "epoch: 1 step: 1531, loss is 0.16813239455223083\n",
      "epoch: 1 step: 1532, loss is 0.19996614754199982\n",
      "epoch: 1 step: 1533, loss is 0.010253719054162502\n",
      "epoch: 1 step: 1534, loss is 0.04169275611639023\n",
      "epoch: 1 step: 1535, loss is 0.3421848714351654\n",
      "epoch: 1 step: 1536, loss is 0.09665456414222717\n",
      "epoch: 1 step: 1537, loss is 0.005047943443059921\n",
      "epoch: 1 step: 1538, loss is 0.03569741174578667\n",
      "epoch: 1 step: 1539, loss is 0.15519683063030243\n",
      "epoch: 1 step: 1540, loss is 0.27224716544151306\n",
      "epoch: 1 step: 1541, loss is 0.107401542365551\n",
      "epoch: 1 step: 1542, loss is 0.1385020613670349\n",
      "epoch: 1 step: 1543, loss is 0.22753021121025085\n",
      "epoch: 1 step: 1544, loss is 0.05226379260420799\n",
      "epoch: 1 step: 1545, loss is 0.048319920897483826\n",
      "epoch: 1 step: 1546, loss is 0.5054016709327698\n",
      "epoch: 1 step: 1547, loss is 0.0881587415933609\n",
      "epoch: 1 step: 1548, loss is 0.07526431977748871\n",
      "epoch: 1 step: 1549, loss is 0.14888523519039154\n",
      "epoch: 1 step: 1550, loss is 0.013537486083805561\n",
      "epoch: 1 step: 1551, loss is 0.2035369873046875\n",
      "epoch: 1 step: 1552, loss is 0.30364689230918884\n",
      "epoch: 1 step: 1553, loss is 0.06768149137496948\n",
      "epoch: 1 step: 1554, loss is 0.19872674345970154\n",
      "epoch: 1 step: 1555, loss is 0.22859378159046173\n",
      "epoch: 1 step: 1556, loss is 0.05006762966513634\n",
      "epoch: 1 step: 1557, loss is 0.058059122413396835\n",
      "epoch: 1 step: 1558, loss is 0.06277681142091751\n",
      "epoch: 1 step: 1559, loss is 0.17821024358272552\n",
      "epoch: 1 step: 1560, loss is 0.027932334691286087\n",
      "epoch: 1 step: 1561, loss is 0.022225666791200638\n",
      "epoch: 1 step: 1562, loss is 0.1410103589296341\n",
      "epoch: 1 step: 1563, loss is 0.01174917258322239\n",
      "epoch: 1 step: 1564, loss is 0.23053181171417236\n",
      "epoch: 1 step: 1565, loss is 0.05278238281607628\n",
      "epoch: 1 step: 1566, loss is 0.23550917208194733\n",
      "epoch: 1 step: 1567, loss is 0.08612155169248581\n",
      "epoch: 1 step: 1568, loss is 0.14996492862701416\n",
      "epoch: 1 step: 1569, loss is 0.046325553208589554\n",
      "epoch: 1 step: 1570, loss is 0.10387583076953888\n",
      "epoch: 1 step: 1571, loss is 0.16452904045581818\n",
      "epoch: 1 step: 1572, loss is 0.43374624848365784\n",
      "epoch: 1 step: 1573, loss is 0.070747010409832\n",
      "epoch: 1 step: 1574, loss is 0.31655287742614746\n",
      "epoch: 1 step: 1575, loss is 0.03603306785225868\n",
      "epoch: 1 step: 1576, loss is 0.10976890474557877\n",
      "epoch: 1 step: 1577, loss is 0.06843003630638123\n",
      "epoch: 1 step: 1578, loss is 0.06225469708442688\n",
      "epoch: 1 step: 1579, loss is 0.08053141832351685\n",
      "epoch: 1 step: 1580, loss is 0.05334947258234024\n",
      "epoch: 1 step: 1581, loss is 0.007481351029127836\n",
      "epoch: 1 step: 1582, loss is 0.26421329379081726\n",
      "epoch: 1 step: 1583, loss is 0.02634424902498722\n",
      "epoch: 1 step: 1584, loss is 0.028616437688469887\n",
      "epoch: 1 step: 1585, loss is 0.30199143290519714\n",
      "epoch: 1 step: 1586, loss is 0.3569760024547577\n",
      "epoch: 1 step: 1587, loss is 0.08559506386518478\n",
      "epoch: 1 step: 1588, loss is 0.14892272651195526\n",
      "epoch: 1 step: 1589, loss is 0.11006879061460495\n",
      "epoch: 1 step: 1590, loss is 0.1224765032529831\n",
      "epoch: 1 step: 1591, loss is 0.3252149820327759\n",
      "epoch: 1 step: 1592, loss is 0.0471581369638443\n",
      "epoch: 1 step: 1593, loss is 0.3404141962528229\n",
      "epoch: 1 step: 1594, loss is 0.09908625483512878\n",
      "epoch: 1 step: 1595, loss is 0.047362569719552994\n",
      "epoch: 1 step: 1596, loss is 0.07805198431015015\n",
      "epoch: 1 step: 1597, loss is 0.20247960090637207\n",
      "epoch: 1 step: 1598, loss is 0.32050982117652893\n",
      "epoch: 1 step: 1599, loss is 0.30610764026641846\n",
      "epoch: 1 step: 1600, loss is 0.10228456556797028\n",
      "epoch: 1 step: 1601, loss is 0.3178786039352417\n",
      "epoch: 1 step: 1602, loss is 0.2041473686695099\n",
      "epoch: 1 step: 1603, loss is 0.08262231200933456\n",
      "epoch: 1 step: 1604, loss is 0.05366317182779312\n",
      "epoch: 1 step: 1605, loss is 0.2249372899532318\n",
      "epoch: 1 step: 1606, loss is 0.06061986833810806\n",
      "epoch: 1 step: 1607, loss is 0.20339937508106232\n",
      "epoch: 1 step: 1608, loss is 0.23353219032287598\n",
      "epoch: 1 step: 1609, loss is 0.20942607522010803\n",
      "epoch: 1 step: 1610, loss is 0.12103605270385742\n",
      "epoch: 1 step: 1611, loss is 0.1103498786687851\n",
      "epoch: 1 step: 1612, loss is 0.10157263278961182\n",
      "epoch: 1 step: 1613, loss is 0.03255274146795273\n",
      "epoch: 1 step: 1614, loss is 0.09912943094968796\n",
      "epoch: 1 step: 1615, loss is 0.4163931906223297\n",
      "epoch: 1 step: 1616, loss is 0.28480517864227295\n",
      "epoch: 1 step: 1617, loss is 0.10624765604734421\n",
      "epoch: 1 step: 1618, loss is 0.10159765183925629\n",
      "epoch: 1 step: 1619, loss is 0.2505384385585785\n",
      "epoch: 1 step: 1620, loss is 0.05493113026022911\n",
      "epoch: 1 step: 1621, loss is 0.1415262073278427\n",
      "epoch: 1 step: 1622, loss is 0.1301582157611847\n",
      "epoch: 1 step: 1623, loss is 0.24296924471855164\n",
      "epoch: 1 step: 1624, loss is 0.2383488267660141\n",
      "epoch: 1 step: 1625, loss is 0.032786957919597626\n",
      "epoch: 1 step: 1626, loss is 0.0467488095164299\n",
      "epoch: 1 step: 1627, loss is 0.00433600228279829\n",
      "epoch: 1 step: 1628, loss is 0.08248794823884964\n",
      "epoch: 1 step: 1629, loss is 0.14168895781040192\n",
      "epoch: 1 step: 1630, loss is 0.25278642773628235\n",
      "epoch: 1 step: 1631, loss is 0.07685619592666626\n",
      "epoch: 1 step: 1632, loss is 0.07582341134548187\n",
      "epoch: 1 step: 1633, loss is 0.044159092009067535\n",
      "epoch: 1 step: 1634, loss is 0.13687998056411743\n",
      "epoch: 1 step: 1635, loss is 0.16929693520069122\n",
      "epoch: 1 step: 1636, loss is 0.1479903757572174\n",
      "epoch: 1 step: 1637, loss is 0.0557711236178875\n",
      "epoch: 1 step: 1638, loss is 0.17096389830112457\n",
      "epoch: 1 step: 1639, loss is 0.20333954691886902\n",
      "epoch: 1 step: 1640, loss is 0.13314567506313324\n",
      "epoch: 1 step: 1641, loss is 0.10458537936210632\n",
      "epoch: 1 step: 1642, loss is 0.08180931955575943\n",
      "epoch: 1 step: 1643, loss is 0.036531299352645874\n",
      "epoch: 1 step: 1644, loss is 0.10776739567518234\n",
      "epoch: 1 step: 1645, loss is 0.2160595804452896\n",
      "epoch: 1 step: 1646, loss is 0.2402084320783615\n",
      "epoch: 1 step: 1647, loss is 0.09363918751478195\n",
      "epoch: 1 step: 1648, loss is 0.4229434132575989\n",
      "epoch: 1 step: 1649, loss is 0.05600816011428833\n",
      "epoch: 1 step: 1650, loss is 0.10144402831792831\n",
      "epoch: 1 step: 1651, loss is 0.16757073998451233\n",
      "epoch: 1 step: 1652, loss is 0.24882884323596954\n",
      "epoch: 1 step: 1653, loss is 0.061166953295469284\n",
      "epoch: 1 step: 1654, loss is 0.31018751859664917\n",
      "epoch: 1 step: 1655, loss is 0.07952888309955597\n",
      "epoch: 1 step: 1656, loss is 0.04162321612238884\n",
      "epoch: 1 step: 1657, loss is 0.048575952649116516\n",
      "epoch: 1 step: 1658, loss is 0.02953755483031273\n",
      "epoch: 1 step: 1659, loss is 0.07611595094203949\n",
      "epoch: 1 step: 1660, loss is 0.07736123353242874\n",
      "epoch: 1 step: 1661, loss is 0.19530075788497925\n",
      "epoch: 1 step: 1662, loss is 0.2331879884004593\n",
      "epoch: 1 step: 1663, loss is 0.202935129404068\n",
      "epoch: 1 step: 1664, loss is 0.010000175796449184\n",
      "epoch: 1 step: 1665, loss is 0.12429406493902206\n",
      "epoch: 1 step: 1666, loss is 0.054116684943437576\n",
      "epoch: 1 step: 1667, loss is 0.14605383574962616\n",
      "epoch: 1 step: 1668, loss is 0.11436957865953445\n",
      "epoch: 1 step: 1669, loss is 0.1565999686717987\n",
      "epoch: 1 step: 1670, loss is 0.26321130990982056\n",
      "epoch: 1 step: 1671, loss is 0.03922967240214348\n",
      "epoch: 1 step: 1672, loss is 0.2857052683830261\n",
      "epoch: 1 step: 1673, loss is 0.131514310836792\n",
      "epoch: 1 step: 1674, loss is 0.12757332623004913\n",
      "epoch: 1 step: 1675, loss is 0.025631196796894073\n",
      "epoch: 1 step: 1676, loss is 0.08873344212770462\n",
      "epoch: 1 step: 1677, loss is 0.047070909291505814\n",
      "epoch: 1 step: 1678, loss is 0.06527473777532578\n",
      "epoch: 1 step: 1679, loss is 0.07963386178016663\n",
      "epoch: 1 step: 1680, loss is 0.26989349722862244\n",
      "epoch: 1 step: 1681, loss is 0.18445903062820435\n",
      "epoch: 1 step: 1682, loss is 0.05750376731157303\n",
      "epoch: 1 step: 1683, loss is 0.004376040305942297\n",
      "epoch: 1 step: 1684, loss is 0.12296891957521439\n",
      "epoch: 1 step: 1685, loss is 0.026796534657478333\n",
      "epoch: 1 step: 1686, loss is 0.008579079993069172\n",
      "epoch: 1 step: 1687, loss is 0.039454348385334015\n",
      "epoch: 1 step: 1688, loss is 0.3235655725002289\n",
      "epoch: 1 step: 1689, loss is 0.0780348852276802\n",
      "epoch: 1 step: 1690, loss is 0.24005985260009766\n",
      "epoch: 1 step: 1691, loss is 0.18188010156154633\n",
      "epoch: 1 step: 1692, loss is 0.10599774867296219\n",
      "epoch: 1 step: 1693, loss is 0.03463232144713402\n",
      "epoch: 1 step: 1694, loss is 0.1119677945971489\n",
      "epoch: 1 step: 1695, loss is 0.10844054818153381\n",
      "epoch: 1 step: 1696, loss is 0.06955383718013763\n",
      "epoch: 1 step: 1697, loss is 0.15362873673439026\n",
      "epoch: 1 step: 1698, loss is 0.022724574431777\n",
      "epoch: 1 step: 1699, loss is 0.079037606716156\n",
      "epoch: 1 step: 1700, loss is 0.0054748752154409885\n",
      "epoch: 1 step: 1701, loss is 0.17439627647399902\n",
      "epoch: 1 step: 1702, loss is 0.18985849618911743\n",
      "epoch: 1 step: 1703, loss is 0.08105766028165817\n",
      "epoch: 1 step: 1704, loss is 0.16494613885879517\n",
      "epoch: 1 step: 1705, loss is 0.06619752943515778\n",
      "epoch: 1 step: 1706, loss is 0.4239490032196045\n",
      "epoch: 1 step: 1707, loss is 0.5050451755523682\n",
      "epoch: 1 step: 1708, loss is 0.07879555970430374\n",
      "epoch: 1 step: 1709, loss is 0.13094569742679596\n",
      "epoch: 1 step: 1710, loss is 0.23637554049491882\n",
      "epoch: 1 step: 1711, loss is 0.2114768922328949\n",
      "epoch: 1 step: 1712, loss is 0.44327256083488464\n",
      "epoch: 1 step: 1713, loss is 0.06973553448915482\n",
      "epoch: 1 step: 1714, loss is 0.12378697842359543\n",
      "epoch: 1 step: 1715, loss is 0.03878705948591232\n",
      "epoch: 1 step: 1716, loss is 0.12287015467882156\n",
      "epoch: 1 step: 1717, loss is 0.07460694015026093\n",
      "epoch: 1 step: 1718, loss is 0.09199534356594086\n",
      "epoch: 1 step: 1719, loss is 0.3900834023952484\n",
      "epoch: 1 step: 1720, loss is 0.10446498543024063\n",
      "epoch: 1 step: 1721, loss is 0.08578356355428696\n",
      "epoch: 1 step: 1722, loss is 0.025008058175444603\n",
      "epoch: 1 step: 1723, loss is 0.31573164463043213\n",
      "epoch: 1 step: 1724, loss is 0.21514320373535156\n",
      "epoch: 1 step: 1725, loss is 0.13537710905075073\n",
      "epoch: 1 step: 1726, loss is 0.10368143022060394\n",
      "epoch: 1 step: 1727, loss is 0.2292761504650116\n",
      "epoch: 1 step: 1728, loss is 0.19968567788600922\n",
      "epoch: 1 step: 1729, loss is 0.23786137998104095\n",
      "epoch: 1 step: 1730, loss is 0.1784421056509018\n",
      "epoch: 1 step: 1731, loss is 0.15919457376003265\n",
      "epoch: 1 step: 1732, loss is 0.024734102189540863\n",
      "epoch: 1 step: 1733, loss is 0.042085204273462296\n",
      "epoch: 1 step: 1734, loss is 0.09189419448375702\n",
      "epoch: 1 step: 1735, loss is 0.21107083559036255\n",
      "epoch: 1 step: 1736, loss is 0.17175404727458954\n",
      "epoch: 1 step: 1737, loss is 0.10022827982902527\n",
      "epoch: 1 step: 1738, loss is 0.02835357002913952\n",
      "epoch: 1 step: 1739, loss is 0.0869823545217514\n",
      "epoch: 1 step: 1740, loss is 0.034426212310791016\n",
      "epoch: 1 step: 1741, loss is 0.14160026609897614\n",
      "epoch: 1 step: 1742, loss is 0.27354151010513306\n",
      "epoch: 1 step: 1743, loss is 0.029631031677126884\n",
      "epoch: 1 step: 1744, loss is 0.17675381898880005\n",
      "epoch: 1 step: 1745, loss is 0.17944028973579407\n",
      "epoch: 1 step: 1746, loss is 0.10743249952793121\n",
      "epoch: 1 step: 1747, loss is 0.034281883388757706\n",
      "epoch: 1 step: 1748, loss is 0.01798425242304802\n",
      "epoch: 1 step: 1749, loss is 0.2012244015932083\n",
      "epoch: 1 step: 1750, loss is 0.2830308973789215\n",
      "epoch: 1 step: 1751, loss is 0.01855740137398243\n",
      "epoch: 1 step: 1752, loss is 0.014924601651728153\n",
      "epoch: 1 step: 1753, loss is 0.034284546971321106\n",
      "epoch: 1 step: 1754, loss is 0.03597204387187958\n",
      "epoch: 1 step: 1755, loss is 0.14502328634262085\n",
      "epoch: 1 step: 1756, loss is 0.24027849733829498\n",
      "epoch: 1 step: 1757, loss is 0.00507552083581686\n",
      "epoch: 1 step: 1758, loss is 0.014895262196660042\n",
      "epoch: 1 step: 1759, loss is 0.29692476987838745\n",
      "epoch: 1 step: 1760, loss is 0.1237814649939537\n",
      "epoch: 1 step: 1761, loss is 0.10526406764984131\n",
      "epoch: 1 step: 1762, loss is 0.04157134145498276\n",
      "epoch: 1 step: 1763, loss is 0.03164403513073921\n",
      "epoch: 1 step: 1764, loss is 0.025127269327640533\n",
      "epoch: 1 step: 1765, loss is 0.1605948656797409\n",
      "epoch: 1 step: 1766, loss is 0.045569486916065216\n",
      "epoch: 1 step: 1767, loss is 0.038299810141325\n",
      "epoch: 1 step: 1768, loss is 0.27301329374313354\n",
      "epoch: 1 step: 1769, loss is 0.19095252454280853\n",
      "epoch: 1 step: 1770, loss is 0.24797168374061584\n",
      "epoch: 1 step: 1771, loss is 0.18734504282474518\n",
      "epoch: 1 step: 1772, loss is 0.033871643245220184\n",
      "epoch: 1 step: 1773, loss is 0.2901541590690613\n",
      "epoch: 1 step: 1774, loss is 0.19100292026996613\n",
      "epoch: 1 step: 1775, loss is 0.052765749394893646\n",
      "epoch: 1 step: 1776, loss is 0.5310138463973999\n",
      "epoch: 1 step: 1777, loss is 0.1846964806318283\n",
      "epoch: 1 step: 1778, loss is 0.05436373129487038\n",
      "epoch: 1 step: 1779, loss is 0.2782570719718933\n",
      "epoch: 1 step: 1780, loss is 0.19469772279262543\n",
      "epoch: 1 step: 1781, loss is 0.13220489025115967\n",
      "epoch: 1 step: 1782, loss is 0.23383785784244537\n",
      "epoch: 1 step: 1783, loss is 0.08448757976293564\n",
      "epoch: 1 step: 1784, loss is 0.16095100343227386\n",
      "epoch: 1 step: 1785, loss is 0.14890217781066895\n",
      "epoch: 1 step: 1786, loss is 0.10963258147239685\n",
      "epoch: 1 step: 1787, loss is 0.12067052721977234\n",
      "epoch: 1 step: 1788, loss is 0.23757505416870117\n",
      "epoch: 1 step: 1789, loss is 0.021562129259109497\n",
      "epoch: 1 step: 1790, loss is 0.044866785407066345\n",
      "epoch: 1 step: 1791, loss is 0.00988885946571827\n",
      "epoch: 1 step: 1792, loss is 0.07915323972702026\n",
      "epoch: 1 step: 1793, loss is 0.15128105878829956\n",
      "epoch: 1 step: 1794, loss is 0.008411668241024017\n",
      "epoch: 1 step: 1795, loss is 0.09514053165912628\n",
      "epoch: 1 step: 1796, loss is 0.16052435338497162\n",
      "epoch: 1 step: 1797, loss is 0.09636125713586807\n",
      "epoch: 1 step: 1798, loss is 0.06680326163768768\n",
      "epoch: 1 step: 1799, loss is 0.06254077702760696\n",
      "epoch: 1 step: 1800, loss is 0.1105814129114151\n",
      "epoch: 1 step: 1801, loss is 0.3158297836780548\n",
      "epoch: 1 step: 1802, loss is 0.16059540212154388\n",
      "epoch: 1 step: 1803, loss is 0.5006901621818542\n",
      "epoch: 1 step: 1804, loss is 0.10123661160469055\n",
      "epoch: 1 step: 1805, loss is 0.3501614034175873\n",
      "epoch: 1 step: 1806, loss is 0.08809169381856918\n",
      "epoch: 1 step: 1807, loss is 0.11675432324409485\n",
      "epoch: 1 step: 1808, loss is 0.18429727852344513\n",
      "epoch: 1 step: 1809, loss is 0.06655232608318329\n",
      "epoch: 1 step: 1810, loss is 0.04830684885382652\n",
      "epoch: 1 step: 1811, loss is 0.17671465873718262\n",
      "epoch: 1 step: 1812, loss is 0.020075269043445587\n",
      "epoch: 1 step: 1813, loss is 0.07962324470281601\n",
      "epoch: 1 step: 1814, loss is 0.12076470255851746\n",
      "epoch: 1 step: 1815, loss is 0.03809767961502075\n",
      "epoch: 1 step: 1816, loss is 0.22642138600349426\n",
      "epoch: 1 step: 1817, loss is 0.03210771456360817\n",
      "epoch: 1 step: 1818, loss is 0.04820375517010689\n",
      "epoch: 1 step: 1819, loss is 0.29996174573898315\n",
      "epoch: 1 step: 1820, loss is 0.10174628347158432\n",
      "epoch: 1 step: 1821, loss is 0.009209011681377888\n",
      "epoch: 1 step: 1822, loss is 0.07486124336719513\n",
      "epoch: 1 step: 1823, loss is 0.08386372774839401\n",
      "epoch: 1 step: 1824, loss is 0.16386297345161438\n",
      "epoch: 1 step: 1825, loss is 0.052995484322309494\n",
      "epoch: 1 step: 1826, loss is 0.006121315062046051\n",
      "epoch: 1 step: 1827, loss is 0.06441313028335571\n",
      "epoch: 1 step: 1828, loss is 0.004695762414485216\n",
      "epoch: 1 step: 1829, loss is 0.11932270228862762\n",
      "epoch: 1 step: 1830, loss is 0.016592713072896004\n",
      "epoch: 1 step: 1831, loss is 0.13193820416927338\n",
      "epoch: 1 step: 1832, loss is 0.26445192098617554\n",
      "epoch: 1 step: 1833, loss is 0.1444758176803589\n",
      "epoch: 1 step: 1834, loss is 0.12055527418851852\n",
      "epoch: 1 step: 1835, loss is 0.09377793222665787\n",
      "epoch: 1 step: 1836, loss is 0.06384927034378052\n",
      "epoch: 1 step: 1837, loss is 0.2239319235086441\n",
      "epoch: 1 step: 1838, loss is 0.2107517570257187\n",
      "epoch: 1 step: 1839, loss is 0.18613453209400177\n",
      "epoch: 1 step: 1840, loss is 0.2643044888973236\n",
      "epoch: 1 step: 1841, loss is 0.174209862947464\n",
      "epoch: 1 step: 1842, loss is 0.154098242521286\n",
      "epoch: 1 step: 1843, loss is 0.14993540942668915\n",
      "epoch: 1 step: 1844, loss is 0.07159842550754547\n",
      "epoch: 1 step: 1845, loss is 0.09787456691265106\n",
      "epoch: 1 step: 1846, loss is 0.021815141662955284\n",
      "epoch: 1 step: 1847, loss is 0.04874001070857048\n",
      "epoch: 1 step: 1848, loss is 0.06450599431991577\n",
      "epoch: 1 step: 1849, loss is 0.057557955384254456\n",
      "epoch: 1 step: 1850, loss is 0.2016681581735611\n",
      "epoch: 1 step: 1851, loss is 0.16718049347400665\n",
      "epoch: 1 step: 1852, loss is 0.18979312479496002\n",
      "epoch: 1 step: 1853, loss is 0.2464754283428192\n",
      "epoch: 1 step: 1854, loss is 0.07871206104755402\n",
      "epoch: 1 step: 1855, loss is 0.10834039747714996\n",
      "epoch: 1 step: 1856, loss is 0.02849152311682701\n",
      "epoch: 1 step: 1857, loss is 0.013414587825536728\n",
      "epoch: 1 step: 1858, loss is 0.1639569252729416\n",
      "epoch: 1 step: 1859, loss is 0.06450512260198593\n",
      "epoch: 1 step: 1860, loss is 0.04891593009233475\n",
      "epoch: 1 step: 1861, loss is 0.03219003230333328\n",
      "epoch: 1 step: 1862, loss is 0.15131241083145142\n",
      "epoch: 1 step: 1863, loss is 0.14498816430568695\n",
      "epoch: 1 step: 1864, loss is 0.06001000851392746\n",
      "epoch: 1 step: 1865, loss is 0.32463333010673523\n",
      "epoch: 1 step: 1866, loss is 0.1255291998386383\n",
      "epoch: 1 step: 1867, loss is 0.0921250730752945\n",
      "epoch: 1 step: 1868, loss is 0.018030604347586632\n",
      "epoch: 1 step: 1869, loss is 0.12519532442092896\n",
      "epoch: 1 step: 1870, loss is 0.06661000102758408\n",
      "epoch: 1 step: 1871, loss is 0.042310506105422974\n",
      "epoch: 1 step: 1872, loss is 0.06644091010093689\n",
      "epoch: 1 step: 1873, loss is 0.010590379126369953\n",
      "epoch: 1 step: 1874, loss is 0.07230254262685776\n",
      "epoch: 1 step: 1875, loss is 0.08708541840314865\n",
      "Train epoch time: 9732.258 ms, per step time: 5.191 ms\n",
      "epoch: 2 step: 1, loss is 0.15749655663967133\n",
      "epoch: 2 step: 2, loss is 0.029252316802740097\n",
      "epoch: 2 step: 3, loss is 0.18959444761276245\n",
      "epoch: 2 step: 4, loss is 0.07241101562976837\n",
      "epoch: 2 step: 5, loss is 0.09758905321359634\n",
      "epoch: 2 step: 6, loss is 0.08810140192508698\n",
      "epoch: 2 step: 7, loss is 0.0049052610993385315\n",
      "epoch: 2 step: 8, loss is 0.015750110149383545\n",
      "epoch: 2 step: 9, loss is 0.16383521258831024\n",
      "epoch: 2 step: 10, loss is 0.03453924134373665\n",
      "epoch: 2 step: 11, loss is 0.030802711844444275\n",
      "epoch: 2 step: 12, loss is 0.08858171850442886\n",
      "epoch: 2 step: 13, loss is 0.26308223605155945\n",
      "epoch: 2 step: 14, loss is 0.17704369127750397\n",
      "epoch: 2 step: 15, loss is 0.43057194352149963\n",
      "epoch: 2 step: 16, loss is 0.03488776832818985\n",
      "epoch: 2 step: 17, loss is 0.2395993173122406\n",
      "epoch: 2 step: 18, loss is 0.04448384419083595\n",
      "epoch: 2 step: 19, loss is 0.00750382523983717\n",
      "epoch: 2 step: 20, loss is 0.15833982825279236\n",
      "epoch: 2 step: 21, loss is 0.19892488420009613\n",
      "epoch: 2 step: 22, loss is 0.14852331578731537\n",
      "epoch: 2 step: 23, loss is 0.03741541504859924\n",
      "epoch: 2 step: 24, loss is 0.02410208433866501\n",
      "epoch: 2 step: 25, loss is 0.04535399377346039\n",
      "epoch: 2 step: 26, loss is 0.026243088766932487\n",
      "epoch: 2 step: 27, loss is 0.21897993981838226\n",
      "epoch: 2 step: 28, loss is 0.028911592438817024\n",
      "epoch: 2 step: 29, loss is 0.08004949241876602\n",
      "epoch: 2 step: 30, loss is 0.023997962474822998\n",
      "epoch: 2 step: 31, loss is 0.011088330298662186\n",
      "epoch: 2 step: 32, loss is 0.15370793640613556\n",
      "epoch: 2 step: 33, loss is 0.058770354837179184\n",
      "epoch: 2 step: 34, loss is 0.23123376071453094\n",
      "epoch: 2 step: 35, loss is 0.14077039062976837\n",
      "epoch: 2 step: 36, loss is 0.06160888075828552\n",
      "epoch: 2 step: 37, loss is 0.16262954473495483\n",
      "epoch: 2 step: 38, loss is 0.08469664305448532\n",
      "epoch: 2 step: 39, loss is 0.043082017451524734\n",
      "epoch: 2 step: 40, loss is 0.016000978648662567\n",
      "epoch: 2 step: 41, loss is 0.11683765798807144\n",
      "epoch: 2 step: 42, loss is 0.023612137883901596\n",
      "epoch: 2 step: 43, loss is 0.17618319392204285\n",
      "epoch: 2 step: 44, loss is 0.23807600140571594\n",
      "epoch: 2 step: 45, loss is 0.01972886733710766\n",
      "epoch: 2 step: 46, loss is 0.2612689137458801\n",
      "epoch: 2 step: 47, loss is 0.08267165720462799\n",
      "epoch: 2 step: 48, loss is 0.10451358556747437\n",
      "epoch: 2 step: 49, loss is 0.21129156649112701\n",
      "epoch: 2 step: 50, loss is 0.05639234185218811\n",
      "epoch: 2 step: 51, loss is 0.0785973072052002\n",
      "epoch: 2 step: 52, loss is 0.048266276717185974\n",
      "epoch: 2 step: 53, loss is 0.019289130344986916\n",
      "epoch: 2 step: 54, loss is 0.10237439721822739\n",
      "epoch: 2 step: 55, loss is 0.09545507282018661\n",
      "epoch: 2 step: 56, loss is 0.15439262986183167\n",
      "epoch: 2 step: 57, loss is 0.09609495848417282\n",
      "epoch: 2 step: 58, loss is 0.0726521909236908\n",
      "epoch: 2 step: 59, loss is 0.0313437283039093\n",
      "epoch: 2 step: 60, loss is 0.04913673177361488\n",
      "epoch: 2 step: 61, loss is 0.01594081148505211\n",
      "epoch: 2 step: 62, loss is 0.05747086927294731\n",
      "epoch: 2 step: 63, loss is 0.15328878164291382\n",
      "epoch: 2 step: 64, loss is 0.27574947476387024\n",
      "epoch: 2 step: 65, loss is 0.050683051347732544\n",
      "epoch: 2 step: 66, loss is 0.0827605202794075\n",
      "epoch: 2 step: 67, loss is 0.035733625292778015\n",
      "epoch: 2 step: 68, loss is 0.059176478534936905\n",
      "epoch: 2 step: 69, loss is 0.2895890772342682\n",
      "epoch: 2 step: 70, loss is 0.15327927470207214\n",
      "epoch: 2 step: 71, loss is 0.2885395288467407\n",
      "epoch: 2 step: 72, loss is 0.028224248439073563\n",
      "epoch: 2 step: 73, loss is 0.1345651000738144\n",
      "epoch: 2 step: 74, loss is 0.16487783193588257\n",
      "epoch: 2 step: 75, loss is 0.04920392483472824\n",
      "epoch: 2 step: 76, loss is 0.01247137039899826\n",
      "epoch: 2 step: 77, loss is 0.008531080558896065\n",
      "epoch: 2 step: 78, loss is 0.1599569022655487\n",
      "epoch: 2 step: 79, loss is 0.01843104511499405\n",
      "epoch: 2 step: 80, loss is 0.07000260055065155\n",
      "epoch: 2 step: 81, loss is 0.015464851632714272\n",
      "epoch: 2 step: 82, loss is 0.022944217547774315\n",
      "epoch: 2 step: 83, loss is 0.005644891411066055\n",
      "epoch: 2 step: 84, loss is 0.023682715371251106\n",
      "epoch: 2 step: 85, loss is 0.06593102216720581\n",
      "epoch: 2 step: 86, loss is 0.12776628136634827\n",
      "epoch: 2 step: 87, loss is 0.050941351801157\n",
      "epoch: 2 step: 88, loss is 0.20883771777153015\n",
      "epoch: 2 step: 89, loss is 0.06251095980405807\n",
      "epoch: 2 step: 90, loss is 0.21800242364406586\n",
      "epoch: 2 step: 91, loss is 0.05021096020936966\n",
      "epoch: 2 step: 92, loss is 0.008691334165632725\n",
      "epoch: 2 step: 93, loss is 0.023039599880576134\n",
      "epoch: 2 step: 94, loss is 0.01319708488881588\n",
      "epoch: 2 step: 95, loss is 0.24581396579742432\n",
      "epoch: 2 step: 96, loss is 0.2221207320690155\n",
      "epoch: 2 step: 97, loss is 0.06469205766916275\n",
      "epoch: 2 step: 98, loss is 0.009426099248230457\n",
      "epoch: 2 step: 99, loss is 0.015454452484846115\n",
      "epoch: 2 step: 100, loss is 0.03621427342295647\n",
      "epoch: 2 step: 101, loss is 0.01090884581208229\n",
      "epoch: 2 step: 102, loss is 0.18240144848823547\n",
      "epoch: 2 step: 103, loss is 0.05486985296010971\n",
      "epoch: 2 step: 104, loss is 0.11093510687351227\n",
      "epoch: 2 step: 105, loss is 0.02278224565088749\n",
      "epoch: 2 step: 106, loss is 0.012343030422925949\n",
      "epoch: 2 step: 107, loss is 0.045942749828100204\n",
      "epoch: 2 step: 108, loss is 0.015498947352170944\n",
      "epoch: 2 step: 109, loss is 0.35334905982017517\n",
      "epoch: 2 step: 110, loss is 0.10629841685295105\n",
      "epoch: 2 step: 111, loss is 0.12686488032341003\n",
      "epoch: 2 step: 112, loss is 0.042587146162986755\n",
      "epoch: 2 step: 113, loss is 0.0811784565448761\n",
      "epoch: 2 step: 114, loss is 0.011090918444097042\n",
      "epoch: 2 step: 115, loss is 0.09922251105308533\n",
      "epoch: 2 step: 116, loss is 0.024727798998355865\n",
      "epoch: 2 step: 117, loss is 0.09282410144805908\n",
      "epoch: 2 step: 118, loss is 0.5123965740203857\n",
      "epoch: 2 step: 119, loss is 0.13649338483810425\n",
      "epoch: 2 step: 120, loss is 0.036716170608997345\n",
      "epoch: 2 step: 121, loss is 0.03602492809295654\n",
      "epoch: 2 step: 122, loss is 0.1278114765882492\n",
      "epoch: 2 step: 123, loss is 0.03376574069261551\n",
      "epoch: 2 step: 124, loss is 0.07927118241786957\n",
      "epoch: 2 step: 125, loss is 0.06263913959264755\n",
      "epoch: 2 step: 126, loss is 0.028040537610650063\n",
      "epoch: 2 step: 127, loss is 0.052320875227451324\n",
      "epoch: 2 step: 128, loss is 0.17443853616714478\n",
      "epoch: 2 step: 129, loss is 0.022647971287369728\n",
      "epoch: 2 step: 130, loss is 0.21327798068523407\n",
      "epoch: 2 step: 131, loss is 0.012670865282416344\n",
      "epoch: 2 step: 132, loss is 0.19161754846572876\n",
      "epoch: 2 step: 133, loss is 0.18179263174533844\n",
      "epoch: 2 step: 134, loss is 0.037885840982198715\n",
      "epoch: 2 step: 135, loss is 0.2746794819831848\n",
      "epoch: 2 step: 136, loss is 0.15111394226551056\n",
      "epoch: 2 step: 137, loss is 0.17935389280319214\n",
      "epoch: 2 step: 138, loss is 0.03564669191837311\n",
      "epoch: 2 step: 139, loss is 0.10816828161478043\n",
      "epoch: 2 step: 140, loss is 0.0710022896528244\n",
      "epoch: 2 step: 141, loss is 0.02689877338707447\n",
      "epoch: 2 step: 142, loss is 0.14187368750572205\n",
      "epoch: 2 step: 143, loss is 0.11562050879001617\n",
      "epoch: 2 step: 144, loss is 0.08345888555049896\n",
      "epoch: 2 step: 145, loss is 0.2659575939178467\n",
      "epoch: 2 step: 146, loss is 0.028343871235847473\n",
      "epoch: 2 step: 147, loss is 0.04266666620969772\n",
      "epoch: 2 step: 148, loss is 0.0657091960310936\n",
      "epoch: 2 step: 149, loss is 0.08699154853820801\n",
      "epoch: 2 step: 150, loss is 0.03493913635611534\n",
      "epoch: 2 step: 151, loss is 0.01399325579404831\n",
      "epoch: 2 step: 152, loss is 0.5158565044403076\n",
      "epoch: 2 step: 153, loss is 0.14211896061897278\n",
      "epoch: 2 step: 154, loss is 0.12797462940216064\n",
      "epoch: 2 step: 155, loss is 0.16596944630146027\n",
      "epoch: 2 step: 156, loss is 0.023059090599417686\n",
      "epoch: 2 step: 157, loss is 0.039959296584129333\n",
      "epoch: 2 step: 158, loss is 0.05571109429001808\n",
      "epoch: 2 step: 159, loss is 0.010352155193686485\n",
      "epoch: 2 step: 160, loss is 0.08586406707763672\n",
      "epoch: 2 step: 161, loss is 0.21743275225162506\n",
      "epoch: 2 step: 162, loss is 0.033425070345401764\n",
      "epoch: 2 step: 163, loss is 0.12963251769542694\n",
      "epoch: 2 step: 164, loss is 0.12278775870800018\n",
      "epoch: 2 step: 165, loss is 0.16617606580257416\n",
      "epoch: 2 step: 166, loss is 0.016413377597928047\n",
      "epoch: 2 step: 167, loss is 0.0809379518032074\n",
      "epoch: 2 step: 168, loss is 0.0803031474351883\n",
      "epoch: 2 step: 169, loss is 0.023228513076901436\n",
      "epoch: 2 step: 170, loss is 0.04446442052721977\n",
      "epoch: 2 step: 171, loss is 0.08915317058563232\n",
      "epoch: 2 step: 172, loss is 0.005216109566390514\n",
      "epoch: 2 step: 173, loss is 0.08553603291511536\n",
      "epoch: 2 step: 174, loss is 0.16741031408309937\n",
      "epoch: 2 step: 175, loss is 0.3301094174385071\n",
      "epoch: 2 step: 176, loss is 0.028697308152914047\n",
      "epoch: 2 step: 177, loss is 0.03861716017127037\n",
      "epoch: 2 step: 178, loss is 0.06755521893501282\n",
      "epoch: 2 step: 179, loss is 0.2284742146730423\n",
      "epoch: 2 step: 180, loss is 0.0360138900578022\n",
      "epoch: 2 step: 181, loss is 0.02893080934882164\n",
      "epoch: 2 step: 182, loss is 0.02706119418144226\n",
      "epoch: 2 step: 183, loss is 0.14364832639694214\n",
      "epoch: 2 step: 184, loss is 0.030932949855923653\n",
      "epoch: 2 step: 185, loss is 0.27032822370529175\n",
      "epoch: 2 step: 186, loss is 0.05537954345345497\n",
      "epoch: 2 step: 187, loss is 0.09206823259592056\n",
      "epoch: 2 step: 188, loss is 0.3594696521759033\n",
      "epoch: 2 step: 189, loss is 0.054836682975292206\n",
      "epoch: 2 step: 190, loss is 0.12786567211151123\n",
      "epoch: 2 step: 191, loss is 0.054638881236314774\n",
      "epoch: 2 step: 192, loss is 0.08229299634695053\n",
      "epoch: 2 step: 193, loss is 0.014051813632249832\n",
      "epoch: 2 step: 194, loss is 0.20252582430839539\n",
      "epoch: 2 step: 195, loss is 0.21218639612197876\n",
      "epoch: 2 step: 196, loss is 0.0662417933344841\n",
      "epoch: 2 step: 197, loss is 0.010502418503165245\n",
      "epoch: 2 step: 198, loss is 0.10378421097993851\n",
      "epoch: 2 step: 199, loss is 0.19883573055267334\n",
      "epoch: 2 step: 200, loss is 0.24559833109378815\n",
      "epoch: 2 step: 201, loss is 0.08936668932437897\n",
      "epoch: 2 step: 202, loss is 0.2520839273929596\n",
      "epoch: 2 step: 203, loss is 0.01996564492583275\n",
      "epoch: 2 step: 204, loss is 0.16571852564811707\n",
      "epoch: 2 step: 205, loss is 0.08306334167718887\n",
      "epoch: 2 step: 206, loss is 0.057110708206892014\n",
      "epoch: 2 step: 207, loss is 0.06190204992890358\n",
      "epoch: 2 step: 208, loss is 0.05363600701093674\n",
      "epoch: 2 step: 209, loss is 0.3545396029949188\n",
      "epoch: 2 step: 210, loss is 0.2244028002023697\n",
      "epoch: 2 step: 211, loss is 0.12768974900245667\n",
      "epoch: 2 step: 212, loss is 0.16451840102672577\n",
      "epoch: 2 step: 213, loss is 0.06494969874620438\n",
      "epoch: 2 step: 214, loss is 0.052174948155879974\n",
      "epoch: 2 step: 215, loss is 0.12708544731140137\n",
      "epoch: 2 step: 216, loss is 0.03605104982852936\n",
      "epoch: 2 step: 217, loss is 0.07572971284389496\n",
      "epoch: 2 step: 218, loss is 0.15512137115001678\n",
      "epoch: 2 step: 219, loss is 0.2232835590839386\n",
      "epoch: 2 step: 220, loss is 0.07483129948377609\n",
      "epoch: 2 step: 221, loss is 0.04569476470351219\n",
      "epoch: 2 step: 222, loss is 0.03860466182231903\n",
      "epoch: 2 step: 223, loss is 0.09062116593122482\n",
      "epoch: 2 step: 224, loss is 0.11707959324121475\n",
      "epoch: 2 step: 225, loss is 0.029622405767440796\n",
      "epoch: 2 step: 226, loss is 0.14415951073169708\n",
      "epoch: 2 step: 227, loss is 0.018033087253570557\n",
      "epoch: 2 step: 228, loss is 0.0687057226896286\n",
      "epoch: 2 step: 229, loss is 0.31823650002479553\n",
      "epoch: 2 step: 230, loss is 0.306258887052536\n",
      "epoch: 2 step: 231, loss is 0.057456281036138535\n",
      "epoch: 2 step: 232, loss is 0.2175183743238449\n",
      "epoch: 2 step: 233, loss is 0.052755557000637054\n",
      "epoch: 2 step: 234, loss is 0.00909618940204382\n",
      "epoch: 2 step: 235, loss is 0.0770733430981636\n",
      "epoch: 2 step: 236, loss is 0.15675142407417297\n",
      "epoch: 2 step: 237, loss is 0.020412366837263107\n",
      "epoch: 2 step: 238, loss is 0.22202983498573303\n",
      "epoch: 2 step: 239, loss is 0.042999789118766785\n",
      "epoch: 2 step: 240, loss is 0.036053746938705444\n",
      "epoch: 2 step: 241, loss is 0.09947429597377777\n",
      "epoch: 2 step: 242, loss is 0.10748676210641861\n",
      "epoch: 2 step: 243, loss is 0.20782902836799622\n",
      "epoch: 2 step: 244, loss is 0.06752794981002808\n",
      "epoch: 2 step: 245, loss is 0.19249308109283447\n",
      "epoch: 2 step: 246, loss is 0.0069013722240924835\n",
      "epoch: 2 step: 247, loss is 0.039222173392772675\n",
      "epoch: 2 step: 248, loss is 0.07701689749956131\n",
      "epoch: 2 step: 249, loss is 0.01764107309281826\n",
      "epoch: 2 step: 250, loss is 0.2261950969696045\n",
      "epoch: 2 step: 251, loss is 0.14318595826625824\n",
      "epoch: 2 step: 252, loss is 0.23810651898384094\n",
      "epoch: 2 step: 253, loss is 0.013082295656204224\n",
      "epoch: 2 step: 254, loss is 0.03999483585357666\n",
      "epoch: 2 step: 255, loss is 0.3037194013595581\n",
      "epoch: 2 step: 256, loss is 0.017669089138507843\n",
      "epoch: 2 step: 257, loss is 0.24819938838481903\n",
      "epoch: 2 step: 258, loss is 0.04395721107721329\n",
      "epoch: 2 step: 259, loss is 0.016252340748906136\n",
      "epoch: 2 step: 260, loss is 0.026708917692303658\n",
      "epoch: 2 step: 261, loss is 0.027389636263251305\n",
      "epoch: 2 step: 262, loss is 0.020127128809690475\n",
      "epoch: 2 step: 263, loss is 0.05633917823433876\n",
      "epoch: 2 step: 264, loss is 0.1530178338289261\n",
      "epoch: 2 step: 265, loss is 0.10253491997718811\n",
      "epoch: 2 step: 266, loss is 0.0518624521791935\n",
      "epoch: 2 step: 267, loss is 0.011841775849461555\n",
      "epoch: 2 step: 268, loss is 0.25124886631965637\n",
      "epoch: 2 step: 269, loss is 0.01655849814414978\n",
      "epoch: 2 step: 270, loss is 0.24547013640403748\n",
      "epoch: 2 step: 271, loss is 0.24746498465538025\n",
      "epoch: 2 step: 272, loss is 0.1290622502565384\n",
      "epoch: 2 step: 273, loss is 0.03826295584440231\n",
      "epoch: 2 step: 274, loss is 0.06798417866230011\n",
      "epoch: 2 step: 275, loss is 0.16793093085289001\n",
      "epoch: 2 step: 276, loss is 0.09180620312690735\n",
      "epoch: 2 step: 277, loss is 0.19402113556861877\n",
      "epoch: 2 step: 278, loss is 0.0309822428971529\n",
      "epoch: 2 step: 279, loss is 0.011653007008135319\n",
      "epoch: 2 step: 280, loss is 0.05286351218819618\n",
      "epoch: 2 step: 281, loss is 0.059966713190078735\n",
      "epoch: 2 step: 282, loss is 0.40556976199150085\n",
      "epoch: 2 step: 283, loss is 0.0672176405787468\n",
      "epoch: 2 step: 284, loss is 0.2596502900123596\n",
      "epoch: 2 step: 285, loss is 0.026492921635508537\n",
      "epoch: 2 step: 286, loss is 0.18060030043125153\n",
      "epoch: 2 step: 287, loss is 0.06983889639377594\n",
      "epoch: 2 step: 288, loss is 0.11899984627962112\n",
      "epoch: 2 step: 289, loss is 0.14334449172019958\n",
      "epoch: 2 step: 290, loss is 0.037888094782829285\n",
      "epoch: 2 step: 291, loss is 0.007862367667257786\n",
      "epoch: 2 step: 292, loss is 0.010172324255108833\n",
      "epoch: 2 step: 293, loss is 0.11360624432563782\n",
      "epoch: 2 step: 294, loss is 0.14046423137187958\n",
      "epoch: 2 step: 295, loss is 0.1909191608428955\n",
      "epoch: 2 step: 296, loss is 0.06643037497997284\n",
      "epoch: 2 step: 297, loss is 0.07516450434923172\n",
      "epoch: 2 step: 298, loss is 0.27197951078414917\n",
      "epoch: 2 step: 299, loss is 0.041412580758333206\n",
      "epoch: 2 step: 300, loss is 0.044626384973526\n",
      "epoch: 2 step: 301, loss is 0.02011442556977272\n",
      "epoch: 2 step: 302, loss is 0.017262332141399384\n",
      "epoch: 2 step: 303, loss is 0.3091532588005066\n",
      "epoch: 2 step: 304, loss is 0.17641441524028778\n",
      "epoch: 2 step: 305, loss is 0.016605032607913017\n",
      "epoch: 2 step: 306, loss is 0.07535072416067123\n",
      "epoch: 2 step: 307, loss is 0.03445565700531006\n",
      "epoch: 2 step: 308, loss is 0.15262910723686218\n",
      "epoch: 2 step: 309, loss is 0.04059294983744621\n",
      "epoch: 2 step: 310, loss is 0.027123108506202698\n",
      "epoch: 2 step: 311, loss is 0.052753422409296036\n",
      "epoch: 2 step: 312, loss is 0.14464379847049713\n",
      "epoch: 2 step: 313, loss is 0.018497034907341003\n",
      "epoch: 2 step: 314, loss is 0.23612302541732788\n",
      "epoch: 2 step: 315, loss is 0.12425997853279114\n",
      "epoch: 2 step: 316, loss is 0.19814157485961914\n",
      "epoch: 2 step: 317, loss is 0.10781630873680115\n",
      "epoch: 2 step: 318, loss is 0.052361007779836655\n",
      "epoch: 2 step: 319, loss is 0.049278076738119125\n",
      "epoch: 2 step: 320, loss is 0.1887407749891281\n",
      "epoch: 2 step: 321, loss is 0.03344014286994934\n",
      "epoch: 2 step: 322, loss is 0.2514990270137787\n",
      "epoch: 2 step: 323, loss is 0.26581352949142456\n",
      "epoch: 2 step: 324, loss is 0.21863527595996857\n",
      "epoch: 2 step: 325, loss is 0.041279472410678864\n",
      "epoch: 2 step: 326, loss is 0.03472602739930153\n",
      "epoch: 2 step: 327, loss is 0.07950083166360855\n",
      "epoch: 2 step: 328, loss is 0.19606630504131317\n",
      "epoch: 2 step: 329, loss is 0.13416582345962524\n",
      "epoch: 2 step: 330, loss is 0.015231354162096977\n",
      "epoch: 2 step: 331, loss is 0.24177472293376923\n",
      "epoch: 2 step: 332, loss is 0.12441542744636536\n",
      "epoch: 2 step: 333, loss is 0.03833473473787308\n",
      "epoch: 2 step: 334, loss is 0.2086353600025177\n",
      "epoch: 2 step: 335, loss is 0.042996980249881744\n",
      "epoch: 2 step: 336, loss is 0.043615084141492844\n",
      "epoch: 2 step: 337, loss is 0.28189632296562195\n",
      "epoch: 2 step: 338, loss is 0.03995713219046593\n",
      "epoch: 2 step: 339, loss is 0.05418965965509415\n",
      "epoch: 2 step: 340, loss is 0.039859332144260406\n",
      "epoch: 2 step: 341, loss is 0.21815650165081024\n",
      "epoch: 2 step: 342, loss is 0.15045201778411865\n",
      "epoch: 2 step: 343, loss is 0.03960496559739113\n",
      "epoch: 2 step: 344, loss is 0.3784704804420471\n",
      "epoch: 2 step: 345, loss is 0.03988582640886307\n",
      "epoch: 2 step: 346, loss is 0.09814384579658508\n",
      "epoch: 2 step: 347, loss is 0.009725275449454784\n",
      "epoch: 2 step: 348, loss is 0.037011124193668365\n",
      "epoch: 2 step: 349, loss is 0.022781619802117348\n",
      "epoch: 2 step: 350, loss is 0.1759863793849945\n",
      "epoch: 2 step: 351, loss is 0.08070071041584015\n",
      "epoch: 2 step: 352, loss is 0.177286759018898\n",
      "epoch: 2 step: 353, loss is 0.42206111550331116\n",
      "epoch: 2 step: 354, loss is 0.14854709804058075\n",
      "epoch: 2 step: 355, loss is 0.023762887343764305\n",
      "epoch: 2 step: 356, loss is 0.05005877837538719\n",
      "epoch: 2 step: 357, loss is 0.1810069978237152\n",
      "epoch: 2 step: 358, loss is 0.024909885600209236\n",
      "epoch: 2 step: 359, loss is 0.03975040838122368\n",
      "epoch: 2 step: 360, loss is 0.0864177718758583\n",
      "epoch: 2 step: 361, loss is 0.04578199237585068\n",
      "epoch: 2 step: 362, loss is 0.039568327367305756\n",
      "epoch: 2 step: 363, loss is 0.10651646554470062\n",
      "epoch: 2 step: 364, loss is 0.11382710933685303\n",
      "epoch: 2 step: 365, loss is 0.05326198413968086\n",
      "epoch: 2 step: 366, loss is 0.05011110380291939\n",
      "epoch: 2 step: 367, loss is 0.08086105436086655\n",
      "epoch: 2 step: 368, loss is 0.11436115950345993\n",
      "epoch: 2 step: 369, loss is 0.047889210283756256\n",
      "epoch: 2 step: 370, loss is 0.1014619842171669\n",
      "epoch: 2 step: 371, loss is 0.09827350080013275\n",
      "epoch: 2 step: 372, loss is 0.06331367045640945\n",
      "epoch: 2 step: 373, loss is 0.3534034788608551\n",
      "epoch: 2 step: 374, loss is 0.012933135032653809\n",
      "epoch: 2 step: 375, loss is 0.19278095662593842\n",
      "epoch: 2 step: 376, loss is 0.08190715312957764\n",
      "epoch: 2 step: 377, loss is 0.006361894775182009\n",
      "epoch: 2 step: 378, loss is 0.07181277871131897\n",
      "epoch: 2 step: 379, loss is 0.030156821012496948\n",
      "epoch: 2 step: 380, loss is 0.06624435633420944\n",
      "epoch: 2 step: 381, loss is 0.01623718999326229\n",
      "epoch: 2 step: 382, loss is 0.10525298863649368\n",
      "epoch: 2 step: 383, loss is 0.03318985924124718\n",
      "epoch: 2 step: 384, loss is 0.004330440890043974\n",
      "epoch: 2 step: 385, loss is 0.02266424149274826\n",
      "epoch: 2 step: 386, loss is 0.31657734513282776\n",
      "epoch: 2 step: 387, loss is 0.07102853804826736\n",
      "epoch: 2 step: 388, loss is 0.009185541421175003\n",
      "epoch: 2 step: 389, loss is 0.08405421674251556\n",
      "epoch: 2 step: 390, loss is 0.006505461875349283\n",
      "epoch: 2 step: 391, loss is 0.025234336033463478\n",
      "epoch: 2 step: 392, loss is 0.05272587388753891\n",
      "epoch: 2 step: 393, loss is 0.09830203652381897\n",
      "epoch: 2 step: 394, loss is 0.09506303817033768\n",
      "epoch: 2 step: 395, loss is 0.061917927116155624\n",
      "epoch: 2 step: 396, loss is 0.35696035623550415\n",
      "epoch: 2 step: 397, loss is 0.2110918015241623\n",
      "epoch: 2 step: 398, loss is 0.016249798238277435\n",
      "epoch: 2 step: 399, loss is 0.192485511302948\n",
      "epoch: 2 step: 400, loss is 0.05164860188961029\n",
      "epoch: 2 step: 401, loss is 0.010576722212135792\n",
      "epoch: 2 step: 402, loss is 0.010330568067729473\n",
      "epoch: 2 step: 403, loss is 0.1235693171620369\n",
      "epoch: 2 step: 404, loss is 0.006200818810611963\n",
      "epoch: 2 step: 405, loss is 0.048839982599020004\n",
      "epoch: 2 step: 406, loss is 0.17949043214321136\n",
      "epoch: 2 step: 407, loss is 0.021690065041184425\n",
      "epoch: 2 step: 408, loss is 0.05096736177802086\n",
      "epoch: 2 step: 409, loss is 0.054805152118206024\n",
      "epoch: 2 step: 410, loss is 0.022066203877329826\n",
      "epoch: 2 step: 411, loss is 0.04433932900428772\n",
      "epoch: 2 step: 412, loss is 0.05667751654982567\n",
      "epoch: 2 step: 413, loss is 0.047544509172439575\n",
      "epoch: 2 step: 414, loss is 0.010157755576074123\n",
      "epoch: 2 step: 415, loss is 0.18793195486068726\n",
      "epoch: 2 step: 416, loss is 0.05094094201922417\n",
      "epoch: 2 step: 417, loss is 0.13936614990234375\n",
      "epoch: 2 step: 418, loss is 0.010859639383852482\n",
      "epoch: 2 step: 419, loss is 0.36778488755226135\n",
      "epoch: 2 step: 420, loss is 0.21161913871765137\n",
      "epoch: 2 step: 421, loss is 0.15253621339797974\n",
      "epoch: 2 step: 422, loss is 0.2654185891151428\n",
      "epoch: 2 step: 423, loss is 0.16693980991840363\n",
      "epoch: 2 step: 424, loss is 0.05718286335468292\n",
      "epoch: 2 step: 425, loss is 0.15694482624530792\n",
      "epoch: 2 step: 426, loss is 0.004603011067956686\n",
      "epoch: 2 step: 427, loss is 0.07601414620876312\n",
      "epoch: 2 step: 428, loss is 0.05704305320978165\n",
      "epoch: 2 step: 429, loss is 0.1813145875930786\n",
      "epoch: 2 step: 430, loss is 0.006661830935627222\n",
      "epoch: 2 step: 431, loss is 0.08500374108552933\n",
      "epoch: 2 step: 432, loss is 0.04962776228785515\n",
      "epoch: 2 step: 433, loss is 0.04227058216929436\n",
      "epoch: 2 step: 434, loss is 0.06596504151821136\n",
      "epoch: 2 step: 435, loss is 0.10166870802640915\n",
      "epoch: 2 step: 436, loss is 0.015682537108659744\n",
      "epoch: 2 step: 437, loss is 0.02440585196018219\n",
      "epoch: 2 step: 438, loss is 0.11633184552192688\n",
      "epoch: 2 step: 439, loss is 0.021798567846417427\n",
      "epoch: 2 step: 440, loss is 0.1755553036928177\n",
      "epoch: 2 step: 441, loss is 0.08327186852693558\n",
      "epoch: 2 step: 442, loss is 0.056474149227142334\n",
      "epoch: 2 step: 443, loss is 0.05725320428609848\n",
      "epoch: 2 step: 444, loss is 0.06804054230451584\n",
      "epoch: 2 step: 445, loss is 0.002824974711984396\n",
      "epoch: 2 step: 446, loss is 0.12312908470630646\n",
      "epoch: 2 step: 447, loss is 0.05790932476520538\n",
      "epoch: 2 step: 448, loss is 0.007920806296169758\n",
      "epoch: 2 step: 449, loss is 0.2543656527996063\n",
      "epoch: 2 step: 450, loss is 0.03685253858566284\n",
      "epoch: 2 step: 451, loss is 0.005221203900873661\n",
      "epoch: 2 step: 452, loss is 0.09189078956842422\n",
      "epoch: 2 step: 453, loss is 0.03268217295408249\n",
      "epoch: 2 step: 454, loss is 0.09146755933761597\n",
      "epoch: 2 step: 455, loss is 0.22476714849472046\n",
      "epoch: 2 step: 456, loss is 0.008760057389736176\n",
      "epoch: 2 step: 457, loss is 0.12044916301965714\n",
      "epoch: 2 step: 458, loss is 0.030608469620347023\n",
      "epoch: 2 step: 459, loss is 0.02431904338300228\n",
      "epoch: 2 step: 460, loss is 0.05238732323050499\n",
      "epoch: 2 step: 461, loss is 0.1276235580444336\n",
      "epoch: 2 step: 462, loss is 0.055259883403778076\n",
      "epoch: 2 step: 463, loss is 0.17372728884220123\n",
      "epoch: 2 step: 464, loss is 0.04564724490046501\n",
      "epoch: 2 step: 465, loss is 0.0483349934220314\n",
      "epoch: 2 step: 466, loss is 0.2913527488708496\n",
      "epoch: 2 step: 467, loss is 0.10325389355421066\n",
      "epoch: 2 step: 468, loss is 0.013830325566232204\n",
      "epoch: 2 step: 469, loss is 0.0054374863393604755\n",
      "epoch: 2 step: 470, loss is 0.05091960355639458\n",
      "epoch: 2 step: 471, loss is 0.06252974271774292\n",
      "epoch: 2 step: 472, loss is 0.23592114448547363\n",
      "epoch: 2 step: 473, loss is 0.018435560166835785\n",
      "epoch: 2 step: 474, loss is 0.194642573595047\n",
      "epoch: 2 step: 475, loss is 0.0634983628988266\n",
      "epoch: 2 step: 476, loss is 0.10781563818454742\n",
      "epoch: 2 step: 477, loss is 0.0037295680958777666\n",
      "epoch: 2 step: 478, loss is 0.016936365514993668\n",
      "epoch: 2 step: 479, loss is 0.16213458776474\n",
      "epoch: 2 step: 480, loss is 0.08784370869398117\n",
      "epoch: 2 step: 481, loss is 0.0445978082716465\n",
      "epoch: 2 step: 482, loss is 0.007630747742950916\n",
      "epoch: 2 step: 483, loss is 0.1515168398618698\n",
      "epoch: 2 step: 484, loss is 0.0328199602663517\n",
      "epoch: 2 step: 485, loss is 0.0867653489112854\n",
      "epoch: 2 step: 486, loss is 0.17909491062164307\n",
      "epoch: 2 step: 487, loss is 0.0163424052298069\n",
      "epoch: 2 step: 488, loss is 0.14214105904102325\n",
      "epoch: 2 step: 489, loss is 0.031836677342653275\n",
      "epoch: 2 step: 490, loss is 0.2228824645280838\n",
      "epoch: 2 step: 491, loss is 0.10419785976409912\n",
      "epoch: 2 step: 492, loss is 0.0073559177108109\n",
      "epoch: 2 step: 493, loss is 0.23845943808555603\n",
      "epoch: 2 step: 494, loss is 0.26006418466567993\n",
      "epoch: 2 step: 495, loss is 0.015048959292471409\n",
      "epoch: 2 step: 496, loss is 0.023189358413219452\n",
      "epoch: 2 step: 497, loss is 0.1738269031047821\n",
      "epoch: 2 step: 498, loss is 0.01621018908917904\n",
      "epoch: 2 step: 499, loss is 0.014761947095394135\n",
      "epoch: 2 step: 500, loss is 0.29665637016296387\n",
      "epoch: 2 step: 501, loss is 0.04008110985159874\n",
      "epoch: 2 step: 502, loss is 0.05438068509101868\n",
      "epoch: 2 step: 503, loss is 0.06577577441930771\n",
      "epoch: 2 step: 504, loss is 0.05374138057231903\n",
      "epoch: 2 step: 505, loss is 0.19996479153633118\n",
      "epoch: 2 step: 506, loss is 0.1315729320049286\n",
      "epoch: 2 step: 507, loss is 0.026977544650435448\n",
      "epoch: 2 step: 508, loss is 0.16655860841274261\n",
      "epoch: 2 step: 509, loss is 0.022856347262859344\n",
      "epoch: 2 step: 510, loss is 0.18812865018844604\n",
      "epoch: 2 step: 511, loss is 0.06569361686706543\n",
      "epoch: 2 step: 512, loss is 0.008105055429041386\n",
      "epoch: 2 step: 513, loss is 0.07908448576927185\n",
      "epoch: 2 step: 514, loss is 0.055156223475933075\n",
      "epoch: 2 step: 515, loss is 0.1362307071685791\n",
      "epoch: 2 step: 516, loss is 0.20174086093902588\n",
      "epoch: 2 step: 517, loss is 0.12412857264280319\n",
      "epoch: 2 step: 518, loss is 0.38460662961006165\n",
      "epoch: 2 step: 519, loss is 0.023838937282562256\n",
      "epoch: 2 step: 520, loss is 0.17591390013694763\n",
      "epoch: 2 step: 521, loss is 0.3312543034553528\n",
      "epoch: 2 step: 522, loss is 0.16339823603630066\n",
      "epoch: 2 step: 523, loss is 0.04874571040272713\n",
      "epoch: 2 step: 524, loss is 0.0927780419588089\n",
      "epoch: 2 step: 525, loss is 0.03418492153286934\n",
      "epoch: 2 step: 526, loss is 0.018261481076478958\n",
      "epoch: 2 step: 527, loss is 0.1252715289592743\n",
      "epoch: 2 step: 528, loss is 0.07538830488920212\n",
      "epoch: 2 step: 529, loss is 0.23133327066898346\n",
      "epoch: 2 step: 530, loss is 0.2911317050457001\n",
      "epoch: 2 step: 531, loss is 0.2588215470314026\n",
      "epoch: 2 step: 532, loss is 0.1713312566280365\n",
      "epoch: 2 step: 533, loss is 0.02045457810163498\n",
      "epoch: 2 step: 534, loss is 0.017567303031682968\n",
      "epoch: 2 step: 535, loss is 0.030629238113760948\n",
      "epoch: 2 step: 536, loss is 0.05267716199159622\n",
      "epoch: 2 step: 537, loss is 0.0532086081802845\n",
      "epoch: 2 step: 538, loss is 0.08043856173753738\n",
      "epoch: 2 step: 539, loss is 0.3125767111778259\n",
      "epoch: 2 step: 540, loss is 0.2344130277633667\n",
      "epoch: 2 step: 541, loss is 0.0812438353896141\n",
      "epoch: 2 step: 542, loss is 0.020544584840536118\n",
      "epoch: 2 step: 543, loss is 0.06741403043270111\n",
      "epoch: 2 step: 544, loss is 0.14110657572746277\n",
      "epoch: 2 step: 545, loss is 0.3033519387245178\n",
      "epoch: 2 step: 546, loss is 0.009397040121257305\n",
      "epoch: 2 step: 547, loss is 0.028508376330137253\n",
      "epoch: 2 step: 548, loss is 0.17771238088607788\n",
      "epoch: 2 step: 549, loss is 0.20809189975261688\n",
      "epoch: 2 step: 550, loss is 0.04239142686128616\n",
      "epoch: 2 step: 551, loss is 0.04019062966108322\n",
      "epoch: 2 step: 552, loss is 0.030017808079719543\n",
      "epoch: 2 step: 553, loss is 0.09851451218128204\n",
      "epoch: 2 step: 554, loss is 0.16740183532238007\n",
      "epoch: 2 step: 555, loss is 0.13355307281017303\n",
      "epoch: 2 step: 556, loss is 0.050360821187496185\n",
      "epoch: 2 step: 557, loss is 0.304007887840271\n",
      "epoch: 2 step: 558, loss is 0.11514951288700104\n",
      "epoch: 2 step: 559, loss is 0.011180113069713116\n",
      "epoch: 2 step: 560, loss is 0.14138822257518768\n",
      "epoch: 2 step: 561, loss is 0.04957478493452072\n",
      "epoch: 2 step: 562, loss is 0.10795614868402481\n",
      "epoch: 2 step: 563, loss is 0.007049963343888521\n",
      "epoch: 2 step: 564, loss is 0.015019917860627174\n",
      "epoch: 2 step: 565, loss is 0.08499083667993546\n",
      "epoch: 2 step: 566, loss is 0.02712113969027996\n",
      "epoch: 2 step: 567, loss is 0.1170937567949295\n",
      "epoch: 2 step: 568, loss is 0.08308574557304382\n",
      "epoch: 2 step: 569, loss is 0.060391105711460114\n",
      "epoch: 2 step: 570, loss is 0.010940373875200748\n",
      "epoch: 2 step: 571, loss is 0.023835154250264168\n",
      "epoch: 2 step: 572, loss is 0.09520437568426132\n",
      "epoch: 2 step: 573, loss is 0.029970012605190277\n",
      "epoch: 2 step: 574, loss is 0.08010365068912506\n",
      "epoch: 2 step: 575, loss is 0.09086164087057114\n",
      "epoch: 2 step: 576, loss is 0.10283207893371582\n",
      "epoch: 2 step: 577, loss is 0.02271917276084423\n",
      "epoch: 2 step: 578, loss is 0.08724280446767807\n",
      "epoch: 2 step: 579, loss is 0.09361083805561066\n",
      "epoch: 2 step: 580, loss is 0.1424950808286667\n",
      "epoch: 2 step: 581, loss is 0.06683960556983948\n",
      "epoch: 2 step: 582, loss is 0.07012391090393066\n",
      "epoch: 2 step: 583, loss is 0.020565852522850037\n",
      "epoch: 2 step: 584, loss is 0.21211259067058563\n",
      "epoch: 2 step: 585, loss is 0.07811170071363449\n",
      "epoch: 2 step: 586, loss is 0.01573805697262287\n",
      "epoch: 2 step: 587, loss is 0.05623660609126091\n",
      "epoch: 2 step: 588, loss is 0.11132562905550003\n",
      "epoch: 2 step: 589, loss is 0.12645463645458221\n",
      "epoch: 2 step: 590, loss is 0.4462244212627411\n",
      "epoch: 2 step: 591, loss is 0.005912473425269127\n",
      "epoch: 2 step: 592, loss is 0.0016425176290795207\n",
      "epoch: 2 step: 593, loss is 0.008037008345127106\n",
      "epoch: 2 step: 594, loss is 0.1577586680650711\n",
      "epoch: 2 step: 595, loss is 0.04116480052471161\n",
      "epoch: 2 step: 596, loss is 0.14453154802322388\n",
      "epoch: 2 step: 597, loss is 0.13547392189502716\n",
      "epoch: 2 step: 598, loss is 0.1440150886774063\n",
      "epoch: 2 step: 599, loss is 0.037205882370471954\n",
      "epoch: 2 step: 600, loss is 0.04744924232363701\n",
      "epoch: 2 step: 601, loss is 0.015058092772960663\n",
      "epoch: 2 step: 602, loss is 0.07770144939422607\n",
      "epoch: 2 step: 603, loss is 0.13427607715129852\n",
      "epoch: 2 step: 604, loss is 0.22367382049560547\n",
      "epoch: 2 step: 605, loss is 0.0154689596965909\n",
      "epoch: 2 step: 606, loss is 0.1667444109916687\n",
      "epoch: 2 step: 607, loss is 0.030187081545591354\n",
      "epoch: 2 step: 608, loss is 0.2004321813583374\n",
      "epoch: 2 step: 609, loss is 0.0828639417886734\n",
      "epoch: 2 step: 610, loss is 0.06939917057752609\n",
      "epoch: 2 step: 611, loss is 0.09711814671754837\n",
      "epoch: 2 step: 612, loss is 0.035374682396650314\n",
      "epoch: 2 step: 613, loss is 0.04435713589191437\n",
      "epoch: 2 step: 614, loss is 0.006972792092710733\n",
      "epoch: 2 step: 615, loss is 0.14875063300132751\n",
      "epoch: 2 step: 616, loss is 0.03510409966111183\n",
      "epoch: 2 step: 617, loss is 0.16624422371387482\n",
      "epoch: 2 step: 618, loss is 0.00642875861376524\n",
      "epoch: 2 step: 619, loss is 0.08793023973703384\n",
      "epoch: 2 step: 620, loss is 0.05346864461898804\n",
      "epoch: 2 step: 621, loss is 0.09458242356777191\n",
      "epoch: 2 step: 622, loss is 0.009038613177835941\n",
      "epoch: 2 step: 623, loss is 0.015844428911805153\n",
      "epoch: 2 step: 624, loss is 0.06284379214048386\n",
      "epoch: 2 step: 625, loss is 0.013126841746270657\n",
      "epoch: 2 step: 626, loss is 0.014174618758261204\n",
      "epoch: 2 step: 627, loss is 0.09178901463747025\n",
      "epoch: 2 step: 628, loss is 0.24109892547130585\n",
      "epoch: 2 step: 629, loss is 0.0034308098256587982\n",
      "epoch: 2 step: 630, loss is 0.006889179814606905\n",
      "epoch: 2 step: 631, loss is 0.11662162840366364\n",
      "epoch: 2 step: 632, loss is 0.02783128060400486\n",
      "epoch: 2 step: 633, loss is 0.06600965559482574\n",
      "epoch: 2 step: 634, loss is 0.003447917988523841\n",
      "epoch: 2 step: 635, loss is 0.14734423160552979\n",
      "epoch: 2 step: 636, loss is 0.0013209488242864609\n",
      "epoch: 2 step: 637, loss is 0.29904913902282715\n",
      "epoch: 2 step: 638, loss is 0.006297613959759474\n",
      "epoch: 2 step: 639, loss is 0.21894633769989014\n",
      "epoch: 2 step: 640, loss is 0.0039116693660616875\n",
      "epoch: 2 step: 641, loss is 0.04541894048452377\n",
      "epoch: 2 step: 642, loss is 0.06301717460155487\n",
      "epoch: 2 step: 643, loss is 0.02599937468767166\n",
      "epoch: 2 step: 644, loss is 0.012420540675520897\n",
      "epoch: 2 step: 645, loss is 0.03983364626765251\n",
      "epoch: 2 step: 646, loss is 0.04531003534793854\n",
      "epoch: 2 step: 647, loss is 0.29511621594429016\n",
      "epoch: 2 step: 648, loss is 0.025522267445921898\n",
      "epoch: 2 step: 649, loss is 0.065093033015728\n",
      "epoch: 2 step: 650, loss is 0.008097352460026741\n",
      "epoch: 2 step: 651, loss is 0.009289806708693504\n",
      "epoch: 2 step: 652, loss is 0.21030758321285248\n",
      "epoch: 2 step: 653, loss is 0.18594996631145477\n",
      "epoch: 2 step: 654, loss is 0.0189669169485569\n",
      "epoch: 2 step: 655, loss is 0.09338709712028503\n",
      "epoch: 2 step: 656, loss is 0.13707946240901947\n",
      "epoch: 2 step: 657, loss is 0.031878434121608734\n",
      "epoch: 2 step: 658, loss is 0.08112019300460815\n",
      "epoch: 2 step: 659, loss is 0.43223315477371216\n",
      "epoch: 2 step: 660, loss is 0.13140977919101715\n",
      "epoch: 2 step: 661, loss is 0.22468794882297516\n",
      "epoch: 2 step: 662, loss is 0.06007228046655655\n",
      "epoch: 2 step: 663, loss is 0.006113103590905666\n",
      "epoch: 2 step: 664, loss is 0.013306768611073494\n",
      "epoch: 2 step: 665, loss is 0.08269783854484558\n",
      "epoch: 2 step: 666, loss is 0.021579742431640625\n",
      "epoch: 2 step: 667, loss is 0.06880657374858856\n",
      "epoch: 2 step: 668, loss is 0.027581514790654182\n",
      "epoch: 2 step: 669, loss is 0.05095075070858002\n",
      "epoch: 2 step: 670, loss is 0.0025061219930648804\n",
      "epoch: 2 step: 671, loss is 0.10098496824502945\n",
      "epoch: 2 step: 672, loss is 0.09076163172721863\n",
      "epoch: 2 step: 673, loss is 0.09558748453855515\n",
      "epoch: 2 step: 674, loss is 0.15164601802825928\n",
      "epoch: 2 step: 675, loss is 0.07528802007436752\n",
      "epoch: 2 step: 676, loss is 0.04269971698522568\n",
      "epoch: 2 step: 677, loss is 0.013647436164319515\n",
      "epoch: 2 step: 678, loss is 0.12712809443473816\n",
      "epoch: 2 step: 679, loss is 0.019766271114349365\n",
      "epoch: 2 step: 680, loss is 0.030299708247184753\n",
      "epoch: 2 step: 681, loss is 0.06294738501310349\n",
      "epoch: 2 step: 682, loss is 0.006075133103877306\n",
      "epoch: 2 step: 683, loss is 0.0031686504371464252\n",
      "epoch: 2 step: 684, loss is 0.1895158737897873\n",
      "epoch: 2 step: 685, loss is 0.030705325305461884\n",
      "epoch: 2 step: 686, loss is 0.15708819031715393\n",
      "epoch: 2 step: 687, loss is 0.09574505686759949\n",
      "epoch: 2 step: 688, loss is 0.27635571360588074\n",
      "epoch: 2 step: 689, loss is 0.09023883938789368\n",
      "epoch: 2 step: 690, loss is 0.13674195110797882\n",
      "epoch: 2 step: 691, loss is 0.005865948740392923\n",
      "epoch: 2 step: 692, loss is 0.008487757295370102\n",
      "epoch: 2 step: 693, loss is 0.16224215924739838\n",
      "epoch: 2 step: 694, loss is 0.11686824262142181\n",
      "epoch: 2 step: 695, loss is 0.03646425902843475\n",
      "epoch: 2 step: 696, loss is 0.14121046662330627\n",
      "epoch: 2 step: 697, loss is 0.015643157064914703\n",
      "epoch: 2 step: 698, loss is 0.015161436051130295\n",
      "epoch: 2 step: 699, loss is 0.24586087465286255\n",
      "epoch: 2 step: 700, loss is 0.027120642364025116\n",
      "epoch: 2 step: 701, loss is 0.02446788363158703\n",
      "epoch: 2 step: 702, loss is 0.021584827452898026\n",
      "epoch: 2 step: 703, loss is 0.019504396244883537\n",
      "epoch: 2 step: 704, loss is 0.052734844386577606\n",
      "epoch: 2 step: 705, loss is 0.01599900610744953\n",
      "epoch: 2 step: 706, loss is 0.15580904483795166\n",
      "epoch: 2 step: 707, loss is 0.03752761706709862\n",
      "epoch: 2 step: 708, loss is 0.01849239133298397\n",
      "epoch: 2 step: 709, loss is 0.04796963557600975\n",
      "epoch: 2 step: 710, loss is 0.07002143561840057\n",
      "epoch: 2 step: 711, loss is 0.11771298944950104\n",
      "epoch: 2 step: 712, loss is 0.1487225741147995\n",
      "epoch: 2 step: 713, loss is 0.023008372634649277\n",
      "epoch: 2 step: 714, loss is 0.050572656095027924\n",
      "epoch: 2 step: 715, loss is 0.010185453109443188\n",
      "epoch: 2 step: 716, loss is 0.23737852275371552\n",
      "epoch: 2 step: 717, loss is 0.010644973255693913\n",
      "epoch: 2 step: 718, loss is 0.07187704741954803\n",
      "epoch: 2 step: 719, loss is 0.10601027309894562\n",
      "epoch: 2 step: 720, loss is 0.05384168028831482\n",
      "epoch: 2 step: 721, loss is 0.017898399382829666\n",
      "epoch: 2 step: 722, loss is 0.005319449584931135\n",
      "epoch: 2 step: 723, loss is 0.0037379523273557425\n",
      "epoch: 2 step: 724, loss is 0.003379712114110589\n",
      "epoch: 2 step: 725, loss is 0.1961972415447235\n",
      "epoch: 2 step: 726, loss is 0.07836609333753586\n",
      "epoch: 2 step: 727, loss is 0.28370028734207153\n",
      "epoch: 2 step: 728, loss is 0.1537492573261261\n",
      "epoch: 2 step: 729, loss is 0.005594420246779919\n",
      "epoch: 2 step: 730, loss is 0.016668422147631645\n",
      "epoch: 2 step: 731, loss is 0.015461845323443413\n",
      "epoch: 2 step: 732, loss is 0.019017016515135765\n",
      "epoch: 2 step: 733, loss is 0.10291698575019836\n",
      "epoch: 2 step: 734, loss is 0.15360163152217865\n",
      "epoch: 2 step: 735, loss is 0.038899511098861694\n",
      "epoch: 2 step: 736, loss is 0.22639209032058716\n",
      "epoch: 2 step: 737, loss is 0.010913457721471786\n",
      "epoch: 2 step: 738, loss is 0.01379586011171341\n",
      "epoch: 2 step: 739, loss is 0.010026691481471062\n",
      "epoch: 2 step: 740, loss is 0.06213688105344772\n",
      "epoch: 2 step: 741, loss is 0.02259708009660244\n",
      "epoch: 2 step: 742, loss is 0.014355745166540146\n",
      "epoch: 2 step: 743, loss is 0.06431778520345688\n",
      "epoch: 2 step: 744, loss is 0.046068064868450165\n",
      "epoch: 2 step: 745, loss is 0.20805785059928894\n",
      "epoch: 2 step: 746, loss is 0.00529119186103344\n",
      "epoch: 2 step: 747, loss is 0.014251286163926125\n",
      "epoch: 2 step: 748, loss is 0.030088327825069427\n",
      "epoch: 2 step: 749, loss is 0.011392183601856232\n",
      "epoch: 2 step: 750, loss is 0.21308468282222748\n",
      "epoch: 2 step: 751, loss is 0.00785016268491745\n",
      "epoch: 2 step: 752, loss is 0.027430180460214615\n",
      "epoch: 2 step: 753, loss is 0.0103550273925066\n",
      "epoch: 2 step: 754, loss is 0.09331158548593521\n",
      "epoch: 2 step: 755, loss is 0.0922582820057869\n",
      "epoch: 2 step: 756, loss is 0.0521497018635273\n",
      "epoch: 2 step: 757, loss is 0.008819661103188992\n",
      "epoch: 2 step: 758, loss is 0.21547794342041016\n",
      "epoch: 2 step: 759, loss is 0.102755106985569\n",
      "epoch: 2 step: 760, loss is 0.003019645344465971\n",
      "epoch: 2 step: 761, loss is 0.005158885847777128\n",
      "epoch: 2 step: 762, loss is 0.250009149312973\n",
      "epoch: 2 step: 763, loss is 0.21568705141544342\n",
      "epoch: 2 step: 764, loss is 0.05929623916745186\n",
      "epoch: 2 step: 765, loss is 0.11035530269145966\n",
      "epoch: 2 step: 766, loss is 0.019623246043920517\n",
      "epoch: 2 step: 767, loss is 0.05197961628437042\n",
      "epoch: 2 step: 768, loss is 0.06103983893990517\n",
      "epoch: 2 step: 769, loss is 0.026712747290730476\n",
      "epoch: 2 step: 770, loss is 0.03853273764252663\n",
      "epoch: 2 step: 771, loss is 0.009706128388643265\n",
      "epoch: 2 step: 772, loss is 0.04184042662382126\n",
      "epoch: 2 step: 773, loss is 0.02193307690322399\n",
      "epoch: 2 step: 774, loss is 0.01673935540020466\n",
      "epoch: 2 step: 775, loss is 0.04295025393366814\n",
      "epoch: 2 step: 776, loss is 0.020618770271539688\n",
      "epoch: 2 step: 777, loss is 0.04304938018321991\n",
      "epoch: 2 step: 778, loss is 0.01084718108177185\n",
      "epoch: 2 step: 779, loss is 0.017930295318365097\n",
      "epoch: 2 step: 780, loss is 0.07111626118421555\n",
      "epoch: 2 step: 781, loss is 0.03910508379340172\n",
      "epoch: 2 step: 782, loss is 0.05588187277317047\n",
      "epoch: 2 step: 783, loss is 0.18405579030513763\n",
      "epoch: 2 step: 784, loss is 0.09840840846300125\n",
      "epoch: 2 step: 785, loss is 0.03640120103955269\n",
      "epoch: 2 step: 786, loss is 0.03671249374747276\n",
      "epoch: 2 step: 787, loss is 0.028417976573109627\n",
      "epoch: 2 step: 788, loss is 0.004564962349832058\n",
      "epoch: 2 step: 789, loss is 0.118224136531353\n",
      "epoch: 2 step: 790, loss is 0.04023052379488945\n",
      "epoch: 2 step: 791, loss is 0.012630661949515343\n",
      "epoch: 2 step: 792, loss is 0.3863195776939392\n",
      "epoch: 2 step: 793, loss is 0.07144730538129807\n",
      "epoch: 2 step: 794, loss is 0.5041247606277466\n",
      "epoch: 2 step: 795, loss is 0.1249883845448494\n",
      "epoch: 2 step: 796, loss is 0.015999773517251015\n",
      "epoch: 2 step: 797, loss is 0.20456890761852264\n",
      "epoch: 2 step: 798, loss is 0.014853271655738354\n",
      "epoch: 2 step: 799, loss is 0.10867266356945038\n",
      "epoch: 2 step: 800, loss is 0.13227054476737976\n",
      "epoch: 2 step: 801, loss is 0.03045741654932499\n",
      "epoch: 2 step: 802, loss is 0.16929036378860474\n",
      "epoch: 2 step: 803, loss is 0.25337788462638855\n",
      "epoch: 2 step: 804, loss is 0.00389101798646152\n",
      "epoch: 2 step: 805, loss is 0.014321543276309967\n",
      "epoch: 2 step: 806, loss is 0.056873105466365814\n",
      "epoch: 2 step: 807, loss is 0.033252693712711334\n",
      "epoch: 2 step: 808, loss is 0.022097324952483177\n",
      "epoch: 2 step: 809, loss is 0.271137535572052\n",
      "epoch: 2 step: 810, loss is 0.0420859269797802\n",
      "epoch: 2 step: 811, loss is 0.054406460374593735\n",
      "epoch: 2 step: 812, loss is 0.5081350207328796\n",
      "epoch: 2 step: 813, loss is 0.06977672874927521\n",
      "epoch: 2 step: 814, loss is 0.04595455527305603\n",
      "epoch: 2 step: 815, loss is 0.009810252115130424\n",
      "epoch: 2 step: 816, loss is 0.1749637871980667\n",
      "epoch: 2 step: 817, loss is 0.036259423941373825\n",
      "epoch: 2 step: 818, loss is 0.023106925189495087\n",
      "epoch: 2 step: 819, loss is 0.12241780757904053\n",
      "epoch: 2 step: 820, loss is 0.05438341572880745\n",
      "epoch: 2 step: 821, loss is 0.20887845754623413\n",
      "epoch: 2 step: 822, loss is 0.11154848337173462\n",
      "epoch: 2 step: 823, loss is 0.1436147689819336\n",
      "epoch: 2 step: 824, loss is 0.03094220720231533\n",
      "epoch: 2 step: 825, loss is 0.03780703246593475\n",
      "epoch: 2 step: 826, loss is 0.322425901889801\n",
      "epoch: 2 step: 827, loss is 0.05809739977121353\n",
      "epoch: 2 step: 828, loss is 0.01322215422987938\n",
      "epoch: 2 step: 829, loss is 0.07090098410844803\n",
      "epoch: 2 step: 830, loss is 0.04496923089027405\n",
      "epoch: 2 step: 831, loss is 0.06175299361348152\n",
      "epoch: 2 step: 832, loss is 0.35066014528274536\n",
      "epoch: 2 step: 833, loss is 0.019327351823449135\n",
      "epoch: 2 step: 834, loss is 0.15298797190189362\n",
      "epoch: 2 step: 835, loss is 0.009596114978194237\n",
      "epoch: 2 step: 836, loss is 0.008118580095469952\n",
      "epoch: 2 step: 837, loss is 0.05862601846456528\n",
      "epoch: 2 step: 838, loss is 0.12782232463359833\n",
      "epoch: 2 step: 839, loss is 0.03542827069759369\n",
      "epoch: 2 step: 840, loss is 0.1716579794883728\n",
      "epoch: 2 step: 841, loss is 0.020617332309484482\n",
      "epoch: 2 step: 842, loss is 0.06302712857723236\n",
      "epoch: 2 step: 843, loss is 0.16780634224414825\n",
      "epoch: 2 step: 844, loss is 0.27866658568382263\n",
      "epoch: 2 step: 845, loss is 0.029211632907390594\n",
      "epoch: 2 step: 846, loss is 0.03580183535814285\n",
      "epoch: 2 step: 847, loss is 0.00685124471783638\n",
      "epoch: 2 step: 848, loss is 0.21473757922649384\n",
      "epoch: 2 step: 849, loss is 0.02624775655567646\n",
      "epoch: 2 step: 850, loss is 0.010500574484467506\n",
      "epoch: 2 step: 851, loss is 0.02276139333844185\n",
      "epoch: 2 step: 852, loss is 0.06427497416734695\n",
      "epoch: 2 step: 853, loss is 0.10141588002443314\n",
      "epoch: 2 step: 854, loss is 0.20966565608978271\n",
      "epoch: 2 step: 855, loss is 0.03732968866825104\n",
      "epoch: 2 step: 856, loss is 0.009018058888614178\n",
      "epoch: 2 step: 857, loss is 0.013156918808817863\n",
      "epoch: 2 step: 858, loss is 0.013986756093800068\n",
      "epoch: 2 step: 859, loss is 0.36875686049461365\n",
      "epoch: 2 step: 860, loss is 0.049434736371040344\n",
      "epoch: 2 step: 861, loss is 0.0033589834347367287\n",
      "epoch: 2 step: 862, loss is 0.3206595778465271\n",
      "epoch: 2 step: 863, loss is 0.00648953951895237\n",
      "epoch: 2 step: 864, loss is 0.06250493228435516\n",
      "epoch: 2 step: 865, loss is 0.15141181647777557\n",
      "epoch: 2 step: 866, loss is 0.09125550091266632\n",
      "epoch: 2 step: 867, loss is 0.12098991870880127\n",
      "epoch: 2 step: 868, loss is 0.029980245977640152\n",
      "epoch: 2 step: 869, loss is 0.05908871814608574\n",
      "epoch: 2 step: 870, loss is 0.19985119998455048\n",
      "epoch: 2 step: 871, loss is 0.05936964228749275\n",
      "epoch: 2 step: 872, loss is 0.028465110808610916\n",
      "epoch: 2 step: 873, loss is 0.011965233832597733\n",
      "epoch: 2 step: 874, loss is 0.10341046750545502\n",
      "epoch: 2 step: 875, loss is 0.0861683264374733\n",
      "epoch: 2 step: 876, loss is 0.05712949112057686\n",
      "epoch: 2 step: 877, loss is 0.03293472155928612\n",
      "epoch: 2 step: 878, loss is 0.061569176614284515\n",
      "epoch: 2 step: 879, loss is 0.048134732991456985\n",
      "epoch: 2 step: 880, loss is 0.03234313428401947\n",
      "epoch: 2 step: 881, loss is 0.0980740562081337\n",
      "epoch: 2 step: 882, loss is 0.13829417526721954\n",
      "epoch: 2 step: 883, loss is 0.1681765913963318\n",
      "epoch: 2 step: 884, loss is 0.17474421858787537\n",
      "epoch: 2 step: 885, loss is 0.18600578606128693\n",
      "epoch: 2 step: 886, loss is 0.09577692300081253\n",
      "epoch: 2 step: 887, loss is 0.11474111676216125\n",
      "epoch: 2 step: 888, loss is 0.012412280775606632\n",
      "epoch: 2 step: 889, loss is 0.01476200856268406\n",
      "epoch: 2 step: 890, loss is 0.039785727858543396\n",
      "epoch: 2 step: 891, loss is 0.01260799914598465\n",
      "epoch: 2 step: 892, loss is 0.1020611897110939\n",
      "epoch: 2 step: 893, loss is 0.09720466285943985\n",
      "epoch: 2 step: 894, loss is 0.028443746268749237\n",
      "epoch: 2 step: 895, loss is 0.13062521815299988\n",
      "epoch: 2 step: 896, loss is 0.23941536247730255\n",
      "epoch: 2 step: 897, loss is 0.023257901892066002\n",
      "epoch: 2 step: 898, loss is 0.10120739787817001\n",
      "epoch: 2 step: 899, loss is 0.08886999636888504\n",
      "epoch: 2 step: 900, loss is 0.04658307880163193\n",
      "epoch: 2 step: 901, loss is 0.03522460162639618\n",
      "epoch: 2 step: 902, loss is 0.05942387878894806\n",
      "epoch: 2 step: 903, loss is 0.013320333324372768\n",
      "epoch: 2 step: 904, loss is 0.08535639941692352\n",
      "epoch: 2 step: 905, loss is 0.03016524948179722\n",
      "epoch: 2 step: 906, loss is 0.05786164104938507\n",
      "epoch: 2 step: 907, loss is 0.2430044412612915\n",
      "epoch: 2 step: 908, loss is 0.04509682580828667\n",
      "epoch: 2 step: 909, loss is 0.00784057192504406\n",
      "epoch: 2 step: 910, loss is 0.017402539029717445\n",
      "epoch: 2 step: 911, loss is 0.2400006204843521\n",
      "epoch: 2 step: 912, loss is 0.11236605793237686\n",
      "epoch: 2 step: 913, loss is 0.04360617697238922\n",
      "epoch: 2 step: 914, loss is 0.03842027857899666\n",
      "epoch: 2 step: 915, loss is 0.03865203633904457\n",
      "epoch: 2 step: 916, loss is 0.14667534828186035\n",
      "epoch: 2 step: 917, loss is 0.019600188359618187\n",
      "epoch: 2 step: 918, loss is 0.06734757870435715\n",
      "epoch: 2 step: 919, loss is 0.08193101733922958\n",
      "epoch: 2 step: 920, loss is 0.07371601462364197\n",
      "epoch: 2 step: 921, loss is 0.0998867005109787\n",
      "epoch: 2 step: 922, loss is 0.026183640584349632\n",
      "epoch: 2 step: 923, loss is 0.09673020988702774\n",
      "epoch: 2 step: 924, loss is 0.022466760128736496\n",
      "epoch: 2 step: 925, loss is 0.09730713814496994\n",
      "epoch: 2 step: 926, loss is 0.04698491841554642\n",
      "epoch: 2 step: 927, loss is 0.009935968555510044\n",
      "epoch: 2 step: 928, loss is 0.17588524520397186\n",
      "epoch: 2 step: 929, loss is 0.15448886156082153\n",
      "epoch: 2 step: 930, loss is 0.05193490907549858\n",
      "epoch: 2 step: 931, loss is 0.02958468347787857\n",
      "epoch: 2 step: 932, loss is 0.006620719563215971\n",
      "epoch: 2 step: 933, loss is 0.04210955277085304\n",
      "epoch: 2 step: 934, loss is 0.004331451375037432\n",
      "epoch: 2 step: 935, loss is 0.06451135128736496\n",
      "epoch: 2 step: 936, loss is 0.19057296216487885\n",
      "epoch: 2 step: 937, loss is 0.030302969738841057\n",
      "epoch: 2 step: 938, loss is 0.02610345557332039\n",
      "epoch: 2 step: 939, loss is 0.11718223989009857\n",
      "epoch: 2 step: 940, loss is 0.10725978016853333\n",
      "epoch: 2 step: 941, loss is 0.09256913512945175\n",
      "epoch: 2 step: 942, loss is 0.03943783417344093\n",
      "epoch: 2 step: 943, loss is 0.012188914231956005\n",
      "epoch: 2 step: 944, loss is 0.1254187673330307\n",
      "epoch: 2 step: 945, loss is 0.0010458024917170405\n",
      "epoch: 2 step: 946, loss is 0.20982101559638977\n",
      "epoch: 2 step: 947, loss is 0.014279860071837902\n",
      "epoch: 2 step: 948, loss is 0.31713375449180603\n",
      "epoch: 2 step: 949, loss is 0.2575433552265167\n",
      "epoch: 2 step: 950, loss is 0.2310759425163269\n",
      "epoch: 2 step: 951, loss is 0.04224955290555954\n",
      "epoch: 2 step: 952, loss is 0.00404818682000041\n",
      "epoch: 2 step: 953, loss is 0.04550061374902725\n",
      "epoch: 2 step: 954, loss is 0.024963704869151115\n",
      "epoch: 2 step: 955, loss is 0.13094303011894226\n",
      "epoch: 2 step: 956, loss is 0.014489677734673023\n",
      "epoch: 2 step: 957, loss is 0.01603083126246929\n",
      "epoch: 2 step: 958, loss is 0.0011849639704450965\n",
      "epoch: 2 step: 959, loss is 0.021253088489174843\n",
      "epoch: 2 step: 960, loss is 0.13366390764713287\n",
      "epoch: 2 step: 961, loss is 0.013628769665956497\n",
      "epoch: 2 step: 962, loss is 0.009929328225553036\n",
      "epoch: 2 step: 963, loss is 0.2889080345630646\n",
      "epoch: 2 step: 964, loss is 0.0834270790219307\n",
      "epoch: 2 step: 965, loss is 0.06588155031204224\n",
      "epoch: 2 step: 966, loss is 0.01637616567313671\n",
      "epoch: 2 step: 967, loss is 0.08457306027412415\n",
      "epoch: 2 step: 968, loss is 0.10472830384969711\n",
      "epoch: 2 step: 969, loss is 0.1237376481294632\n",
      "epoch: 2 step: 970, loss is 0.06787478178739548\n",
      "epoch: 2 step: 971, loss is 0.13749341666698456\n",
      "epoch: 2 step: 972, loss is 0.033183299005031586\n",
      "epoch: 2 step: 973, loss is 0.004441780969500542\n",
      "epoch: 2 step: 974, loss is 0.02417723834514618\n",
      "epoch: 2 step: 975, loss is 0.2042216807603836\n",
      "epoch: 2 step: 976, loss is 0.020016642287373543\n",
      "epoch: 2 step: 977, loss is 0.004213032778352499\n",
      "epoch: 2 step: 978, loss is 0.028319986537098885\n",
      "epoch: 2 step: 979, loss is 0.2202894240617752\n",
      "epoch: 2 step: 980, loss is 0.033771004527807236\n",
      "epoch: 2 step: 981, loss is 0.06043696403503418\n",
      "epoch: 2 step: 982, loss is 0.055993638932704926\n",
      "epoch: 2 step: 983, loss is 0.012819832190871239\n",
      "epoch: 2 step: 984, loss is 0.015183866024017334\n",
      "epoch: 2 step: 985, loss is 0.00871854554861784\n",
      "epoch: 2 step: 986, loss is 0.028606709092855453\n",
      "epoch: 2 step: 987, loss is 0.1895769089460373\n",
      "epoch: 2 step: 988, loss is 0.20440571010112762\n",
      "epoch: 2 step: 989, loss is 0.050047531723976135\n",
      "epoch: 2 step: 990, loss is 0.01962323673069477\n",
      "epoch: 2 step: 991, loss is 0.029658690094947815\n",
      "epoch: 2 step: 992, loss is 0.11419180035591125\n",
      "epoch: 2 step: 993, loss is 0.05996277555823326\n",
      "epoch: 2 step: 994, loss is 0.05542152747511864\n",
      "epoch: 2 step: 995, loss is 0.02941070683300495\n",
      "epoch: 2 step: 996, loss is 0.058451056480407715\n",
      "epoch: 2 step: 997, loss is 0.06154094636440277\n",
      "epoch: 2 step: 998, loss is 0.044870588928461075\n",
      "epoch: 2 step: 999, loss is 0.10071681439876556\n",
      "epoch: 2 step: 1000, loss is 0.07437882572412491\n",
      "epoch: 2 step: 1001, loss is 0.057431019842624664\n",
      "epoch: 2 step: 1002, loss is 0.10934285074472427\n",
      "epoch: 2 step: 1003, loss is 0.026321295648813248\n",
      "epoch: 2 step: 1004, loss is 0.05780482292175293\n",
      "epoch: 2 step: 1005, loss is 0.2644279897212982\n",
      "epoch: 2 step: 1006, loss is 0.009457350708544254\n",
      "epoch: 2 step: 1007, loss is 0.3109273612499237\n",
      "epoch: 2 step: 1008, loss is 0.0018714830512180924\n",
      "epoch: 2 step: 1009, loss is 0.015199374407529831\n",
      "epoch: 2 step: 1010, loss is 0.02668297290802002\n",
      "epoch: 2 step: 1011, loss is 0.007092858664691448\n",
      "epoch: 2 step: 1012, loss is 0.01441873423755169\n",
      "epoch: 2 step: 1013, loss is 0.1182665005326271\n",
      "epoch: 2 step: 1014, loss is 0.29972052574157715\n",
      "epoch: 2 step: 1015, loss is 0.4150148034095764\n",
      "epoch: 2 step: 1016, loss is 0.10111582279205322\n",
      "epoch: 2 step: 1017, loss is 0.015432889573276043\n",
      "epoch: 2 step: 1018, loss is 0.04662483185529709\n",
      "epoch: 2 step: 1019, loss is 0.03895725682377815\n",
      "epoch: 2 step: 1020, loss is 0.052536651492118835\n",
      "epoch: 2 step: 1021, loss is 0.024064119905233383\n",
      "epoch: 2 step: 1022, loss is 0.16179826855659485\n",
      "epoch: 2 step: 1023, loss is 0.08165008574724197\n",
      "epoch: 2 step: 1024, loss is 0.04412713646888733\n",
      "epoch: 2 step: 1025, loss is 0.09241544455289841\n",
      "epoch: 2 step: 1026, loss is 0.063431017100811\n",
      "epoch: 2 step: 1027, loss is 0.08785473555326462\n",
      "epoch: 2 step: 1028, loss is 0.09267755597829819\n",
      "epoch: 2 step: 1029, loss is 0.011500694788992405\n",
      "epoch: 2 step: 1030, loss is 0.02694944478571415\n",
      "epoch: 2 step: 1031, loss is 0.07088635861873627\n",
      "epoch: 2 step: 1032, loss is 0.17750206589698792\n",
      "epoch: 2 step: 1033, loss is 0.01540208887308836\n",
      "epoch: 2 step: 1034, loss is 0.17744283378124237\n",
      "epoch: 2 step: 1035, loss is 0.023188859224319458\n",
      "epoch: 2 step: 1036, loss is 0.03489472717046738\n",
      "epoch: 2 step: 1037, loss is 0.07837459444999695\n",
      "epoch: 2 step: 1038, loss is 0.011913783848285675\n",
      "epoch: 2 step: 1039, loss is 0.009474064223468304\n",
      "epoch: 2 step: 1040, loss is 0.11507069319486618\n",
      "epoch: 2 step: 1041, loss is 0.0063982862047851086\n",
      "epoch: 2 step: 1042, loss is 0.042804982513189316\n",
      "epoch: 2 step: 1043, loss is 0.15454353392124176\n",
      "epoch: 2 step: 1044, loss is 0.020653042942285538\n",
      "epoch: 2 step: 1045, loss is 0.013905142433941364\n",
      "epoch: 2 step: 1046, loss is 0.0019911210983991623\n",
      "epoch: 2 step: 1047, loss is 0.04739256948232651\n",
      "epoch: 2 step: 1048, loss is 0.04667782038450241\n",
      "epoch: 2 step: 1049, loss is 0.0035243756137788296\n",
      "epoch: 2 step: 1050, loss is 0.007799661252647638\n",
      "epoch: 2 step: 1051, loss is 0.001146474969573319\n",
      "epoch: 2 step: 1052, loss is 0.0026623127050697803\n",
      "epoch: 2 step: 1053, loss is 0.10671187192201614\n",
      "epoch: 2 step: 1054, loss is 0.011810262687504292\n",
      "epoch: 2 step: 1055, loss is 0.0029072007164359093\n",
      "epoch: 2 step: 1056, loss is 0.03849024325609207\n",
      "epoch: 2 step: 1057, loss is 0.0388818196952343\n",
      "epoch: 2 step: 1058, loss is 0.011019513942301273\n",
      "epoch: 2 step: 1059, loss is 0.010809801518917084\n",
      "epoch: 2 step: 1060, loss is 0.3382021188735962\n",
      "epoch: 2 step: 1061, loss is 0.02477618120610714\n",
      "epoch: 2 step: 1062, loss is 0.004885263275355101\n",
      "epoch: 2 step: 1063, loss is 0.19283002614974976\n",
      "epoch: 2 step: 1064, loss is 0.005296756513416767\n",
      "epoch: 2 step: 1065, loss is 0.06199920177459717\n",
      "epoch: 2 step: 1066, loss is 0.1395203173160553\n",
      "epoch: 2 step: 1067, loss is 0.021197982132434845\n",
      "epoch: 2 step: 1068, loss is 0.04889662563800812\n",
      "epoch: 2 step: 1069, loss is 0.009522846899926662\n",
      "epoch: 2 step: 1070, loss is 0.106449656188488\n",
      "epoch: 2 step: 1071, loss is 0.20655226707458496\n",
      "epoch: 2 step: 1072, loss is 0.08886302262544632\n",
      "epoch: 2 step: 1073, loss is 0.08953879028558731\n",
      "epoch: 2 step: 1074, loss is 0.07731745392084122\n",
      "epoch: 2 step: 1075, loss is 0.007011783309280872\n",
      "epoch: 2 step: 1076, loss is 0.0835409015417099\n",
      "epoch: 2 step: 1077, loss is 0.05993315577507019\n",
      "epoch: 2 step: 1078, loss is 0.012783347629010677\n",
      "epoch: 2 step: 1079, loss is 0.010924791917204857\n",
      "epoch: 2 step: 1080, loss is 0.21513669192790985\n",
      "epoch: 2 step: 1081, loss is 0.02756611630320549\n",
      "epoch: 2 step: 1082, loss is 0.003061907133087516\n",
      "epoch: 2 step: 1083, loss is 0.05001123249530792\n",
      "epoch: 2 step: 1084, loss is 0.05661817267537117\n",
      "epoch: 2 step: 1085, loss is 0.058917317539453506\n",
      "epoch: 2 step: 1086, loss is 0.008722671307623386\n",
      "epoch: 2 step: 1087, loss is 0.06236155331134796\n",
      "epoch: 2 step: 1088, loss is 0.07412225753068924\n",
      "epoch: 2 step: 1089, loss is 0.014960885979235172\n",
      "epoch: 2 step: 1090, loss is 0.10711906105279922\n",
      "epoch: 2 step: 1091, loss is 0.016133539378643036\n",
      "epoch: 2 step: 1092, loss is 0.10948437452316284\n",
      "epoch: 2 step: 1093, loss is 0.07078158855438232\n",
      "epoch: 2 step: 1094, loss is 0.044947780668735504\n",
      "epoch: 2 step: 1095, loss is 0.008093229494988918\n",
      "epoch: 2 step: 1096, loss is 0.22246988117694855\n",
      "epoch: 2 step: 1097, loss is 0.1141810342669487\n",
      "epoch: 2 step: 1098, loss is 0.017782360315322876\n",
      "epoch: 2 step: 1099, loss is 0.03231321647763252\n",
      "epoch: 2 step: 1100, loss is 0.02807096764445305\n",
      "epoch: 2 step: 1101, loss is 0.022799205034971237\n",
      "epoch: 2 step: 1102, loss is 0.037024904042482376\n",
      "epoch: 2 step: 1103, loss is 0.009174009785056114\n",
      "epoch: 2 step: 1104, loss is 0.0008010729216039181\n",
      "epoch: 2 step: 1105, loss is 0.03538689389824867\n",
      "epoch: 2 step: 1106, loss is 0.0456157848238945\n",
      "epoch: 2 step: 1107, loss is 0.006157077848911285\n",
      "epoch: 2 step: 1108, loss is 0.11102990061044693\n",
      "epoch: 2 step: 1109, loss is 0.004285992123186588\n",
      "epoch: 2 step: 1110, loss is 0.0946747437119484\n",
      "epoch: 2 step: 1111, loss is 0.17030069231987\n",
      "epoch: 2 step: 1112, loss is 0.13307145237922668\n",
      "epoch: 2 step: 1113, loss is 0.0062943678349256516\n",
      "epoch: 2 step: 1114, loss is 0.01920158788561821\n",
      "epoch: 2 step: 1115, loss is 0.011171745136380196\n",
      "epoch: 2 step: 1116, loss is 0.04241319000720978\n",
      "epoch: 2 step: 1117, loss is 0.059007853269577026\n",
      "epoch: 2 step: 1118, loss is 0.007024028804153204\n",
      "epoch: 2 step: 1119, loss is 0.03462512791156769\n",
      "epoch: 2 step: 1120, loss is 0.004192071501165628\n",
      "epoch: 2 step: 1121, loss is 0.09601667523384094\n",
      "epoch: 2 step: 1122, loss is 0.0287167988717556\n",
      "epoch: 2 step: 1123, loss is 0.023998990654945374\n",
      "epoch: 2 step: 1124, loss is 0.0018789108144119382\n",
      "epoch: 2 step: 1125, loss is 0.22333039343357086\n",
      "epoch: 2 step: 1126, loss is 0.0071441782638430595\n",
      "epoch: 2 step: 1127, loss is 0.31937676668167114\n",
      "epoch: 2 step: 1128, loss is 0.12217942625284195\n",
      "epoch: 2 step: 1129, loss is 0.002569644246250391\n",
      "epoch: 2 step: 1130, loss is 0.0026188530027866364\n",
      "epoch: 2 step: 1131, loss is 0.3619571626186371\n",
      "epoch: 2 step: 1132, loss is 0.007947433739900589\n",
      "epoch: 2 step: 1133, loss is 0.07218638062477112\n",
      "epoch: 2 step: 1134, loss is 0.04628424718976021\n",
      "epoch: 2 step: 1135, loss is 0.008408165536820889\n",
      "epoch: 2 step: 1136, loss is 0.0355246365070343\n",
      "epoch: 2 step: 1137, loss is 0.046418849378824234\n",
      "epoch: 2 step: 1138, loss is 0.0009694013861007988\n",
      "epoch: 2 step: 1139, loss is 0.055738579481840134\n",
      "epoch: 2 step: 1140, loss is 0.15110716223716736\n",
      "epoch: 2 step: 1141, loss is 0.12132565677165985\n",
      "epoch: 2 step: 1142, loss is 0.027975166216492653\n",
      "epoch: 2 step: 1143, loss is 0.1762012392282486\n",
      "epoch: 2 step: 1144, loss is 0.021335776895284653\n",
      "epoch: 2 step: 1145, loss is 0.17131219804286957\n",
      "epoch: 2 step: 1146, loss is 0.009184553287923336\n",
      "epoch: 2 step: 1147, loss is 0.009635425172746181\n",
      "epoch: 2 step: 1148, loss is 0.1441923975944519\n",
      "epoch: 2 step: 1149, loss is 0.1541585773229599\n",
      "epoch: 2 step: 1150, loss is 0.08795639127492905\n",
      "epoch: 2 step: 1151, loss is 0.04256643354892731\n",
      "epoch: 2 step: 1152, loss is 0.028534961864352226\n",
      "epoch: 2 step: 1153, loss is 0.003674503415822983\n",
      "epoch: 2 step: 1154, loss is 0.026373689994215965\n",
      "epoch: 2 step: 1155, loss is 0.006506780628114939\n",
      "epoch: 2 step: 1156, loss is 0.10775671154260635\n",
      "epoch: 2 step: 1157, loss is 0.12925414741039276\n",
      "epoch: 2 step: 1158, loss is 0.1538098156452179\n",
      "epoch: 2 step: 1159, loss is 0.018432186916470528\n",
      "epoch: 2 step: 1160, loss is 0.01764986850321293\n",
      "epoch: 2 step: 1161, loss is 0.17766644060611725\n",
      "epoch: 2 step: 1162, loss is 0.1033797338604927\n",
      "epoch: 2 step: 1163, loss is 0.018271412700414658\n",
      "epoch: 2 step: 1164, loss is 0.16320613026618958\n",
      "epoch: 2 step: 1165, loss is 0.015979090705513954\n",
      "epoch: 2 step: 1166, loss is 0.0496770441532135\n",
      "epoch: 2 step: 1167, loss is 0.06162475794553757\n",
      "epoch: 2 step: 1168, loss is 0.28830888867378235\n",
      "epoch: 2 step: 1169, loss is 0.02878694050014019\n",
      "epoch: 2 step: 1170, loss is 0.005221948958933353\n",
      "epoch: 2 step: 1171, loss is 0.19225576519966125\n",
      "epoch: 2 step: 1172, loss is 0.009186184033751488\n",
      "epoch: 2 step: 1173, loss is 0.03019162081182003\n",
      "epoch: 2 step: 1174, loss is 0.024866890162229538\n",
      "epoch: 2 step: 1175, loss is 0.011536694131791592\n",
      "epoch: 2 step: 1176, loss is 0.019190266728401184\n",
      "epoch: 2 step: 1177, loss is 0.014764838851988316\n",
      "epoch: 2 step: 1178, loss is 0.01717549003660679\n",
      "epoch: 2 step: 1179, loss is 0.046492088586091995\n",
      "epoch: 2 step: 1180, loss is 0.027806174010038376\n",
      "epoch: 2 step: 1181, loss is 0.08436521887779236\n",
      "epoch: 2 step: 1182, loss is 0.03656861186027527\n",
      "epoch: 2 step: 1183, loss is 0.08451194316148758\n",
      "epoch: 2 step: 1184, loss is 0.002339110244065523\n",
      "epoch: 2 step: 1185, loss is 0.011002994142472744\n",
      "epoch: 2 step: 1186, loss is 0.023818472400307655\n",
      "epoch: 2 step: 1187, loss is 0.1588154286146164\n",
      "epoch: 2 step: 1188, loss is 0.0038025446701794863\n",
      "epoch: 2 step: 1189, loss is 0.06024383008480072\n",
      "epoch: 2 step: 1190, loss is 0.0798862874507904\n",
      "epoch: 2 step: 1191, loss is 0.48238927125930786\n",
      "epoch: 2 step: 1192, loss is 0.0064001730643212795\n",
      "epoch: 2 step: 1193, loss is 0.05851589888334274\n",
      "epoch: 2 step: 1194, loss is 0.02582181803882122\n",
      "epoch: 2 step: 1195, loss is 0.12123485654592514\n",
      "epoch: 2 step: 1196, loss is 0.0377478301525116\n",
      "epoch: 2 step: 1197, loss is 0.015201322734355927\n",
      "epoch: 2 step: 1198, loss is 0.015933388844132423\n",
      "epoch: 2 step: 1199, loss is 0.2655971348285675\n",
      "epoch: 2 step: 1200, loss is 0.11494814604520798\n",
      "epoch: 2 step: 1201, loss is 0.007381713017821312\n",
      "epoch: 2 step: 1202, loss is 0.0366617888212204\n",
      "epoch: 2 step: 1203, loss is 0.004780570976436138\n",
      "epoch: 2 step: 1204, loss is 0.030456651002168655\n",
      "epoch: 2 step: 1205, loss is 0.08536773920059204\n",
      "epoch: 2 step: 1206, loss is 0.11198511719703674\n",
      "epoch: 2 step: 1207, loss is 0.14052331447601318\n",
      "epoch: 2 step: 1208, loss is 0.02690931409597397\n",
      "epoch: 2 step: 1209, loss is 0.040519751608371735\n",
      "epoch: 2 step: 1210, loss is 0.0028530671261250973\n",
      "epoch: 2 step: 1211, loss is 0.10716062784194946\n",
      "epoch: 2 step: 1212, loss is 0.01630524918437004\n",
      "epoch: 2 step: 1213, loss is 0.031016459688544273\n",
      "epoch: 2 step: 1214, loss is 0.03090355359017849\n",
      "epoch: 2 step: 1215, loss is 0.031071042641997337\n",
      "epoch: 2 step: 1216, loss is 0.03941246122121811\n",
      "epoch: 2 step: 1217, loss is 0.007497748360037804\n",
      "epoch: 2 step: 1218, loss is 0.021667730063199997\n",
      "epoch: 2 step: 1219, loss is 0.008301757276058197\n",
      "epoch: 2 step: 1220, loss is 0.10422791540622711\n",
      "epoch: 2 step: 1221, loss is 0.11468026041984558\n",
      "epoch: 2 step: 1222, loss is 0.03779847174882889\n",
      "epoch: 2 step: 1223, loss is 0.012165970169007778\n",
      "epoch: 2 step: 1224, loss is 0.08061868697404861\n",
      "epoch: 2 step: 1225, loss is 0.020003261044621468\n",
      "epoch: 2 step: 1226, loss is 0.06165766343474388\n",
      "epoch: 2 step: 1227, loss is 0.00821291096508503\n",
      "epoch: 2 step: 1228, loss is 0.0054865190759301186\n",
      "epoch: 2 step: 1229, loss is 0.18148447573184967\n",
      "epoch: 2 step: 1230, loss is 0.06960126757621765\n",
      "epoch: 2 step: 1231, loss is 0.023679355159401894\n",
      "epoch: 2 step: 1232, loss is 0.057684723287820816\n",
      "epoch: 2 step: 1233, loss is 0.019233455881476402\n",
      "epoch: 2 step: 1234, loss is 0.002077046548947692\n",
      "epoch: 2 step: 1235, loss is 0.012500006705522537\n",
      "epoch: 2 step: 1236, loss is 0.11525559425354004\n",
      "epoch: 2 step: 1237, loss is 0.01684495061635971\n",
      "epoch: 2 step: 1238, loss is 0.0034501575864851475\n",
      "epoch: 2 step: 1239, loss is 0.071248359978199\n",
      "epoch: 2 step: 1240, loss is 0.09078020602464676\n",
      "epoch: 2 step: 1241, loss is 0.15210729837417603\n",
      "epoch: 2 step: 1242, loss is 0.00838171411305666\n",
      "epoch: 2 step: 1243, loss is 0.08777962625026703\n",
      "epoch: 2 step: 1244, loss is 0.06848578155040741\n",
      "epoch: 2 step: 1245, loss is 0.019597094506025314\n",
      "epoch: 2 step: 1246, loss is 0.18996402621269226\n",
      "epoch: 2 step: 1247, loss is 0.0059369513764977455\n",
      "epoch: 2 step: 1248, loss is 0.006535446271300316\n",
      "epoch: 2 step: 1249, loss is 0.016964638605713844\n",
      "epoch: 2 step: 1250, loss is 0.0018528327345848083\n",
      "epoch: 2 step: 1251, loss is 0.03673854470252991\n",
      "epoch: 2 step: 1252, loss is 0.13609835505485535\n",
      "epoch: 2 step: 1253, loss is 0.015648119151592255\n",
      "epoch: 2 step: 1254, loss is 0.012318183667957783\n",
      "epoch: 2 step: 1255, loss is 0.08319921046495438\n",
      "epoch: 2 step: 1256, loss is 0.032533224672079086\n",
      "epoch: 2 step: 1257, loss is 0.049489520490169525\n",
      "epoch: 2 step: 1258, loss is 0.013493952341377735\n",
      "epoch: 2 step: 1259, loss is 0.019126564264297485\n",
      "epoch: 2 step: 1260, loss is 0.1951238512992859\n",
      "epoch: 2 step: 1261, loss is 0.08213606476783752\n",
      "epoch: 2 step: 1262, loss is 0.07514306902885437\n",
      "epoch: 2 step: 1263, loss is 0.07189428061246872\n",
      "epoch: 2 step: 1264, loss is 0.4309697449207306\n",
      "epoch: 2 step: 1265, loss is 0.007944242097437382\n",
      "epoch: 2 step: 1266, loss is 0.19666801393032074\n",
      "epoch: 2 step: 1267, loss is 0.5054205060005188\n",
      "epoch: 2 step: 1268, loss is 0.008096657693386078\n",
      "epoch: 2 step: 1269, loss is 0.15664765238761902\n",
      "epoch: 2 step: 1270, loss is 0.024538598954677582\n",
      "epoch: 2 step: 1271, loss is 0.02894739992916584\n",
      "epoch: 2 step: 1272, loss is 0.2987780272960663\n",
      "epoch: 2 step: 1273, loss is 0.07677273452281952\n",
      "epoch: 2 step: 1274, loss is 0.03807327523827553\n",
      "epoch: 2 step: 1275, loss is 0.06912674009799957\n",
      "epoch: 2 step: 1276, loss is 0.031046591699123383\n",
      "epoch: 2 step: 1277, loss is 0.07971175014972687\n",
      "epoch: 2 step: 1278, loss is 0.09152988344430923\n",
      "epoch: 2 step: 1279, loss is 0.09188848733901978\n",
      "epoch: 2 step: 1280, loss is 0.015304581262171268\n",
      "epoch: 2 step: 1281, loss is 0.005607190541923046\n",
      "epoch: 2 step: 1282, loss is 0.05692293494939804\n",
      "epoch: 2 step: 1283, loss is 0.020807676017284393\n",
      "epoch: 2 step: 1284, loss is 0.0715116560459137\n",
      "epoch: 2 step: 1285, loss is 0.06491770595312119\n",
      "epoch: 2 step: 1286, loss is 0.19934572279453278\n",
      "epoch: 2 step: 1287, loss is 0.20896194875240326\n",
      "epoch: 2 step: 1288, loss is 0.06669136136770248\n",
      "epoch: 2 step: 1289, loss is 0.12410265952348709\n",
      "epoch: 2 step: 1290, loss is 0.07955984026193619\n",
      "epoch: 2 step: 1291, loss is 0.028324207291007042\n",
      "epoch: 2 step: 1292, loss is 0.06896977126598358\n",
      "epoch: 2 step: 1293, loss is 0.012782287783920765\n",
      "epoch: 2 step: 1294, loss is 0.018577635288238525\n",
      "epoch: 2 step: 1295, loss is 0.018102265894412994\n",
      "epoch: 2 step: 1296, loss is 0.015176039189100266\n",
      "epoch: 2 step: 1297, loss is 0.015927383676171303\n",
      "epoch: 2 step: 1298, loss is 0.12533794343471527\n",
      "epoch: 2 step: 1299, loss is 0.11136095225811005\n",
      "epoch: 2 step: 1300, loss is 0.01928648166358471\n",
      "epoch: 2 step: 1301, loss is 0.03577973321080208\n",
      "epoch: 2 step: 1302, loss is 0.09405046701431274\n",
      "epoch: 2 step: 1303, loss is 0.2512904405593872\n",
      "epoch: 2 step: 1304, loss is 0.022618116810917854\n",
      "epoch: 2 step: 1305, loss is 0.026829835027456284\n",
      "epoch: 2 step: 1306, loss is 0.005567340180277824\n",
      "epoch: 2 step: 1307, loss is 0.036163751035928726\n",
      "epoch: 2 step: 1308, loss is 0.07541194558143616\n",
      "epoch: 2 step: 1309, loss is 0.018442030996084213\n",
      "epoch: 2 step: 1310, loss is 0.032240018248558044\n",
      "epoch: 2 step: 1311, loss is 0.09226491302251816\n",
      "epoch: 2 step: 1312, loss is 0.2118542194366455\n",
      "epoch: 2 step: 1313, loss is 0.07260923087596893\n",
      "epoch: 2 step: 1314, loss is 0.004505144897848368\n",
      "epoch: 2 step: 1315, loss is 0.033209607005119324\n",
      "epoch: 2 step: 1316, loss is 0.06960613280534744\n",
      "epoch: 2 step: 1317, loss is 0.05344053730368614\n",
      "epoch: 2 step: 1318, loss is 0.2007693201303482\n",
      "epoch: 2 step: 1319, loss is 0.0653981864452362\n",
      "epoch: 2 step: 1320, loss is 0.03591719642281532\n",
      "epoch: 2 step: 1321, loss is 0.07405776530504227\n",
      "epoch: 2 step: 1322, loss is 0.002426855731755495\n",
      "epoch: 2 step: 1323, loss is 0.0818527415394783\n",
      "epoch: 2 step: 1324, loss is 0.017054157331585884\n",
      "epoch: 2 step: 1325, loss is 0.11650330573320389\n",
      "epoch: 2 step: 1326, loss is 0.03508410230278969\n",
      "epoch: 2 step: 1327, loss is 0.08531972765922546\n",
      "epoch: 2 step: 1328, loss is 0.05429142713546753\n",
      "epoch: 2 step: 1329, loss is 0.011205555871129036\n",
      "epoch: 2 step: 1330, loss is 0.014199472963809967\n",
      "epoch: 2 step: 1331, loss is 0.002455509966239333\n",
      "epoch: 2 step: 1332, loss is 0.1335698813199997\n",
      "epoch: 2 step: 1333, loss is 0.1256514936685562\n",
      "epoch: 2 step: 1334, loss is 0.052982550114393234\n",
      "epoch: 2 step: 1335, loss is 0.006637223996222019\n",
      "epoch: 2 step: 1336, loss is 0.1910569965839386\n",
      "epoch: 2 step: 1337, loss is 0.0856948271393776\n",
      "epoch: 2 step: 1338, loss is 0.018526175990700722\n",
      "epoch: 2 step: 1339, loss is 0.00613193865865469\n",
      "epoch: 2 step: 1340, loss is 0.0030634477734565735\n",
      "epoch: 2 step: 1341, loss is 0.01716897264122963\n",
      "epoch: 2 step: 1342, loss is 0.08845027536153793\n",
      "epoch: 2 step: 1343, loss is 0.20824182033538818\n",
      "epoch: 2 step: 1344, loss is 0.04739290475845337\n",
      "epoch: 2 step: 1345, loss is 0.06588160246610641\n",
      "epoch: 2 step: 1346, loss is 0.060352541506290436\n",
      "epoch: 2 step: 1347, loss is 0.056184086948633194\n",
      "epoch: 2 step: 1348, loss is 0.17347556352615356\n",
      "epoch: 2 step: 1349, loss is 0.25757959485054016\n",
      "epoch: 2 step: 1350, loss is 0.016448000445961952\n",
      "epoch: 2 step: 1351, loss is 0.009599138982594013\n",
      "epoch: 2 step: 1352, loss is 0.08352940529584885\n",
      "epoch: 2 step: 1353, loss is 0.031222566962242126\n",
      "epoch: 2 step: 1354, loss is 0.01713206060230732\n",
      "epoch: 2 step: 1355, loss is 0.04069065302610397\n",
      "epoch: 2 step: 1356, loss is 0.02532215416431427\n",
      "epoch: 2 step: 1357, loss is 0.18923598527908325\n",
      "epoch: 2 step: 1358, loss is 0.007021461613476276\n",
      "epoch: 2 step: 1359, loss is 0.0028630956076085567\n",
      "epoch: 2 step: 1360, loss is 0.07871998101472855\n",
      "epoch: 2 step: 1361, loss is 0.02829568460583687\n",
      "epoch: 2 step: 1362, loss is 0.1249375268816948\n",
      "epoch: 2 step: 1363, loss is 0.19099508225917816\n",
      "epoch: 2 step: 1364, loss is 0.09429257363080978\n",
      "epoch: 2 step: 1365, loss is 0.1181594729423523\n",
      "epoch: 2 step: 1366, loss is 0.053729575127363205\n",
      "epoch: 2 step: 1367, loss is 0.02679162286221981\n",
      "epoch: 2 step: 1368, loss is 0.04327859729528427\n",
      "epoch: 2 step: 1369, loss is 0.36335286498069763\n",
      "epoch: 2 step: 1370, loss is 0.013847699388861656\n",
      "epoch: 2 step: 1371, loss is 0.017986487597227097\n",
      "epoch: 2 step: 1372, loss is 0.1232982724905014\n",
      "epoch: 2 step: 1373, loss is 0.12870734930038452\n",
      "epoch: 2 step: 1374, loss is 0.01075509563088417\n",
      "epoch: 2 step: 1375, loss is 0.034343138337135315\n",
      "epoch: 2 step: 1376, loss is 0.007437614724040031\n",
      "epoch: 2 step: 1377, loss is 0.03303225710988045\n",
      "epoch: 2 step: 1378, loss is 0.1741916388273239\n",
      "epoch: 2 step: 1379, loss is 0.036253832280635834\n",
      "epoch: 2 step: 1380, loss is 0.026424163952469826\n",
      "epoch: 2 step: 1381, loss is 0.010871519334614277\n",
      "epoch: 2 step: 1382, loss is 0.1014556959271431\n",
      "epoch: 2 step: 1383, loss is 0.1905083805322647\n",
      "epoch: 2 step: 1384, loss is 0.02813049405813217\n",
      "epoch: 2 step: 1385, loss is 0.028076380491256714\n",
      "epoch: 2 step: 1386, loss is 0.32178327441215515\n",
      "epoch: 2 step: 1387, loss is 0.021150579676032066\n",
      "epoch: 2 step: 1388, loss is 0.006726068444550037\n",
      "epoch: 2 step: 1389, loss is 0.13048282265663147\n",
      "epoch: 2 step: 1390, loss is 0.3265135884284973\n",
      "epoch: 2 step: 1391, loss is 0.06990917772054672\n",
      "epoch: 2 step: 1392, loss is 0.12422987073659897\n",
      "epoch: 2 step: 1393, loss is 0.1188964992761612\n",
      "epoch: 2 step: 1394, loss is 0.0049172635190188885\n",
      "epoch: 2 step: 1395, loss is 0.006560609210282564\n",
      "epoch: 2 step: 1396, loss is 0.019425123929977417\n",
      "epoch: 2 step: 1397, loss is 0.023492960259318352\n",
      "epoch: 2 step: 1398, loss is 0.010062157176434994\n",
      "epoch: 2 step: 1399, loss is 0.05434567853808403\n",
      "epoch: 2 step: 1400, loss is 0.02346092462539673\n",
      "epoch: 2 step: 1401, loss is 0.4009025990962982\n",
      "epoch: 2 step: 1402, loss is 0.03316116705536842\n",
      "epoch: 2 step: 1403, loss is 0.12609252333641052\n",
      "epoch: 2 step: 1404, loss is 0.03222689777612686\n",
      "epoch: 2 step: 1405, loss is 0.12495909631252289\n",
      "epoch: 2 step: 1406, loss is 0.028785256668925285\n",
      "epoch: 2 step: 1407, loss is 0.2449103146791458\n",
      "epoch: 2 step: 1408, loss is 0.040861308574676514\n",
      "epoch: 2 step: 1409, loss is 0.18467433750629425\n",
      "epoch: 2 step: 1410, loss is 0.2724502980709076\n",
      "epoch: 2 step: 1411, loss is 0.2721118628978729\n",
      "epoch: 2 step: 1412, loss is 0.009761528111994267\n",
      "epoch: 2 step: 1413, loss is 0.12495709955692291\n",
      "epoch: 2 step: 1414, loss is 0.00898685585707426\n",
      "epoch: 2 step: 1415, loss is 0.021335352212190628\n",
      "epoch: 2 step: 1416, loss is 0.03158661723136902\n",
      "epoch: 2 step: 1417, loss is 0.05022076889872551\n",
      "epoch: 2 step: 1418, loss is 0.0817285105586052\n",
      "epoch: 2 step: 1419, loss is 0.45473265647888184\n",
      "epoch: 2 step: 1420, loss is 0.1874898225069046\n",
      "epoch: 2 step: 1421, loss is 0.022183362394571304\n",
      "epoch: 2 step: 1422, loss is 0.06059084087610245\n",
      "epoch: 2 step: 1423, loss is 0.14661437273025513\n",
      "epoch: 2 step: 1424, loss is 0.05566135421395302\n",
      "epoch: 2 step: 1425, loss is 0.12773703038692474\n",
      "epoch: 2 step: 1426, loss is 0.0032950094901025295\n",
      "epoch: 2 step: 1427, loss is 0.0405580997467041\n",
      "epoch: 2 step: 1428, loss is 0.03568137437105179\n",
      "epoch: 2 step: 1429, loss is 0.011661776341497898\n",
      "epoch: 2 step: 1430, loss is 0.032502029091119766\n",
      "epoch: 2 step: 1431, loss is 0.10489743947982788\n",
      "epoch: 2 step: 1432, loss is 0.10335811972618103\n",
      "epoch: 2 step: 1433, loss is 0.022535746917128563\n",
      "epoch: 2 step: 1434, loss is 0.305929571390152\n",
      "epoch: 2 step: 1435, loss is 0.027999266982078552\n",
      "epoch: 2 step: 1436, loss is 0.09224677085876465\n",
      "epoch: 2 step: 1437, loss is 0.0033052521757781506\n",
      "epoch: 2 step: 1438, loss is 0.13547983765602112\n",
      "epoch: 2 step: 1439, loss is 0.013189934194087982\n",
      "epoch: 2 step: 1440, loss is 0.005112386308610439\n",
      "epoch: 2 step: 1441, loss is 0.0755753368139267\n",
      "epoch: 2 step: 1442, loss is 0.055906906723976135\n",
      "epoch: 2 step: 1443, loss is 0.008613754995167255\n",
      "epoch: 2 step: 1444, loss is 0.07600166648626328\n",
      "epoch: 2 step: 1445, loss is 0.02065829001367092\n",
      "epoch: 2 step: 1446, loss is 0.05700787901878357\n",
      "epoch: 2 step: 1447, loss is 0.004262264352291822\n",
      "epoch: 2 step: 1448, loss is 0.017887171357870102\n",
      "epoch: 2 step: 1449, loss is 0.026767445728182793\n",
      "epoch: 2 step: 1450, loss is 0.10357367247343063\n",
      "epoch: 2 step: 1451, loss is 0.0032039196230471134\n",
      "epoch: 2 step: 1452, loss is 0.006981121841818094\n",
      "epoch: 2 step: 1453, loss is 0.05292254313826561\n",
      "epoch: 2 step: 1454, loss is 0.02888701856136322\n",
      "epoch: 2 step: 1455, loss is 0.10359877347946167\n",
      "epoch: 2 step: 1456, loss is 0.005123103503137827\n",
      "epoch: 2 step: 1457, loss is 0.11924301832914352\n",
      "epoch: 2 step: 1458, loss is 0.0006412160582840443\n",
      "epoch: 2 step: 1459, loss is 0.006686439272016287\n",
      "epoch: 2 step: 1460, loss is 0.00542160589247942\n",
      "epoch: 2 step: 1461, loss is 0.0057184430770576\n",
      "epoch: 2 step: 1462, loss is 0.04997427761554718\n",
      "epoch: 2 step: 1463, loss is 0.0190903227776289\n",
      "epoch: 2 step: 1464, loss is 0.02869218960404396\n",
      "epoch: 2 step: 1465, loss is 0.005328009370714426\n",
      "epoch: 2 step: 1466, loss is 0.005491592921316624\n",
      "epoch: 2 step: 1467, loss is 0.2744755446910858\n",
      "epoch: 2 step: 1468, loss is 0.09926579892635345\n",
      "epoch: 2 step: 1469, loss is 0.013264299370348454\n",
      "epoch: 2 step: 1470, loss is 0.0046199532225728035\n",
      "epoch: 2 step: 1471, loss is 0.004907809663563967\n",
      "epoch: 2 step: 1472, loss is 0.016047492623329163\n",
      "epoch: 2 step: 1473, loss is 0.01229899749159813\n",
      "epoch: 2 step: 1474, loss is 0.0470627062022686\n",
      "epoch: 2 step: 1475, loss is 0.25963538885116577\n",
      "epoch: 2 step: 1476, loss is 0.002192811109125614\n",
      "epoch: 2 step: 1477, loss is 0.09684887528419495\n",
      "epoch: 2 step: 1478, loss is 0.015108310617506504\n",
      "epoch: 2 step: 1479, loss is 0.10627944022417068\n",
      "epoch: 2 step: 1480, loss is 0.020504890009760857\n",
      "epoch: 2 step: 1481, loss is 0.009523860178887844\n",
      "epoch: 2 step: 1482, loss is 0.0064315637573599815\n",
      "epoch: 2 step: 1483, loss is 0.015797104686498642\n",
      "epoch: 2 step: 1484, loss is 0.03151953965425491\n",
      "epoch: 2 step: 1485, loss is 0.0017770642880350351\n",
      "epoch: 2 step: 1486, loss is 0.04162473976612091\n",
      "epoch: 2 step: 1487, loss is 0.023568768054246902\n",
      "epoch: 2 step: 1488, loss is 0.09730750322341919\n",
      "epoch: 2 step: 1489, loss is 0.026082435622811317\n",
      "epoch: 2 step: 1490, loss is 0.015623446553945541\n",
      "epoch: 2 step: 1491, loss is 0.3517933189868927\n",
      "epoch: 2 step: 1492, loss is 0.13342204689979553\n",
      "epoch: 2 step: 1493, loss is 0.05759953334927559\n",
      "epoch: 2 step: 1494, loss is 0.2560040056705475\n",
      "epoch: 2 step: 1495, loss is 0.0257934108376503\n",
      "epoch: 2 step: 1496, loss is 0.03081667609512806\n",
      "epoch: 2 step: 1497, loss is 0.00860601756721735\n",
      "epoch: 2 step: 1498, loss is 0.09859342128038406\n",
      "epoch: 2 step: 1499, loss is 0.23982326686382294\n",
      "epoch: 2 step: 1500, loss is 0.04244103282690048\n",
      "epoch: 2 step: 1501, loss is 0.02351089008152485\n",
      "epoch: 2 step: 1502, loss is 0.009675700217485428\n",
      "epoch: 2 step: 1503, loss is 0.0039433930069208145\n",
      "epoch: 2 step: 1504, loss is 0.03281852975487709\n",
      "epoch: 2 step: 1505, loss is 0.019896814599633217\n",
      "epoch: 2 step: 1506, loss is 0.045557163655757904\n",
      "epoch: 2 step: 1507, loss is 0.005373525898903608\n",
      "epoch: 2 step: 1508, loss is 0.22998301684856415\n",
      "epoch: 2 step: 1509, loss is 0.07456006854772568\n",
      "epoch: 2 step: 1510, loss is 0.09198197722434998\n",
      "epoch: 2 step: 1511, loss is 0.05454767122864723\n",
      "epoch: 2 step: 1512, loss is 0.029760733246803284\n",
      "epoch: 2 step: 1513, loss is 0.010841052047908306\n",
      "epoch: 2 step: 1514, loss is 0.014743699692189693\n",
      "epoch: 2 step: 1515, loss is 0.011366534046828747\n",
      "epoch: 2 step: 1516, loss is 0.03776184469461441\n",
      "epoch: 2 step: 1517, loss is 0.024715887382626534\n",
      "epoch: 2 step: 1518, loss is 0.008807558566331863\n",
      "epoch: 2 step: 1519, loss is 0.15367236733436584\n",
      "epoch: 2 step: 1520, loss is 0.009070955216884613\n",
      "epoch: 2 step: 1521, loss is 0.004505304619669914\n",
      "epoch: 2 step: 1522, loss is 0.02977347932755947\n",
      "epoch: 2 step: 1523, loss is 0.03317271173000336\n",
      "epoch: 2 step: 1524, loss is 0.08947119116783142\n",
      "epoch: 2 step: 1525, loss is 0.1221856102347374\n",
      "epoch: 2 step: 1526, loss is 0.06598449498414993\n",
      "epoch: 2 step: 1527, loss is 0.05553971603512764\n",
      "epoch: 2 step: 1528, loss is 0.008779872208833694\n",
      "epoch: 2 step: 1529, loss is 0.017456866800785065\n",
      "epoch: 2 step: 1530, loss is 0.08963268995285034\n",
      "epoch: 2 step: 1531, loss is 0.04431799799203873\n",
      "epoch: 2 step: 1532, loss is 0.013912263326346874\n",
      "epoch: 2 step: 1533, loss is 0.27391982078552246\n",
      "epoch: 2 step: 1534, loss is 0.043876320123672485\n",
      "epoch: 2 step: 1535, loss is 0.012528873980045319\n",
      "epoch: 2 step: 1536, loss is 0.048893917351961136\n",
      "epoch: 2 step: 1537, loss is 0.06642848253250122\n",
      "epoch: 2 step: 1538, loss is 0.0023642261512577534\n",
      "epoch: 2 step: 1539, loss is 0.20380915701389313\n",
      "epoch: 2 step: 1540, loss is 0.17857147753238678\n",
      "epoch: 2 step: 1541, loss is 0.023722480982542038\n",
      "epoch: 2 step: 1542, loss is 0.05522213131189346\n",
      "epoch: 2 step: 1543, loss is 0.05701439455151558\n",
      "epoch: 2 step: 1544, loss is 0.0320747010409832\n",
      "epoch: 2 step: 1545, loss is 0.02734304592013359\n",
      "epoch: 2 step: 1546, loss is 0.16385425627231598\n",
      "epoch: 2 step: 1547, loss is 0.06896407902240753\n",
      "epoch: 2 step: 1548, loss is 0.05972839519381523\n",
      "epoch: 2 step: 1549, loss is 0.007154692895710468\n",
      "epoch: 2 step: 1550, loss is 0.0776396095752716\n",
      "epoch: 2 step: 1551, loss is 0.109105683863163\n",
      "epoch: 2 step: 1552, loss is 0.09296507388353348\n",
      "epoch: 2 step: 1553, loss is 0.004341932479292154\n",
      "epoch: 2 step: 1554, loss is 0.12009643763303757\n",
      "epoch: 2 step: 1555, loss is 0.05893981456756592\n",
      "epoch: 2 step: 1556, loss is 0.04869849979877472\n",
      "epoch: 2 step: 1557, loss is 0.2523958384990692\n",
      "epoch: 2 step: 1558, loss is 0.07995545864105225\n",
      "epoch: 2 step: 1559, loss is 0.08770430088043213\n",
      "epoch: 2 step: 1560, loss is 0.03459691256284714\n",
      "epoch: 2 step: 1561, loss is 0.049947887659072876\n",
      "epoch: 2 step: 1562, loss is 0.016092544421553612\n",
      "epoch: 2 step: 1563, loss is 0.013004601001739502\n",
      "epoch: 2 step: 1564, loss is 0.048703160136938095\n",
      "epoch: 2 step: 1565, loss is 0.24127091467380524\n",
      "epoch: 2 step: 1566, loss is 0.23959845304489136\n",
      "epoch: 2 step: 1567, loss is 0.06581561267375946\n",
      "epoch: 2 step: 1568, loss is 0.016637643799185753\n",
      "epoch: 2 step: 1569, loss is 0.0742839127779007\n",
      "epoch: 2 step: 1570, loss is 0.0403372123837471\n",
      "epoch: 2 step: 1571, loss is 0.2062833458185196\n",
      "epoch: 2 step: 1572, loss is 0.018823612481355667\n",
      "epoch: 2 step: 1573, loss is 0.05898011475801468\n",
      "epoch: 2 step: 1574, loss is 0.17492136359214783\n",
      "epoch: 2 step: 1575, loss is 0.046892665326595306\n",
      "epoch: 2 step: 1576, loss is 0.039722148329019547\n",
      "epoch: 2 step: 1577, loss is 0.01709630712866783\n",
      "epoch: 2 step: 1578, loss is 0.007663617841899395\n",
      "epoch: 2 step: 1579, loss is 0.14030808210372925\n",
      "epoch: 2 step: 1580, loss is 0.1471288651227951\n",
      "epoch: 2 step: 1581, loss is 0.12622332572937012\n",
      "epoch: 2 step: 1582, loss is 0.13472692668437958\n",
      "epoch: 2 step: 1583, loss is 0.20339961349964142\n",
      "epoch: 2 step: 1584, loss is 0.18303638696670532\n",
      "epoch: 2 step: 1585, loss is 0.02019341289997101\n",
      "epoch: 2 step: 1586, loss is 0.08685892075300217\n",
      "epoch: 2 step: 1587, loss is 0.20583416521549225\n",
      "epoch: 2 step: 1588, loss is 0.08595681190490723\n",
      "epoch: 2 step: 1589, loss is 0.06219294294714928\n",
      "epoch: 2 step: 1590, loss is 0.027110029011964798\n",
      "epoch: 2 step: 1591, loss is 0.03789810836315155\n",
      "epoch: 2 step: 1592, loss is 0.07145236432552338\n",
      "epoch: 2 step: 1593, loss is 0.04567072540521622\n",
      "epoch: 2 step: 1594, loss is 0.0102153979241848\n",
      "epoch: 2 step: 1595, loss is 0.008100241422653198\n",
      "epoch: 2 step: 1596, loss is 0.010705409571528435\n",
      "epoch: 2 step: 1597, loss is 0.00674970680847764\n",
      "epoch: 2 step: 1598, loss is 0.06057801842689514\n",
      "epoch: 2 step: 1599, loss is 0.007029153872281313\n",
      "epoch: 2 step: 1600, loss is 0.1055743396282196\n",
      "epoch: 2 step: 1601, loss is 0.0054884483106434345\n",
      "epoch: 2 step: 1602, loss is 0.006381361745297909\n",
      "epoch: 2 step: 1603, loss is 0.013544037006795406\n",
      "epoch: 2 step: 1604, loss is 0.009270375594496727\n",
      "epoch: 2 step: 1605, loss is 0.007914597168564796\n",
      "epoch: 2 step: 1606, loss is 0.2181846797466278\n",
      "epoch: 2 step: 1607, loss is 0.002451240783557296\n",
      "epoch: 2 step: 1608, loss is 0.14599253237247467\n",
      "epoch: 2 step: 1609, loss is 0.005282457917928696\n",
      "epoch: 2 step: 1610, loss is 0.13875705003738403\n",
      "epoch: 2 step: 1611, loss is 0.00581423519179225\n",
      "epoch: 2 step: 1612, loss is 0.06000952050089836\n",
      "epoch: 2 step: 1613, loss is 0.0009140458423644304\n",
      "epoch: 2 step: 1614, loss is 0.09593472629785538\n",
      "epoch: 2 step: 1615, loss is 0.01158961746841669\n",
      "epoch: 2 step: 1616, loss is 0.00617605447769165\n",
      "epoch: 2 step: 1617, loss is 0.13754814863204956\n",
      "epoch: 2 step: 1618, loss is 0.2606164216995239\n",
      "epoch: 2 step: 1619, loss is 0.23175115883350372\n",
      "epoch: 2 step: 1620, loss is 0.08573638647794724\n",
      "epoch: 2 step: 1621, loss is 0.056934159249067307\n",
      "epoch: 2 step: 1622, loss is 0.09696097671985626\n",
      "epoch: 2 step: 1623, loss is 0.03434944897890091\n",
      "epoch: 2 step: 1624, loss is 0.06477850675582886\n",
      "epoch: 2 step: 1625, loss is 0.02638455666601658\n",
      "epoch: 2 step: 1626, loss is 0.02130144275724888\n",
      "epoch: 2 step: 1627, loss is 0.017381921410560608\n",
      "epoch: 2 step: 1628, loss is 0.02848520129919052\n",
      "epoch: 2 step: 1629, loss is 0.03149747475981712\n",
      "epoch: 2 step: 1630, loss is 0.1605626940727234\n",
      "epoch: 2 step: 1631, loss is 0.006986716762185097\n",
      "epoch: 2 step: 1632, loss is 0.04802954941987991\n",
      "epoch: 2 step: 1633, loss is 0.018821680918335915\n",
      "epoch: 2 step: 1634, loss is 0.34414586424827576\n",
      "epoch: 2 step: 1635, loss is 0.09006203711032867\n",
      "epoch: 2 step: 1636, loss is 0.1449006050825119\n",
      "epoch: 2 step: 1637, loss is 0.2696742117404938\n",
      "epoch: 2 step: 1638, loss is 0.10259098559617996\n",
      "epoch: 2 step: 1639, loss is 0.04883188754320145\n",
      "epoch: 2 step: 1640, loss is 0.023304274305701256\n",
      "epoch: 2 step: 1641, loss is 0.03615260496735573\n",
      "epoch: 2 step: 1642, loss is 0.007464023306965828\n",
      "epoch: 2 step: 1643, loss is 0.02658388577401638\n",
      "epoch: 2 step: 1644, loss is 0.28297415375709534\n",
      "epoch: 2 step: 1645, loss is 0.013701912015676498\n",
      "epoch: 2 step: 1646, loss is 0.02288658916950226\n",
      "epoch: 2 step: 1647, loss is 0.10622186958789825\n",
      "epoch: 2 step: 1648, loss is 0.006428408902138472\n",
      "epoch: 2 step: 1649, loss is 0.1453305333852768\n",
      "epoch: 2 step: 1650, loss is 0.012155159376561642\n",
      "epoch: 2 step: 1651, loss is 0.010328583419322968\n",
      "epoch: 2 step: 1652, loss is 0.01574758067727089\n",
      "epoch: 2 step: 1653, loss is 0.021084289997816086\n",
      "epoch: 2 step: 1654, loss is 0.14159440994262695\n",
      "epoch: 2 step: 1655, loss is 0.10926586389541626\n",
      "epoch: 2 step: 1656, loss is 0.012212364003062248\n",
      "epoch: 2 step: 1657, loss is 0.09355814009904861\n",
      "epoch: 2 step: 1658, loss is 0.020745636895298958\n",
      "epoch: 2 step: 1659, loss is 0.10435814410448074\n",
      "epoch: 2 step: 1660, loss is 0.1527101844549179\n",
      "epoch: 2 step: 1661, loss is 0.0050977254286408424\n",
      "epoch: 2 step: 1662, loss is 0.0042574149556458\n",
      "epoch: 2 step: 1663, loss is 0.02570788748562336\n",
      "epoch: 2 step: 1664, loss is 0.04594569653272629\n",
      "epoch: 2 step: 1665, loss is 0.0434875562787056\n",
      "epoch: 2 step: 1666, loss is 0.14040374755859375\n",
      "epoch: 2 step: 1667, loss is 0.02363758720457554\n",
      "epoch: 2 step: 1668, loss is 0.008703340776264668\n",
      "epoch: 2 step: 1669, loss is 0.02162112109363079\n",
      "epoch: 2 step: 1670, loss is 0.00460567744448781\n",
      "epoch: 2 step: 1671, loss is 0.1410229504108429\n",
      "epoch: 2 step: 1672, loss is 0.060571711510419846\n",
      "epoch: 2 step: 1673, loss is 0.033645518124103546\n",
      "epoch: 2 step: 1674, loss is 0.23613710701465607\n",
      "epoch: 2 step: 1675, loss is 0.013623518869280815\n",
      "epoch: 2 step: 1676, loss is 0.015585936605930328\n",
      "epoch: 2 step: 1677, loss is 0.23626255989074707\n",
      "epoch: 2 step: 1678, loss is 0.06913632899522781\n",
      "epoch: 2 step: 1679, loss is 0.0233493410050869\n",
      "epoch: 2 step: 1680, loss is 0.0038407822139561176\n",
      "epoch: 2 step: 1681, loss is 0.09631884098052979\n",
      "epoch: 2 step: 1682, loss is 0.003769987728446722\n",
      "epoch: 2 step: 1683, loss is 0.08157110214233398\n",
      "epoch: 2 step: 1684, loss is 0.23313429951667786\n",
      "epoch: 2 step: 1685, loss is 0.013316689990460873\n",
      "epoch: 2 step: 1686, loss is 0.022833015769720078\n",
      "epoch: 2 step: 1687, loss is 0.14257855713367462\n",
      "epoch: 2 step: 1688, loss is 0.029858142137527466\n",
      "epoch: 2 step: 1689, loss is 0.0022602523677051067\n",
      "epoch: 2 step: 1690, loss is 0.05489930510520935\n",
      "epoch: 2 step: 1691, loss is 0.024391435086727142\n",
      "epoch: 2 step: 1692, loss is 0.031278468668460846\n",
      "epoch: 2 step: 1693, loss is 0.004588524345308542\n",
      "epoch: 2 step: 1694, loss is 0.010654548183083534\n",
      "epoch: 2 step: 1695, loss is 0.06838604062795639\n",
      "epoch: 2 step: 1696, loss is 0.16531287133693695\n",
      "epoch: 2 step: 1697, loss is 0.08073215186595917\n",
      "epoch: 2 step: 1698, loss is 0.06694149225950241\n",
      "epoch: 2 step: 1699, loss is 0.035827890038490295\n",
      "epoch: 2 step: 1700, loss is 0.12259560078382492\n",
      "epoch: 2 step: 1701, loss is 0.007864191196858883\n",
      "epoch: 2 step: 1702, loss is 0.0064354389905929565\n",
      "epoch: 2 step: 1703, loss is 0.024565575644373894\n",
      "epoch: 2 step: 1704, loss is 0.004351369570940733\n",
      "epoch: 2 step: 1705, loss is 0.01029311865568161\n",
      "epoch: 2 step: 1706, loss is 0.003436714643612504\n",
      "epoch: 2 step: 1707, loss is 0.006324151065200567\n",
      "epoch: 2 step: 1708, loss is 0.009295926429331303\n",
      "epoch: 2 step: 1709, loss is 0.06462006270885468\n",
      "epoch: 2 step: 1710, loss is 0.011534174904227257\n",
      "epoch: 2 step: 1711, loss is 0.004644813947379589\n",
      "epoch: 2 step: 1712, loss is 0.09569905698299408\n",
      "epoch: 2 step: 1713, loss is 0.0032409592531621456\n",
      "epoch: 2 step: 1714, loss is 0.10723937302827835\n",
      "epoch: 2 step: 1715, loss is 0.0016343563329428434\n",
      "epoch: 2 step: 1716, loss is 0.03155390918254852\n",
      "epoch: 2 step: 1717, loss is 0.0028300867415964603\n",
      "epoch: 2 step: 1718, loss is 0.013556107878684998\n",
      "epoch: 2 step: 1719, loss is 0.04921616241335869\n",
      "epoch: 2 step: 1720, loss is 0.007000027224421501\n",
      "epoch: 2 step: 1721, loss is 0.013534720987081528\n",
      "epoch: 2 step: 1722, loss is 0.032300326973199844\n",
      "epoch: 2 step: 1723, loss is 0.03983635827898979\n",
      "epoch: 2 step: 1724, loss is 0.04858272895216942\n",
      "epoch: 2 step: 1725, loss is 0.007603954058140516\n",
      "epoch: 2 step: 1726, loss is 0.005584165919572115\n",
      "epoch: 2 step: 1727, loss is 0.004911336116492748\n",
      "epoch: 2 step: 1728, loss is 0.014268321916460991\n",
      "epoch: 2 step: 1729, loss is 0.0210744459182024\n",
      "epoch: 2 step: 1730, loss is 0.034117959439754486\n",
      "epoch: 2 step: 1731, loss is 0.07207252085208893\n",
      "epoch: 2 step: 1732, loss is 0.39465224742889404\n",
      "epoch: 2 step: 1733, loss is 0.16392283141613007\n",
      "epoch: 2 step: 1734, loss is 0.0006601344794034958\n",
      "epoch: 2 step: 1735, loss is 0.13594140112400055\n",
      "epoch: 2 step: 1736, loss is 0.027205539867281914\n",
      "epoch: 2 step: 1737, loss is 0.08792375028133392\n",
      "epoch: 2 step: 1738, loss is 0.021953918039798737\n",
      "epoch: 2 step: 1739, loss is 0.016168389469385147\n",
      "epoch: 2 step: 1740, loss is 0.0027918017003685236\n",
      "epoch: 2 step: 1741, loss is 0.12851081788539886\n",
      "epoch: 2 step: 1742, loss is 0.12746702134609222\n",
      "epoch: 2 step: 1743, loss is 0.01127962302416563\n",
      "epoch: 2 step: 1744, loss is 0.01643596589565277\n",
      "epoch: 2 step: 1745, loss is 0.3332952857017517\n",
      "epoch: 2 step: 1746, loss is 0.01833866722881794\n",
      "epoch: 2 step: 1747, loss is 0.009584917686879635\n",
      "epoch: 2 step: 1748, loss is 0.1015254557132721\n",
      "epoch: 2 step: 1749, loss is 0.01757144182920456\n",
      "epoch: 2 step: 1750, loss is 0.24694927036762238\n",
      "epoch: 2 step: 1751, loss is 0.22814300656318665\n",
      "epoch: 2 step: 1752, loss is 0.09004220366477966\n",
      "epoch: 2 step: 1753, loss is 0.008554956875741482\n",
      "epoch: 2 step: 1754, loss is 0.09119249880313873\n",
      "epoch: 2 step: 1755, loss is 0.043617624789476395\n",
      "epoch: 2 step: 1756, loss is 0.25340938568115234\n",
      "epoch: 2 step: 1757, loss is 0.11868951469659805\n",
      "epoch: 2 step: 1758, loss is 0.006405862048268318\n",
      "epoch: 2 step: 1759, loss is 0.030949899926781654\n",
      "epoch: 2 step: 1760, loss is 0.20628684759140015\n",
      "epoch: 2 step: 1761, loss is 0.04858589917421341\n",
      "epoch: 2 step: 1762, loss is 0.1308557540178299\n",
      "epoch: 2 step: 1763, loss is 0.026916466653347015\n",
      "epoch: 2 step: 1764, loss is 0.004418539814651012\n",
      "epoch: 2 step: 1765, loss is 0.023968925699591637\n",
      "epoch: 2 step: 1766, loss is 0.04677623510360718\n",
      "epoch: 2 step: 1767, loss is 0.0322483591735363\n",
      "epoch: 2 step: 1768, loss is 0.011943706311285496\n",
      "epoch: 2 step: 1769, loss is 0.058914706110954285\n",
      "epoch: 2 step: 1770, loss is 0.03358915075659752\n",
      "epoch: 2 step: 1771, loss is 0.21076340973377228\n",
      "epoch: 2 step: 1772, loss is 0.4141804575920105\n",
      "epoch: 2 step: 1773, loss is 0.044114794582128525\n",
      "epoch: 2 step: 1774, loss is 0.0053543816320598125\n",
      "epoch: 2 step: 1775, loss is 0.00960012711584568\n",
      "epoch: 2 step: 1776, loss is 0.005069534759968519\n",
      "epoch: 2 step: 1777, loss is 0.005311230663210154\n",
      "epoch: 2 step: 1778, loss is 0.00693666422739625\n",
      "epoch: 2 step: 1779, loss is 0.019215058535337448\n",
      "epoch: 2 step: 1780, loss is 0.009645395912230015\n",
      "epoch: 2 step: 1781, loss is 0.006683587562292814\n",
      "epoch: 2 step: 1782, loss is 0.1390886753797531\n",
      "epoch: 2 step: 1783, loss is 0.0033213626593351364\n",
      "epoch: 2 step: 1784, loss is 0.039040789008140564\n",
      "epoch: 2 step: 1785, loss is 0.18045490980148315\n",
      "epoch: 2 step: 1786, loss is 0.0069211265072226524\n",
      "epoch: 2 step: 1787, loss is 0.002018480794504285\n",
      "epoch: 2 step: 1788, loss is 0.07412784546613693\n",
      "epoch: 2 step: 1789, loss is 0.05142104625701904\n",
      "epoch: 2 step: 1790, loss is 0.018559392541646957\n",
      "epoch: 2 step: 1791, loss is 0.08501888811588287\n",
      "epoch: 2 step: 1792, loss is 0.18193303048610687\n",
      "epoch: 2 step: 1793, loss is 0.05880215764045715\n",
      "epoch: 2 step: 1794, loss is 0.052833206951618195\n",
      "epoch: 2 step: 1795, loss is 0.07029474526643753\n",
      "epoch: 2 step: 1796, loss is 0.014601911418139935\n",
      "epoch: 2 step: 1797, loss is 0.017302149906754494\n",
      "epoch: 2 step: 1798, loss is 0.03537120297551155\n",
      "epoch: 2 step: 1799, loss is 0.17519308626651764\n",
      "epoch: 2 step: 1800, loss is 0.0013106599217280746\n",
      "epoch: 2 step: 1801, loss is 0.023897338658571243\n",
      "epoch: 2 step: 1802, loss is 0.12949369847774506\n",
      "epoch: 2 step: 1803, loss is 0.01618763990700245\n",
      "epoch: 2 step: 1804, loss is 0.012989754788577557\n",
      "epoch: 2 step: 1805, loss is 0.008463100530207157\n",
      "epoch: 2 step: 1806, loss is 0.00832397397607565\n",
      "epoch: 2 step: 1807, loss is 0.08758509904146194\n",
      "epoch: 2 step: 1808, loss is 0.15891972184181213\n",
      "epoch: 2 step: 1809, loss is 0.008169671520590782\n",
      "epoch: 2 step: 1810, loss is 0.18896465003490448\n",
      "epoch: 2 step: 1811, loss is 0.26917609572410583\n",
      "epoch: 2 step: 1812, loss is 0.04070476070046425\n",
      "epoch: 2 step: 1813, loss is 0.20835956931114197\n",
      "epoch: 2 step: 1814, loss is 0.005662740673869848\n",
      "epoch: 2 step: 1815, loss is 0.001394360326230526\n",
      "epoch: 2 step: 1816, loss is 0.1447950154542923\n",
      "epoch: 2 step: 1817, loss is 0.012093826197087765\n",
      "epoch: 2 step: 1818, loss is 0.0022403616458177567\n",
      "epoch: 2 step: 1819, loss is 0.07260315865278244\n",
      "epoch: 2 step: 1820, loss is 0.00391281908378005\n",
      "epoch: 2 step: 1821, loss is 0.00697401724755764\n",
      "epoch: 2 step: 1822, loss is 0.07457470893859863\n",
      "epoch: 2 step: 1823, loss is 0.13519443571567535\n",
      "epoch: 2 step: 1824, loss is 0.012636474333703518\n",
      "epoch: 2 step: 1825, loss is 0.07787742465734482\n",
      "epoch: 2 step: 1826, loss is 0.006124625448137522\n",
      "epoch: 2 step: 1827, loss is 0.14246049523353577\n",
      "epoch: 2 step: 1828, loss is 0.06955940276384354\n",
      "epoch: 2 step: 1829, loss is 0.016409367322921753\n",
      "epoch: 2 step: 1830, loss is 0.018091965466737747\n",
      "epoch: 2 step: 1831, loss is 0.021219363436102867\n",
      "epoch: 2 step: 1832, loss is 0.04956653341650963\n",
      "epoch: 2 step: 1833, loss is 0.0655149444937706\n",
      "epoch: 2 step: 1834, loss is 0.10081403702497482\n",
      "epoch: 2 step: 1835, loss is 0.010111627168953419\n",
      "epoch: 2 step: 1836, loss is 0.0041615283116698265\n",
      "epoch: 2 step: 1837, loss is 0.007393775042146444\n",
      "epoch: 2 step: 1838, loss is 0.019607018679380417\n",
      "epoch: 2 step: 1839, loss is 0.00816153734922409\n",
      "epoch: 2 step: 1840, loss is 0.06179167702794075\n",
      "epoch: 2 step: 1841, loss is 0.06658980995416641\n",
      "epoch: 2 step: 1842, loss is 0.004588064271956682\n",
      "epoch: 2 step: 1843, loss is 0.14997239410877228\n",
      "epoch: 2 step: 1844, loss is 0.0692913606762886\n",
      "epoch: 2 step: 1845, loss is 0.02523902989923954\n",
      "epoch: 2 step: 1846, loss is 0.01742846518754959\n",
      "epoch: 2 step: 1847, loss is 0.0026225389447063208\n",
      "epoch: 2 step: 1848, loss is 0.16645759344100952\n",
      "epoch: 2 step: 1849, loss is 0.03929474204778671\n",
      "epoch: 2 step: 1850, loss is 0.003877082606777549\n",
      "epoch: 2 step: 1851, loss is 0.0087824622169137\n",
      "epoch: 2 step: 1852, loss is 0.001957494532689452\n",
      "epoch: 2 step: 1853, loss is 0.00215095654129982\n",
      "epoch: 2 step: 1854, loss is 0.002498432993888855\n",
      "epoch: 2 step: 1855, loss is 0.03188777342438698\n",
      "epoch: 2 step: 1856, loss is 0.011459763161838055\n",
      "epoch: 2 step: 1857, loss is 0.000817351508885622\n",
      "epoch: 2 step: 1858, loss is 0.0055693369358778\n",
      "epoch: 2 step: 1859, loss is 0.03903651982545853\n",
      "epoch: 2 step: 1860, loss is 0.04498247057199478\n",
      "epoch: 2 step: 1861, loss is 0.0006528954836539924\n",
      "epoch: 2 step: 1862, loss is 0.002185909543186426\n",
      "epoch: 2 step: 1863, loss is 0.015034730546176434\n",
      "epoch: 2 step: 1864, loss is 0.03791729733347893\n",
      "epoch: 2 step: 1865, loss is 0.23094026744365692\n",
      "epoch: 2 step: 1866, loss is 0.020740902051329613\n",
      "epoch: 2 step: 1867, loss is 0.029005207121372223\n",
      "epoch: 2 step: 1868, loss is 0.18818624317646027\n",
      "epoch: 2 step: 1869, loss is 0.14338211715221405\n",
      "epoch: 2 step: 1870, loss is 0.05297788605093956\n",
      "epoch: 2 step: 1871, loss is 0.17891258001327515\n",
      "epoch: 2 step: 1872, loss is 0.010895545594394207\n",
      "epoch: 2 step: 1873, loss is 0.10044978559017181\n",
      "epoch: 2 step: 1874, loss is 0.0023210537619888783\n",
      "epoch: 2 step: 1875, loss is 0.05276968702673912\n",
      "Train epoch time: 9296.634 ms, per step time: 4.958 ms\n",
      "epoch: 3 step: 1, loss is 0.0111966198310256\n",
      "epoch: 3 step: 2, loss is 0.0050524077378213406\n",
      "epoch: 3 step: 3, loss is 0.1677357405424118\n",
      "epoch: 3 step: 4, loss is 0.0016937977634370327\n",
      "epoch: 3 step: 5, loss is 0.4133833646774292\n",
      "epoch: 3 step: 6, loss is 0.008693594485521317\n",
      "epoch: 3 step: 7, loss is 0.054447151720523834\n",
      "epoch: 3 step: 8, loss is 0.04180191457271576\n",
      "epoch: 3 step: 9, loss is 0.015458693727850914\n",
      "epoch: 3 step: 10, loss is 0.11211742460727692\n",
      "epoch: 3 step: 11, loss is 0.020641768351197243\n",
      "epoch: 3 step: 12, loss is 0.17573116719722748\n",
      "epoch: 3 step: 13, loss is 0.062046896666288376\n",
      "epoch: 3 step: 14, loss is 0.011718487367033958\n",
      "epoch: 3 step: 15, loss is 0.02133036218583584\n",
      "epoch: 3 step: 16, loss is 0.0014020920498296618\n",
      "epoch: 3 step: 17, loss is 0.189254030585289\n",
      "epoch: 3 step: 18, loss is 0.1052679494023323\n",
      "epoch: 3 step: 19, loss is 0.22929182648658752\n",
      "epoch: 3 step: 20, loss is 0.1605897694826126\n",
      "epoch: 3 step: 21, loss is 0.07981762290000916\n",
      "epoch: 3 step: 22, loss is 0.04021010920405388\n",
      "epoch: 3 step: 23, loss is 0.03334714472293854\n",
      "epoch: 3 step: 24, loss is 0.18108709156513214\n",
      "epoch: 3 step: 25, loss is 0.12539799511432648\n",
      "epoch: 3 step: 26, loss is 0.004460005555301905\n",
      "epoch: 3 step: 27, loss is 0.09489898383617401\n",
      "epoch: 3 step: 28, loss is 0.14962421357631683\n",
      "epoch: 3 step: 29, loss is 0.12072498351335526\n",
      "epoch: 3 step: 30, loss is 0.017338687554001808\n",
      "epoch: 3 step: 31, loss is 0.0062692053616046906\n",
      "epoch: 3 step: 32, loss is 0.02952750027179718\n",
      "epoch: 3 step: 33, loss is 0.02901892550289631\n",
      "epoch: 3 step: 34, loss is 0.20123976469039917\n",
      "epoch: 3 step: 35, loss is 0.057707034051418304\n",
      "epoch: 3 step: 36, loss is 0.006969116628170013\n",
      "epoch: 3 step: 37, loss is 0.16767238080501556\n",
      "epoch: 3 step: 38, loss is 0.013204098679125309\n",
      "epoch: 3 step: 39, loss is 0.044175323098897934\n",
      "epoch: 3 step: 40, loss is 0.008478417061269283\n",
      "epoch: 3 step: 41, loss is 0.01338187512010336\n",
      "epoch: 3 step: 42, loss is 0.044159483164548874\n",
      "epoch: 3 step: 43, loss is 0.005334966816008091\n",
      "epoch: 3 step: 44, loss is 0.006572004407644272\n",
      "epoch: 3 step: 45, loss is 0.19375020265579224\n",
      "epoch: 3 step: 46, loss is 0.09797404706478119\n",
      "epoch: 3 step: 47, loss is 0.10364498198032379\n",
      "epoch: 3 step: 48, loss is 0.0023417328484356403\n",
      "epoch: 3 step: 49, loss is 0.010777270421385765\n",
      "epoch: 3 step: 50, loss is 0.010482225567102432\n",
      "epoch: 3 step: 51, loss is 0.10181038081645966\n",
      "epoch: 3 step: 52, loss is 0.016740890219807625\n",
      "epoch: 3 step: 53, loss is 0.08681077510118484\n",
      "epoch: 3 step: 54, loss is 0.2552243769168854\n",
      "epoch: 3 step: 55, loss is 0.010722375474870205\n",
      "epoch: 3 step: 56, loss is 0.1885254830121994\n",
      "epoch: 3 step: 57, loss is 0.05885042995214462\n",
      "epoch: 3 step: 58, loss is 0.10282166302204132\n",
      "epoch: 3 step: 59, loss is 0.12562952935695648\n",
      "epoch: 3 step: 60, loss is 0.31656336784362793\n",
      "epoch: 3 step: 61, loss is 0.04163990542292595\n",
      "epoch: 3 step: 62, loss is 0.06505532562732697\n",
      "epoch: 3 step: 63, loss is 0.08740648627281189\n",
      "epoch: 3 step: 64, loss is 0.06746687740087509\n",
      "epoch: 3 step: 65, loss is 0.025998638942837715\n",
      "epoch: 3 step: 66, loss is 0.002276059240102768\n",
      "epoch: 3 step: 67, loss is 0.008421649225056171\n",
      "epoch: 3 step: 68, loss is 0.014797315932810307\n",
      "epoch: 3 step: 69, loss is 0.2731285095214844\n",
      "epoch: 3 step: 70, loss is 0.002343882340937853\n",
      "epoch: 3 step: 71, loss is 0.025247439742088318\n",
      "epoch: 3 step: 72, loss is 0.019588131457567215\n",
      "epoch: 3 step: 73, loss is 0.058240678161382675\n",
      "epoch: 3 step: 74, loss is 0.1836683601140976\n",
      "epoch: 3 step: 75, loss is 0.06766732037067413\n",
      "epoch: 3 step: 76, loss is 0.020549362525343895\n",
      "epoch: 3 step: 77, loss is 0.04360650107264519\n",
      "epoch: 3 step: 78, loss is 0.16558609902858734\n",
      "epoch: 3 step: 79, loss is 0.13798293471336365\n",
      "epoch: 3 step: 80, loss is 0.05277545005083084\n",
      "epoch: 3 step: 81, loss is 0.0004774034605361521\n",
      "epoch: 3 step: 82, loss is 0.14224468171596527\n",
      "epoch: 3 step: 83, loss is 0.07666667550802231\n",
      "epoch: 3 step: 84, loss is 0.16777463257312775\n",
      "epoch: 3 step: 85, loss is 0.005331038497388363\n",
      "epoch: 3 step: 86, loss is 0.02587161771953106\n",
      "epoch: 3 step: 87, loss is 0.004521755967289209\n",
      "epoch: 3 step: 88, loss is 0.0031135668978095055\n",
      "epoch: 3 step: 89, loss is 0.034338463097810745\n",
      "epoch: 3 step: 90, loss is 0.011574356816709042\n",
      "epoch: 3 step: 91, loss is 0.006298656575381756\n",
      "epoch: 3 step: 92, loss is 0.022700058296322823\n",
      "epoch: 3 step: 93, loss is 0.007673353888094425\n",
      "epoch: 3 step: 94, loss is 0.0376497358083725\n",
      "epoch: 3 step: 95, loss is 0.04230070859193802\n",
      "epoch: 3 step: 96, loss is 0.22182373702526093\n",
      "epoch: 3 step: 97, loss is 0.009007131680846214\n",
      "epoch: 3 step: 98, loss is 0.01676897704601288\n",
      "epoch: 3 step: 99, loss is 0.09412223100662231\n",
      "epoch: 3 step: 100, loss is 0.01830923929810524\n",
      "epoch: 3 step: 101, loss is 0.016716543585062027\n",
      "epoch: 3 step: 102, loss is 0.04717504233121872\n",
      "epoch: 3 step: 103, loss is 0.015347207896411419\n",
      "epoch: 3 step: 104, loss is 0.026791486889123917\n",
      "epoch: 3 step: 105, loss is 0.05350121110677719\n",
      "epoch: 3 step: 106, loss is 0.07063978910446167\n",
      "epoch: 3 step: 107, loss is 0.002479200717061758\n",
      "epoch: 3 step: 108, loss is 0.006604237016290426\n",
      "epoch: 3 step: 109, loss is 0.23943619430065155\n",
      "epoch: 3 step: 110, loss is 0.02630649134516716\n",
      "epoch: 3 step: 111, loss is 0.047806788235902786\n",
      "epoch: 3 step: 112, loss is 0.004918347112834454\n",
      "epoch: 3 step: 113, loss is 0.0029961969703435898\n",
      "epoch: 3 step: 114, loss is 0.050916172564029694\n",
      "epoch: 3 step: 115, loss is 0.09259103238582611\n",
      "epoch: 3 step: 116, loss is 0.016748998314142227\n",
      "epoch: 3 step: 117, loss is 0.029276281595230103\n",
      "epoch: 3 step: 118, loss is 0.21903866529464722\n",
      "epoch: 3 step: 119, loss is 0.16188888251781464\n",
      "epoch: 3 step: 120, loss is 0.04662112519145012\n",
      "epoch: 3 step: 121, loss is 0.001991930650547147\n",
      "epoch: 3 step: 122, loss is 0.0023807876277714968\n",
      "epoch: 3 step: 123, loss is 0.27949830889701843\n",
      "epoch: 3 step: 124, loss is 0.03259677812457085\n",
      "epoch: 3 step: 125, loss is 0.004537278786301613\n",
      "epoch: 3 step: 126, loss is 0.06205194443464279\n",
      "epoch: 3 step: 127, loss is 0.10485067218542099\n",
      "epoch: 3 step: 128, loss is 0.002052849857136607\n",
      "epoch: 3 step: 129, loss is 0.005163256078958511\n",
      "epoch: 3 step: 130, loss is 0.06187330558896065\n",
      "epoch: 3 step: 131, loss is 0.09359614551067352\n",
      "epoch: 3 step: 132, loss is 0.06712222844362259\n",
      "epoch: 3 step: 133, loss is 0.045160386711359024\n",
      "epoch: 3 step: 134, loss is 0.01249874196946621\n",
      "epoch: 3 step: 135, loss is 0.022035909816622734\n",
      "epoch: 3 step: 136, loss is 0.003691719612106681\n",
      "epoch: 3 step: 137, loss is 0.010125678963959217\n",
      "epoch: 3 step: 138, loss is 0.08611328899860382\n",
      "epoch: 3 step: 139, loss is 0.02393951825797558\n",
      "epoch: 3 step: 140, loss is 0.01617300696671009\n",
      "epoch: 3 step: 141, loss is 0.11725612729787827\n",
      "epoch: 3 step: 142, loss is 0.059467434883117676\n",
      "epoch: 3 step: 143, loss is 0.08005604147911072\n",
      "epoch: 3 step: 144, loss is 0.006717897020280361\n",
      "epoch: 3 step: 145, loss is 0.02877764403820038\n",
      "epoch: 3 step: 146, loss is 0.03350439667701721\n",
      "epoch: 3 step: 147, loss is 0.2995058596134186\n",
      "epoch: 3 step: 148, loss is 0.058441199362277985\n",
      "epoch: 3 step: 149, loss is 0.0025715716183185577\n",
      "epoch: 3 step: 150, loss is 0.0026745705399662256\n",
      "epoch: 3 step: 151, loss is 0.02821182832121849\n",
      "epoch: 3 step: 152, loss is 0.012624941766262054\n",
      "epoch: 3 step: 153, loss is 0.0619235634803772\n",
      "epoch: 3 step: 154, loss is 0.0031596876215189695\n",
      "epoch: 3 step: 155, loss is 0.14068540930747986\n",
      "epoch: 3 step: 156, loss is 0.012912007048726082\n",
      "epoch: 3 step: 157, loss is 0.010851672850549221\n",
      "epoch: 3 step: 158, loss is 0.046606551855802536\n",
      "epoch: 3 step: 159, loss is 0.006320332642644644\n",
      "epoch: 3 step: 160, loss is 0.013674336485564709\n",
      "epoch: 3 step: 161, loss is 0.05685895308852196\n",
      "epoch: 3 step: 162, loss is 0.1539900153875351\n",
      "epoch: 3 step: 163, loss is 0.04569532722234726\n",
      "epoch: 3 step: 164, loss is 0.0016048526158556342\n",
      "epoch: 3 step: 165, loss is 0.0048012686893343925\n",
      "epoch: 3 step: 166, loss is 0.0024684786330908537\n",
      "epoch: 3 step: 167, loss is 0.1087157353758812\n",
      "epoch: 3 step: 168, loss is 0.0005410709418356419\n",
      "epoch: 3 step: 169, loss is 0.027369080111384392\n",
      "epoch: 3 step: 170, loss is 0.018831316381692886\n",
      "epoch: 3 step: 171, loss is 0.029799513518810272\n",
      "epoch: 3 step: 172, loss is 0.02319851890206337\n",
      "epoch: 3 step: 173, loss is 0.004711208399385214\n",
      "epoch: 3 step: 174, loss is 0.06475736945867538\n",
      "epoch: 3 step: 175, loss is 0.13313868641853333\n",
      "epoch: 3 step: 176, loss is 0.0007191665936261415\n",
      "epoch: 3 step: 177, loss is 0.0021861742716282606\n",
      "epoch: 3 step: 178, loss is 0.014126334339380264\n",
      "epoch: 3 step: 179, loss is 0.005694133695214987\n",
      "epoch: 3 step: 180, loss is 0.0035676874686032534\n",
      "epoch: 3 step: 181, loss is 0.002354552038013935\n",
      "epoch: 3 step: 182, loss is 0.045375533401966095\n",
      "epoch: 3 step: 183, loss is 0.19825692474842072\n",
      "epoch: 3 step: 184, loss is 0.008712554350495338\n",
      "epoch: 3 step: 185, loss is 0.050183527171611786\n",
      "epoch: 3 step: 186, loss is 0.03360320255160332\n",
      "epoch: 3 step: 187, loss is 0.13109925389289856\n",
      "epoch: 3 step: 188, loss is 0.0016323633026331663\n",
      "epoch: 3 step: 189, loss is 0.00541731808334589\n",
      "epoch: 3 step: 190, loss is 0.014298847876489162\n",
      "epoch: 3 step: 191, loss is 0.007158207707107067\n",
      "epoch: 3 step: 192, loss is 0.0016329132486134768\n",
      "epoch: 3 step: 193, loss is 0.06831414997577667\n",
      "epoch: 3 step: 194, loss is 0.006027938332408667\n",
      "epoch: 3 step: 195, loss is 0.003907959442585707\n",
      "epoch: 3 step: 196, loss is 0.07178349792957306\n",
      "epoch: 3 step: 197, loss is 0.06982830166816711\n",
      "epoch: 3 step: 198, loss is 0.0008942837594076991\n",
      "epoch: 3 step: 199, loss is 0.004175511188805103\n",
      "epoch: 3 step: 200, loss is 0.00932040624320507\n",
      "epoch: 3 step: 201, loss is 0.0092256348580122\n",
      "epoch: 3 step: 202, loss is 0.05067100003361702\n",
      "epoch: 3 step: 203, loss is 0.08020087331533432\n",
      "epoch: 3 step: 204, loss is 0.09770753234624863\n",
      "epoch: 3 step: 205, loss is 0.0011181664885953069\n",
      "epoch: 3 step: 206, loss is 0.0042693414725363255\n",
      "epoch: 3 step: 207, loss is 0.06904014199972153\n",
      "epoch: 3 step: 208, loss is 0.007201614789664745\n",
      "epoch: 3 step: 209, loss is 0.005659021437168121\n",
      "epoch: 3 step: 210, loss is 0.05956309661269188\n",
      "epoch: 3 step: 211, loss is 0.001861556083895266\n",
      "epoch: 3 step: 212, loss is 0.004475646652281284\n",
      "epoch: 3 step: 213, loss is 0.10121630132198334\n",
      "epoch: 3 step: 214, loss is 0.008825965225696564\n",
      "epoch: 3 step: 215, loss is 0.25933295488357544\n",
      "epoch: 3 step: 216, loss is 0.010095286183059216\n",
      "epoch: 3 step: 217, loss is 0.00012136154691688716\n",
      "epoch: 3 step: 218, loss is 0.0019255895167589188\n",
      "epoch: 3 step: 219, loss is 0.0006218828493729234\n",
      "epoch: 3 step: 220, loss is 0.013286300003528595\n",
      "epoch: 3 step: 221, loss is 0.07973223924636841\n",
      "epoch: 3 step: 222, loss is 0.029090730473399162\n",
      "epoch: 3 step: 223, loss is 0.13071230053901672\n",
      "epoch: 3 step: 224, loss is 0.016415024176239967\n",
      "epoch: 3 step: 225, loss is 0.06949067860841751\n",
      "epoch: 3 step: 226, loss is 0.02432398684322834\n",
      "epoch: 3 step: 227, loss is 0.005159829277545214\n",
      "epoch: 3 step: 228, loss is 0.00034897547448053956\n",
      "epoch: 3 step: 229, loss is 0.11126656830310822\n",
      "epoch: 3 step: 230, loss is 0.0070557985454797745\n",
      "epoch: 3 step: 231, loss is 0.030088279396295547\n",
      "epoch: 3 step: 232, loss is 0.0588671900331974\n",
      "epoch: 3 step: 233, loss is 0.010297082364559174\n",
      "epoch: 3 step: 234, loss is 0.10449448972940445\n",
      "epoch: 3 step: 235, loss is 0.23073358833789825\n",
      "epoch: 3 step: 236, loss is 0.0005044040153734386\n",
      "epoch: 3 step: 237, loss is 0.14091932773590088\n",
      "epoch: 3 step: 238, loss is 0.055522702634334564\n",
      "epoch: 3 step: 239, loss is 0.004637525882571936\n",
      "epoch: 3 step: 240, loss is 0.006492672488093376\n",
      "epoch: 3 step: 241, loss is 0.13329294323921204\n",
      "epoch: 3 step: 242, loss is 0.013088325038552284\n",
      "epoch: 3 step: 243, loss is 0.0029830734711140394\n",
      "epoch: 3 step: 244, loss is 0.029595984145998955\n",
      "epoch: 3 step: 245, loss is 0.008305479772388935\n",
      "epoch: 3 step: 246, loss is 0.0056495387107133865\n",
      "epoch: 3 step: 247, loss is 0.024193216115236282\n",
      "epoch: 3 step: 248, loss is 0.027464330196380615\n",
      "epoch: 3 step: 249, loss is 0.06783399730920792\n",
      "epoch: 3 step: 250, loss is 0.002482494805008173\n",
      "epoch: 3 step: 251, loss is 0.0012731575407087803\n",
      "epoch: 3 step: 252, loss is 0.04865797609090805\n",
      "epoch: 3 step: 253, loss is 0.026682663708925247\n",
      "epoch: 3 step: 254, loss is 0.009672407060861588\n",
      "epoch: 3 step: 255, loss is 0.11867556720972061\n",
      "epoch: 3 step: 256, loss is 0.039689917117357254\n",
      "epoch: 3 step: 257, loss is 0.007879245094954967\n",
      "epoch: 3 step: 258, loss is 0.02439013309776783\n",
      "epoch: 3 step: 259, loss is 0.018280968070030212\n",
      "epoch: 3 step: 260, loss is 0.18683147430419922\n",
      "epoch: 3 step: 261, loss is 0.003073648316785693\n",
      "epoch: 3 step: 262, loss is 0.00939799752086401\n",
      "epoch: 3 step: 263, loss is 0.01320344302803278\n",
      "epoch: 3 step: 264, loss is 0.012091007083654404\n",
      "epoch: 3 step: 265, loss is 0.024198133498430252\n",
      "epoch: 3 step: 266, loss is 0.025550495833158493\n",
      "epoch: 3 step: 267, loss is 0.10435885936021805\n",
      "epoch: 3 step: 268, loss is 0.0025683247949928045\n",
      "epoch: 3 step: 269, loss is 0.0011906325817108154\n",
      "epoch: 3 step: 270, loss is 0.04861375689506531\n",
      "epoch: 3 step: 271, loss is 0.019313737750053406\n",
      "epoch: 3 step: 272, loss is 0.13123854994773865\n",
      "epoch: 3 step: 273, loss is 0.15405753254890442\n",
      "epoch: 3 step: 274, loss is 0.05378517508506775\n",
      "epoch: 3 step: 275, loss is 0.06756714731454849\n",
      "epoch: 3 step: 276, loss is 0.002816335763782263\n",
      "epoch: 3 step: 277, loss is 0.005301014520227909\n",
      "epoch: 3 step: 278, loss is 0.16926005482673645\n",
      "epoch: 3 step: 279, loss is 0.0002873038174584508\n",
      "epoch: 3 step: 280, loss is 0.00203531002625823\n",
      "epoch: 3 step: 281, loss is 0.012713754549622536\n",
      "epoch: 3 step: 282, loss is 0.016728060320019722\n",
      "epoch: 3 step: 283, loss is 0.00032095343340188265\n",
      "epoch: 3 step: 284, loss is 0.4716125428676605\n",
      "epoch: 3 step: 285, loss is 0.3656312823295593\n",
      "epoch: 3 step: 286, loss is 0.0015359176322817802\n",
      "epoch: 3 step: 287, loss is 0.0646398663520813\n",
      "epoch: 3 step: 288, loss is 0.2982824444770813\n",
      "epoch: 3 step: 289, loss is 0.012089036405086517\n",
      "epoch: 3 step: 290, loss is 0.08119179308414459\n",
      "epoch: 3 step: 291, loss is 0.013390963897109032\n",
      "epoch: 3 step: 292, loss is 0.03563447669148445\n",
      "epoch: 3 step: 293, loss is 0.031109783798456192\n",
      "epoch: 3 step: 294, loss is 0.10448490083217621\n",
      "epoch: 3 step: 295, loss is 0.008606437593698502\n",
      "epoch: 3 step: 296, loss is 0.40166428685188293\n",
      "epoch: 3 step: 297, loss is 0.019687378779053688\n",
      "epoch: 3 step: 298, loss is 0.014138296246528625\n",
      "epoch: 3 step: 299, loss is 0.07107016444206238\n",
      "epoch: 3 step: 300, loss is 0.01831481233239174\n",
      "epoch: 3 step: 301, loss is 0.08346008509397507\n",
      "epoch: 3 step: 302, loss is 0.013218025676906109\n",
      "epoch: 3 step: 303, loss is 0.009158249013125896\n",
      "epoch: 3 step: 304, loss is 0.013925277628004551\n",
      "epoch: 3 step: 305, loss is 0.011087894439697266\n",
      "epoch: 3 step: 306, loss is 0.011306554079055786\n",
      "epoch: 3 step: 307, loss is 0.040922585874795914\n",
      "epoch: 3 step: 308, loss is 0.04710536077618599\n",
      "epoch: 3 step: 309, loss is 0.0016597640933468938\n",
      "epoch: 3 step: 310, loss is 0.07620863616466522\n",
      "epoch: 3 step: 311, loss is 0.04454682767391205\n",
      "epoch: 3 step: 312, loss is 0.10728383809328079\n",
      "epoch: 3 step: 313, loss is 0.15383826196193695\n",
      "epoch: 3 step: 314, loss is 0.01772942952811718\n",
      "epoch: 3 step: 315, loss is 0.057994600385427475\n",
      "epoch: 3 step: 316, loss is 0.0218600295484066\n",
      "epoch: 3 step: 317, loss is 0.04242177680134773\n",
      "epoch: 3 step: 318, loss is 0.05314197391271591\n",
      "epoch: 3 step: 319, loss is 0.013226442039012909\n",
      "epoch: 3 step: 320, loss is 0.0047151693142950535\n",
      "epoch: 3 step: 321, loss is 0.026447050273418427\n",
      "epoch: 3 step: 322, loss is 0.0038602587301284075\n",
      "epoch: 3 step: 323, loss is 0.0028272001072764397\n",
      "epoch: 3 step: 324, loss is 0.0244368314743042\n",
      "epoch: 3 step: 325, loss is 0.01698177494108677\n",
      "epoch: 3 step: 326, loss is 0.009285814128816128\n",
      "epoch: 3 step: 327, loss is 0.005642717704176903\n",
      "epoch: 3 step: 328, loss is 0.052212756127119064\n",
      "epoch: 3 step: 329, loss is 0.17780399322509766\n",
      "epoch: 3 step: 330, loss is 0.04079226776957512\n",
      "epoch: 3 step: 331, loss is 0.016586728394031525\n",
      "epoch: 3 step: 332, loss is 0.04521172121167183\n",
      "epoch: 3 step: 333, loss is 0.18232235312461853\n",
      "epoch: 3 step: 334, loss is 0.06462407857179642\n",
      "epoch: 3 step: 335, loss is 0.0019277065293863416\n",
      "epoch: 3 step: 336, loss is 0.11838699132204056\n",
      "epoch: 3 step: 337, loss is 0.005078916437923908\n",
      "epoch: 3 step: 338, loss is 0.02124778926372528\n",
      "epoch: 3 step: 339, loss is 0.12507937848567963\n",
      "epoch: 3 step: 340, loss is 0.11342932283878326\n",
      "epoch: 3 step: 341, loss is 0.09398506581783295\n",
      "epoch: 3 step: 342, loss is 0.06435206532478333\n",
      "epoch: 3 step: 343, loss is 0.06502402573823929\n",
      "epoch: 3 step: 344, loss is 0.008234020322561264\n",
      "epoch: 3 step: 345, loss is 0.046858806163072586\n",
      "epoch: 3 step: 346, loss is 0.07229018956422806\n",
      "epoch: 3 step: 347, loss is 0.012570394203066826\n",
      "epoch: 3 step: 348, loss is 0.00851999782025814\n",
      "epoch: 3 step: 349, loss is 0.002892739837989211\n",
      "epoch: 3 step: 350, loss is 0.009503382258117199\n",
      "epoch: 3 step: 351, loss is 0.0024728355929255486\n",
      "epoch: 3 step: 352, loss is 0.07441115379333496\n",
      "epoch: 3 step: 353, loss is 0.14648926258087158\n",
      "epoch: 3 step: 354, loss is 0.013667364604771137\n",
      "epoch: 3 step: 355, loss is 0.34248292446136475\n",
      "epoch: 3 step: 356, loss is 0.0047595263458788395\n",
      "epoch: 3 step: 357, loss is 0.014946624636650085\n",
      "epoch: 3 step: 358, loss is 0.002063190331682563\n",
      "epoch: 3 step: 359, loss is 0.15261316299438477\n",
      "epoch: 3 step: 360, loss is 0.002727063139900565\n",
      "epoch: 3 step: 361, loss is 0.0215456560254097\n",
      "epoch: 3 step: 362, loss is 0.004910137038677931\n",
      "epoch: 3 step: 363, loss is 0.11782705038785934\n",
      "epoch: 3 step: 364, loss is 0.05463257059454918\n",
      "epoch: 3 step: 365, loss is 0.016447830945253372\n",
      "epoch: 3 step: 366, loss is 0.012602039612829685\n",
      "epoch: 3 step: 367, loss is 0.13669638335704803\n",
      "epoch: 3 step: 368, loss is 0.11262000352144241\n",
      "epoch: 3 step: 369, loss is 0.013326547108590603\n",
      "epoch: 3 step: 370, loss is 0.03983364626765251\n",
      "epoch: 3 step: 371, loss is 0.14013831317424774\n",
      "epoch: 3 step: 372, loss is 0.004887860268354416\n",
      "epoch: 3 step: 373, loss is 0.29323610663414\n",
      "epoch: 3 step: 374, loss is 0.004858505912125111\n",
      "epoch: 3 step: 375, loss is 0.0299819465726614\n",
      "epoch: 3 step: 376, loss is 0.056863315403461456\n",
      "epoch: 3 step: 377, loss is 0.06938516348600388\n",
      "epoch: 3 step: 378, loss is 0.016915054991841316\n",
      "epoch: 3 step: 379, loss is 0.0827820897102356\n",
      "epoch: 3 step: 380, loss is 0.004914883989840746\n",
      "epoch: 3 step: 381, loss is 0.17284271121025085\n",
      "epoch: 3 step: 382, loss is 0.0416712760925293\n",
      "epoch: 3 step: 383, loss is 0.06435354799032211\n",
      "epoch: 3 step: 384, loss is 0.11918855458498001\n",
      "epoch: 3 step: 385, loss is 0.2966640889644623\n",
      "epoch: 3 step: 386, loss is 0.0387105755507946\n",
      "epoch: 3 step: 387, loss is 0.07797522097826004\n",
      "epoch: 3 step: 388, loss is 0.03778017312288284\n",
      "epoch: 3 step: 389, loss is 0.05756480246782303\n",
      "epoch: 3 step: 390, loss is 0.014404444955289364\n",
      "epoch: 3 step: 391, loss is 0.0026827810797840357\n",
      "epoch: 3 step: 392, loss is 0.1332797110080719\n",
      "epoch: 3 step: 393, loss is 0.005290290340781212\n",
      "epoch: 3 step: 394, loss is 0.005723768379539251\n",
      "epoch: 3 step: 395, loss is 0.08317507058382034\n",
      "epoch: 3 step: 396, loss is 0.19155368208885193\n",
      "epoch: 3 step: 397, loss is 0.013089191168546677\n",
      "epoch: 3 step: 398, loss is 0.022732805460691452\n",
      "epoch: 3 step: 399, loss is 0.053830835968256\n",
      "epoch: 3 step: 400, loss is 0.003120515262708068\n",
      "epoch: 3 step: 401, loss is 0.09334611892700195\n",
      "epoch: 3 step: 402, loss is 0.03258455544710159\n",
      "epoch: 3 step: 403, loss is 0.08375681191682816\n",
      "epoch: 3 step: 404, loss is 0.014827282167971134\n",
      "epoch: 3 step: 405, loss is 0.024200668558478355\n",
      "epoch: 3 step: 406, loss is 0.054328352212905884\n",
      "epoch: 3 step: 407, loss is 0.07168968766927719\n",
      "epoch: 3 step: 408, loss is 0.012727562338113785\n",
      "epoch: 3 step: 409, loss is 0.1719401329755783\n",
      "epoch: 3 step: 410, loss is 0.010068309493362904\n",
      "epoch: 3 step: 411, loss is 0.023987416177988052\n",
      "epoch: 3 step: 412, loss is 0.022154565900564194\n",
      "epoch: 3 step: 413, loss is 0.2460278421640396\n",
      "epoch: 3 step: 414, loss is 0.045638855546712875\n",
      "epoch: 3 step: 415, loss is 0.08523037284612656\n",
      "epoch: 3 step: 416, loss is 0.016605356708168983\n",
      "epoch: 3 step: 417, loss is 0.038591157644987106\n",
      "epoch: 3 step: 418, loss is 0.08485114574432373\n",
      "epoch: 3 step: 419, loss is 0.025731287896633148\n",
      "epoch: 3 step: 420, loss is 0.004642175976186991\n",
      "epoch: 3 step: 421, loss is 0.11599624156951904\n",
      "epoch: 3 step: 422, loss is 0.004026378039270639\n",
      "epoch: 3 step: 423, loss is 0.17971596121788025\n",
      "epoch: 3 step: 424, loss is 0.0026162113063037395\n",
      "epoch: 3 step: 425, loss is 0.032308030873537064\n",
      "epoch: 3 step: 426, loss is 0.023538948968052864\n",
      "epoch: 3 step: 427, loss is 0.13258299231529236\n",
      "epoch: 3 step: 428, loss is 0.016742054373025894\n",
      "epoch: 3 step: 429, loss is 0.0676087960600853\n",
      "epoch: 3 step: 430, loss is 0.10370082408189774\n",
      "epoch: 3 step: 431, loss is 0.07681088894605637\n",
      "epoch: 3 step: 432, loss is 0.007631203159689903\n",
      "epoch: 3 step: 433, loss is 0.09173031151294708\n",
      "epoch: 3 step: 434, loss is 0.008540362119674683\n",
      "epoch: 3 step: 435, loss is 0.050752121955156326\n",
      "epoch: 3 step: 436, loss is 0.037722472101449966\n",
      "epoch: 3 step: 437, loss is 0.05471577122807503\n",
      "epoch: 3 step: 438, loss is 0.0035352909471839666\n",
      "epoch: 3 step: 439, loss is 0.007765180431306362\n",
      "epoch: 3 step: 440, loss is 0.07172852009534836\n",
      "epoch: 3 step: 441, loss is 0.02042725682258606\n",
      "epoch: 3 step: 442, loss is 0.002837103558704257\n",
      "epoch: 3 step: 443, loss is 0.2966342866420746\n",
      "epoch: 3 step: 444, loss is 0.05854283645749092\n",
      "epoch: 3 step: 445, loss is 0.015757741406559944\n",
      "epoch: 3 step: 446, loss is 0.011108475737273693\n",
      "epoch: 3 step: 447, loss is 0.05417598783969879\n",
      "epoch: 3 step: 448, loss is 0.04687827080488205\n",
      "epoch: 3 step: 449, loss is 0.0022059360053390265\n",
      "epoch: 3 step: 450, loss is 0.000474874657811597\n",
      "epoch: 3 step: 451, loss is 0.009158159606158733\n",
      "epoch: 3 step: 452, loss is 0.037837736308574677\n",
      "epoch: 3 step: 453, loss is 0.021743997931480408\n",
      "epoch: 3 step: 454, loss is 0.060120683163404465\n",
      "epoch: 3 step: 455, loss is 0.0035538908559828997\n",
      "epoch: 3 step: 456, loss is 0.40794801712036133\n",
      "epoch: 3 step: 457, loss is 0.009059345349669456\n",
      "epoch: 3 step: 458, loss is 0.00862699281424284\n",
      "epoch: 3 step: 459, loss is 0.02375617064535618\n",
      "epoch: 3 step: 460, loss is 0.02875056117773056\n",
      "epoch: 3 step: 461, loss is 0.019232984632253647\n",
      "epoch: 3 step: 462, loss is 0.0030719952192157507\n",
      "epoch: 3 step: 463, loss is 0.0005441816756501794\n",
      "epoch: 3 step: 464, loss is 0.0016267793253064156\n",
      "epoch: 3 step: 465, loss is 0.08081183582544327\n",
      "epoch: 3 step: 466, loss is 0.09213417768478394\n",
      "epoch: 3 step: 467, loss is 0.023131711408495903\n",
      "epoch: 3 step: 468, loss is 0.007969282567501068\n",
      "epoch: 3 step: 469, loss is 0.2514771819114685\n",
      "epoch: 3 step: 470, loss is 0.13852053880691528\n",
      "epoch: 3 step: 471, loss is 0.05231580510735512\n",
      "epoch: 3 step: 472, loss is 0.02399345673620701\n",
      "epoch: 3 step: 473, loss is 0.013237366452813148\n",
      "epoch: 3 step: 474, loss is 0.06365666538476944\n",
      "epoch: 3 step: 475, loss is 0.0739118903875351\n",
      "epoch: 3 step: 476, loss is 0.07888726145029068\n",
      "epoch: 3 step: 477, loss is 0.0068636732175946236\n",
      "epoch: 3 step: 478, loss is 0.0026438746135681868\n",
      "epoch: 3 step: 479, loss is 0.1456962376832962\n",
      "epoch: 3 step: 480, loss is 0.003965578507632017\n",
      "epoch: 3 step: 481, loss is 0.0021587442606687546\n",
      "epoch: 3 step: 482, loss is 0.010276797227561474\n",
      "epoch: 3 step: 483, loss is 0.0781458169221878\n",
      "epoch: 3 step: 484, loss is 0.09188664704561234\n",
      "epoch: 3 step: 485, loss is 0.0035209127236157656\n",
      "epoch: 3 step: 486, loss is 0.028790298849344254\n",
      "epoch: 3 step: 487, loss is 0.0563926175236702\n",
      "epoch: 3 step: 488, loss is 0.004204198252409697\n",
      "epoch: 3 step: 489, loss is 0.04420565441250801\n",
      "epoch: 3 step: 490, loss is 0.07163164764642715\n",
      "epoch: 3 step: 491, loss is 0.13998840749263763\n",
      "epoch: 3 step: 492, loss is 0.44918185472488403\n",
      "epoch: 3 step: 493, loss is 0.09074404090642929\n",
      "epoch: 3 step: 494, loss is 0.01650848798453808\n",
      "epoch: 3 step: 495, loss is 0.0046921176835894585\n",
      "epoch: 3 step: 496, loss is 0.0044516706839203835\n",
      "epoch: 3 step: 497, loss is 0.10189985483884811\n",
      "epoch: 3 step: 498, loss is 0.11828666925430298\n",
      "epoch: 3 step: 499, loss is 0.0663011446595192\n",
      "epoch: 3 step: 500, loss is 0.06527991592884064\n",
      "epoch: 3 step: 501, loss is 0.08478604257106781\n",
      "epoch: 3 step: 502, loss is 0.11359693109989166\n",
      "epoch: 3 step: 503, loss is 0.01328371837735176\n",
      "epoch: 3 step: 504, loss is 0.040354080498218536\n",
      "epoch: 3 step: 505, loss is 0.013693979941308498\n",
      "epoch: 3 step: 506, loss is 0.003979851957410574\n",
      "epoch: 3 step: 507, loss is 0.026793943718075752\n",
      "epoch: 3 step: 508, loss is 0.13800731301307678\n",
      "epoch: 3 step: 509, loss is 0.25443482398986816\n",
      "epoch: 3 step: 510, loss is 0.029925797134637833\n",
      "epoch: 3 step: 511, loss is 0.025048092007637024\n",
      "epoch: 3 step: 512, loss is 0.004802793264389038\n",
      "epoch: 3 step: 513, loss is 0.007941348478198051\n",
      "epoch: 3 step: 514, loss is 0.057092081755399704\n",
      "epoch: 3 step: 515, loss is 0.00281337252818048\n",
      "epoch: 3 step: 516, loss is 0.0186020378023386\n",
      "epoch: 3 step: 517, loss is 0.03121526911854744\n",
      "epoch: 3 step: 518, loss is 0.058648936450481415\n",
      "epoch: 3 step: 519, loss is 0.009390243329107761\n",
      "epoch: 3 step: 520, loss is 0.002914431970566511\n",
      "epoch: 3 step: 521, loss is 0.018326587975025177\n",
      "epoch: 3 step: 522, loss is 0.045165859162807465\n",
      "epoch: 3 step: 523, loss is 0.0030249720439314842\n",
      "epoch: 3 step: 524, loss is 0.013334263116121292\n",
      "epoch: 3 step: 525, loss is 0.03480318933725357\n",
      "epoch: 3 step: 526, loss is 0.0032033312600106\n",
      "epoch: 3 step: 527, loss is 0.006224529352039099\n",
      "epoch: 3 step: 528, loss is 0.012332137674093246\n",
      "epoch: 3 step: 529, loss is 0.0023619006387889385\n",
      "epoch: 3 step: 530, loss is 0.011701902374625206\n",
      "epoch: 3 step: 531, loss is 0.09296634793281555\n",
      "epoch: 3 step: 532, loss is 0.008230791427195072\n",
      "epoch: 3 step: 533, loss is 0.006002205424010754\n",
      "epoch: 3 step: 534, loss is 0.15309692919254303\n",
      "epoch: 3 step: 535, loss is 0.0009140666807070374\n",
      "epoch: 3 step: 536, loss is 0.010037828236818314\n",
      "epoch: 3 step: 537, loss is 0.008148626424372196\n",
      "epoch: 3 step: 538, loss is 0.0020756220910698175\n",
      "epoch: 3 step: 539, loss is 0.008460145443677902\n",
      "epoch: 3 step: 540, loss is 0.019107675179839134\n",
      "epoch: 3 step: 541, loss is 0.005029813852161169\n",
      "epoch: 3 step: 542, loss is 0.006516383029520512\n",
      "epoch: 3 step: 543, loss is 0.01314566470682621\n",
      "epoch: 3 step: 544, loss is 0.03798062354326248\n",
      "epoch: 3 step: 545, loss is 0.022577857598662376\n",
      "epoch: 3 step: 546, loss is 0.26027563214302063\n",
      "epoch: 3 step: 547, loss is 0.09000945836305618\n",
      "epoch: 3 step: 548, loss is 0.00015498168068006635\n",
      "epoch: 3 step: 549, loss is 0.01439259946346283\n",
      "epoch: 3 step: 550, loss is 0.006025323178619146\n",
      "epoch: 3 step: 551, loss is 0.22256812453269958\n",
      "epoch: 3 step: 552, loss is 0.001664640149101615\n",
      "epoch: 3 step: 553, loss is 0.003801144892349839\n",
      "epoch: 3 step: 554, loss is 0.005317665170878172\n",
      "epoch: 3 step: 555, loss is 0.18603400886058807\n",
      "epoch: 3 step: 556, loss is 0.0010708972113206983\n",
      "epoch: 3 step: 557, loss is 0.1829075813293457\n",
      "epoch: 3 step: 558, loss is 0.026021599769592285\n",
      "epoch: 3 step: 559, loss is 0.004211296793073416\n",
      "epoch: 3 step: 560, loss is 0.0022329408675432205\n",
      "epoch: 3 step: 561, loss is 0.0323772169649601\n",
      "epoch: 3 step: 562, loss is 0.038100969046354294\n",
      "epoch: 3 step: 563, loss is 0.05250025913119316\n",
      "epoch: 3 step: 564, loss is 0.006608178373426199\n",
      "epoch: 3 step: 565, loss is 0.20270149409770966\n",
      "epoch: 3 step: 566, loss is 0.008782228454947472\n",
      "epoch: 3 step: 567, loss is 0.0016613589832559228\n",
      "epoch: 3 step: 568, loss is 0.024159176275134087\n",
      "epoch: 3 step: 569, loss is 0.03261793777346611\n",
      "epoch: 3 step: 570, loss is 0.11087561398744583\n",
      "epoch: 3 step: 571, loss is 0.49557816982269287\n",
      "epoch: 3 step: 572, loss is 0.06549268215894699\n",
      "epoch: 3 step: 573, loss is 0.03572657331824303\n",
      "epoch: 3 step: 574, loss is 0.06164216622710228\n",
      "epoch: 3 step: 575, loss is 0.014790253713726997\n",
      "epoch: 3 step: 576, loss is 0.05310962349176407\n",
      "epoch: 3 step: 577, loss is 0.007183482404798269\n",
      "epoch: 3 step: 578, loss is 0.13015155494213104\n",
      "epoch: 3 step: 579, loss is 0.004769779276102781\n",
      "epoch: 3 step: 580, loss is 0.06201576441526413\n",
      "epoch: 3 step: 581, loss is 0.006406876258552074\n",
      "epoch: 3 step: 582, loss is 0.2166205793619156\n",
      "epoch: 3 step: 583, loss is 0.007665581535547972\n",
      "epoch: 3 step: 584, loss is 0.12433695793151855\n",
      "epoch: 3 step: 585, loss is 0.14041917026042938\n",
      "epoch: 3 step: 586, loss is 0.07364071160554886\n",
      "epoch: 3 step: 587, loss is 0.04409421235322952\n",
      "epoch: 3 step: 588, loss is 0.1337466537952423\n",
      "epoch: 3 step: 589, loss is 0.11358455568552017\n",
      "epoch: 3 step: 590, loss is 0.012317266315221786\n",
      "epoch: 3 step: 591, loss is 0.008307748474180698\n",
      "epoch: 3 step: 592, loss is 0.04534374177455902\n",
      "epoch: 3 step: 593, loss is 0.1402725726366043\n",
      "epoch: 3 step: 594, loss is 0.01784268394112587\n",
      "epoch: 3 step: 595, loss is 0.17217697203159332\n",
      "epoch: 3 step: 596, loss is 0.055471524596214294\n",
      "epoch: 3 step: 597, loss is 0.07499270886182785\n",
      "epoch: 3 step: 598, loss is 0.14116589725017548\n",
      "epoch: 3 step: 599, loss is 0.06442556530237198\n",
      "epoch: 3 step: 600, loss is 0.08273091912269592\n",
      "epoch: 3 step: 601, loss is 0.03302290663123131\n",
      "epoch: 3 step: 602, loss is 0.0061368257738649845\n",
      "epoch: 3 step: 603, loss is 0.02681923285126686\n",
      "epoch: 3 step: 604, loss is 0.028560496866703033\n",
      "epoch: 3 step: 605, loss is 0.2249411642551422\n",
      "epoch: 3 step: 606, loss is 0.016162680462002754\n",
      "epoch: 3 step: 607, loss is 0.056596651673316956\n",
      "epoch: 3 step: 608, loss is 0.01128835417330265\n",
      "epoch: 3 step: 609, loss is 0.08348929136991501\n",
      "epoch: 3 step: 610, loss is 0.010927348397672176\n",
      "epoch: 3 step: 611, loss is 0.009066308848559856\n",
      "epoch: 3 step: 612, loss is 0.002108990913257003\n",
      "epoch: 3 step: 613, loss is 0.13438652455806732\n",
      "epoch: 3 step: 614, loss is 0.01291720662266016\n",
      "epoch: 3 step: 615, loss is 0.22111362218856812\n",
      "epoch: 3 step: 616, loss is 0.013215916231274605\n",
      "epoch: 3 step: 617, loss is 0.02448890171945095\n",
      "epoch: 3 step: 618, loss is 0.04476232826709747\n",
      "epoch: 3 step: 619, loss is 0.020176159217953682\n",
      "epoch: 3 step: 620, loss is 0.11692604422569275\n",
      "epoch: 3 step: 621, loss is 0.0021555763669312\n",
      "epoch: 3 step: 622, loss is 0.10210245102643967\n",
      "epoch: 3 step: 623, loss is 0.019500961527228355\n",
      "epoch: 3 step: 624, loss is 0.007288329768925905\n",
      "epoch: 3 step: 625, loss is 0.015933437272906303\n",
      "epoch: 3 step: 626, loss is 0.051921144127845764\n",
      "epoch: 3 step: 627, loss is 0.004715376067906618\n",
      "epoch: 3 step: 628, loss is 0.004283587913960218\n",
      "epoch: 3 step: 629, loss is 0.08558740466833115\n",
      "epoch: 3 step: 630, loss is 0.008475145325064659\n",
      "epoch: 3 step: 631, loss is 0.17854104936122894\n",
      "epoch: 3 step: 632, loss is 0.004395639523863792\n",
      "epoch: 3 step: 633, loss is 0.004253108520060778\n",
      "epoch: 3 step: 634, loss is 0.003054223721846938\n",
      "epoch: 3 step: 635, loss is 0.14192107319831848\n",
      "epoch: 3 step: 636, loss is 0.524512767791748\n",
      "epoch: 3 step: 637, loss is 0.019039936363697052\n",
      "epoch: 3 step: 638, loss is 0.007183549925684929\n",
      "epoch: 3 step: 639, loss is 0.07913346588611603\n",
      "epoch: 3 step: 640, loss is 0.0032129576429724693\n",
      "epoch: 3 step: 641, loss is 0.006127019412815571\n",
      "epoch: 3 step: 642, loss is 0.051041379570961\n",
      "epoch: 3 step: 643, loss is 0.09676873683929443\n",
      "epoch: 3 step: 644, loss is 0.015389996580779552\n",
      "epoch: 3 step: 645, loss is 0.022415701299905777\n",
      "epoch: 3 step: 646, loss is 0.010154876857995987\n",
      "epoch: 3 step: 647, loss is 0.06956981122493744\n",
      "epoch: 3 step: 648, loss is 0.004789556842297316\n",
      "epoch: 3 step: 649, loss is 0.026414774358272552\n",
      "epoch: 3 step: 650, loss is 0.004728608764708042\n",
      "epoch: 3 step: 651, loss is 0.009819197468459606\n",
      "epoch: 3 step: 652, loss is 0.07228808104991913\n",
      "epoch: 3 step: 653, loss is 0.004546680487692356\n",
      "epoch: 3 step: 654, loss is 0.003144262358546257\n",
      "epoch: 3 step: 655, loss is 0.0040440429002046585\n",
      "epoch: 3 step: 656, loss is 0.02121358923614025\n",
      "epoch: 3 step: 657, loss is 0.11327788978815079\n",
      "epoch: 3 step: 658, loss is 0.08419464528560638\n",
      "epoch: 3 step: 659, loss is 0.1850166767835617\n",
      "epoch: 3 step: 660, loss is 0.00793127529323101\n",
      "epoch: 3 step: 661, loss is 0.10608742386102676\n",
      "epoch: 3 step: 662, loss is 0.03758598864078522\n",
      "epoch: 3 step: 663, loss is 0.02846979722380638\n",
      "epoch: 3 step: 664, loss is 0.025927601382136345\n",
      "epoch: 3 step: 665, loss is 0.007994395680725574\n",
      "epoch: 3 step: 666, loss is 0.0556463897228241\n",
      "epoch: 3 step: 667, loss is 0.15109024941921234\n",
      "epoch: 3 step: 668, loss is 0.0010065012611448765\n",
      "epoch: 3 step: 669, loss is 0.0048408969305455685\n",
      "epoch: 3 step: 670, loss is 0.12385964393615723\n",
      "epoch: 3 step: 671, loss is 0.11822241544723511\n",
      "epoch: 3 step: 672, loss is 0.01045263186097145\n",
      "epoch: 3 step: 673, loss is 0.0035519408993422985\n",
      "epoch: 3 step: 674, loss is 0.007551148533821106\n",
      "epoch: 3 step: 675, loss is 0.02631339244544506\n",
      "epoch: 3 step: 676, loss is 0.033821698278188705\n",
      "epoch: 3 step: 677, loss is 0.012909138575196266\n",
      "epoch: 3 step: 678, loss is 0.09982270002365112\n",
      "epoch: 3 step: 679, loss is 0.003669023048132658\n",
      "epoch: 3 step: 680, loss is 0.009774385020136833\n",
      "epoch: 3 step: 681, loss is 0.004929658491164446\n",
      "epoch: 3 step: 682, loss is 0.0014660704182460904\n",
      "epoch: 3 step: 683, loss is 0.13099025189876556\n",
      "epoch: 3 step: 684, loss is 0.0043989866971969604\n",
      "epoch: 3 step: 685, loss is 0.03592474386096001\n",
      "epoch: 3 step: 686, loss is 0.06134027615189552\n",
      "epoch: 3 step: 687, loss is 0.008763219229876995\n",
      "epoch: 3 step: 688, loss is 0.03558419644832611\n",
      "epoch: 3 step: 689, loss is 0.0092269666492939\n",
      "epoch: 3 step: 690, loss is 0.030112387612462044\n",
      "epoch: 3 step: 691, loss is 0.00038776439032517374\n",
      "epoch: 3 step: 692, loss is 0.003153367666527629\n",
      "epoch: 3 step: 693, loss is 0.002422147197648883\n",
      "epoch: 3 step: 694, loss is 0.0006657824269495904\n",
      "epoch: 3 step: 695, loss is 0.04836476594209671\n",
      "epoch: 3 step: 696, loss is 0.0014299883041530848\n",
      "epoch: 3 step: 697, loss is 0.017283041030168533\n",
      "epoch: 3 step: 698, loss is 0.024026600643992424\n",
      "epoch: 3 step: 699, loss is 0.1676137000322342\n",
      "epoch: 3 step: 700, loss is 0.23077932000160217\n",
      "epoch: 3 step: 701, loss is 0.0560573972761631\n",
      "epoch: 3 step: 702, loss is 0.052330441772937775\n",
      "epoch: 3 step: 703, loss is 0.02603936567902565\n",
      "epoch: 3 step: 704, loss is 0.003935659304261208\n",
      "epoch: 3 step: 705, loss is 0.13173729181289673\n",
      "epoch: 3 step: 706, loss is 0.12007705122232437\n",
      "epoch: 3 step: 707, loss is 0.056859374046325684\n",
      "epoch: 3 step: 708, loss is 0.0017928757006302476\n",
      "epoch: 3 step: 709, loss is 0.018315715715289116\n",
      "epoch: 3 step: 710, loss is 0.019126541912555695\n",
      "epoch: 3 step: 711, loss is 0.0116434246301651\n",
      "epoch: 3 step: 712, loss is 0.05410910025238991\n",
      "epoch: 3 step: 713, loss is 0.25381436944007874\n",
      "epoch: 3 step: 714, loss is 0.007528683636337519\n",
      "epoch: 3 step: 715, loss is 0.022243617102503777\n",
      "epoch: 3 step: 716, loss is 0.0022063341457396746\n",
      "epoch: 3 step: 717, loss is 0.5244196057319641\n",
      "epoch: 3 step: 718, loss is 0.0026236854027956724\n",
      "epoch: 3 step: 719, loss is 0.015136810019612312\n",
      "epoch: 3 step: 720, loss is 0.0035180742852389812\n",
      "epoch: 3 step: 721, loss is 0.04487162455916405\n",
      "epoch: 3 step: 722, loss is 0.02420433610677719\n",
      "epoch: 3 step: 723, loss is 0.00661337049677968\n",
      "epoch: 3 step: 724, loss is 0.005559684708714485\n",
      "epoch: 3 step: 725, loss is 0.004202814772725105\n",
      "epoch: 3 step: 726, loss is 0.11483263969421387\n",
      "epoch: 3 step: 727, loss is 0.004058075603097677\n",
      "epoch: 3 step: 728, loss is 0.007142397575080395\n",
      "epoch: 3 step: 729, loss is 0.14840315282344818\n",
      "epoch: 3 step: 730, loss is 0.017875568941235542\n",
      "epoch: 3 step: 731, loss is 0.006640268489718437\n",
      "epoch: 3 step: 732, loss is 0.046922069042921066\n",
      "epoch: 3 step: 733, loss is 0.003987579140812159\n",
      "epoch: 3 step: 734, loss is 0.06325546652078629\n",
      "epoch: 3 step: 735, loss is 0.002076119650155306\n",
      "epoch: 3 step: 736, loss is 0.009222674183547497\n",
      "epoch: 3 step: 737, loss is 0.015247656032443047\n",
      "epoch: 3 step: 738, loss is 0.008520777337253094\n",
      "epoch: 3 step: 739, loss is 0.13160118460655212\n",
      "epoch: 3 step: 740, loss is 0.0085990559309721\n",
      "epoch: 3 step: 741, loss is 0.004809083417057991\n",
      "epoch: 3 step: 742, loss is 0.06021755561232567\n",
      "epoch: 3 step: 743, loss is 0.03882945701479912\n",
      "epoch: 3 step: 744, loss is 0.009308744221925735\n",
      "epoch: 3 step: 745, loss is 0.11411243677139282\n",
      "epoch: 3 step: 746, loss is 0.07177577167749405\n",
      "epoch: 3 step: 747, loss is 0.0012553257402032614\n",
      "epoch: 3 step: 748, loss is 0.003956374246627092\n",
      "epoch: 3 step: 749, loss is 0.003652395447716117\n",
      "epoch: 3 step: 750, loss is 0.005008349660784006\n",
      "epoch: 3 step: 751, loss is 0.07280159741640091\n",
      "epoch: 3 step: 752, loss is 0.030425824224948883\n",
      "epoch: 3 step: 753, loss is 0.002381783677265048\n",
      "epoch: 3 step: 754, loss is 0.09685033559799194\n",
      "epoch: 3 step: 755, loss is 0.046275652945041656\n",
      "epoch: 3 step: 756, loss is 0.005382430274039507\n",
      "epoch: 3 step: 757, loss is 0.0038985456340014935\n",
      "epoch: 3 step: 758, loss is 0.04854441061615944\n",
      "epoch: 3 step: 759, loss is 0.05819268524646759\n",
      "epoch: 3 step: 760, loss is 0.009890669956803322\n",
      "epoch: 3 step: 761, loss is 0.007331467233598232\n",
      "epoch: 3 step: 762, loss is 0.00604213448241353\n",
      "epoch: 3 step: 763, loss is 0.2479877769947052\n",
      "epoch: 3 step: 764, loss is 0.0020126341842114925\n",
      "epoch: 3 step: 765, loss is 0.13916312158107758\n",
      "epoch: 3 step: 766, loss is 0.030065493658185005\n",
      "epoch: 3 step: 767, loss is 0.0043187933042645454\n",
      "epoch: 3 step: 768, loss is 0.05630466341972351\n",
      "epoch: 3 step: 769, loss is 0.01259678229689598\n",
      "epoch: 3 step: 770, loss is 0.016077155247330666\n",
      "epoch: 3 step: 771, loss is 0.023511853069067\n",
      "epoch: 3 step: 772, loss is 0.002830160316079855\n",
      "epoch: 3 step: 773, loss is 0.06595407426357269\n",
      "epoch: 3 step: 774, loss is 0.05148542299866676\n",
      "epoch: 3 step: 775, loss is 0.2485419362783432\n",
      "epoch: 3 step: 776, loss is 0.013291544280946255\n",
      "epoch: 3 step: 777, loss is 0.009347090497612953\n",
      "epoch: 3 step: 778, loss is 0.011358179152011871\n",
      "epoch: 3 step: 779, loss is 0.007031960878521204\n",
      "epoch: 3 step: 780, loss is 0.11626490205526352\n",
      "epoch: 3 step: 781, loss is 0.015816720202565193\n",
      "epoch: 3 step: 782, loss is 0.02919609658420086\n",
      "epoch: 3 step: 783, loss is 0.006585353519767523\n",
      "epoch: 3 step: 784, loss is 0.001454572076909244\n",
      "epoch: 3 step: 785, loss is 0.04066474363207817\n",
      "epoch: 3 step: 786, loss is 0.036939576268196106\n",
      "epoch: 3 step: 787, loss is 0.4385471045970917\n",
      "epoch: 3 step: 788, loss is 0.01577857881784439\n",
      "epoch: 3 step: 789, loss is 0.022994859144091606\n",
      "epoch: 3 step: 790, loss is 0.01631810888648033\n",
      "epoch: 3 step: 791, loss is 0.011031987145543098\n",
      "epoch: 3 step: 792, loss is 0.12158013135194778\n",
      "epoch: 3 step: 793, loss is 0.00803404301404953\n",
      "epoch: 3 step: 794, loss is 0.07237052917480469\n",
      "epoch: 3 step: 795, loss is 0.012761133722960949\n",
      "epoch: 3 step: 796, loss is 0.08461298048496246\n",
      "epoch: 3 step: 797, loss is 0.013194863684475422\n",
      "epoch: 3 step: 798, loss is 0.0013254042714834213\n",
      "epoch: 3 step: 799, loss is 0.03181443363428116\n",
      "epoch: 3 step: 800, loss is 0.027377529069781303\n",
      "epoch: 3 step: 801, loss is 0.052165113389492035\n",
      "epoch: 3 step: 802, loss is 0.04601598531007767\n",
      "epoch: 3 step: 803, loss is 0.048655226826667786\n",
      "epoch: 3 step: 804, loss is 0.06006115674972534\n",
      "epoch: 3 step: 805, loss is 0.020845703780651093\n",
      "epoch: 3 step: 806, loss is 0.01688489131629467\n",
      "epoch: 3 step: 807, loss is 0.0025508273392915726\n",
      "epoch: 3 step: 808, loss is 0.2963526248931885\n",
      "epoch: 3 step: 809, loss is 0.002962314523756504\n",
      "epoch: 3 step: 810, loss is 0.0320417694747448\n",
      "epoch: 3 step: 811, loss is 0.020626986399292946\n",
      "epoch: 3 step: 812, loss is 0.008562728762626648\n",
      "epoch: 3 step: 813, loss is 0.03665328770875931\n",
      "epoch: 3 step: 814, loss is 0.001532724592834711\n",
      "epoch: 3 step: 815, loss is 0.03302763029932976\n",
      "epoch: 3 step: 816, loss is 0.06092214211821556\n",
      "epoch: 3 step: 817, loss is 0.005213723052293062\n",
      "epoch: 3 step: 818, loss is 0.06153469160199165\n",
      "epoch: 3 step: 819, loss is 0.058603521436452866\n",
      "epoch: 3 step: 820, loss is 0.016849873587489128\n",
      "epoch: 3 step: 821, loss is 0.0007389287929981947\n",
      "epoch: 3 step: 822, loss is 0.024431753903627396\n",
      "epoch: 3 step: 823, loss is 0.0949905514717102\n",
      "epoch: 3 step: 824, loss is 0.25139251351356506\n",
      "epoch: 3 step: 825, loss is 0.06884858757257462\n",
      "epoch: 3 step: 826, loss is 0.05543822422623634\n",
      "epoch: 3 step: 827, loss is 0.009229938499629498\n",
      "epoch: 3 step: 828, loss is 0.011027456261217594\n",
      "epoch: 3 step: 829, loss is 0.15614059567451477\n",
      "epoch: 3 step: 830, loss is 0.031608205288648605\n",
      "epoch: 3 step: 831, loss is 0.028915561735630035\n",
      "epoch: 3 step: 832, loss is 0.00952407717704773\n",
      "epoch: 3 step: 833, loss is 0.15853238105773926\n",
      "epoch: 3 step: 834, loss is 0.10469763725996017\n",
      "epoch: 3 step: 835, loss is 0.03518445044755936\n",
      "epoch: 3 step: 836, loss is 0.0300297848880291\n",
      "epoch: 3 step: 837, loss is 0.008658837527036667\n",
      "epoch: 3 step: 838, loss is 0.011538774706423283\n",
      "epoch: 3 step: 839, loss is 0.012532699853181839\n",
      "epoch: 3 step: 840, loss is 0.024301907047629356\n",
      "epoch: 3 step: 841, loss is 0.010762350633740425\n",
      "epoch: 3 step: 842, loss is 0.006373424548655748\n",
      "epoch: 3 step: 843, loss is 0.014053015038371086\n",
      "epoch: 3 step: 844, loss is 0.48768356442451477\n",
      "epoch: 3 step: 845, loss is 0.024235989898443222\n",
      "epoch: 3 step: 846, loss is 0.055984385311603546\n",
      "epoch: 3 step: 847, loss is 0.007106466218829155\n",
      "epoch: 3 step: 848, loss is 0.12338066846132278\n",
      "epoch: 3 step: 849, loss is 0.05789107456803322\n",
      "epoch: 3 step: 850, loss is 0.007408191449940205\n",
      "epoch: 3 step: 851, loss is 0.009095834568142891\n",
      "epoch: 3 step: 852, loss is 0.04080824553966522\n",
      "epoch: 3 step: 853, loss is 0.07100068032741547\n",
      "epoch: 3 step: 854, loss is 0.19744093716144562\n",
      "epoch: 3 step: 855, loss is 0.052547406405210495\n",
      "epoch: 3 step: 856, loss is 0.031232088804244995\n",
      "epoch: 3 step: 857, loss is 0.015040691941976547\n",
      "epoch: 3 step: 858, loss is 0.024140749126672745\n",
      "epoch: 3 step: 859, loss is 0.007226512301713228\n",
      "epoch: 3 step: 860, loss is 0.03645099326968193\n",
      "epoch: 3 step: 861, loss is 0.005416338797658682\n",
      "epoch: 3 step: 862, loss is 0.003537215990945697\n",
      "epoch: 3 step: 863, loss is 0.0014073947677388787\n",
      "epoch: 3 step: 864, loss is 0.1261647641658783\n",
      "epoch: 3 step: 865, loss is 0.0005607562488876283\n",
      "epoch: 3 step: 866, loss is 0.11825370043516159\n",
      "epoch: 3 step: 867, loss is 0.006224818527698517\n",
      "epoch: 3 step: 868, loss is 0.06364801526069641\n",
      "epoch: 3 step: 869, loss is 0.005375161301344633\n",
      "epoch: 3 step: 870, loss is 0.03840424492955208\n",
      "epoch: 3 step: 871, loss is 0.02357226423919201\n",
      "epoch: 3 step: 872, loss is 0.11478934437036514\n",
      "epoch: 3 step: 873, loss is 0.04342757165431976\n",
      "epoch: 3 step: 874, loss is 0.0024174994323402643\n",
      "epoch: 3 step: 875, loss is 0.0048668826930224895\n",
      "epoch: 3 step: 876, loss is 0.01687120459973812\n",
      "epoch: 3 step: 877, loss is 0.008484574034810066\n",
      "epoch: 3 step: 878, loss is 0.027213063091039658\n",
      "epoch: 3 step: 879, loss is 0.02764768712222576\n",
      "epoch: 3 step: 880, loss is 0.09297256171703339\n",
      "epoch: 3 step: 881, loss is 0.0870700255036354\n",
      "epoch: 3 step: 882, loss is 0.003991741221398115\n",
      "epoch: 3 step: 883, loss is 0.12405600398778915\n",
      "epoch: 3 step: 884, loss is 0.017689628526568413\n",
      "epoch: 3 step: 885, loss is 0.026645472273230553\n",
      "epoch: 3 step: 886, loss is 0.0010824240744113922\n",
      "epoch: 3 step: 887, loss is 0.07339257001876831\n",
      "epoch: 3 step: 888, loss is 0.0013806255301460624\n",
      "epoch: 3 step: 889, loss is 0.014208812266588211\n",
      "epoch: 3 step: 890, loss is 0.02308483049273491\n",
      "epoch: 3 step: 891, loss is 0.009456133469939232\n",
      "epoch: 3 step: 892, loss is 0.19937150180339813\n",
      "epoch: 3 step: 893, loss is 0.021458109840750694\n",
      "epoch: 3 step: 894, loss is 0.011442047543823719\n",
      "epoch: 3 step: 895, loss is 0.005977472756057978\n",
      "epoch: 3 step: 896, loss is 0.2195088267326355\n",
      "epoch: 3 step: 897, loss is 0.010632893070578575\n",
      "epoch: 3 step: 898, loss is 0.14522738754749298\n",
      "epoch: 3 step: 899, loss is 0.009064139798283577\n",
      "epoch: 3 step: 900, loss is 0.0024271621368825436\n",
      "epoch: 3 step: 901, loss is 0.011744599789381027\n",
      "epoch: 3 step: 902, loss is 0.025578292086720467\n",
      "epoch: 3 step: 903, loss is 0.0017103231512010098\n",
      "epoch: 3 step: 904, loss is 0.08823169767856598\n",
      "epoch: 3 step: 905, loss is 0.002050099428743124\n",
      "epoch: 3 step: 906, loss is 0.004501359071582556\n",
      "epoch: 3 step: 907, loss is 0.001617048284970224\n",
      "epoch: 3 step: 908, loss is 0.0037908044178038836\n",
      "epoch: 3 step: 909, loss is 0.12488880753517151\n",
      "epoch: 3 step: 910, loss is 0.008723199367523193\n",
      "epoch: 3 step: 911, loss is 0.0019306280883029103\n",
      "epoch: 3 step: 912, loss is 0.07516174763441086\n",
      "epoch: 3 step: 913, loss is 0.015490645542740822\n",
      "epoch: 3 step: 914, loss is 0.001569138839840889\n",
      "epoch: 3 step: 915, loss is 0.004725164268165827\n",
      "epoch: 3 step: 916, loss is 0.03000170923769474\n",
      "epoch: 3 step: 917, loss is 0.05643271654844284\n",
      "epoch: 3 step: 918, loss is 0.011041832156479359\n",
      "epoch: 3 step: 919, loss is 0.08441662043333054\n",
      "epoch: 3 step: 920, loss is 0.04358597844839096\n",
      "epoch: 3 step: 921, loss is 0.0026348952669650316\n",
      "epoch: 3 step: 922, loss is 0.0010005903895944357\n",
      "epoch: 3 step: 923, loss is 0.029021158814430237\n",
      "epoch: 3 step: 924, loss is 0.004432059824466705\n",
      "epoch: 3 step: 925, loss is 0.0028226154390722513\n",
      "epoch: 3 step: 926, loss is 0.003899788251146674\n",
      "epoch: 3 step: 927, loss is 0.03812393918633461\n",
      "epoch: 3 step: 928, loss is 0.16596250236034393\n",
      "epoch: 3 step: 929, loss is 0.009842807427048683\n",
      "epoch: 3 step: 930, loss is 0.014612803235650063\n",
      "epoch: 3 step: 931, loss is 0.10343000292778015\n",
      "epoch: 3 step: 932, loss is 0.0036301123909652233\n",
      "epoch: 3 step: 933, loss is 0.12563300132751465\n",
      "epoch: 3 step: 934, loss is 0.023207377642393112\n",
      "epoch: 3 step: 935, loss is 0.014617242850363255\n",
      "epoch: 3 step: 936, loss is 0.07102008908987045\n",
      "epoch: 3 step: 937, loss is 0.3230154812335968\n",
      "epoch: 3 step: 938, loss is 0.10037408769130707\n",
      "epoch: 3 step: 939, loss is 0.34908774495124817\n",
      "epoch: 3 step: 940, loss is 0.002010697964578867\n",
      "epoch: 3 step: 941, loss is 0.001956386724486947\n",
      "epoch: 3 step: 942, loss is 0.24102532863616943\n",
      "epoch: 3 step: 943, loss is 0.16940395534038544\n",
      "epoch: 3 step: 944, loss is 0.005948224570602179\n",
      "epoch: 3 step: 945, loss is 0.05440015718340874\n",
      "epoch: 3 step: 946, loss is 0.045045364648103714\n",
      "epoch: 3 step: 947, loss is 0.0051633445546031\n",
      "epoch: 3 step: 948, loss is 0.051349952816963196\n",
      "epoch: 3 step: 949, loss is 0.004892273340374231\n",
      "epoch: 3 step: 950, loss is 0.011104553006589413\n",
      "epoch: 3 step: 951, loss is 0.32774507999420166\n",
      "epoch: 3 step: 952, loss is 0.060125064104795456\n",
      "epoch: 3 step: 953, loss is 0.03669849410653114\n",
      "epoch: 3 step: 954, loss is 0.08857514709234238\n",
      "epoch: 3 step: 955, loss is 0.007594590540975332\n",
      "epoch: 3 step: 956, loss is 0.006596206221729517\n",
      "epoch: 3 step: 957, loss is 0.007976938039064407\n",
      "epoch: 3 step: 958, loss is 0.0315113291144371\n",
      "epoch: 3 step: 959, loss is 0.01691318117082119\n",
      "epoch: 3 step: 960, loss is 0.05815501511096954\n",
      "epoch: 3 step: 961, loss is 0.008088630624115467\n",
      "epoch: 3 step: 962, loss is 0.008071674965322018\n",
      "epoch: 3 step: 963, loss is 0.007643700111657381\n",
      "epoch: 3 step: 964, loss is 0.007510377559810877\n",
      "epoch: 3 step: 965, loss is 0.072084441781044\n",
      "epoch: 3 step: 966, loss is 0.008995188400149345\n",
      "epoch: 3 step: 967, loss is 0.03017859347164631\n",
      "epoch: 3 step: 968, loss is 0.013180348090827465\n",
      "epoch: 3 step: 969, loss is 0.0022216765210032463\n",
      "epoch: 3 step: 970, loss is 0.021326174959540367\n",
      "epoch: 3 step: 971, loss is 0.03721608221530914\n",
      "epoch: 3 step: 972, loss is 0.028610270470380783\n",
      "epoch: 3 step: 973, loss is 0.009601556695997715\n",
      "epoch: 3 step: 974, loss is 0.0010344116017222404\n",
      "epoch: 3 step: 975, loss is 0.050111278891563416\n",
      "epoch: 3 step: 976, loss is 0.06142101809382439\n",
      "epoch: 3 step: 977, loss is 0.00830298662185669\n",
      "epoch: 3 step: 978, loss is 0.0047444733791053295\n",
      "epoch: 3 step: 979, loss is 0.002455849666148424\n",
      "epoch: 3 step: 980, loss is 0.002555695828050375\n",
      "epoch: 3 step: 981, loss is 0.04293394833803177\n",
      "epoch: 3 step: 982, loss is 0.006344360765069723\n",
      "epoch: 3 step: 983, loss is 0.0016455366276204586\n",
      "epoch: 3 step: 984, loss is 0.013629880733788013\n",
      "epoch: 3 step: 985, loss is 0.001514206174761057\n",
      "epoch: 3 step: 986, loss is 0.02959432080388069\n",
      "epoch: 3 step: 987, loss is 0.015120514668524265\n",
      "epoch: 3 step: 988, loss is 0.007070471066981554\n",
      "epoch: 3 step: 989, loss is 0.007709503639489412\n",
      "epoch: 3 step: 990, loss is 0.0062992507591843605\n",
      "epoch: 3 step: 991, loss is 0.0037936668377369642\n",
      "epoch: 3 step: 992, loss is 0.029295165091753006\n",
      "epoch: 3 step: 993, loss is 0.0015647459076717496\n",
      "epoch: 3 step: 994, loss is 0.034534282982349396\n",
      "epoch: 3 step: 995, loss is 0.0065375217236578465\n",
      "epoch: 3 step: 996, loss is 0.0012273779138922691\n",
      "epoch: 3 step: 997, loss is 0.0047592418268322945\n",
      "epoch: 3 step: 998, loss is 0.0702141597867012\n",
      "epoch: 3 step: 999, loss is 0.0021770282182842493\n",
      "epoch: 3 step: 1000, loss is 0.0533025823533535\n",
      "epoch: 3 step: 1001, loss is 0.008661671541631222\n",
      "epoch: 3 step: 1002, loss is 0.1077139750123024\n",
      "epoch: 3 step: 1003, loss is 0.03227744996547699\n",
      "epoch: 3 step: 1004, loss is 0.016151124611496925\n",
      "epoch: 3 step: 1005, loss is 0.05817517265677452\n",
      "epoch: 3 step: 1006, loss is 0.002089997986331582\n",
      "epoch: 3 step: 1007, loss is 0.008852476254105568\n",
      "epoch: 3 step: 1008, loss is 0.005152770783752203\n",
      "epoch: 3 step: 1009, loss is 0.028144489973783493\n",
      "epoch: 3 step: 1010, loss is 0.11955438554286957\n",
      "epoch: 3 step: 1011, loss is 0.00023996419622562826\n",
      "epoch: 3 step: 1012, loss is 0.0006963791674934328\n",
      "epoch: 3 step: 1013, loss is 0.006645803339779377\n",
      "epoch: 3 step: 1014, loss is 0.0031151489820331335\n",
      "epoch: 3 step: 1015, loss is 0.019846361130475998\n",
      "epoch: 3 step: 1016, loss is 0.010133631527423859\n",
      "epoch: 3 step: 1017, loss is 0.010844233445823193\n",
      "epoch: 3 step: 1018, loss is 0.08686874061822891\n",
      "epoch: 3 step: 1019, loss is 0.002513943240046501\n",
      "epoch: 3 step: 1020, loss is 0.0019004950299859047\n",
      "epoch: 3 step: 1021, loss is 0.04954799264669418\n",
      "epoch: 3 step: 1022, loss is 0.030846750363707542\n",
      "epoch: 3 step: 1023, loss is 0.009245133958756924\n",
      "epoch: 3 step: 1024, loss is 0.0022020910400897264\n",
      "epoch: 3 step: 1025, loss is 0.10441417247056961\n",
      "epoch: 3 step: 1026, loss is 0.004224372562021017\n",
      "epoch: 3 step: 1027, loss is 0.05498385801911354\n",
      "epoch: 3 step: 1028, loss is 0.13876883685588837\n",
      "epoch: 3 step: 1029, loss is 0.015906160697340965\n",
      "epoch: 3 step: 1030, loss is 0.002900495892390609\n",
      "epoch: 3 step: 1031, loss is 0.04369400069117546\n",
      "epoch: 3 step: 1032, loss is 0.010746386833488941\n",
      "epoch: 3 step: 1033, loss is 0.008103253319859505\n",
      "epoch: 3 step: 1034, loss is 0.05569877848029137\n",
      "epoch: 3 step: 1035, loss is 0.007521723862737417\n",
      "epoch: 3 step: 1036, loss is 0.0024681142531335354\n",
      "epoch: 3 step: 1037, loss is 0.0017843241803348064\n",
      "epoch: 3 step: 1038, loss is 0.0015811810735613108\n",
      "epoch: 3 step: 1039, loss is 0.026232905685901642\n",
      "epoch: 3 step: 1040, loss is 0.0013468993129208684\n",
      "epoch: 3 step: 1041, loss is 0.00098017870914191\n",
      "epoch: 3 step: 1042, loss is 0.0695100799202919\n",
      "epoch: 3 step: 1043, loss is 0.07491420209407806\n",
      "epoch: 3 step: 1044, loss is 0.0346870943903923\n",
      "epoch: 3 step: 1045, loss is 0.02797148935496807\n",
      "epoch: 3 step: 1046, loss is 0.0672188475728035\n",
      "epoch: 3 step: 1047, loss is 0.07807004451751709\n",
      "epoch: 3 step: 1048, loss is 0.054751086980104446\n",
      "epoch: 3 step: 1049, loss is 0.005644448101520538\n",
      "epoch: 3 step: 1050, loss is 0.00861874409019947\n",
      "epoch: 3 step: 1051, loss is 0.02921653911471367\n",
      "epoch: 3 step: 1052, loss is 0.0022655390202999115\n",
      "epoch: 3 step: 1053, loss is 0.010010833851993084\n",
      "epoch: 3 step: 1054, loss is 0.22982677817344666\n",
      "epoch: 3 step: 1055, loss is 0.03474448621273041\n",
      "epoch: 3 step: 1056, loss is 0.2610796391963959\n",
      "epoch: 3 step: 1057, loss is 0.00021620167535729706\n",
      "epoch: 3 step: 1058, loss is 0.016171816736459732\n",
      "epoch: 3 step: 1059, loss is 0.052136510610580444\n",
      "epoch: 3 step: 1060, loss is 0.012267477810382843\n",
      "epoch: 3 step: 1061, loss is 0.026734434068202972\n",
      "epoch: 3 step: 1062, loss is 0.10667693614959717\n",
      "epoch: 3 step: 1063, loss is 0.00531112402677536\n",
      "epoch: 3 step: 1064, loss is 0.0033069923520088196\n",
      "epoch: 3 step: 1065, loss is 0.0010825342033058405\n",
      "epoch: 3 step: 1066, loss is 0.3069548010826111\n",
      "epoch: 3 step: 1067, loss is 0.004447985906153917\n",
      "epoch: 3 step: 1068, loss is 0.06848519295454025\n",
      "epoch: 3 step: 1069, loss is 0.03938784822821617\n",
      "epoch: 3 step: 1070, loss is 0.07390756905078888\n",
      "epoch: 3 step: 1071, loss is 0.3071533441543579\n",
      "epoch: 3 step: 1072, loss is 0.1532101035118103\n",
      "epoch: 3 step: 1073, loss is 0.013949754647910595\n",
      "epoch: 3 step: 1074, loss is 0.26619234681129456\n",
      "epoch: 3 step: 1075, loss is 0.05198878049850464\n",
      "epoch: 3 step: 1076, loss is 0.0008640036685392261\n",
      "epoch: 3 step: 1077, loss is 0.09938812255859375\n",
      "epoch: 3 step: 1078, loss is 0.003185548819601536\n",
      "epoch: 3 step: 1079, loss is 0.0021679236087948084\n",
      "epoch: 3 step: 1080, loss is 0.05501572787761688\n",
      "epoch: 3 step: 1081, loss is 0.031346291303634644\n",
      "epoch: 3 step: 1082, loss is 0.10301060229539871\n",
      "epoch: 3 step: 1083, loss is 0.03968222066760063\n",
      "epoch: 3 step: 1084, loss is 0.12660884857177734\n",
      "epoch: 3 step: 1085, loss is 0.014106623828411102\n",
      "epoch: 3 step: 1086, loss is 0.04590677469968796\n",
      "epoch: 3 step: 1087, loss is 0.1217961385846138\n",
      "epoch: 3 step: 1088, loss is 0.1343279480934143\n",
      "epoch: 3 step: 1089, loss is 0.002358030527830124\n",
      "epoch: 3 step: 1090, loss is 0.02292332425713539\n",
      "epoch: 3 step: 1091, loss is 0.03628106415271759\n",
      "epoch: 3 step: 1092, loss is 0.01387691218405962\n",
      "epoch: 3 step: 1093, loss is 0.1889914721250534\n",
      "epoch: 3 step: 1094, loss is 0.22363512217998505\n",
      "epoch: 3 step: 1095, loss is 0.13680125772953033\n",
      "epoch: 3 step: 1096, loss is 0.010955443605780602\n",
      "epoch: 3 step: 1097, loss is 0.030640725046396255\n",
      "epoch: 3 step: 1098, loss is 0.052118975669145584\n",
      "epoch: 3 step: 1099, loss is 0.09289248287677765\n",
      "epoch: 3 step: 1100, loss is 0.0016118834028020501\n",
      "epoch: 3 step: 1101, loss is 0.09134237468242645\n",
      "epoch: 3 step: 1102, loss is 0.03537825495004654\n",
      "epoch: 3 step: 1103, loss is 0.004456542897969484\n",
      "epoch: 3 step: 1104, loss is 0.021371381357312202\n",
      "epoch: 3 step: 1105, loss is 0.013031055219471455\n",
      "epoch: 3 step: 1106, loss is 0.11598804593086243\n",
      "epoch: 3 step: 1107, loss is 0.025067854672670364\n",
      "epoch: 3 step: 1108, loss is 0.13270074129104614\n",
      "epoch: 3 step: 1109, loss is 0.022897372022271156\n",
      "epoch: 3 step: 1110, loss is 0.07318472117185593\n",
      "epoch: 3 step: 1111, loss is 0.006612984463572502\n",
      "epoch: 3 step: 1112, loss is 0.033281099051237106\n",
      "epoch: 3 step: 1113, loss is 0.05131995305418968\n",
      "epoch: 3 step: 1114, loss is 0.07147330790758133\n",
      "epoch: 3 step: 1115, loss is 0.008921976201236248\n",
      "epoch: 3 step: 1116, loss is 0.002807432087138295\n",
      "epoch: 3 step: 1117, loss is 0.009741755202412605\n",
      "epoch: 3 step: 1118, loss is 0.0009770998731255531\n",
      "epoch: 3 step: 1119, loss is 0.0786556750535965\n",
      "epoch: 3 step: 1120, loss is 0.022674182429909706\n",
      "epoch: 3 step: 1121, loss is 0.06144660711288452\n",
      "epoch: 3 step: 1122, loss is 0.012926971539855003\n",
      "epoch: 3 step: 1123, loss is 0.02292410098016262\n",
      "epoch: 3 step: 1124, loss is 0.05802878364920616\n",
      "epoch: 3 step: 1125, loss is 0.1059168130159378\n",
      "epoch: 3 step: 1126, loss is 0.19273248314857483\n",
      "epoch: 3 step: 1127, loss is 0.0067555042915046215\n",
      "epoch: 3 step: 1128, loss is 0.03207492455840111\n",
      "epoch: 3 step: 1129, loss is 0.009336150251328945\n",
      "epoch: 3 step: 1130, loss is 0.007827057503163815\n",
      "epoch: 3 step: 1131, loss is 0.02315426804125309\n",
      "epoch: 3 step: 1132, loss is 0.006385645363479853\n",
      "epoch: 3 step: 1133, loss is 0.0028581388760358095\n",
      "epoch: 3 step: 1134, loss is 0.007495536468923092\n",
      "epoch: 3 step: 1135, loss is 0.1320267617702484\n",
      "epoch: 3 step: 1136, loss is 0.10903037339448929\n",
      "epoch: 3 step: 1137, loss is 0.030553439632058144\n",
      "epoch: 3 step: 1138, loss is 0.1533588171005249\n",
      "epoch: 3 step: 1139, loss is 0.02140670455992222\n",
      "epoch: 3 step: 1140, loss is 0.08321639150381088\n",
      "epoch: 3 step: 1141, loss is 0.011356798000633717\n",
      "epoch: 3 step: 1142, loss is 0.11937133967876434\n",
      "epoch: 3 step: 1143, loss is 0.04634009301662445\n",
      "epoch: 3 step: 1144, loss is 0.01896996609866619\n",
      "epoch: 3 step: 1145, loss is 0.08411592245101929\n",
      "epoch: 3 step: 1146, loss is 0.05094623565673828\n",
      "epoch: 3 step: 1147, loss is 0.1261148452758789\n",
      "epoch: 3 step: 1148, loss is 0.009509009309113026\n",
      "epoch: 3 step: 1149, loss is 0.019694175571203232\n",
      "epoch: 3 step: 1150, loss is 0.0013816378777846694\n",
      "epoch: 3 step: 1151, loss is 0.020502550527453423\n",
      "epoch: 3 step: 1152, loss is 0.009838882833719254\n",
      "epoch: 3 step: 1153, loss is 0.0015705055557191372\n",
      "epoch: 3 step: 1154, loss is 0.000886480207554996\n",
      "epoch: 3 step: 1155, loss is 0.002604424487799406\n",
      "epoch: 3 step: 1156, loss is 0.002831004559993744\n",
      "epoch: 3 step: 1157, loss is 0.1205330342054367\n",
      "epoch: 3 step: 1158, loss is 0.1066933423280716\n",
      "epoch: 3 step: 1159, loss is 0.05738368257880211\n",
      "epoch: 3 step: 1160, loss is 0.001321102725341916\n",
      "epoch: 3 step: 1161, loss is 0.011962159536778927\n",
      "epoch: 3 step: 1162, loss is 0.04088405519723892\n",
      "epoch: 3 step: 1163, loss is 0.04220486804842949\n",
      "epoch: 3 step: 1164, loss is 0.004570615012198687\n",
      "epoch: 3 step: 1165, loss is 0.006979511119425297\n",
      "epoch: 3 step: 1166, loss is 0.14308074116706848\n",
      "epoch: 3 step: 1167, loss is 0.0816827267408371\n",
      "epoch: 3 step: 1168, loss is 0.0021468931809067726\n",
      "epoch: 3 step: 1169, loss is 0.08640670031309128\n",
      "epoch: 3 step: 1170, loss is 0.0011182541493326426\n",
      "epoch: 3 step: 1171, loss is 0.060901839286088943\n",
      "epoch: 3 step: 1172, loss is 0.026109935715794563\n",
      "epoch: 3 step: 1173, loss is 0.12086142599582672\n",
      "epoch: 3 step: 1174, loss is 0.02586745284497738\n",
      "epoch: 3 step: 1175, loss is 0.008356364443898201\n",
      "epoch: 3 step: 1176, loss is 0.013162572868168354\n",
      "epoch: 3 step: 1177, loss is 0.0027167191728949547\n",
      "epoch: 3 step: 1178, loss is 0.1358506977558136\n",
      "epoch: 3 step: 1179, loss is 0.009374482557177544\n",
      "epoch: 3 step: 1180, loss is 0.09993205964565277\n",
      "epoch: 3 step: 1181, loss is 0.022283809259533882\n",
      "epoch: 3 step: 1182, loss is 0.0036141537129878998\n",
      "epoch: 3 step: 1183, loss is 0.015920210629701614\n",
      "epoch: 3 step: 1184, loss is 0.011673296801745892\n",
      "epoch: 3 step: 1185, loss is 0.016332823783159256\n",
      "epoch: 3 step: 1186, loss is 0.24228093028068542\n",
      "epoch: 3 step: 1187, loss is 0.08272511512041092\n",
      "epoch: 3 step: 1188, loss is 0.005465635564178228\n",
      "epoch: 3 step: 1189, loss is 0.02405911684036255\n",
      "epoch: 3 step: 1190, loss is 0.1265457570552826\n",
      "epoch: 3 step: 1191, loss is 0.008557034656405449\n",
      "epoch: 3 step: 1192, loss is 0.03360645845532417\n",
      "epoch: 3 step: 1193, loss is 0.007364998571574688\n",
      "epoch: 3 step: 1194, loss is 0.022467030212283134\n",
      "epoch: 3 step: 1195, loss is 0.04366075620055199\n",
      "epoch: 3 step: 1196, loss is 0.002609801013022661\n",
      "epoch: 3 step: 1197, loss is 0.008678875863552094\n",
      "epoch: 3 step: 1198, loss is 0.04349102824926376\n",
      "epoch: 3 step: 1199, loss is 0.17363767325878143\n",
      "epoch: 3 step: 1200, loss is 0.1741000860929489\n",
      "epoch: 3 step: 1201, loss is 0.0006847484037280083\n",
      "epoch: 3 step: 1202, loss is 0.03626920282840729\n",
      "epoch: 3 step: 1203, loss is 0.00281004561111331\n",
      "epoch: 3 step: 1204, loss is 0.33601802587509155\n",
      "epoch: 3 step: 1205, loss is 0.02300436608493328\n",
      "epoch: 3 step: 1206, loss is 0.11771947145462036\n",
      "epoch: 3 step: 1207, loss is 0.031152093783020973\n",
      "epoch: 3 step: 1208, loss is 0.007554028648883104\n",
      "epoch: 3 step: 1209, loss is 0.013319652527570724\n",
      "epoch: 3 step: 1210, loss is 0.00483835069462657\n",
      "epoch: 3 step: 1211, loss is 0.07033409178256989\n",
      "epoch: 3 step: 1212, loss is 0.02554330974817276\n",
      "epoch: 3 step: 1213, loss is 0.034593962132930756\n",
      "epoch: 3 step: 1214, loss is 0.19727078080177307\n",
      "epoch: 3 step: 1215, loss is 0.08838701993227005\n",
      "epoch: 3 step: 1216, loss is 0.05972512811422348\n",
      "epoch: 3 step: 1217, loss is 0.01460149697959423\n",
      "epoch: 3 step: 1218, loss is 0.031378403306007385\n",
      "epoch: 3 step: 1219, loss is 0.20784710347652435\n",
      "epoch: 3 step: 1220, loss is 0.042833950370550156\n",
      "epoch: 3 step: 1221, loss is 0.005482332780957222\n",
      "epoch: 3 step: 1222, loss is 0.0825226902961731\n",
      "epoch: 3 step: 1223, loss is 0.003379111411049962\n",
      "epoch: 3 step: 1224, loss is 0.0725012943148613\n",
      "epoch: 3 step: 1225, loss is 0.01098006684333086\n",
      "epoch: 3 step: 1226, loss is 0.0013070289278402925\n",
      "epoch: 3 step: 1227, loss is 0.005454800557345152\n",
      "epoch: 3 step: 1228, loss is 0.03284427896142006\n",
      "epoch: 3 step: 1229, loss is 0.005722092930227518\n",
      "epoch: 3 step: 1230, loss is 0.0071598561480641365\n",
      "epoch: 3 step: 1231, loss is 0.004825616721063852\n",
      "epoch: 3 step: 1232, loss is 0.00578015111386776\n",
      "epoch: 3 step: 1233, loss is 0.0015490592923015356\n",
      "epoch: 3 step: 1234, loss is 0.020210744813084602\n",
      "epoch: 3 step: 1235, loss is 0.3487643003463745\n",
      "epoch: 3 step: 1236, loss is 0.008111461997032166\n",
      "epoch: 3 step: 1237, loss is 0.013640392571687698\n",
      "epoch: 3 step: 1238, loss is 0.01391033548861742\n",
      "epoch: 3 step: 1239, loss is 0.026921089738607407\n",
      "epoch: 3 step: 1240, loss is 0.00636369688436389\n",
      "epoch: 3 step: 1241, loss is 0.005646010395139456\n",
      "epoch: 3 step: 1242, loss is 0.054983314126729965\n",
      "epoch: 3 step: 1243, loss is 0.08555421233177185\n",
      "epoch: 3 step: 1244, loss is 0.005806125234812498\n",
      "epoch: 3 step: 1245, loss is 0.05055388808250427\n",
      "epoch: 3 step: 1246, loss is 0.007697854191064835\n",
      "epoch: 3 step: 1247, loss is 0.005733672063797712\n",
      "epoch: 3 step: 1248, loss is 0.12502439320087433\n",
      "epoch: 3 step: 1249, loss is 0.000906847242731601\n",
      "epoch: 3 step: 1250, loss is 0.006725311279296875\n",
      "epoch: 3 step: 1251, loss is 0.0009065697086043656\n",
      "epoch: 3 step: 1252, loss is 0.1308351457118988\n",
      "epoch: 3 step: 1253, loss is 0.015883756801486015\n",
      "epoch: 3 step: 1254, loss is 0.0727568119764328\n",
      "epoch: 3 step: 1255, loss is 0.01061856560409069\n",
      "epoch: 3 step: 1256, loss is 0.0004477541078813374\n",
      "epoch: 3 step: 1257, loss is 0.027235761284828186\n",
      "epoch: 3 step: 1258, loss is 0.01257633138448\n",
      "epoch: 3 step: 1259, loss is 0.0017018623184412718\n",
      "epoch: 3 step: 1260, loss is 0.01009563822299242\n",
      "epoch: 3 step: 1261, loss is 0.007094969041645527\n",
      "epoch: 3 step: 1262, loss is 0.00104762171395123\n",
      "epoch: 3 step: 1263, loss is 0.18222500383853912\n",
      "epoch: 3 step: 1264, loss is 0.003446553135290742\n",
      "epoch: 3 step: 1265, loss is 0.0035855199676007032\n",
      "epoch: 3 step: 1266, loss is 0.004174996167421341\n",
      "epoch: 3 step: 1267, loss is 0.03467334434390068\n",
      "epoch: 3 step: 1268, loss is 0.017623065039515495\n",
      "epoch: 3 step: 1269, loss is 0.00046284610289148986\n",
      "epoch: 3 step: 1270, loss is 0.06839009374380112\n",
      "epoch: 3 step: 1271, loss is 0.012333043850958347\n",
      "epoch: 3 step: 1272, loss is 0.0050180708058178425\n",
      "epoch: 3 step: 1273, loss is 0.0315018892288208\n",
      "epoch: 3 step: 1274, loss is 0.04258471354842186\n",
      "epoch: 3 step: 1275, loss is 0.004837939981371164\n",
      "epoch: 3 step: 1276, loss is 0.2044723927974701\n",
      "epoch: 3 step: 1277, loss is 0.002790797036141157\n",
      "epoch: 3 step: 1278, loss is 0.00047456336324103177\n",
      "epoch: 3 step: 1279, loss is 0.06230553239583969\n",
      "epoch: 3 step: 1280, loss is 0.0019122813828289509\n",
      "epoch: 3 step: 1281, loss is 0.15958358347415924\n",
      "epoch: 3 step: 1282, loss is 0.0014035464264452457\n",
      "epoch: 3 step: 1283, loss is 0.005202602595090866\n",
      "epoch: 3 step: 1284, loss is 0.10998877137899399\n",
      "epoch: 3 step: 1285, loss is 0.010896200314164162\n",
      "epoch: 3 step: 1286, loss is 0.0024464274756610394\n",
      "epoch: 3 step: 1287, loss is 0.009689918719232082\n",
      "epoch: 3 step: 1288, loss is 0.13726750016212463\n",
      "epoch: 3 step: 1289, loss is 0.009442941285669804\n",
      "epoch: 3 step: 1290, loss is 0.003969013225287199\n",
      "epoch: 3 step: 1291, loss is 0.011791729368269444\n",
      "epoch: 3 step: 1292, loss is 0.01908349245786667\n",
      "epoch: 3 step: 1293, loss is 0.0030691514257341623\n",
      "epoch: 3 step: 1294, loss is 0.030549101531505585\n",
      "epoch: 3 step: 1295, loss is 0.01268091332167387\n",
      "epoch: 3 step: 1296, loss is 0.0035706458147615194\n",
      "epoch: 3 step: 1297, loss is 0.26767289638519287\n",
      "epoch: 3 step: 1298, loss is 0.07821805030107498\n",
      "epoch: 3 step: 1299, loss is 0.0008659042650833726\n",
      "epoch: 3 step: 1300, loss is 0.0041819242760539055\n",
      "epoch: 3 step: 1301, loss is 0.0018266646657139063\n",
      "epoch: 3 step: 1302, loss is 0.003221623133867979\n",
      "epoch: 3 step: 1303, loss is 0.0034335062373429537\n",
      "epoch: 3 step: 1304, loss is 0.008136413991451263\n",
      "epoch: 3 step: 1305, loss is 0.0012870357604697347\n",
      "epoch: 3 step: 1306, loss is 0.16869822144508362\n",
      "epoch: 3 step: 1307, loss is 0.03254813700914383\n",
      "epoch: 3 step: 1308, loss is 0.026066100224852562\n",
      "epoch: 3 step: 1309, loss is 0.002843457506969571\n",
      "epoch: 3 step: 1310, loss is 0.034331075847148895\n",
      "epoch: 3 step: 1311, loss is 0.05778789147734642\n",
      "epoch: 3 step: 1312, loss is 0.00024073281383607537\n",
      "epoch: 3 step: 1313, loss is 0.09926774352788925\n",
      "epoch: 3 step: 1314, loss is 0.0023324741050601006\n",
      "epoch: 3 step: 1315, loss is 0.0036514929961413145\n",
      "epoch: 3 step: 1316, loss is 0.07735458016395569\n",
      "epoch: 3 step: 1317, loss is 0.0020816377364099026\n",
      "epoch: 3 step: 1318, loss is 0.0006360701518133283\n",
      "epoch: 3 step: 1319, loss is 0.032212045043706894\n",
      "epoch: 3 step: 1320, loss is 0.061069339513778687\n",
      "epoch: 3 step: 1321, loss is 0.029270872473716736\n",
      "epoch: 3 step: 1322, loss is 0.000841901171952486\n",
      "epoch: 3 step: 1323, loss is 0.12086232751607895\n",
      "epoch: 3 step: 1324, loss is 0.01898564212024212\n",
      "epoch: 3 step: 1325, loss is 0.04632893204689026\n",
      "epoch: 3 step: 1326, loss is 0.03299647197127342\n",
      "epoch: 3 step: 1327, loss is 0.2487516701221466\n",
      "epoch: 3 step: 1328, loss is 0.0817243903875351\n",
      "epoch: 3 step: 1329, loss is 0.08637519925832748\n",
      "epoch: 3 step: 1330, loss is 0.0047795395366847515\n",
      "epoch: 3 step: 1331, loss is 0.2344164401292801\n",
      "epoch: 3 step: 1332, loss is 0.014725682325661182\n",
      "epoch: 3 step: 1333, loss is 0.001717317383736372\n",
      "epoch: 3 step: 1334, loss is 0.09015580266714096\n",
      "epoch: 3 step: 1335, loss is 0.026817474514245987\n",
      "epoch: 3 step: 1336, loss is 0.00840697716921568\n",
      "epoch: 3 step: 1337, loss is 0.009811274707317352\n",
      "epoch: 3 step: 1338, loss is 0.02047935500741005\n",
      "epoch: 3 step: 1339, loss is 0.01216953806579113\n",
      "epoch: 3 step: 1340, loss is 0.18820004165172577\n",
      "epoch: 3 step: 1341, loss is 0.002798446686938405\n",
      "epoch: 3 step: 1342, loss is 0.019124159589409828\n",
      "epoch: 3 step: 1343, loss is 0.006195622496306896\n",
      "epoch: 3 step: 1344, loss is 0.08187664300203323\n",
      "epoch: 3 step: 1345, loss is 0.041217196732759476\n",
      "epoch: 3 step: 1346, loss is 0.006450195796787739\n",
      "epoch: 3 step: 1347, loss is 0.012016035616397858\n",
      "epoch: 3 step: 1348, loss is 0.045151323080062866\n",
      "epoch: 3 step: 1349, loss is 0.1015196219086647\n",
      "epoch: 3 step: 1350, loss is 0.0003365290758665651\n",
      "epoch: 3 step: 1351, loss is 0.09238457679748535\n",
      "epoch: 3 step: 1352, loss is 0.019442079588770866\n",
      "epoch: 3 step: 1353, loss is 0.002452726708725095\n",
      "epoch: 3 step: 1354, loss is 0.018798356875777245\n",
      "epoch: 3 step: 1355, loss is 0.0006987890228629112\n",
      "epoch: 3 step: 1356, loss is 0.0008052320335991681\n",
      "epoch: 3 step: 1357, loss is 0.024281468242406845\n",
      "epoch: 3 step: 1358, loss is 0.05325164273381233\n",
      "epoch: 3 step: 1359, loss is 0.0011255029821768403\n",
      "epoch: 3 step: 1360, loss is 0.05053921788930893\n",
      "epoch: 3 step: 1361, loss is 0.0938178226351738\n",
      "epoch: 3 step: 1362, loss is 0.02690737508237362\n",
      "epoch: 3 step: 1363, loss is 0.05699444189667702\n",
      "epoch: 3 step: 1364, loss is 0.062045868486166\n",
      "epoch: 3 step: 1365, loss is 0.000995897687971592\n",
      "epoch: 3 step: 1366, loss is 0.052317485213279724\n",
      "epoch: 3 step: 1367, loss is 0.018761442974209785\n",
      "epoch: 3 step: 1368, loss is 0.0015257623745128512\n",
      "epoch: 3 step: 1369, loss is 0.02117520198225975\n",
      "epoch: 3 step: 1370, loss is 0.019388945773243904\n",
      "epoch: 3 step: 1371, loss is 0.00011010197340510786\n",
      "epoch: 3 step: 1372, loss is 0.06612130254507065\n",
      "epoch: 3 step: 1373, loss is 0.02562243491411209\n",
      "epoch: 3 step: 1374, loss is 0.053924158215522766\n",
      "epoch: 3 step: 1375, loss is 0.02106318064033985\n",
      "epoch: 3 step: 1376, loss is 0.010357804596424103\n",
      "epoch: 3 step: 1377, loss is 0.0030295157339423895\n",
      "epoch: 3 step: 1378, loss is 0.0015623109648004174\n",
      "epoch: 3 step: 1379, loss is 0.00395206781104207\n",
      "epoch: 3 step: 1380, loss is 0.32101017236709595\n",
      "epoch: 3 step: 1381, loss is 0.003418530570343137\n",
      "epoch: 3 step: 1382, loss is 0.0022852609399706125\n",
      "epoch: 3 step: 1383, loss is 0.016989802941679955\n",
      "epoch: 3 step: 1384, loss is 0.0018888155464082956\n",
      "epoch: 3 step: 1385, loss is 0.005679040681570768\n",
      "epoch: 3 step: 1386, loss is 0.0027754190377891064\n",
      "epoch: 3 step: 1387, loss is 0.006613453850150108\n",
      "epoch: 3 step: 1388, loss is 0.09426171332597733\n",
      "epoch: 3 step: 1389, loss is 0.0034616924822330475\n",
      "epoch: 3 step: 1390, loss is 0.007347689475864172\n",
      "epoch: 3 step: 1391, loss is 0.12582750618457794\n",
      "epoch: 3 step: 1392, loss is 0.0009560726466588676\n",
      "epoch: 3 step: 1393, loss is 0.007294658571481705\n",
      "epoch: 3 step: 1394, loss is 0.0040125614032149315\n",
      "epoch: 3 step: 1395, loss is 0.0014355190796777606\n",
      "epoch: 3 step: 1396, loss is 0.32380226254463196\n",
      "epoch: 3 step: 1397, loss is 0.012490847148001194\n",
      "epoch: 3 step: 1398, loss is 0.0018030971987172961\n",
      "epoch: 3 step: 1399, loss is 0.16958138346672058\n",
      "epoch: 3 step: 1400, loss is 0.0074336170218884945\n",
      "epoch: 3 step: 1401, loss is 0.006285231560468674\n",
      "epoch: 3 step: 1402, loss is 0.0014010606100782752\n",
      "epoch: 3 step: 1403, loss is 0.15183639526367188\n",
      "epoch: 3 step: 1404, loss is 0.0008664734777994454\n",
      "epoch: 3 step: 1405, loss is 0.006656025070697069\n",
      "epoch: 3 step: 1406, loss is 0.01061536930501461\n",
      "epoch: 3 step: 1407, loss is 0.06643307954072952\n",
      "epoch: 3 step: 1408, loss is 0.005982883274555206\n",
      "epoch: 3 step: 1409, loss is 0.18722416460514069\n",
      "epoch: 3 step: 1410, loss is 0.16631771624088287\n",
      "epoch: 3 step: 1411, loss is 0.015528582036495209\n",
      "epoch: 3 step: 1412, loss is 0.011393129825592041\n",
      "epoch: 3 step: 1413, loss is 0.007361788302659988\n",
      "epoch: 3 step: 1414, loss is 0.08537674695253372\n",
      "epoch: 3 step: 1415, loss is 0.004655393306165934\n",
      "epoch: 3 step: 1416, loss is 0.16180239617824554\n",
      "epoch: 3 step: 1417, loss is 0.0062394957058131695\n",
      "epoch: 3 step: 1418, loss is 0.0236009880900383\n",
      "epoch: 3 step: 1419, loss is 0.035176195204257965\n",
      "epoch: 3 step: 1420, loss is 0.017192360013723373\n",
      "epoch: 3 step: 1421, loss is 0.012340346351265907\n",
      "epoch: 3 step: 1422, loss is 0.012714462354779243\n",
      "epoch: 3 step: 1423, loss is 0.07688987255096436\n",
      "epoch: 3 step: 1424, loss is 0.02629462257027626\n",
      "epoch: 3 step: 1425, loss is 0.001573005341924727\n",
      "epoch: 3 step: 1426, loss is 0.008982319384813309\n",
      "epoch: 3 step: 1427, loss is 0.007049776613712311\n",
      "epoch: 3 step: 1428, loss is 0.028108611702919006\n",
      "epoch: 3 step: 1429, loss is 0.10650502145290375\n",
      "epoch: 3 step: 1430, loss is 0.005547028966248035\n",
      "epoch: 3 step: 1431, loss is 0.0032884059473872185\n",
      "epoch: 3 step: 1432, loss is 0.03436396270990372\n",
      "epoch: 3 step: 1433, loss is 0.05824730545282364\n",
      "epoch: 3 step: 1434, loss is 0.010950099676847458\n",
      "epoch: 3 step: 1435, loss is 0.0022630600724369287\n",
      "epoch: 3 step: 1436, loss is 0.004615661688148975\n",
      "epoch: 3 step: 1437, loss is 0.0042493040673434734\n",
      "epoch: 3 step: 1438, loss is 0.10147280991077423\n",
      "epoch: 3 step: 1439, loss is 0.03581969439983368\n",
      "epoch: 3 step: 1440, loss is 0.21767981350421906\n",
      "epoch: 3 step: 1441, loss is 0.004577229265123606\n",
      "epoch: 3 step: 1442, loss is 0.20771126449108124\n",
      "epoch: 3 step: 1443, loss is 0.06167992204427719\n",
      "epoch: 3 step: 1444, loss is 0.1408190280199051\n",
      "epoch: 3 step: 1445, loss is 0.0006256519700400531\n",
      "epoch: 3 step: 1446, loss is 0.008282081224024296\n",
      "epoch: 3 step: 1447, loss is 0.0916610136628151\n",
      "epoch: 3 step: 1448, loss is 0.03408482298254967\n",
      "epoch: 3 step: 1449, loss is 0.18367359042167664\n",
      "epoch: 3 step: 1450, loss is 0.005837580189108849\n",
      "epoch: 3 step: 1451, loss is 0.013664228841662407\n",
      "epoch: 3 step: 1452, loss is 0.03541821986436844\n",
      "epoch: 3 step: 1453, loss is 0.0005398736102506518\n",
      "epoch: 3 step: 1454, loss is 0.04117140173912048\n",
      "epoch: 3 step: 1455, loss is 0.026754913851618767\n",
      "epoch: 3 step: 1456, loss is 0.0016240624245256186\n",
      "epoch: 3 step: 1457, loss is 0.027354497462511063\n",
      "epoch: 3 step: 1458, loss is 0.004840295761823654\n",
      "epoch: 3 step: 1459, loss is 0.004710829351097345\n",
      "epoch: 3 step: 1460, loss is 0.0015505291521549225\n",
      "epoch: 3 step: 1461, loss is 0.008397065103054047\n",
      "epoch: 3 step: 1462, loss is 0.07129336148500443\n",
      "epoch: 3 step: 1463, loss is 0.10347940772771835\n",
      "epoch: 3 step: 1464, loss is 0.31974124908447266\n",
      "epoch: 3 step: 1465, loss is 0.0001970754674402997\n",
      "epoch: 3 step: 1466, loss is 0.004667624831199646\n",
      "epoch: 3 step: 1467, loss is 0.03511377424001694\n",
      "epoch: 3 step: 1468, loss is 0.028899801895022392\n",
      "epoch: 3 step: 1469, loss is 0.08668770641088486\n",
      "epoch: 3 step: 1470, loss is 0.008768243715167046\n",
      "epoch: 3 step: 1471, loss is 0.009162488393485546\n",
      "epoch: 3 step: 1472, loss is 0.08963944762945175\n",
      "epoch: 3 step: 1473, loss is 0.016413195058703423\n",
      "epoch: 3 step: 1474, loss is 0.35091981291770935\n",
      "epoch: 3 step: 1475, loss is 0.0032231479417532682\n",
      "epoch: 3 step: 1476, loss is 0.1602126508951187\n",
      "epoch: 3 step: 1477, loss is 0.033642470836639404\n",
      "epoch: 3 step: 1478, loss is 0.05618708208203316\n",
      "epoch: 3 step: 1479, loss is 0.12145799398422241\n",
      "epoch: 3 step: 1480, loss is 0.07063575088977814\n",
      "epoch: 3 step: 1481, loss is 0.1438218355178833\n",
      "epoch: 3 step: 1482, loss is 0.05815734714269638\n",
      "epoch: 3 step: 1483, loss is 0.021548917517066002\n",
      "epoch: 3 step: 1484, loss is 0.0015624372754245996\n",
      "epoch: 3 step: 1485, loss is 0.016225650906562805\n",
      "epoch: 3 step: 1486, loss is 0.008230172097682953\n",
      "epoch: 3 step: 1487, loss is 0.025070512667298317\n",
      "epoch: 3 step: 1488, loss is 0.09572915732860565\n",
      "epoch: 3 step: 1489, loss is 0.005339376628398895\n",
      "epoch: 3 step: 1490, loss is 0.003935390152037144\n",
      "epoch: 3 step: 1491, loss is 0.1350134164094925\n",
      "epoch: 3 step: 1492, loss is 0.1266123652458191\n",
      "epoch: 3 step: 1493, loss is 0.1071057915687561\n",
      "epoch: 3 step: 1494, loss is 0.1264454573392868\n",
      "epoch: 3 step: 1495, loss is 0.0102219358086586\n",
      "epoch: 3 step: 1496, loss is 0.015046477317810059\n",
      "epoch: 3 step: 1497, loss is 0.0026536472141742706\n",
      "epoch: 3 step: 1498, loss is 0.1327122151851654\n",
      "epoch: 3 step: 1499, loss is 0.034445419907569885\n",
      "epoch: 3 step: 1500, loss is 0.005859490018337965\n",
      "epoch: 3 step: 1501, loss is 0.01609908975660801\n",
      "epoch: 3 step: 1502, loss is 0.003850069595500827\n",
      "epoch: 3 step: 1503, loss is 0.029170647263526917\n",
      "epoch: 3 step: 1504, loss is 0.09399827569723129\n",
      "epoch: 3 step: 1505, loss is 0.003954994957894087\n",
      "epoch: 3 step: 1506, loss is 0.07149847596883774\n",
      "epoch: 3 step: 1507, loss is 0.006954241544008255\n",
      "epoch: 3 step: 1508, loss is 0.14478029310703278\n",
      "epoch: 3 step: 1509, loss is 0.0018236808246001601\n",
      "epoch: 3 step: 1510, loss is 0.05557998642325401\n",
      "epoch: 3 step: 1511, loss is 0.15256370604038239\n",
      "epoch: 3 step: 1512, loss is 0.01613638550043106\n",
      "epoch: 3 step: 1513, loss is 0.02430165559053421\n",
      "epoch: 3 step: 1514, loss is 0.18709325790405273\n",
      "epoch: 3 step: 1515, loss is 0.019038820639252663\n",
      "epoch: 3 step: 1516, loss is 0.0258187185972929\n",
      "epoch: 3 step: 1517, loss is 0.058304354548454285\n",
      "epoch: 3 step: 1518, loss is 0.09928113967180252\n",
      "epoch: 3 step: 1519, loss is 0.06852074712514877\n",
      "epoch: 3 step: 1520, loss is 0.0013507833937183022\n",
      "epoch: 3 step: 1521, loss is 0.0037449842784553766\n",
      "epoch: 3 step: 1522, loss is 0.04625486209988594\n",
      "epoch: 3 step: 1523, loss is 0.006118900142610073\n",
      "epoch: 3 step: 1524, loss is 0.2880922555923462\n",
      "epoch: 3 step: 1525, loss is 0.019839581102132797\n",
      "epoch: 3 step: 1526, loss is 0.0529538094997406\n",
      "epoch: 3 step: 1527, loss is 0.16920259594917297\n",
      "epoch: 3 step: 1528, loss is 0.03518820181488991\n",
      "epoch: 3 step: 1529, loss is 0.05046122148633003\n",
      "epoch: 3 step: 1530, loss is 0.009480210952460766\n",
      "epoch: 3 step: 1531, loss is 0.022243423387408257\n",
      "epoch: 3 step: 1532, loss is 0.10169641673564911\n",
      "epoch: 3 step: 1533, loss is 0.024060353636741638\n",
      "epoch: 3 step: 1534, loss is 0.028038907796144485\n",
      "epoch: 3 step: 1535, loss is 0.0048757740296423435\n",
      "epoch: 3 step: 1536, loss is 0.0072518340311944485\n",
      "epoch: 3 step: 1537, loss is 0.02738278917968273\n",
      "epoch: 3 step: 1538, loss is 0.00960743147879839\n",
      "epoch: 3 step: 1539, loss is 0.014124247245490551\n",
      "epoch: 3 step: 1540, loss is 0.16095179319381714\n",
      "epoch: 3 step: 1541, loss is 0.17018482089042664\n",
      "epoch: 3 step: 1542, loss is 0.03697420284152031\n",
      "epoch: 3 step: 1543, loss is 0.07397063076496124\n",
      "epoch: 3 step: 1544, loss is 0.013656028546392918\n",
      "epoch: 3 step: 1545, loss is 0.01438919734209776\n",
      "epoch: 3 step: 1546, loss is 0.0014756567543372512\n",
      "epoch: 3 step: 1547, loss is 0.0026669087819755077\n",
      "epoch: 3 step: 1548, loss is 0.08591713011264801\n",
      "epoch: 3 step: 1549, loss is 0.022412782534956932\n",
      "epoch: 3 step: 1550, loss is 0.00920860655605793\n",
      "epoch: 3 step: 1551, loss is 0.004442586097866297\n",
      "epoch: 3 step: 1552, loss is 0.002218008041381836\n",
      "epoch: 3 step: 1553, loss is 0.004109732806682587\n",
      "epoch: 3 step: 1554, loss is 0.00827582087367773\n",
      "epoch: 3 step: 1555, loss is 0.037284109741449356\n",
      "epoch: 3 step: 1556, loss is 0.1348685324192047\n",
      "epoch: 3 step: 1557, loss is 0.08824140578508377\n",
      "epoch: 3 step: 1558, loss is 0.04549994319677353\n",
      "epoch: 3 step: 1559, loss is 0.08287440985441208\n",
      "epoch: 3 step: 1560, loss is 0.002385183935984969\n",
      "epoch: 3 step: 1561, loss is 0.010440056212246418\n",
      "epoch: 3 step: 1562, loss is 0.03270633891224861\n",
      "epoch: 3 step: 1563, loss is 0.001739120576530695\n",
      "epoch: 3 step: 1564, loss is 0.015654003247618675\n",
      "epoch: 3 step: 1565, loss is 0.11714603006839752\n",
      "epoch: 3 step: 1566, loss is 0.0014246555510908365\n",
      "epoch: 3 step: 1567, loss is 0.05478120967745781\n",
      "epoch: 3 step: 1568, loss is 0.12860938906669617\n",
      "epoch: 3 step: 1569, loss is 0.0022704650182276964\n",
      "epoch: 3 step: 1570, loss is 0.0018829407636076212\n",
      "epoch: 3 step: 1571, loss is 0.010448329150676727\n",
      "epoch: 3 step: 1572, loss is 0.034912075847387314\n",
      "epoch: 3 step: 1573, loss is 0.012599865905940533\n",
      "epoch: 3 step: 1574, loss is 0.015885688364505768\n",
      "epoch: 3 step: 1575, loss is 0.027504658326506615\n",
      "epoch: 3 step: 1576, loss is 0.002890376839786768\n",
      "epoch: 3 step: 1577, loss is 0.13799379765987396\n",
      "epoch: 3 step: 1578, loss is 0.03894702345132828\n",
      "epoch: 3 step: 1579, loss is 0.003277847543358803\n",
      "epoch: 3 step: 1580, loss is 0.003017665585502982\n",
      "epoch: 3 step: 1581, loss is 0.17923565208911896\n",
      "epoch: 3 step: 1582, loss is 0.0067751118913292885\n",
      "epoch: 3 step: 1583, loss is 0.002989155473187566\n",
      "epoch: 3 step: 1584, loss is 0.006801155395805836\n",
      "epoch: 3 step: 1585, loss is 0.0021902238950133324\n",
      "epoch: 3 step: 1586, loss is 0.010172668844461441\n",
      "epoch: 3 step: 1587, loss is 0.048550814390182495\n",
      "epoch: 3 step: 1588, loss is 0.003443916793912649\n",
      "epoch: 3 step: 1589, loss is 0.014485002495348454\n",
      "epoch: 3 step: 1590, loss is 0.05376727506518364\n",
      "epoch: 3 step: 1591, loss is 0.16147111356258392\n",
      "epoch: 3 step: 1592, loss is 0.002510992344468832\n",
      "epoch: 3 step: 1593, loss is 0.07477346062660217\n",
      "epoch: 3 step: 1594, loss is 0.003151650307700038\n",
      "epoch: 3 step: 1595, loss is 0.0031834545079618692\n",
      "epoch: 3 step: 1596, loss is 0.015280133113265038\n",
      "epoch: 3 step: 1597, loss is 0.10119639337062836\n",
      "epoch: 3 step: 1598, loss is 0.001542768906801939\n",
      "epoch: 3 step: 1599, loss is 0.0032649519853293896\n",
      "epoch: 3 step: 1600, loss is 0.0012939383741468191\n",
      "epoch: 3 step: 1601, loss is 0.0016161949606612325\n",
      "epoch: 3 step: 1602, loss is 0.030100557953119278\n",
      "epoch: 3 step: 1603, loss is 0.051210712641477585\n",
      "epoch: 3 step: 1604, loss is 0.022266019135713577\n",
      "epoch: 3 step: 1605, loss is 0.05561656504869461\n",
      "epoch: 3 step: 1606, loss is 0.0951479971408844\n",
      "epoch: 3 step: 1607, loss is 0.011764289811253548\n",
      "epoch: 3 step: 1608, loss is 0.06160036846995354\n",
      "epoch: 3 step: 1609, loss is 0.008112969808280468\n",
      "epoch: 3 step: 1610, loss is 0.035353027284145355\n",
      "epoch: 3 step: 1611, loss is 0.007026187144219875\n",
      "epoch: 3 step: 1612, loss is 0.32287004590034485\n",
      "epoch: 3 step: 1613, loss is 0.01720833219587803\n",
      "epoch: 3 step: 1614, loss is 0.00642765499651432\n",
      "epoch: 3 step: 1615, loss is 0.023389196023344994\n",
      "epoch: 3 step: 1616, loss is 0.04154588654637337\n",
      "epoch: 3 step: 1617, loss is 0.0010823819320648909\n",
      "epoch: 3 step: 1618, loss is 0.02344319596886635\n",
      "epoch: 3 step: 1619, loss is 0.0058712647296488285\n",
      "epoch: 3 step: 1620, loss is 0.007002608850598335\n",
      "epoch: 3 step: 1621, loss is 0.12513482570648193\n",
      "epoch: 3 step: 1622, loss is 0.013378568924963474\n",
      "epoch: 3 step: 1623, loss is 0.011915336363017559\n",
      "epoch: 3 step: 1624, loss is 0.03307034447789192\n",
      "epoch: 3 step: 1625, loss is 0.014115254394710064\n",
      "epoch: 3 step: 1626, loss is 0.10219836235046387\n",
      "epoch: 3 step: 1627, loss is 0.23427946865558624\n",
      "epoch: 3 step: 1628, loss is 0.07786516845226288\n",
      "epoch: 3 step: 1629, loss is 0.009701479226350784\n",
      "epoch: 3 step: 1630, loss is 0.0021409187465906143\n",
      "epoch: 3 step: 1631, loss is 0.0324881486594677\n",
      "epoch: 3 step: 1632, loss is 0.01739070937037468\n",
      "epoch: 3 step: 1633, loss is 0.03255581110715866\n",
      "epoch: 3 step: 1634, loss is 0.06640230864286423\n",
      "epoch: 3 step: 1635, loss is 0.12882539629936218\n",
      "epoch: 3 step: 1636, loss is 0.04694041982293129\n",
      "epoch: 3 step: 1637, loss is 0.02168319933116436\n",
      "epoch: 3 step: 1638, loss is 0.01924280822277069\n",
      "epoch: 3 step: 1639, loss is 0.0168113112449646\n",
      "epoch: 3 step: 1640, loss is 0.005639601033180952\n",
      "epoch: 3 step: 1641, loss is 0.007750427350401878\n",
      "epoch: 3 step: 1642, loss is 0.03150644525885582\n",
      "epoch: 3 step: 1643, loss is 0.018996525555849075\n",
      "epoch: 3 step: 1644, loss is 0.002820750931277871\n",
      "epoch: 3 step: 1645, loss is 0.019019844010472298\n",
      "epoch: 3 step: 1646, loss is 0.1039588451385498\n",
      "epoch: 3 step: 1647, loss is 0.015702813863754272\n",
      "epoch: 3 step: 1648, loss is 0.021085012704133987\n",
      "epoch: 3 step: 1649, loss is 0.016443928703665733\n",
      "epoch: 3 step: 1650, loss is 0.001450890558771789\n",
      "epoch: 3 step: 1651, loss is 0.005915521178394556\n",
      "epoch: 3 step: 1652, loss is 0.002178400056436658\n",
      "epoch: 3 step: 1653, loss is 0.004301746841520071\n",
      "epoch: 3 step: 1654, loss is 0.016851361840963364\n",
      "epoch: 3 step: 1655, loss is 0.03815917298197746\n",
      "epoch: 3 step: 1656, loss is 0.01722358539700508\n",
      "epoch: 3 step: 1657, loss is 0.04317175969481468\n",
      "epoch: 3 step: 1658, loss is 0.15758690237998962\n",
      "epoch: 3 step: 1659, loss is 0.008482517674565315\n",
      "epoch: 3 step: 1660, loss is 0.04215662181377411\n",
      "epoch: 3 step: 1661, loss is 0.037586964666843414\n",
      "epoch: 3 step: 1662, loss is 0.0034564484376460314\n",
      "epoch: 3 step: 1663, loss is 0.0005068390164524317\n",
      "epoch: 3 step: 1664, loss is 0.012118867598474026\n",
      "epoch: 3 step: 1665, loss is 0.014460909180343151\n",
      "epoch: 3 step: 1666, loss is 0.0038087256252765656\n",
      "epoch: 3 step: 1667, loss is 0.14437395334243774\n",
      "epoch: 3 step: 1668, loss is 0.07847623527050018\n",
      "epoch: 3 step: 1669, loss is 0.0024911239743232727\n",
      "epoch: 3 step: 1670, loss is 0.13481958210468292\n",
      "epoch: 3 step: 1671, loss is 0.0034301471896469593\n",
      "epoch: 3 step: 1672, loss is 0.0008487759623676538\n",
      "epoch: 3 step: 1673, loss is 0.017230844125151634\n",
      "epoch: 3 step: 1674, loss is 0.0012208642438054085\n",
      "epoch: 3 step: 1675, loss is 0.26915261149406433\n",
      "epoch: 3 step: 1676, loss is 0.17536693811416626\n",
      "epoch: 3 step: 1677, loss is 0.00892264861613512\n",
      "epoch: 3 step: 1678, loss is 0.002726236591115594\n",
      "epoch: 3 step: 1679, loss is 0.010856153443455696\n",
      "epoch: 3 step: 1680, loss is 0.002507667290046811\n",
      "epoch: 3 step: 1681, loss is 0.0063392692245543\n",
      "epoch: 3 step: 1682, loss is 0.0046606347896158695\n",
      "epoch: 3 step: 1683, loss is 0.0013978580245748162\n",
      "epoch: 3 step: 1684, loss is 0.009142348542809486\n",
      "epoch: 3 step: 1685, loss is 0.26909545063972473\n",
      "epoch: 3 step: 1686, loss is 0.009161172434687614\n",
      "epoch: 3 step: 1687, loss is 0.0031952683348208666\n",
      "epoch: 3 step: 1688, loss is 0.007898442447185516\n",
      "epoch: 3 step: 1689, loss is 0.010463100858032703\n",
      "epoch: 3 step: 1690, loss is 0.2576836049556732\n",
      "epoch: 3 step: 1691, loss is 0.04099618270993233\n",
      "epoch: 3 step: 1692, loss is 0.0054287295788526535\n",
      "epoch: 3 step: 1693, loss is 0.0011006637942045927\n",
      "epoch: 3 step: 1694, loss is 0.02781357616186142\n",
      "epoch: 3 step: 1695, loss is 0.058651190251111984\n",
      "epoch: 3 step: 1696, loss is 0.0023261266760528088\n",
      "epoch: 3 step: 1697, loss is 0.050879329442977905\n",
      "epoch: 3 step: 1698, loss is 0.07245681434869766\n",
      "epoch: 3 step: 1699, loss is 0.011450813151896\n",
      "epoch: 3 step: 1700, loss is 0.020984651520848274\n",
      "epoch: 3 step: 1701, loss is 0.004444716032594442\n",
      "epoch: 3 step: 1702, loss is 0.0011408769059926271\n",
      "epoch: 3 step: 1703, loss is 0.00730342511087656\n",
      "epoch: 3 step: 1704, loss is 0.0006031752564013004\n",
      "epoch: 3 step: 1705, loss is 0.004654309246689081\n",
      "epoch: 3 step: 1706, loss is 0.006982315797358751\n",
      "epoch: 3 step: 1707, loss is 0.009578993543982506\n",
      "epoch: 3 step: 1708, loss is 0.026320837438106537\n",
      "epoch: 3 step: 1709, loss is 0.0005115663516335189\n",
      "epoch: 3 step: 1710, loss is 0.051804933696985245\n",
      "epoch: 3 step: 1711, loss is 0.0021802170667797327\n",
      "epoch: 3 step: 1712, loss is 0.02701769769191742\n",
      "epoch: 3 step: 1713, loss is 0.18725349009037018\n",
      "epoch: 3 step: 1714, loss is 0.0033276784233748913\n",
      "epoch: 3 step: 1715, loss is 0.018565919250249863\n",
      "epoch: 3 step: 1716, loss is 0.004267089534550905\n",
      "epoch: 3 step: 1717, loss is 0.007806006353348494\n",
      "epoch: 3 step: 1718, loss is 0.07351116091012955\n",
      "epoch: 3 step: 1719, loss is 0.2520281970500946\n",
      "epoch: 3 step: 1720, loss is 0.0053170835599303246\n",
      "epoch: 3 step: 1721, loss is 0.009177274070680141\n",
      "epoch: 3 step: 1722, loss is 0.045291949063539505\n",
      "epoch: 3 step: 1723, loss is 0.001915534376166761\n",
      "epoch: 3 step: 1724, loss is 0.030685264617204666\n",
      "epoch: 3 step: 1725, loss is 0.04844177886843681\n",
      "epoch: 3 step: 1726, loss is 0.004588475916534662\n",
      "epoch: 3 step: 1727, loss is 0.002919524209573865\n",
      "epoch: 3 step: 1728, loss is 0.0027078555431216955\n",
      "epoch: 3 step: 1729, loss is 0.007926140911877155\n",
      "epoch: 3 step: 1730, loss is 0.009007377550005913\n",
      "epoch: 3 step: 1731, loss is 0.0003279366937931627\n",
      "epoch: 3 step: 1732, loss is 0.055495813488960266\n",
      "epoch: 3 step: 1733, loss is 0.002427358878776431\n",
      "epoch: 3 step: 1734, loss is 0.0007514330791309476\n",
      "epoch: 3 step: 1735, loss is 0.23619642853736877\n",
      "epoch: 3 step: 1736, loss is 0.004871299955993891\n",
      "epoch: 3 step: 1737, loss is 0.18541564047336578\n",
      "epoch: 3 step: 1738, loss is 0.3424602448940277\n",
      "epoch: 3 step: 1739, loss is 0.006235063076019287\n",
      "epoch: 3 step: 1740, loss is 0.005545472260564566\n",
      "epoch: 3 step: 1741, loss is 0.02020614966750145\n",
      "epoch: 3 step: 1742, loss is 0.055643003433942795\n",
      "epoch: 3 step: 1743, loss is 0.0038128248415887356\n",
      "epoch: 3 step: 1744, loss is 0.16539375483989716\n",
      "epoch: 3 step: 1745, loss is 0.0794585645198822\n",
      "epoch: 3 step: 1746, loss is 0.005891691893339157\n",
      "epoch: 3 step: 1747, loss is 0.05909304320812225\n",
      "epoch: 3 step: 1748, loss is 0.32534682750701904\n",
      "epoch: 3 step: 1749, loss is 0.002567603485658765\n",
      "epoch: 3 step: 1750, loss is 0.03128552809357643\n",
      "epoch: 3 step: 1751, loss is 0.16364675760269165\n",
      "epoch: 3 step: 1752, loss is 0.08419445157051086\n",
      "epoch: 3 step: 1753, loss is 0.009591853246092796\n",
      "epoch: 3 step: 1754, loss is 0.12432938069105148\n",
      "epoch: 3 step: 1755, loss is 0.06318265944719315\n",
      "epoch: 3 step: 1756, loss is 0.0148320896551013\n",
      "epoch: 3 step: 1757, loss is 0.020768387243151665\n",
      "epoch: 3 step: 1758, loss is 0.18132513761520386\n",
      "epoch: 3 step: 1759, loss is 0.07904720306396484\n",
      "epoch: 3 step: 1760, loss is 0.021580643951892853\n",
      "epoch: 3 step: 1761, loss is 0.0012842522701248527\n",
      "epoch: 3 step: 1762, loss is 0.3620346784591675\n",
      "epoch: 3 step: 1763, loss is 0.13295961916446686\n",
      "epoch: 3 step: 1764, loss is 0.02751513198018074\n",
      "epoch: 3 step: 1765, loss is 0.15499475598335266\n",
      "epoch: 3 step: 1766, loss is 0.09683432430028915\n",
      "epoch: 3 step: 1767, loss is 0.01285582222044468\n",
      "epoch: 3 step: 1768, loss is 0.13825933635234833\n",
      "epoch: 3 step: 1769, loss is 0.0031325924210250378\n",
      "epoch: 3 step: 1770, loss is 0.01790706440806389\n",
      "epoch: 3 step: 1771, loss is 0.01381226908415556\n",
      "epoch: 3 step: 1772, loss is 0.029719246551394463\n",
      "epoch: 3 step: 1773, loss is 0.0057656592689454556\n",
      "epoch: 3 step: 1774, loss is 0.004048189148306847\n",
      "epoch: 3 step: 1775, loss is 0.01572956144809723\n",
      "epoch: 3 step: 1776, loss is 0.009087858721613884\n",
      "epoch: 3 step: 1777, loss is 0.2832161486148834\n",
      "epoch: 3 step: 1778, loss is 0.0013694694498553872\n",
      "epoch: 3 step: 1779, loss is 0.001811111462302506\n",
      "epoch: 3 step: 1780, loss is 0.03130470961332321\n",
      "epoch: 3 step: 1781, loss is 0.04749806970357895\n",
      "epoch: 3 step: 1782, loss is 0.025847243145108223\n",
      "epoch: 3 step: 1783, loss is 0.016960131004452705\n",
      "epoch: 3 step: 1784, loss is 0.08748915046453476\n",
      "epoch: 3 step: 1785, loss is 0.027062484994530678\n",
      "epoch: 3 step: 1786, loss is 0.02497069537639618\n",
      "epoch: 3 step: 1787, loss is 0.005051165819168091\n",
      "epoch: 3 step: 1788, loss is 0.08026523888111115\n",
      "epoch: 3 step: 1789, loss is 0.05953012406826019\n",
      "epoch: 3 step: 1790, loss is 0.016994377598166466\n",
      "epoch: 3 step: 1791, loss is 0.1853240430355072\n",
      "epoch: 3 step: 1792, loss is 0.10110406577587128\n",
      "epoch: 3 step: 1793, loss is 0.11000499129295349\n",
      "epoch: 3 step: 1794, loss is 0.23251064121723175\n",
      "epoch: 3 step: 1795, loss is 0.03787580132484436\n",
      "epoch: 3 step: 1796, loss is 0.0550987608730793\n",
      "epoch: 3 step: 1797, loss is 0.003972796257585287\n",
      "epoch: 3 step: 1798, loss is 0.03703432157635689\n",
      "epoch: 3 step: 1799, loss is 0.03669958934187889\n",
      "epoch: 3 step: 1800, loss is 0.10425260663032532\n",
      "epoch: 3 step: 1801, loss is 0.01637616939842701\n",
      "epoch: 3 step: 1802, loss is 0.11334691196680069\n",
      "epoch: 3 step: 1803, loss is 0.1314782053232193\n",
      "epoch: 3 step: 1804, loss is 0.03043939173221588\n",
      "epoch: 3 step: 1805, loss is 0.017309188842773438\n",
      "epoch: 3 step: 1806, loss is 0.007400340400636196\n",
      "epoch: 3 step: 1807, loss is 0.062337834388017654\n",
      "epoch: 3 step: 1808, loss is 0.016062693670392036\n",
      "epoch: 3 step: 1809, loss is 0.0067823827266693115\n",
      "epoch: 3 step: 1810, loss is 0.09450047463178635\n",
      "epoch: 3 step: 1811, loss is 0.025583581998944283\n",
      "epoch: 3 step: 1812, loss is 0.011100328527390957\n",
      "epoch: 3 step: 1813, loss is 0.03545413166284561\n",
      "epoch: 3 step: 1814, loss is 0.06954571604728699\n",
      "epoch: 3 step: 1815, loss is 0.016416586935520172\n",
      "epoch: 3 step: 1816, loss is 0.0008753053843975067\n",
      "epoch: 3 step: 1817, loss is 0.0030342673417180777\n",
      "epoch: 3 step: 1818, loss is 0.007385062053799629\n",
      "epoch: 3 step: 1819, loss is 0.029129544273018837\n",
      "epoch: 3 step: 1820, loss is 0.0010303773451596498\n",
      "epoch: 3 step: 1821, loss is 0.012017934583127499\n",
      "epoch: 3 step: 1822, loss is 0.007761897053569555\n",
      "epoch: 3 step: 1823, loss is 0.036346435546875\n",
      "epoch: 3 step: 1824, loss is 0.06415987014770508\n",
      "epoch: 3 step: 1825, loss is 0.037619538605213165\n",
      "epoch: 3 step: 1826, loss is 0.02051529474556446\n",
      "epoch: 3 step: 1827, loss is 0.09335369616746902\n",
      "epoch: 3 step: 1828, loss is 0.11959550529718399\n",
      "epoch: 3 step: 1829, loss is 0.1591627299785614\n",
      "epoch: 3 step: 1830, loss is 0.0031993072479963303\n",
      "epoch: 3 step: 1831, loss is 0.04811821132898331\n",
      "epoch: 3 step: 1832, loss is 0.019033748656511307\n",
      "epoch: 3 step: 1833, loss is 0.035700853914022446\n",
      "epoch: 3 step: 1834, loss is 0.0006770131294615567\n",
      "epoch: 3 step: 1835, loss is 0.008717970922589302\n",
      "epoch: 3 step: 1836, loss is 0.0008876433130353689\n",
      "epoch: 3 step: 1837, loss is 0.13966993987560272\n",
      "epoch: 3 step: 1838, loss is 0.022648096084594727\n",
      "epoch: 3 step: 1839, loss is 0.09690061956644058\n",
      "epoch: 3 step: 1840, loss is 0.08017774671316147\n",
      "epoch: 3 step: 1841, loss is 0.00047056202311068773\n",
      "epoch: 3 step: 1842, loss is 0.0029902481473982334\n",
      "epoch: 3 step: 1843, loss is 0.007271756883710623\n",
      "epoch: 3 step: 1844, loss is 0.0328267440199852\n",
      "epoch: 3 step: 1845, loss is 0.10091546177864075\n",
      "epoch: 3 step: 1846, loss is 0.0076086693443357944\n",
      "epoch: 3 step: 1847, loss is 0.04623402655124664\n",
      "epoch: 3 step: 1848, loss is 0.20926496386528015\n",
      "epoch: 3 step: 1849, loss is 0.01603301428258419\n",
      "epoch: 3 step: 1850, loss is 0.0015282667009159923\n",
      "epoch: 3 step: 1851, loss is 0.18183356523513794\n",
      "epoch: 3 step: 1852, loss is 0.010885394178330898\n",
      "epoch: 3 step: 1853, loss is 0.1625240445137024\n",
      "epoch: 3 step: 1854, loss is 0.1773432493209839\n",
      "epoch: 3 step: 1855, loss is 0.07262086868286133\n",
      "epoch: 3 step: 1856, loss is 0.13181409239768982\n",
      "epoch: 3 step: 1857, loss is 0.2693675756454468\n",
      "epoch: 3 step: 1858, loss is 0.004382442682981491\n",
      "epoch: 3 step: 1859, loss is 0.08063959330320358\n",
      "epoch: 3 step: 1860, loss is 0.004560805857181549\n",
      "epoch: 3 step: 1861, loss is 0.047389641404151917\n",
      "epoch: 3 step: 1862, loss is 0.004618912003934383\n",
      "epoch: 3 step: 1863, loss is 0.03767872229218483\n",
      "epoch: 3 step: 1864, loss is 0.21957625448703766\n",
      "epoch: 3 step: 1865, loss is 0.06600656360387802\n",
      "epoch: 3 step: 1866, loss is 0.15422306954860687\n",
      "epoch: 3 step: 1867, loss is 0.006370649207383394\n",
      "epoch: 3 step: 1868, loss is 0.016705695539712906\n",
      "epoch: 3 step: 1869, loss is 0.03416227549314499\n",
      "epoch: 3 step: 1870, loss is 0.019847862422466278\n",
      "epoch: 3 step: 1871, loss is 0.01888650469481945\n",
      "epoch: 3 step: 1872, loss is 0.08264924585819244\n",
      "epoch: 3 step: 1873, loss is 0.038611624389886856\n",
      "epoch: 3 step: 1874, loss is 0.14444226026535034\n",
      "epoch: 3 step: 1875, loss is 0.017527073621749878\n",
      "Train epoch time: 9893.114 ms, per step time: 5.276 ms\n",
      "epoch: 4 step: 1, loss is 0.04017668217420578\n",
      "epoch: 4 step: 2, loss is 0.0018816636875271797\n",
      "epoch: 4 step: 3, loss is 0.02544926479458809\n",
      "epoch: 4 step: 4, loss is 0.05338699743151665\n",
      "epoch: 4 step: 5, loss is 0.012108925729990005\n",
      "epoch: 4 step: 6, loss is 0.006826785858720541\n",
      "epoch: 4 step: 7, loss is 0.0032559444662183523\n",
      "epoch: 4 step: 8, loss is 0.00645443657413125\n",
      "epoch: 4 step: 9, loss is 0.1253972053527832\n",
      "epoch: 4 step: 10, loss is 0.0021286404225975275\n",
      "epoch: 4 step: 11, loss is 0.034304093569517136\n",
      "epoch: 4 step: 12, loss is 0.16318152844905853\n",
      "epoch: 4 step: 13, loss is 0.011830446310341358\n",
      "epoch: 4 step: 14, loss is 0.12803351879119873\n",
      "epoch: 4 step: 15, loss is 0.030874432995915413\n",
      "epoch: 4 step: 16, loss is 0.002366381697356701\n",
      "epoch: 4 step: 17, loss is 0.007230160292237997\n",
      "epoch: 4 step: 18, loss is 0.03514542430639267\n",
      "epoch: 4 step: 19, loss is 0.032354503870010376\n",
      "epoch: 4 step: 20, loss is 0.012122957967221737\n",
      "epoch: 4 step: 21, loss is 0.014719411730766296\n",
      "epoch: 4 step: 22, loss is 0.009752893820405006\n",
      "epoch: 4 step: 23, loss is 0.02126944251358509\n",
      "epoch: 4 step: 24, loss is 0.06769547611474991\n",
      "epoch: 4 step: 25, loss is 0.000972901179920882\n",
      "epoch: 4 step: 26, loss is 0.011578493751585484\n",
      "epoch: 4 step: 27, loss is 0.04491180181503296\n",
      "epoch: 4 step: 28, loss is 0.003443583846092224\n",
      "epoch: 4 step: 29, loss is 0.014351123943924904\n",
      "epoch: 4 step: 30, loss is 0.005404259078204632\n",
      "epoch: 4 step: 31, loss is 0.0017896124627441168\n",
      "epoch: 4 step: 32, loss is 0.007700839079916477\n",
      "epoch: 4 step: 33, loss is 0.0022705248557031155\n",
      "epoch: 4 step: 34, loss is 0.03461878374218941\n",
      "epoch: 4 step: 35, loss is 0.011949577368795872\n",
      "epoch: 4 step: 36, loss is 0.0020707459188997746\n",
      "epoch: 4 step: 37, loss is 0.0015864500310271978\n",
      "epoch: 4 step: 38, loss is 0.017503153532743454\n",
      "epoch: 4 step: 39, loss is 0.0019365391926839948\n",
      "epoch: 4 step: 40, loss is 0.007871023379266262\n",
      "epoch: 4 step: 41, loss is 0.002130926353856921\n",
      "epoch: 4 step: 42, loss is 0.002291204873472452\n",
      "epoch: 4 step: 43, loss is 0.0027815126813948154\n",
      "epoch: 4 step: 44, loss is 0.0025336523540318012\n",
      "epoch: 4 step: 45, loss is 0.009566807188093662\n",
      "epoch: 4 step: 46, loss is 0.011723342351615429\n",
      "epoch: 4 step: 47, loss is 0.0865161344408989\n",
      "epoch: 4 step: 48, loss is 0.0006502783508040011\n",
      "epoch: 4 step: 49, loss is 0.09392362087965012\n",
      "epoch: 4 step: 50, loss is 0.0027180034667253494\n",
      "epoch: 4 step: 51, loss is 0.0013189960736781359\n",
      "epoch: 4 step: 52, loss is 0.018389759585261345\n",
      "epoch: 4 step: 53, loss is 0.0005660208989866078\n",
      "epoch: 4 step: 54, loss is 0.014946116134524345\n",
      "epoch: 4 step: 55, loss is 0.016304761171340942\n",
      "epoch: 4 step: 56, loss is 0.0021417655516415834\n",
      "epoch: 4 step: 57, loss is 0.05543328449130058\n",
      "epoch: 4 step: 58, loss is 0.1860944628715515\n",
      "epoch: 4 step: 59, loss is 0.0006130122928880155\n",
      "epoch: 4 step: 60, loss is 0.1445898413658142\n",
      "epoch: 4 step: 61, loss is 0.0059771062806248665\n",
      "epoch: 4 step: 62, loss is 0.039132699370384216\n",
      "epoch: 4 step: 63, loss is 0.001389139797538519\n",
      "epoch: 4 step: 64, loss is 0.011028974317014217\n",
      "epoch: 4 step: 65, loss is 0.0005458065425045788\n",
      "epoch: 4 step: 66, loss is 0.0023477610666304827\n",
      "epoch: 4 step: 67, loss is 0.011686379089951515\n",
      "epoch: 4 step: 68, loss is 0.0036594972480088472\n",
      "epoch: 4 step: 69, loss is 0.0015781460097059608\n",
      "epoch: 4 step: 70, loss is 0.03173449635505676\n",
      "epoch: 4 step: 71, loss is 0.09258201718330383\n",
      "epoch: 4 step: 72, loss is 0.06862656027078629\n",
      "epoch: 4 step: 73, loss is 0.0047369045205414295\n",
      "epoch: 4 step: 74, loss is 0.014391960576176643\n",
      "epoch: 4 step: 75, loss is 0.08261557668447495\n",
      "epoch: 4 step: 76, loss is 0.06694739311933517\n",
      "epoch: 4 step: 77, loss is 0.001676702406257391\n",
      "epoch: 4 step: 78, loss is 0.33924493193626404\n",
      "epoch: 4 step: 79, loss is 0.16190819442272186\n",
      "epoch: 4 step: 80, loss is 0.02306208200752735\n",
      "epoch: 4 step: 81, loss is 0.044130004942417145\n",
      "epoch: 4 step: 82, loss is 0.008601410314440727\n",
      "epoch: 4 step: 83, loss is 0.02207101322710514\n",
      "epoch: 4 step: 84, loss is 0.008842401206493378\n",
      "epoch: 4 step: 85, loss is 0.05359851196408272\n",
      "epoch: 4 step: 86, loss is 0.0029387210961431265\n",
      "epoch: 4 step: 87, loss is 0.036138955503702164\n",
      "epoch: 4 step: 88, loss is 0.003862394718453288\n",
      "epoch: 4 step: 89, loss is 0.01006294135004282\n",
      "epoch: 4 step: 90, loss is 0.01187566202133894\n",
      "epoch: 4 step: 91, loss is 0.018832175061106682\n",
      "epoch: 4 step: 92, loss is 0.02745286375284195\n",
      "epoch: 4 step: 93, loss is 0.0075921909883618355\n",
      "epoch: 4 step: 94, loss is 0.03665594011545181\n",
      "epoch: 4 step: 95, loss is 0.006212633103132248\n",
      "epoch: 4 step: 96, loss is 0.07013775408267975\n",
      "epoch: 4 step: 97, loss is 0.015634208917617798\n",
      "epoch: 4 step: 98, loss is 0.02625947631895542\n",
      "epoch: 4 step: 99, loss is 0.04981422424316406\n",
      "epoch: 4 step: 100, loss is 0.03850162774324417\n",
      "epoch: 4 step: 101, loss is 0.0031834847759455442\n",
      "epoch: 4 step: 102, loss is 0.0035835281014442444\n",
      "epoch: 4 step: 103, loss is 0.26149094104766846\n",
      "epoch: 4 step: 104, loss is 0.028474492952227592\n",
      "epoch: 4 step: 105, loss is 0.0026344479992985725\n",
      "epoch: 4 step: 106, loss is 0.009263813495635986\n",
      "epoch: 4 step: 107, loss is 0.028343182057142258\n",
      "epoch: 4 step: 108, loss is 0.06434793770313263\n",
      "epoch: 4 step: 109, loss is 0.04117704927921295\n",
      "epoch: 4 step: 110, loss is 0.35683661699295044\n",
      "epoch: 4 step: 111, loss is 0.00467238062992692\n",
      "epoch: 4 step: 112, loss is 0.0637655109167099\n",
      "epoch: 4 step: 113, loss is 0.013839950785040855\n",
      "epoch: 4 step: 114, loss is 0.18329288065433502\n",
      "epoch: 4 step: 115, loss is 0.004503095988184214\n",
      "epoch: 4 step: 116, loss is 0.02647419460117817\n",
      "epoch: 4 step: 117, loss is 0.019826041534543037\n",
      "epoch: 4 step: 118, loss is 0.015694668516516685\n",
      "epoch: 4 step: 119, loss is 0.011207150295376778\n",
      "epoch: 4 step: 120, loss is 0.0027826433070003986\n",
      "epoch: 4 step: 121, loss is 0.02937956340610981\n",
      "epoch: 4 step: 122, loss is 0.0024864680599421263\n",
      "epoch: 4 step: 123, loss is 0.2110133022069931\n",
      "epoch: 4 step: 124, loss is 0.027609478682279587\n",
      "epoch: 4 step: 125, loss is 0.001630779355764389\n",
      "epoch: 4 step: 126, loss is 0.010074714198708534\n",
      "epoch: 4 step: 127, loss is 0.0035989549942314625\n",
      "epoch: 4 step: 128, loss is 0.08011619001626968\n",
      "epoch: 4 step: 129, loss is 0.06731495261192322\n",
      "epoch: 4 step: 130, loss is 0.005679811351001263\n",
      "epoch: 4 step: 131, loss is 0.0009640129283070564\n",
      "epoch: 4 step: 132, loss is 0.003246638458222151\n",
      "epoch: 4 step: 133, loss is 0.014381645247340202\n",
      "epoch: 4 step: 134, loss is 0.02598256804049015\n",
      "epoch: 4 step: 135, loss is 0.003372396342456341\n",
      "epoch: 4 step: 136, loss is 0.0005046218866482377\n",
      "epoch: 4 step: 137, loss is 0.004448846448212862\n",
      "epoch: 4 step: 138, loss is 0.008849186822772026\n",
      "epoch: 4 step: 139, loss is 0.029673391953110695\n",
      "epoch: 4 step: 140, loss is 0.020609010010957718\n",
      "epoch: 4 step: 141, loss is 0.006870371755212545\n",
      "epoch: 4 step: 142, loss is 0.17476175725460052\n",
      "epoch: 4 step: 143, loss is 0.004851026460528374\n",
      "epoch: 4 step: 144, loss is 0.024221334606409073\n",
      "epoch: 4 step: 145, loss is 0.003581627272069454\n",
      "epoch: 4 step: 146, loss is 0.005350380204617977\n",
      "epoch: 4 step: 147, loss is 0.03207901120185852\n",
      "epoch: 4 step: 148, loss is 0.04309599846601486\n",
      "epoch: 4 step: 149, loss is 0.05389948561787605\n",
      "epoch: 4 step: 150, loss is 0.00017360436322633177\n",
      "epoch: 4 step: 151, loss is 0.020591367036104202\n",
      "epoch: 4 step: 152, loss is 0.053792230784893036\n",
      "epoch: 4 step: 153, loss is 0.009739031083881855\n",
      "epoch: 4 step: 154, loss is 0.0035658078268170357\n",
      "epoch: 4 step: 155, loss is 0.00126925902441144\n",
      "epoch: 4 step: 156, loss is 0.05987231060862541\n",
      "epoch: 4 step: 157, loss is 0.007090047001838684\n",
      "epoch: 4 step: 158, loss is 0.015066157095134258\n",
      "epoch: 4 step: 159, loss is 0.0031220123637467623\n",
      "epoch: 4 step: 160, loss is 0.10391136258840561\n",
      "epoch: 4 step: 161, loss is 0.008899431675672531\n",
      "epoch: 4 step: 162, loss is 0.001668559736572206\n",
      "epoch: 4 step: 163, loss is 0.09816627204418182\n",
      "epoch: 4 step: 164, loss is 0.0016870745457708836\n",
      "epoch: 4 step: 165, loss is 0.02790338359773159\n",
      "epoch: 4 step: 166, loss is 0.0015342297265306115\n",
      "epoch: 4 step: 167, loss is 0.013457577675580978\n",
      "epoch: 4 step: 168, loss is 0.0011446966091170907\n",
      "epoch: 4 step: 169, loss is 0.009380228817462921\n",
      "epoch: 4 step: 170, loss is 0.0604541040956974\n",
      "epoch: 4 step: 171, loss is 0.01878429390490055\n",
      "epoch: 4 step: 172, loss is 0.009640250355005264\n",
      "epoch: 4 step: 173, loss is 0.10382536798715591\n",
      "epoch: 4 step: 174, loss is 0.00408395379781723\n",
      "epoch: 4 step: 175, loss is 0.0057512978091835976\n",
      "epoch: 4 step: 176, loss is 0.00013600483362097293\n",
      "epoch: 4 step: 177, loss is 0.06294149905443192\n",
      "epoch: 4 step: 178, loss is 0.01928163692355156\n",
      "epoch: 4 step: 179, loss is 0.001681488356553018\n",
      "epoch: 4 step: 180, loss is 0.0041540758684277534\n",
      "epoch: 4 step: 181, loss is 0.028238117694854736\n",
      "epoch: 4 step: 182, loss is 0.06968800723552704\n",
      "epoch: 4 step: 183, loss is 0.000972567533608526\n",
      "epoch: 4 step: 184, loss is 0.04292183369398117\n",
      "epoch: 4 step: 185, loss is 0.0003630440041888505\n",
      "epoch: 4 step: 186, loss is 0.029593855142593384\n",
      "epoch: 4 step: 187, loss is 0.0006079640006646514\n",
      "epoch: 4 step: 188, loss is 0.04744594171643257\n",
      "epoch: 4 step: 189, loss is 0.006379159167408943\n",
      "epoch: 4 step: 190, loss is 0.14085109531879425\n",
      "epoch: 4 step: 191, loss is 0.0008561313734389842\n",
      "epoch: 4 step: 192, loss is 0.014418985694646835\n",
      "epoch: 4 step: 193, loss is 0.004184390883892775\n",
      "epoch: 4 step: 194, loss is 0.022056667134165764\n",
      "epoch: 4 step: 195, loss is 0.0019332760712131858\n",
      "epoch: 4 step: 196, loss is 0.0022365127224475145\n",
      "epoch: 4 step: 197, loss is 0.013577901758253574\n",
      "epoch: 4 step: 198, loss is 0.03021145798265934\n",
      "epoch: 4 step: 199, loss is 0.009432309307157993\n",
      "epoch: 4 step: 200, loss is 0.028637122362852097\n",
      "epoch: 4 step: 201, loss is 0.004249239340424538\n",
      "epoch: 4 step: 202, loss is 0.002669203095138073\n",
      "epoch: 4 step: 203, loss is 0.04930039867758751\n",
      "epoch: 4 step: 204, loss is 0.036471664905548096\n",
      "epoch: 4 step: 205, loss is 0.0020429010037332773\n",
      "epoch: 4 step: 206, loss is 0.0022425956558436155\n",
      "epoch: 4 step: 207, loss is 0.04649966210126877\n",
      "epoch: 4 step: 208, loss is 0.01214707363396883\n",
      "epoch: 4 step: 209, loss is 0.1334283947944641\n",
      "epoch: 4 step: 210, loss is 0.013262021355330944\n",
      "epoch: 4 step: 211, loss is 0.0009221403743140399\n",
      "epoch: 4 step: 212, loss is 0.004142715595662594\n",
      "epoch: 4 step: 213, loss is 0.01634717732667923\n",
      "epoch: 4 step: 214, loss is 0.03328896686434746\n",
      "epoch: 4 step: 215, loss is 0.0006202667718753219\n",
      "epoch: 4 step: 216, loss is 0.014241570606827736\n",
      "epoch: 4 step: 217, loss is 0.01720762439072132\n",
      "epoch: 4 step: 218, loss is 0.0014255503192543983\n",
      "epoch: 4 step: 219, loss is 0.028893912211060524\n",
      "epoch: 4 step: 220, loss is 0.010287808254361153\n",
      "epoch: 4 step: 221, loss is 0.16279242932796478\n",
      "epoch: 4 step: 222, loss is 0.13422009348869324\n",
      "epoch: 4 step: 223, loss is 0.019036952406167984\n",
      "epoch: 4 step: 224, loss is 0.0026497328653931618\n",
      "epoch: 4 step: 225, loss is 0.007702242117375135\n",
      "epoch: 4 step: 226, loss is 0.044302888214588165\n",
      "epoch: 4 step: 227, loss is 0.011729913763701916\n",
      "epoch: 4 step: 228, loss is 0.047558482736349106\n",
      "epoch: 4 step: 229, loss is 0.001840692013502121\n",
      "epoch: 4 step: 230, loss is 0.004301742650568485\n",
      "epoch: 4 step: 231, loss is 0.2568967640399933\n",
      "epoch: 4 step: 232, loss is 0.0024983305484056473\n",
      "epoch: 4 step: 233, loss is 0.002837671898305416\n",
      "epoch: 4 step: 234, loss is 0.005488163325935602\n",
      "epoch: 4 step: 235, loss is 0.30841493606567383\n",
      "epoch: 4 step: 236, loss is 0.003642419120296836\n",
      "epoch: 4 step: 237, loss is 0.05050722882151604\n",
      "epoch: 4 step: 238, loss is 0.07383862137794495\n",
      "epoch: 4 step: 239, loss is 0.18852147459983826\n",
      "epoch: 4 step: 240, loss is 0.00313576846383512\n",
      "epoch: 4 step: 241, loss is 0.1209062784910202\n",
      "epoch: 4 step: 242, loss is 0.0008224727353081107\n",
      "epoch: 4 step: 243, loss is 0.001042769057676196\n",
      "epoch: 4 step: 244, loss is 0.0015471599763259292\n",
      "epoch: 4 step: 245, loss is 0.006774570792913437\n",
      "epoch: 4 step: 246, loss is 0.023977505043148994\n",
      "epoch: 4 step: 247, loss is 0.0060619753785431385\n",
      "epoch: 4 step: 248, loss is 0.009703016839921474\n",
      "epoch: 4 step: 249, loss is 0.04872956871986389\n",
      "epoch: 4 step: 250, loss is 0.030470987781882286\n",
      "epoch: 4 step: 251, loss is 0.06396624445915222\n",
      "epoch: 4 step: 252, loss is 0.07713133841753006\n",
      "epoch: 4 step: 253, loss is 0.002540008397772908\n",
      "epoch: 4 step: 254, loss is 0.15057864785194397\n",
      "epoch: 4 step: 255, loss is 0.03656700626015663\n",
      "epoch: 4 step: 256, loss is 0.007729110307991505\n",
      "epoch: 4 step: 257, loss is 0.008658125065267086\n",
      "epoch: 4 step: 258, loss is 0.04231061786413193\n",
      "epoch: 4 step: 259, loss is 0.015941444784402847\n",
      "epoch: 4 step: 260, loss is 0.03963656723499298\n",
      "epoch: 4 step: 261, loss is 0.15536430478096008\n",
      "epoch: 4 step: 262, loss is 0.048315584659576416\n",
      "epoch: 4 step: 263, loss is 0.02042452059686184\n",
      "epoch: 4 step: 264, loss is 0.0677802786231041\n",
      "epoch: 4 step: 265, loss is 0.003401794470846653\n",
      "epoch: 4 step: 266, loss is 0.01809902861714363\n",
      "epoch: 4 step: 267, loss is 0.05448059365153313\n",
      "epoch: 4 step: 268, loss is 0.00554425735026598\n",
      "epoch: 4 step: 269, loss is 0.02498660422861576\n",
      "epoch: 4 step: 270, loss is 0.0968232974410057\n",
      "epoch: 4 step: 271, loss is 0.006349105387926102\n",
      "epoch: 4 step: 272, loss is 0.0005827985005453229\n",
      "epoch: 4 step: 273, loss is 0.0949617251753807\n",
      "epoch: 4 step: 274, loss is 0.09352262318134308\n",
      "epoch: 4 step: 275, loss is 0.005941468290984631\n",
      "epoch: 4 step: 276, loss is 0.004799272865056992\n",
      "epoch: 4 step: 277, loss is 0.019528253003954887\n",
      "epoch: 4 step: 278, loss is 0.056210681796073914\n",
      "epoch: 4 step: 279, loss is 0.009571199305355549\n",
      "epoch: 4 step: 280, loss is 0.008079617284238338\n",
      "epoch: 4 step: 281, loss is 0.007889853790402412\n",
      "epoch: 4 step: 282, loss is 0.12601684033870697\n",
      "epoch: 4 step: 283, loss is 0.02096070535480976\n",
      "epoch: 4 step: 284, loss is 0.02618281915783882\n",
      "epoch: 4 step: 285, loss is 0.02011730521917343\n",
      "epoch: 4 step: 286, loss is 0.020267032086849213\n",
      "epoch: 4 step: 287, loss is 0.07063741236925125\n",
      "epoch: 4 step: 288, loss is 0.002492207335308194\n",
      "epoch: 4 step: 289, loss is 0.02055293321609497\n",
      "epoch: 4 step: 290, loss is 0.044346608221530914\n",
      "epoch: 4 step: 291, loss is 0.00039951709914021194\n",
      "epoch: 4 step: 292, loss is 0.03636334463953972\n",
      "epoch: 4 step: 293, loss is 0.013706113211810589\n",
      "epoch: 4 step: 294, loss is 0.0006088122026994824\n",
      "epoch: 4 step: 295, loss is 0.004495660774409771\n",
      "epoch: 4 step: 296, loss is 0.18290695548057556\n",
      "epoch: 4 step: 297, loss is 0.004378347657620907\n",
      "epoch: 4 step: 298, loss is 0.008212047629058361\n",
      "epoch: 4 step: 299, loss is 0.03403047099709511\n",
      "epoch: 4 step: 300, loss is 0.0006908962968736887\n",
      "epoch: 4 step: 301, loss is 0.14161404967308044\n",
      "epoch: 4 step: 302, loss is 0.011919180862605572\n",
      "epoch: 4 step: 303, loss is 0.0012466710759326816\n",
      "epoch: 4 step: 304, loss is 0.007297254633158445\n",
      "epoch: 4 step: 305, loss is 0.08536889404058456\n",
      "epoch: 4 step: 306, loss is 0.0003594088484533131\n",
      "epoch: 4 step: 307, loss is 0.0009864571038633585\n",
      "epoch: 4 step: 308, loss is 0.10194335132837296\n",
      "epoch: 4 step: 309, loss is 0.017853714525699615\n",
      "epoch: 4 step: 310, loss is 0.02119222655892372\n",
      "epoch: 4 step: 311, loss is 0.0025328367482870817\n",
      "epoch: 4 step: 312, loss is 0.04497653618454933\n",
      "epoch: 4 step: 313, loss is 0.01309120561927557\n",
      "epoch: 4 step: 314, loss is 0.04982952028512955\n",
      "epoch: 4 step: 315, loss is 0.15673105418682098\n",
      "epoch: 4 step: 316, loss is 0.010735470801591873\n",
      "epoch: 4 step: 317, loss is 0.03736608847975731\n",
      "epoch: 4 step: 318, loss is 0.00023076606157701463\n",
      "epoch: 4 step: 319, loss is 0.0021140282042324543\n",
      "epoch: 4 step: 320, loss is 0.01268604677170515\n",
      "epoch: 4 step: 321, loss is 0.21426047384738922\n",
      "epoch: 4 step: 322, loss is 0.003132957499474287\n",
      "epoch: 4 step: 323, loss is 0.010915325954556465\n",
      "epoch: 4 step: 324, loss is 0.06363685429096222\n",
      "epoch: 4 step: 325, loss is 0.17513036727905273\n",
      "epoch: 4 step: 326, loss is 0.025824328884482384\n",
      "epoch: 4 step: 327, loss is 0.05297261103987694\n",
      "epoch: 4 step: 328, loss is 0.0031917986925691366\n",
      "epoch: 4 step: 329, loss is 0.0780925303697586\n",
      "epoch: 4 step: 330, loss is 0.05046597495675087\n",
      "epoch: 4 step: 331, loss is 0.0032127159647643566\n",
      "epoch: 4 step: 332, loss is 0.25017043948173523\n",
      "epoch: 4 step: 333, loss is 8.735594747122377e-05\n",
      "epoch: 4 step: 334, loss is 0.0037351055070757866\n",
      "epoch: 4 step: 335, loss is 0.006588074378669262\n",
      "epoch: 4 step: 336, loss is 0.014553245157003403\n",
      "epoch: 4 step: 337, loss is 0.011618191376328468\n",
      "epoch: 4 step: 338, loss is 0.00335960416123271\n",
      "epoch: 4 step: 339, loss is 0.011473803780972958\n",
      "epoch: 4 step: 340, loss is 0.00351059902459383\n",
      "epoch: 4 step: 341, loss is 0.07704557478427887\n",
      "epoch: 4 step: 342, loss is 0.065019391477108\n",
      "epoch: 4 step: 343, loss is 0.08310612291097641\n",
      "epoch: 4 step: 344, loss is 0.0023569921031594276\n",
      "epoch: 4 step: 345, loss is 0.052037667483091354\n",
      "epoch: 4 step: 346, loss is 0.04862425476312637\n",
      "epoch: 4 step: 347, loss is 0.056299902498722076\n",
      "epoch: 4 step: 348, loss is 0.0015256708720698953\n",
      "epoch: 4 step: 349, loss is 0.03516968712210655\n",
      "epoch: 4 step: 350, loss is 0.006866946816444397\n",
      "epoch: 4 step: 351, loss is 0.001053770538419485\n",
      "epoch: 4 step: 352, loss is 0.07749422639608383\n",
      "epoch: 4 step: 353, loss is 0.02128237672150135\n",
      "epoch: 4 step: 354, loss is 0.07093511521816254\n",
      "epoch: 4 step: 355, loss is 0.04754236713051796\n",
      "epoch: 4 step: 356, loss is 0.08036577701568604\n",
      "epoch: 4 step: 357, loss is 0.012445277534425259\n",
      "epoch: 4 step: 358, loss is 0.009192150086164474\n",
      "epoch: 4 step: 359, loss is 0.03421308845281601\n",
      "epoch: 4 step: 360, loss is 0.062056005001068115\n",
      "epoch: 4 step: 361, loss is 0.0018115424318239093\n",
      "epoch: 4 step: 362, loss is 0.01751396432518959\n",
      "epoch: 4 step: 363, loss is 0.0016973211895674467\n",
      "epoch: 4 step: 364, loss is 0.01858786679804325\n",
      "epoch: 4 step: 365, loss is 0.0017706844955682755\n",
      "epoch: 4 step: 366, loss is 0.1635131537914276\n",
      "epoch: 4 step: 367, loss is 0.12080015987157822\n",
      "epoch: 4 step: 368, loss is 0.01442081481218338\n",
      "epoch: 4 step: 369, loss is 0.00792419258505106\n",
      "epoch: 4 step: 370, loss is 0.005288692656904459\n",
      "epoch: 4 step: 371, loss is 0.16512034833431244\n",
      "epoch: 4 step: 372, loss is 0.1638842672109604\n",
      "epoch: 4 step: 373, loss is 0.15381699800491333\n",
      "epoch: 4 step: 374, loss is 0.017374634742736816\n",
      "epoch: 4 step: 375, loss is 0.03572222217917442\n",
      "epoch: 4 step: 376, loss is 0.0025754233356565237\n",
      "epoch: 4 step: 377, loss is 0.063478983938694\n",
      "epoch: 4 step: 378, loss is 0.08976338803768158\n",
      "epoch: 4 step: 379, loss is 0.06793434172868729\n",
      "epoch: 4 step: 380, loss is 0.015924543142318726\n",
      "epoch: 4 step: 381, loss is 0.009942890144884586\n",
      "epoch: 4 step: 382, loss is 0.32824400067329407\n",
      "epoch: 4 step: 383, loss is 0.002382962265983224\n",
      "epoch: 4 step: 384, loss is 0.011987045407295227\n",
      "epoch: 4 step: 385, loss is 0.022354882210493088\n",
      "epoch: 4 step: 386, loss is 0.10822553932666779\n",
      "epoch: 4 step: 387, loss is 0.01960265263915062\n",
      "epoch: 4 step: 388, loss is 0.02111520618200302\n",
      "epoch: 4 step: 389, loss is 0.0010814430424943566\n",
      "epoch: 4 step: 390, loss is 0.003077818313613534\n",
      "epoch: 4 step: 391, loss is 0.09595640003681183\n",
      "epoch: 4 step: 392, loss is 0.0021878639236092567\n",
      "epoch: 4 step: 393, loss is 0.008656332269310951\n",
      "epoch: 4 step: 394, loss is 0.058889057487249374\n",
      "epoch: 4 step: 395, loss is 0.14103448390960693\n",
      "epoch: 4 step: 396, loss is 0.004334436729550362\n",
      "epoch: 4 step: 397, loss is 0.011768891476094723\n",
      "epoch: 4 step: 398, loss is 0.0025849633384495974\n",
      "epoch: 4 step: 399, loss is 0.0064630513079464436\n",
      "epoch: 4 step: 400, loss is 0.0014119603438302875\n",
      "epoch: 4 step: 401, loss is 0.010899988934397697\n",
      "epoch: 4 step: 402, loss is 0.10926157236099243\n",
      "epoch: 4 step: 403, loss is 0.007578158285468817\n",
      "epoch: 4 step: 404, loss is 0.0551227405667305\n",
      "epoch: 4 step: 405, loss is 0.05042782053351402\n",
      "epoch: 4 step: 406, loss is 0.018122175708413124\n",
      "epoch: 4 step: 407, loss is 0.01744009740650654\n",
      "epoch: 4 step: 408, loss is 0.002637990517541766\n",
      "epoch: 4 step: 409, loss is 0.06942090392112732\n",
      "epoch: 4 step: 410, loss is 0.0007794799748808146\n",
      "epoch: 4 step: 411, loss is 0.006128148175776005\n",
      "epoch: 4 step: 412, loss is 0.013328606262803078\n",
      "epoch: 4 step: 413, loss is 0.022685134783387184\n",
      "epoch: 4 step: 414, loss is 0.004773516207933426\n",
      "epoch: 4 step: 415, loss is 0.0011482955887913704\n",
      "epoch: 4 step: 416, loss is 0.004335081670433283\n",
      "epoch: 4 step: 417, loss is 0.009206265211105347\n",
      "epoch: 4 step: 418, loss is 0.090657077729702\n",
      "epoch: 4 step: 419, loss is 0.17823585867881775\n",
      "epoch: 4 step: 420, loss is 0.0030000279657542706\n",
      "epoch: 4 step: 421, loss is 0.009557530283927917\n",
      "epoch: 4 step: 422, loss is 0.008300640620291233\n",
      "epoch: 4 step: 423, loss is 0.3371523320674896\n",
      "epoch: 4 step: 424, loss is 0.0013055835152044892\n",
      "epoch: 4 step: 425, loss is 0.027748607099056244\n",
      "epoch: 4 step: 426, loss is 0.0005600243457593024\n",
      "epoch: 4 step: 427, loss is 0.0200747549533844\n",
      "epoch: 4 step: 428, loss is 0.07506173849105835\n",
      "epoch: 4 step: 429, loss is 0.0008639777079224586\n",
      "epoch: 4 step: 430, loss is 0.0155325373634696\n",
      "epoch: 4 step: 431, loss is 0.031530484557151794\n",
      "epoch: 4 step: 432, loss is 0.0355740450322628\n",
      "epoch: 4 step: 433, loss is 0.009072422049939632\n",
      "epoch: 4 step: 434, loss is 0.016813715919852257\n",
      "epoch: 4 step: 435, loss is 0.025583328679203987\n",
      "epoch: 4 step: 436, loss is 0.13046488165855408\n",
      "epoch: 4 step: 437, loss is 0.0019704587757587433\n",
      "epoch: 4 step: 438, loss is 0.019481243565678596\n",
      "epoch: 4 step: 439, loss is 0.39339679479599\n",
      "epoch: 4 step: 440, loss is 0.00035669642966240644\n",
      "epoch: 4 step: 441, loss is 0.0024833017960190773\n",
      "epoch: 4 step: 442, loss is 0.012689067050814629\n",
      "epoch: 4 step: 443, loss is 0.02131075970828533\n",
      "epoch: 4 step: 444, loss is 0.04540353640913963\n",
      "epoch: 4 step: 445, loss is 0.02228359691798687\n",
      "epoch: 4 step: 446, loss is 0.004598511382937431\n",
      "epoch: 4 step: 447, loss is 0.0015109360683709383\n",
      "epoch: 4 step: 448, loss is 0.05371025949716568\n",
      "epoch: 4 step: 449, loss is 0.002846261952072382\n",
      "epoch: 4 step: 450, loss is 0.10128502547740936\n",
      "epoch: 4 step: 451, loss is 0.003415275365114212\n",
      "epoch: 4 step: 452, loss is 0.041120365262031555\n",
      "epoch: 4 step: 453, loss is 0.017199596390128136\n",
      "epoch: 4 step: 454, loss is 0.004663006868213415\n",
      "epoch: 4 step: 455, loss is 0.009122133255004883\n",
      "epoch: 4 step: 456, loss is 0.0012035587569698691\n",
      "epoch: 4 step: 457, loss is 0.008222928270697594\n",
      "epoch: 4 step: 458, loss is 0.013464598916471004\n",
      "epoch: 4 step: 459, loss is 0.024770770221948624\n",
      "epoch: 4 step: 460, loss is 0.186154305934906\n",
      "epoch: 4 step: 461, loss is 0.0024755520280450583\n",
      "epoch: 4 step: 462, loss is 0.01851506344974041\n",
      "epoch: 4 step: 463, loss is 0.0044553461484611034\n",
      "epoch: 4 step: 464, loss is 0.002295097103342414\n",
      "epoch: 4 step: 465, loss is 0.004896814003586769\n",
      "epoch: 4 step: 466, loss is 0.01875760219991207\n",
      "epoch: 4 step: 467, loss is 0.024129869416356087\n",
      "epoch: 4 step: 468, loss is 0.0283212773501873\n",
      "epoch: 4 step: 469, loss is 0.0037663704715669155\n",
      "epoch: 4 step: 470, loss is 0.0041153645142912865\n",
      "epoch: 4 step: 471, loss is 0.04947458580136299\n",
      "epoch: 4 step: 472, loss is 0.005185345187783241\n",
      "epoch: 4 step: 473, loss is 0.00380842387676239\n",
      "epoch: 4 step: 474, loss is 0.0011995312524959445\n",
      "epoch: 4 step: 475, loss is 0.005903921090066433\n",
      "epoch: 4 step: 476, loss is 0.0939108282327652\n",
      "epoch: 4 step: 477, loss is 0.006541352719068527\n",
      "epoch: 4 step: 478, loss is 0.0145790483802557\n",
      "epoch: 4 step: 479, loss is 0.008234630338847637\n",
      "epoch: 4 step: 480, loss is 0.0017067999579012394\n",
      "epoch: 4 step: 481, loss is 0.011880231089890003\n",
      "epoch: 4 step: 482, loss is 0.005493356380611658\n",
      "epoch: 4 step: 483, loss is 0.00021454654051922262\n",
      "epoch: 4 step: 484, loss is 0.0006579908658750355\n",
      "epoch: 4 step: 485, loss is 0.010196628049015999\n",
      "epoch: 4 step: 486, loss is 0.012857149355113506\n",
      "epoch: 4 step: 487, loss is 0.028352363035082817\n",
      "epoch: 4 step: 488, loss is 0.009254618547856808\n",
      "epoch: 4 step: 489, loss is 0.0007524464162997901\n",
      "epoch: 4 step: 490, loss is 0.0038750048261135817\n",
      "epoch: 4 step: 491, loss is 0.005566174164414406\n",
      "epoch: 4 step: 492, loss is 0.04262412339448929\n",
      "epoch: 4 step: 493, loss is 0.006597946397960186\n",
      "epoch: 4 step: 494, loss is 0.01940855011343956\n",
      "epoch: 4 step: 495, loss is 0.00087924231775105\n",
      "epoch: 4 step: 496, loss is 0.002330642892047763\n",
      "epoch: 4 step: 497, loss is 0.002194782253354788\n",
      "epoch: 4 step: 498, loss is 0.0026547724846750498\n",
      "epoch: 4 step: 499, loss is 0.0010958644561469555\n",
      "epoch: 4 step: 500, loss is 0.00331669463776052\n",
      "epoch: 4 step: 501, loss is 0.07514815777540207\n",
      "epoch: 4 step: 502, loss is 0.04391224682331085\n",
      "epoch: 4 step: 503, loss is 0.04716292768716812\n",
      "epoch: 4 step: 504, loss is 0.0016544379759579897\n",
      "epoch: 4 step: 505, loss is 0.0012725329725071788\n",
      "epoch: 4 step: 506, loss is 0.0012397357495501637\n",
      "epoch: 4 step: 507, loss is 0.0027161044999957085\n",
      "epoch: 4 step: 508, loss is 0.0780145451426506\n",
      "epoch: 4 step: 509, loss is 0.00012385955778881907\n",
      "epoch: 4 step: 510, loss is 0.0024195113219320774\n",
      "epoch: 4 step: 511, loss is 0.0015650360146537423\n",
      "epoch: 4 step: 512, loss is 0.0014717936282977462\n",
      "epoch: 4 step: 513, loss is 0.017521565780043602\n",
      "epoch: 4 step: 514, loss is 0.011941421777009964\n",
      "epoch: 4 step: 515, loss is 0.022277269512414932\n",
      "epoch: 4 step: 516, loss is 0.07139837741851807\n",
      "epoch: 4 step: 517, loss is 0.006654131691902876\n",
      "epoch: 4 step: 518, loss is 0.07460391521453857\n",
      "epoch: 4 step: 519, loss is 0.018420640379190445\n",
      "epoch: 4 step: 520, loss is 0.2682449221611023\n",
      "epoch: 4 step: 521, loss is 0.1362299621105194\n",
      "epoch: 4 step: 522, loss is 0.021325886249542236\n",
      "epoch: 4 step: 523, loss is 0.0007088166894391179\n",
      "epoch: 4 step: 524, loss is 0.0015638850163668394\n",
      "epoch: 4 step: 525, loss is 0.003172051627188921\n",
      "epoch: 4 step: 526, loss is 0.2093629091978073\n",
      "epoch: 4 step: 527, loss is 0.18430951237678528\n",
      "epoch: 4 step: 528, loss is 0.04865321144461632\n",
      "epoch: 4 step: 529, loss is 0.22599145770072937\n",
      "epoch: 4 step: 530, loss is 0.028097564354538918\n",
      "epoch: 4 step: 531, loss is 0.1302950084209442\n",
      "epoch: 4 step: 532, loss is 0.12678973376750946\n",
      "epoch: 4 step: 533, loss is 0.009806877933442593\n",
      "epoch: 4 step: 534, loss is 0.12853068113327026\n",
      "epoch: 4 step: 535, loss is 0.01602085493505001\n",
      "epoch: 4 step: 536, loss is 0.007601281628012657\n",
      "epoch: 4 step: 537, loss is 0.0014334071893244982\n",
      "epoch: 4 step: 538, loss is 0.014876318164169788\n",
      "epoch: 4 step: 539, loss is 0.003040475770831108\n",
      "epoch: 4 step: 540, loss is 0.06870736926794052\n",
      "epoch: 4 step: 541, loss is 0.013651846908032894\n",
      "epoch: 4 step: 542, loss is 0.015569271519780159\n",
      "epoch: 4 step: 543, loss is 0.04333913326263428\n",
      "epoch: 4 step: 544, loss is 0.028330447152256966\n",
      "epoch: 4 step: 545, loss is 0.0013997451169416308\n",
      "epoch: 4 step: 546, loss is 0.00617736391723156\n",
      "epoch: 4 step: 547, loss is 0.13687388598918915\n",
      "epoch: 4 step: 548, loss is 0.02595418132841587\n",
      "epoch: 4 step: 549, loss is 0.012638591229915619\n",
      "epoch: 4 step: 550, loss is 0.001683082664385438\n",
      "epoch: 4 step: 551, loss is 0.009371642023324966\n",
      "epoch: 4 step: 552, loss is 0.04401222616434097\n",
      "epoch: 4 step: 553, loss is 0.0008038496016524732\n",
      "epoch: 4 step: 554, loss is 0.01219114474952221\n",
      "epoch: 4 step: 555, loss is 0.00280544301494956\n",
      "epoch: 4 step: 556, loss is 0.006025743205100298\n",
      "epoch: 4 step: 557, loss is 0.04381630942225456\n",
      "epoch: 4 step: 558, loss is 0.0016889615217223763\n",
      "epoch: 4 step: 559, loss is 0.0005064848810434341\n",
      "epoch: 4 step: 560, loss is 0.0007541642989963293\n",
      "epoch: 4 step: 561, loss is 0.009259123355150223\n",
      "epoch: 4 step: 562, loss is 0.1756083071231842\n",
      "epoch: 4 step: 563, loss is 0.0008982912404462695\n",
      "epoch: 4 step: 564, loss is 0.0047708009369671345\n",
      "epoch: 4 step: 565, loss is 0.02077583782374859\n",
      "epoch: 4 step: 566, loss is 0.014158112928271294\n",
      "epoch: 4 step: 567, loss is 0.006846339907497168\n",
      "epoch: 4 step: 568, loss is 0.018579311668872833\n",
      "epoch: 4 step: 569, loss is 0.00029400124913081527\n",
      "epoch: 4 step: 570, loss is 0.0036048709880560637\n",
      "epoch: 4 step: 571, loss is 8.355281897820532e-05\n",
      "epoch: 4 step: 572, loss is 0.22824883460998535\n",
      "epoch: 4 step: 573, loss is 0.0009212809964083135\n",
      "epoch: 4 step: 574, loss is 0.003307773731648922\n",
      "epoch: 4 step: 575, loss is 0.12130758166313171\n",
      "epoch: 4 step: 576, loss is 0.011646834202110767\n",
      "epoch: 4 step: 577, loss is 0.005437061190605164\n",
      "epoch: 4 step: 578, loss is 0.023607661947607994\n",
      "epoch: 4 step: 579, loss is 0.0033477621618658304\n",
      "epoch: 4 step: 580, loss is 0.016482891514897346\n",
      "epoch: 4 step: 581, loss is 0.00028886814834550023\n",
      "epoch: 4 step: 582, loss is 0.021366974338889122\n",
      "epoch: 4 step: 583, loss is 0.00394840445369482\n",
      "epoch: 4 step: 584, loss is 0.0058576976880431175\n",
      "epoch: 4 step: 585, loss is 0.05704255402088165\n",
      "epoch: 4 step: 586, loss is 0.0005985668394714594\n",
      "epoch: 4 step: 587, loss is 0.0721704438328743\n",
      "epoch: 4 step: 588, loss is 0.0027887970209121704\n",
      "epoch: 4 step: 589, loss is 0.0017140090931206942\n",
      "epoch: 4 step: 590, loss is 0.14171558618545532\n",
      "epoch: 4 step: 591, loss is 0.08862995356321335\n",
      "epoch: 4 step: 592, loss is 0.03251931443810463\n",
      "epoch: 4 step: 593, loss is 0.010544516146183014\n",
      "epoch: 4 step: 594, loss is 0.010095912031829357\n",
      "epoch: 4 step: 595, loss is 0.005596581380814314\n",
      "epoch: 4 step: 596, loss is 0.26853814721107483\n",
      "epoch: 4 step: 597, loss is 0.17953820526599884\n",
      "epoch: 4 step: 598, loss is 0.07014983147382736\n",
      "epoch: 4 step: 599, loss is 0.00877442117780447\n",
      "epoch: 4 step: 600, loss is 0.00032947916770353913\n",
      "epoch: 4 step: 601, loss is 0.002773864660412073\n",
      "epoch: 4 step: 602, loss is 0.0010357658611610532\n",
      "epoch: 4 step: 603, loss is 0.011070769280195236\n",
      "epoch: 4 step: 604, loss is 0.04310102015733719\n",
      "epoch: 4 step: 605, loss is 0.0031809117645025253\n",
      "epoch: 4 step: 606, loss is 0.03771074116230011\n",
      "epoch: 4 step: 607, loss is 0.0009175921441055834\n",
      "epoch: 4 step: 608, loss is 0.0799991637468338\n",
      "epoch: 4 step: 609, loss is 0.042717546224594116\n",
      "epoch: 4 step: 610, loss is 0.003452530363574624\n",
      "epoch: 4 step: 611, loss is 0.0033127772621810436\n",
      "epoch: 4 step: 612, loss is 0.17745418846607208\n",
      "epoch: 4 step: 613, loss is 0.0019587650895118713\n",
      "epoch: 4 step: 614, loss is 0.09846005588769913\n",
      "epoch: 4 step: 615, loss is 0.09395027905702591\n",
      "epoch: 4 step: 616, loss is 0.004765688441693783\n",
      "epoch: 4 step: 617, loss is 0.0015897052362561226\n",
      "epoch: 4 step: 618, loss is 0.052802518010139465\n",
      "epoch: 4 step: 619, loss is 0.0011129227932542562\n",
      "epoch: 4 step: 620, loss is 0.016783885657787323\n",
      "epoch: 4 step: 621, loss is 0.019535163417458534\n",
      "epoch: 4 step: 622, loss is 0.011136967688798904\n",
      "epoch: 4 step: 623, loss is 0.004682488273829222\n",
      "epoch: 4 step: 624, loss is 0.09872909635305405\n",
      "epoch: 4 step: 625, loss is 0.15426403284072876\n",
      "epoch: 4 step: 626, loss is 0.0043486012145876884\n",
      "epoch: 4 step: 627, loss is 0.001011566142551601\n",
      "epoch: 4 step: 628, loss is 0.07446052879095078\n",
      "epoch: 4 step: 629, loss is 0.004322327673435211\n",
      "epoch: 4 step: 630, loss is 0.019386086612939835\n",
      "epoch: 4 step: 631, loss is 0.009296919219195843\n",
      "epoch: 4 step: 632, loss is 0.020991452038288116\n",
      "epoch: 4 step: 633, loss is 0.014769503846764565\n",
      "epoch: 4 step: 634, loss is 0.11324065178632736\n",
      "epoch: 4 step: 635, loss is 0.009185172617435455\n",
      "epoch: 4 step: 636, loss is 0.016033459454774857\n",
      "epoch: 4 step: 637, loss is 0.05139081925153732\n",
      "epoch: 4 step: 638, loss is 0.020840030163526535\n",
      "epoch: 4 step: 639, loss is 0.021425072103738785\n",
      "epoch: 4 step: 640, loss is 0.009878327138721943\n",
      "epoch: 4 step: 641, loss is 0.037151724100112915\n",
      "epoch: 4 step: 642, loss is 0.001599939540028572\n",
      "epoch: 4 step: 643, loss is 0.002288276795297861\n",
      "epoch: 4 step: 644, loss is 0.19141636788845062\n",
      "epoch: 4 step: 645, loss is 0.0433242991566658\n",
      "epoch: 4 step: 646, loss is 0.03109520487487316\n",
      "epoch: 4 step: 647, loss is 0.07765129953622818\n",
      "epoch: 4 step: 648, loss is 0.0026977008674293756\n",
      "epoch: 4 step: 649, loss is 0.003545597195625305\n",
      "epoch: 4 step: 650, loss is 0.025576775893568993\n",
      "epoch: 4 step: 651, loss is 0.006870632991194725\n",
      "epoch: 4 step: 652, loss is 0.014340724796056747\n",
      "epoch: 4 step: 653, loss is 0.006060637533664703\n",
      "epoch: 4 step: 654, loss is 0.0012138746678829193\n",
      "epoch: 4 step: 655, loss is 0.009562225081026554\n",
      "epoch: 4 step: 656, loss is 0.032415904104709625\n",
      "epoch: 4 step: 657, loss is 0.004143948666751385\n",
      "epoch: 4 step: 658, loss is 0.0027122991159558296\n",
      "epoch: 4 step: 659, loss is 0.07485710084438324\n",
      "epoch: 4 step: 660, loss is 0.033751584589481354\n",
      "epoch: 4 step: 661, loss is 0.01707637310028076\n",
      "epoch: 4 step: 662, loss is 0.0009309329325333238\n",
      "epoch: 4 step: 663, loss is 0.0017688657389953732\n",
      "epoch: 4 step: 664, loss is 0.002746599493548274\n",
      "epoch: 4 step: 665, loss is 0.03194248676300049\n",
      "epoch: 4 step: 666, loss is 0.0030189440585672855\n",
      "epoch: 4 step: 667, loss is 0.00675869919359684\n",
      "epoch: 4 step: 668, loss is 0.20042943954467773\n",
      "epoch: 4 step: 669, loss is 0.021514445543289185\n",
      "epoch: 4 step: 670, loss is 0.0007825971697457135\n",
      "epoch: 4 step: 671, loss is 0.00231246673502028\n",
      "epoch: 4 step: 672, loss is 0.011976771056652069\n",
      "epoch: 4 step: 673, loss is 0.0058516208082437515\n",
      "epoch: 4 step: 674, loss is 0.05950813367962837\n",
      "epoch: 4 step: 675, loss is 0.004709187429398298\n",
      "epoch: 4 step: 676, loss is 0.0029863507952541113\n",
      "epoch: 4 step: 677, loss is 0.014309647493064404\n",
      "epoch: 4 step: 678, loss is 0.003937960136681795\n",
      "epoch: 4 step: 679, loss is 0.0024906652979552746\n",
      "epoch: 4 step: 680, loss is 0.002105563646182418\n",
      "epoch: 4 step: 681, loss is 0.030920246616005898\n",
      "epoch: 4 step: 682, loss is 0.03184526041150093\n",
      "epoch: 4 step: 683, loss is 0.1852613240480423\n",
      "epoch: 4 step: 684, loss is 0.1097298339009285\n",
      "epoch: 4 step: 685, loss is 0.0031638750806450844\n",
      "epoch: 4 step: 686, loss is 0.0015415820525959134\n",
      "epoch: 4 step: 687, loss is 0.004639715887606144\n",
      "epoch: 4 step: 688, loss is 0.02530108578503132\n",
      "epoch: 4 step: 689, loss is 0.0015571614494547248\n",
      "epoch: 4 step: 690, loss is 0.0030514507088810205\n",
      "epoch: 4 step: 691, loss is 0.0029856327455490828\n",
      "epoch: 4 step: 692, loss is 0.0549955815076828\n",
      "epoch: 4 step: 693, loss is 0.057292383164167404\n",
      "epoch: 4 step: 694, loss is 0.00412088492885232\n",
      "epoch: 4 step: 695, loss is 0.010192987509071827\n",
      "epoch: 4 step: 696, loss is 0.0002631435636430979\n",
      "epoch: 4 step: 697, loss is 0.007992954924702644\n",
      "epoch: 4 step: 698, loss is 0.012434398755431175\n",
      "epoch: 4 step: 699, loss is 0.02034321054816246\n",
      "epoch: 4 step: 700, loss is 0.10354925692081451\n",
      "epoch: 4 step: 701, loss is 0.09158600866794586\n",
      "epoch: 4 step: 702, loss is 0.012430334463715553\n",
      "epoch: 4 step: 703, loss is 0.05629215016961098\n",
      "epoch: 4 step: 704, loss is 0.0013611852191388607\n",
      "epoch: 4 step: 705, loss is 0.044868458062410355\n",
      "epoch: 4 step: 706, loss is 0.0004361006722319871\n",
      "epoch: 4 step: 707, loss is 0.047886718064546585\n",
      "epoch: 4 step: 708, loss is 0.10280346870422363\n",
      "epoch: 4 step: 709, loss is 0.017044799402356148\n",
      "epoch: 4 step: 710, loss is 0.0022430557291954756\n",
      "epoch: 4 step: 711, loss is 0.024417694658041\n",
      "epoch: 4 step: 712, loss is 0.037932392209768295\n",
      "epoch: 4 step: 713, loss is 0.000819032546132803\n",
      "epoch: 4 step: 714, loss is 0.009811674244701862\n",
      "epoch: 4 step: 715, loss is 0.03630862757563591\n",
      "epoch: 4 step: 716, loss is 0.015819648280739784\n",
      "epoch: 4 step: 717, loss is 0.0016359000001102686\n",
      "epoch: 4 step: 718, loss is 0.0010557075729593635\n",
      "epoch: 4 step: 719, loss is 0.004903905559331179\n",
      "epoch: 4 step: 720, loss is 0.08532363176345825\n",
      "epoch: 4 step: 721, loss is 0.15898291766643524\n",
      "epoch: 4 step: 722, loss is 0.01784328930079937\n",
      "epoch: 4 step: 723, loss is 0.0053169094026088715\n",
      "epoch: 4 step: 724, loss is 0.0025656179059296846\n",
      "epoch: 4 step: 725, loss is 0.0015545919304713607\n",
      "epoch: 4 step: 726, loss is 0.0007073188317008317\n",
      "epoch: 4 step: 727, loss is 0.003138388739898801\n",
      "epoch: 4 step: 728, loss is 0.08358924090862274\n",
      "epoch: 4 step: 729, loss is 0.0009679722716100514\n",
      "epoch: 4 step: 730, loss is 0.0060540735721588135\n",
      "epoch: 4 step: 731, loss is 0.2519665062427521\n",
      "epoch: 4 step: 732, loss is 0.008618796244263649\n",
      "epoch: 4 step: 733, loss is 0.06166868656873703\n",
      "epoch: 4 step: 734, loss is 0.0006068076472729445\n",
      "epoch: 4 step: 735, loss is 0.008869128301739693\n",
      "epoch: 4 step: 736, loss is 0.00047474936582148075\n",
      "epoch: 4 step: 737, loss is 0.001705279923044145\n",
      "epoch: 4 step: 738, loss is 0.03239454701542854\n",
      "epoch: 4 step: 739, loss is 0.0005338827613741159\n",
      "epoch: 4 step: 740, loss is 0.03861411660909653\n",
      "epoch: 4 step: 741, loss is 0.0062436205334961414\n",
      "epoch: 4 step: 742, loss is 0.1393834948539734\n",
      "epoch: 4 step: 743, loss is 0.18196550011634827\n",
      "epoch: 4 step: 744, loss is 0.3330629765987396\n",
      "epoch: 4 step: 745, loss is 0.0003857806441374123\n",
      "epoch: 4 step: 746, loss is 0.0023684215266257524\n",
      "epoch: 4 step: 747, loss is 0.0008850687299855053\n",
      "epoch: 4 step: 748, loss is 0.07297082245349884\n",
      "epoch: 4 step: 749, loss is 0.011855662800371647\n",
      "epoch: 4 step: 750, loss is 0.16971543431282043\n",
      "epoch: 4 step: 751, loss is 0.06964730471372604\n",
      "epoch: 4 step: 752, loss is 0.004777427297085524\n",
      "epoch: 4 step: 753, loss is 0.00566085847094655\n",
      "epoch: 4 step: 754, loss is 0.026680491864681244\n",
      "epoch: 4 step: 755, loss is 0.02651970088481903\n",
      "epoch: 4 step: 756, loss is 0.0014474624767899513\n",
      "epoch: 4 step: 757, loss is 0.015076341107487679\n",
      "epoch: 4 step: 758, loss is 0.08894103765487671\n",
      "epoch: 4 step: 759, loss is 0.07588660717010498\n",
      "epoch: 4 step: 760, loss is 0.0027634561993181705\n",
      "epoch: 4 step: 761, loss is 0.0030842784326523542\n",
      "epoch: 4 step: 762, loss is 0.10813779383897781\n",
      "epoch: 4 step: 763, loss is 0.00014883154653944075\n",
      "epoch: 4 step: 764, loss is 0.021660927683115005\n",
      "epoch: 4 step: 765, loss is 0.0011972923530265689\n",
      "epoch: 4 step: 766, loss is 0.003377863671630621\n",
      "epoch: 4 step: 767, loss is 0.000508825178258121\n",
      "epoch: 4 step: 768, loss is 0.006113453768193722\n",
      "epoch: 4 step: 769, loss is 0.05075778067111969\n",
      "epoch: 4 step: 770, loss is 0.018773004412651062\n",
      "epoch: 4 step: 771, loss is 0.0031765205785632133\n",
      "epoch: 4 step: 772, loss is 0.12726515531539917\n",
      "epoch: 4 step: 773, loss is 0.12518545985221863\n",
      "epoch: 4 step: 774, loss is 0.0032329901587218046\n",
      "epoch: 4 step: 775, loss is 0.07848425954580307\n",
      "epoch: 4 step: 776, loss is 0.1696273535490036\n",
      "epoch: 4 step: 777, loss is 0.05115146189928055\n",
      "epoch: 4 step: 778, loss is 0.0006859805434942245\n",
      "epoch: 4 step: 779, loss is 0.03861112520098686\n",
      "epoch: 4 step: 780, loss is 0.03234230354428291\n",
      "epoch: 4 step: 781, loss is 0.009206539951264858\n",
      "epoch: 4 step: 782, loss is 0.17427508533000946\n",
      "epoch: 4 step: 783, loss is 0.0009597087046131492\n",
      "epoch: 4 step: 784, loss is 0.0021663070656359196\n",
      "epoch: 4 step: 785, loss is 0.004857833031564951\n",
      "epoch: 4 step: 786, loss is 0.11587396264076233\n",
      "epoch: 4 step: 787, loss is 0.10999289155006409\n",
      "epoch: 4 step: 788, loss is 0.008234632201492786\n",
      "epoch: 4 step: 789, loss is 0.023646600544452667\n",
      "epoch: 4 step: 790, loss is 0.013581831008195877\n",
      "epoch: 4 step: 791, loss is 0.00526887271553278\n",
      "epoch: 4 step: 792, loss is 0.0029705523047596216\n",
      "epoch: 4 step: 793, loss is 0.0143102603033185\n",
      "epoch: 4 step: 794, loss is 0.03351307287812233\n",
      "epoch: 4 step: 795, loss is 0.02508237399160862\n",
      "epoch: 4 step: 796, loss is 0.16297852993011475\n",
      "epoch: 4 step: 797, loss is 0.004507456440478563\n",
      "epoch: 4 step: 798, loss is 0.0022807701025158167\n",
      "epoch: 4 step: 799, loss is 0.005442080087959766\n",
      "epoch: 4 step: 800, loss is 0.0030666240490972996\n",
      "epoch: 4 step: 801, loss is 0.0010923537192866206\n",
      "epoch: 4 step: 802, loss is 0.024501845240592957\n",
      "epoch: 4 step: 803, loss is 0.003494189353659749\n",
      "epoch: 4 step: 804, loss is 0.039495229721069336\n",
      "epoch: 4 step: 805, loss is 0.002058481564745307\n",
      "epoch: 4 step: 806, loss is 0.0005586184561252594\n",
      "epoch: 4 step: 807, loss is 0.08771925419569016\n",
      "epoch: 4 step: 808, loss is 0.07025767117738724\n",
      "epoch: 4 step: 809, loss is 0.001236422685906291\n",
      "epoch: 4 step: 810, loss is 0.008428705856204033\n",
      "epoch: 4 step: 811, loss is 0.0030075572431087494\n",
      "epoch: 4 step: 812, loss is 0.02399478666484356\n",
      "epoch: 4 step: 813, loss is 0.02324141003191471\n",
      "epoch: 4 step: 814, loss is 0.0024315582122653723\n",
      "epoch: 4 step: 815, loss is 0.0032322178594768047\n",
      "epoch: 4 step: 816, loss is 0.040967416018247604\n",
      "epoch: 4 step: 817, loss is 0.010199261829257011\n",
      "epoch: 4 step: 818, loss is 0.14320948719978333\n",
      "epoch: 4 step: 819, loss is 0.009464925155043602\n",
      "epoch: 4 step: 820, loss is 0.0008717919117771089\n",
      "epoch: 4 step: 821, loss is 0.041528262197971344\n",
      "epoch: 4 step: 822, loss is 0.03474738448858261\n",
      "epoch: 4 step: 823, loss is 0.06988726556301117\n",
      "epoch: 4 step: 824, loss is 0.018918059766292572\n",
      "epoch: 4 step: 825, loss is 0.0052077327854931355\n",
      "epoch: 4 step: 826, loss is 0.057367950677871704\n",
      "epoch: 4 step: 827, loss is 0.004893924575299025\n",
      "epoch: 4 step: 828, loss is 0.0419667586684227\n",
      "epoch: 4 step: 829, loss is 0.026547815650701523\n",
      "epoch: 4 step: 830, loss is 0.0011829399736598134\n",
      "epoch: 4 step: 831, loss is 0.006995806470513344\n",
      "epoch: 4 step: 832, loss is 0.0007348121143877506\n",
      "epoch: 4 step: 833, loss is 0.10209006071090698\n",
      "epoch: 4 step: 834, loss is 0.0010353680700063705\n",
      "epoch: 4 step: 835, loss is 0.00971461646258831\n",
      "epoch: 4 step: 836, loss is 0.08708447962999344\n",
      "epoch: 4 step: 837, loss is 0.003641702700406313\n",
      "epoch: 4 step: 838, loss is 0.01281854510307312\n",
      "epoch: 4 step: 839, loss is 0.007281086407601833\n",
      "epoch: 4 step: 840, loss is 0.006823043338954449\n",
      "epoch: 4 step: 841, loss is 0.009815556928515434\n",
      "epoch: 4 step: 842, loss is 0.0007847117958590388\n",
      "epoch: 4 step: 843, loss is 0.002781585557386279\n",
      "epoch: 4 step: 844, loss is 0.03346735239028931\n",
      "epoch: 4 step: 845, loss is 0.021075893193483353\n",
      "epoch: 4 step: 846, loss is 0.14196862280368805\n",
      "epoch: 4 step: 847, loss is 0.00036339182406663895\n",
      "epoch: 4 step: 848, loss is 0.008876550942659378\n",
      "epoch: 4 step: 849, loss is 0.18473558127880096\n",
      "epoch: 4 step: 850, loss is 0.05075272172689438\n",
      "epoch: 4 step: 851, loss is 0.0004265313036739826\n",
      "epoch: 4 step: 852, loss is 0.0022055725567042828\n",
      "epoch: 4 step: 853, loss is 0.0611586831510067\n",
      "epoch: 4 step: 854, loss is 0.0033715986646711826\n",
      "epoch: 4 step: 855, loss is 0.05482744053006172\n",
      "epoch: 4 step: 856, loss is 0.0019368922803550959\n",
      "epoch: 4 step: 857, loss is 0.030894292518496513\n",
      "epoch: 4 step: 858, loss is 0.0001349247177131474\n",
      "epoch: 4 step: 859, loss is 0.002781171817332506\n",
      "epoch: 4 step: 860, loss is 0.01426042802631855\n",
      "epoch: 4 step: 861, loss is 0.008275443688035011\n",
      "epoch: 4 step: 862, loss is 0.004015528131276369\n",
      "epoch: 4 step: 863, loss is 0.00037150224670767784\n",
      "epoch: 4 step: 864, loss is 0.0009068364161066711\n",
      "epoch: 4 step: 865, loss is 0.1004694327712059\n",
      "epoch: 4 step: 866, loss is 0.007875044830143452\n",
      "epoch: 4 step: 867, loss is 0.003765240777283907\n",
      "epoch: 4 step: 868, loss is 0.019549744203686714\n",
      "epoch: 4 step: 869, loss is 0.016222219914197922\n",
      "epoch: 4 step: 870, loss is 0.007239515893161297\n",
      "epoch: 4 step: 871, loss is 0.4104466140270233\n",
      "epoch: 4 step: 872, loss is 0.06234029307961464\n",
      "epoch: 4 step: 873, loss is 0.06282354891300201\n",
      "epoch: 4 step: 874, loss is 0.001092991093173623\n",
      "epoch: 4 step: 875, loss is 0.0011612583184614778\n",
      "epoch: 4 step: 876, loss is 0.086982861161232\n",
      "epoch: 4 step: 877, loss is 0.10885079950094223\n",
      "epoch: 4 step: 878, loss is 0.0036378693766891956\n",
      "epoch: 4 step: 879, loss is 0.043304406106472015\n",
      "epoch: 4 step: 880, loss is 0.0027131028473377228\n",
      "epoch: 4 step: 881, loss is 0.00019041058840230107\n",
      "epoch: 4 step: 882, loss is 0.0307315681129694\n",
      "epoch: 4 step: 883, loss is 0.18854515254497528\n",
      "epoch: 4 step: 884, loss is 0.002386560197919607\n",
      "epoch: 4 step: 885, loss is 0.2777172327041626\n",
      "epoch: 4 step: 886, loss is 0.003844514489173889\n",
      "epoch: 4 step: 887, loss is 0.056039631366729736\n",
      "epoch: 4 step: 888, loss is 0.00385109381750226\n",
      "epoch: 4 step: 889, loss is 0.0018058738205581903\n",
      "epoch: 4 step: 890, loss is 0.004809247329831123\n",
      "epoch: 4 step: 891, loss is 0.008968140929937363\n",
      "epoch: 4 step: 892, loss is 0.06390280276536942\n",
      "epoch: 4 step: 893, loss is 0.11404584348201752\n",
      "epoch: 4 step: 894, loss is 0.015986368060112\n",
      "epoch: 4 step: 895, loss is 0.001490487833507359\n",
      "epoch: 4 step: 896, loss is 0.050028085708618164\n",
      "epoch: 4 step: 897, loss is 0.00448793126270175\n",
      "epoch: 4 step: 898, loss is 0.0021293098106980324\n",
      "epoch: 4 step: 899, loss is 0.015104234218597412\n",
      "epoch: 4 step: 900, loss is 0.07926177233457565\n",
      "epoch: 4 step: 901, loss is 0.04840534180402756\n",
      "epoch: 4 step: 902, loss is 0.09777829051017761\n",
      "epoch: 4 step: 903, loss is 0.014958842657506466\n",
      "epoch: 4 step: 904, loss is 0.09818404912948608\n",
      "epoch: 4 step: 905, loss is 0.03374747559428215\n",
      "epoch: 4 step: 906, loss is 0.0828179344534874\n",
      "epoch: 4 step: 907, loss is 0.06912758201360703\n",
      "epoch: 4 step: 908, loss is 0.042169734835624695\n",
      "epoch: 4 step: 909, loss is 0.145393967628479\n",
      "epoch: 4 step: 910, loss is 0.00039989157812669873\n",
      "epoch: 4 step: 911, loss is 0.10652243345975876\n",
      "epoch: 4 step: 912, loss is 0.0002334108139621094\n",
      "epoch: 4 step: 913, loss is 0.014275847002863884\n",
      "epoch: 4 step: 914, loss is 0.08006114512681961\n",
      "epoch: 4 step: 915, loss is 0.004459609277546406\n",
      "epoch: 4 step: 916, loss is 0.24180977046489716\n",
      "epoch: 4 step: 917, loss is 0.07248107343912125\n",
      "epoch: 4 step: 918, loss is 0.003324651625007391\n",
      "epoch: 4 step: 919, loss is 0.014948150143027306\n",
      "epoch: 4 step: 920, loss is 0.021126942709088326\n",
      "epoch: 4 step: 921, loss is 0.03880339860916138\n",
      "epoch: 4 step: 922, loss is 0.009605715051293373\n",
      "epoch: 4 step: 923, loss is 0.12587574124336243\n",
      "epoch: 4 step: 924, loss is 0.003430634271353483\n",
      "epoch: 4 step: 925, loss is 0.07185572385787964\n",
      "epoch: 4 step: 926, loss is 0.004372484516352415\n",
      "epoch: 4 step: 927, loss is 0.12173766642808914\n",
      "epoch: 4 step: 928, loss is 0.1665755957365036\n",
      "epoch: 4 step: 929, loss is 0.009623877704143524\n",
      "epoch: 4 step: 930, loss is 0.2099751979112625\n",
      "epoch: 4 step: 931, loss is 0.0010886621894314885\n",
      "epoch: 4 step: 932, loss is 0.030489854514598846\n",
      "epoch: 4 step: 933, loss is 0.0027237136382609606\n",
      "epoch: 4 step: 934, loss is 0.03824996203184128\n",
      "epoch: 4 step: 935, loss is 0.011937658302485943\n",
      "epoch: 4 step: 936, loss is 0.008950777351856232\n",
      "epoch: 4 step: 937, loss is 0.017089439556002617\n",
      "epoch: 4 step: 938, loss is 0.059406109154224396\n",
      "epoch: 4 step: 939, loss is 0.08140093833208084\n",
      "epoch: 4 step: 940, loss is 0.019631225615739822\n",
      "epoch: 4 step: 941, loss is 0.04126177355647087\n",
      "epoch: 4 step: 942, loss is 0.020525213330984116\n",
      "epoch: 4 step: 943, loss is 0.007472265046089888\n",
      "epoch: 4 step: 944, loss is 0.10374590754508972\n",
      "epoch: 4 step: 945, loss is 0.008134124800562859\n",
      "epoch: 4 step: 946, loss is 0.004040638450533152\n",
      "epoch: 4 step: 947, loss is 0.004381123464554548\n",
      "epoch: 4 step: 948, loss is 0.006677909754216671\n",
      "epoch: 4 step: 949, loss is 0.049911800771951675\n",
      "epoch: 4 step: 950, loss is 0.007830537855625153\n",
      "epoch: 4 step: 951, loss is 0.006238020956516266\n",
      "epoch: 4 step: 952, loss is 0.0010360304731875658\n",
      "epoch: 4 step: 953, loss is 0.09785819053649902\n",
      "epoch: 4 step: 954, loss is 0.013501361943781376\n",
      "epoch: 4 step: 955, loss is 0.0683412104845047\n",
      "epoch: 4 step: 956, loss is 0.002833997132256627\n",
      "epoch: 4 step: 957, loss is 0.0035704253241419792\n",
      "epoch: 4 step: 958, loss is 0.1949404776096344\n",
      "epoch: 4 step: 959, loss is 0.07192166894674301\n",
      "epoch: 4 step: 960, loss is 0.18157455325126648\n",
      "epoch: 4 step: 961, loss is 0.01969529688358307\n",
      "epoch: 4 step: 962, loss is 0.0010730307549238205\n",
      "epoch: 4 step: 963, loss is 0.12837640941143036\n",
      "epoch: 4 step: 964, loss is 0.01694430038332939\n",
      "epoch: 4 step: 965, loss is 0.0029269258957356215\n",
      "epoch: 4 step: 966, loss is 0.030925916507840157\n",
      "epoch: 4 step: 967, loss is 0.12689900398254395\n",
      "epoch: 4 step: 968, loss is 0.12029022723436356\n",
      "epoch: 4 step: 969, loss is 0.012383116409182549\n",
      "epoch: 4 step: 970, loss is 0.0004066716064698994\n",
      "epoch: 4 step: 971, loss is 0.07039252668619156\n",
      "epoch: 4 step: 972, loss is 0.12649944424629211\n",
      "epoch: 4 step: 973, loss is 0.0016832135152071714\n",
      "epoch: 4 step: 974, loss is 0.0023498227819800377\n",
      "epoch: 4 step: 975, loss is 0.003340980503708124\n",
      "epoch: 4 step: 976, loss is 0.06305625289678574\n",
      "epoch: 4 step: 977, loss is 0.02051684446632862\n",
      "epoch: 4 step: 978, loss is 0.010905928909778595\n",
      "epoch: 4 step: 979, loss is 0.0005805975524708629\n",
      "epoch: 4 step: 980, loss is 0.024749159812927246\n",
      "epoch: 4 step: 981, loss is 0.006925774272531271\n",
      "epoch: 4 step: 982, loss is 0.06127222254872322\n",
      "epoch: 4 step: 983, loss is 0.04525325074791908\n",
      "epoch: 4 step: 984, loss is 0.0255582332611084\n",
      "epoch: 4 step: 985, loss is 0.12058653682470322\n",
      "epoch: 4 step: 986, loss is 0.0007061072392389178\n",
      "epoch: 4 step: 987, loss is 0.011068947613239288\n",
      "epoch: 4 step: 988, loss is 0.004373784177005291\n",
      "epoch: 4 step: 989, loss is 0.03074122592806816\n",
      "epoch: 4 step: 990, loss is 0.006805687677115202\n",
      "epoch: 4 step: 991, loss is 0.014428948052227497\n",
      "epoch: 4 step: 992, loss is 0.0048643192276358604\n",
      "epoch: 4 step: 993, loss is 0.0002684591745492071\n",
      "epoch: 4 step: 994, loss is 0.002840012079104781\n",
      "epoch: 4 step: 995, loss is 0.013359432108700275\n",
      "epoch: 4 step: 996, loss is 0.0009094811393879354\n",
      "epoch: 4 step: 997, loss is 0.0015368933090940118\n",
      "epoch: 4 step: 998, loss is 0.004073297139257193\n",
      "epoch: 4 step: 999, loss is 0.008552639745175838\n",
      "epoch: 4 step: 1000, loss is 0.0179568063467741\n",
      "epoch: 4 step: 1001, loss is 0.007276111748069525\n",
      "epoch: 4 step: 1002, loss is 0.013882678933441639\n",
      "epoch: 4 step: 1003, loss is 0.01706118695437908\n",
      "epoch: 4 step: 1004, loss is 0.0009844813030213118\n",
      "epoch: 4 step: 1005, loss is 0.004734130576252937\n",
      "epoch: 4 step: 1006, loss is 0.002089656423777342\n",
      "epoch: 4 step: 1007, loss is 0.0983581393957138\n",
      "epoch: 4 step: 1008, loss is 0.007666291203349829\n",
      "epoch: 4 step: 1009, loss is 0.01911631040275097\n",
      "epoch: 4 step: 1010, loss is 0.0021031417418271303\n",
      "epoch: 4 step: 1011, loss is 0.15134447813034058\n",
      "epoch: 4 step: 1012, loss is 0.020144058391451836\n",
      "epoch: 4 step: 1013, loss is 0.0012417667312547565\n",
      "epoch: 4 step: 1014, loss is 0.01066113542765379\n",
      "epoch: 4 step: 1015, loss is 0.0022868469823151827\n",
      "epoch: 4 step: 1016, loss is 0.026507191359996796\n",
      "epoch: 4 step: 1017, loss is 0.01933641918003559\n",
      "epoch: 4 step: 1018, loss is 0.0022785395849496126\n",
      "epoch: 4 step: 1019, loss is 0.0005769970593973994\n",
      "epoch: 4 step: 1020, loss is 0.017269371077418327\n",
      "epoch: 4 step: 1021, loss is 0.0024151206016540527\n",
      "epoch: 4 step: 1022, loss is 0.004271317273378372\n",
      "epoch: 4 step: 1023, loss is 0.19734062254428864\n",
      "epoch: 4 step: 1024, loss is 0.010725375264883041\n",
      "epoch: 4 step: 1025, loss is 0.009743251837790012\n",
      "epoch: 4 step: 1026, loss is 0.0007520217914134264\n",
      "epoch: 4 step: 1027, loss is 0.0017107856692746282\n",
      "epoch: 4 step: 1028, loss is 0.005128820892423391\n",
      "epoch: 4 step: 1029, loss is 0.011891379952430725\n",
      "epoch: 4 step: 1030, loss is 0.00045253417920321226\n",
      "epoch: 4 step: 1031, loss is 0.004861575085669756\n",
      "epoch: 4 step: 1032, loss is 0.0012435588287189603\n",
      "epoch: 4 step: 1033, loss is 0.12813648581504822\n",
      "epoch: 4 step: 1034, loss is 0.006901168264448643\n",
      "epoch: 4 step: 1035, loss is 0.001562646240927279\n",
      "epoch: 4 step: 1036, loss is 0.02871881239116192\n",
      "epoch: 4 step: 1037, loss is 0.12650880217552185\n",
      "epoch: 4 step: 1038, loss is 0.001712548895739019\n",
      "epoch: 4 step: 1039, loss is 0.0063336980529129505\n",
      "epoch: 4 step: 1040, loss is 0.0023270591627806425\n",
      "epoch: 4 step: 1041, loss is 0.0014807813568040729\n",
      "epoch: 4 step: 1042, loss is 0.011824664659798145\n",
      "epoch: 4 step: 1043, loss is 0.0023346147499978542\n",
      "epoch: 4 step: 1044, loss is 0.014923528768122196\n",
      "epoch: 4 step: 1045, loss is 0.03805779665708542\n",
      "epoch: 4 step: 1046, loss is 0.03338806331157684\n",
      "epoch: 4 step: 1047, loss is 0.01599002256989479\n",
      "epoch: 4 step: 1048, loss is 0.051689594984054565\n",
      "epoch: 4 step: 1049, loss is 0.08390988409519196\n",
      "epoch: 4 step: 1050, loss is 0.0002954450319521129\n",
      "epoch: 4 step: 1051, loss is 0.0029841456562280655\n",
      "epoch: 4 step: 1052, loss is 0.23750513792037964\n",
      "epoch: 4 step: 1053, loss is 0.010038670152425766\n",
      "epoch: 4 step: 1054, loss is 0.07395956665277481\n",
      "epoch: 4 step: 1055, loss is 0.0019381886813789606\n",
      "epoch: 4 step: 1056, loss is 0.0009085205383598804\n",
      "epoch: 4 step: 1057, loss is 0.006499557755887508\n",
      "epoch: 4 step: 1058, loss is 0.06479568779468536\n",
      "epoch: 4 step: 1059, loss is 0.048138003796339035\n",
      "epoch: 4 step: 1060, loss is 0.007056954316794872\n",
      "epoch: 4 step: 1061, loss is 0.07534658163785934\n",
      "epoch: 4 step: 1062, loss is 0.29949676990509033\n",
      "epoch: 4 step: 1063, loss is 0.06992100924253464\n",
      "epoch: 4 step: 1064, loss is 0.08297283947467804\n",
      "epoch: 4 step: 1065, loss is 0.2316872477531433\n",
      "epoch: 4 step: 1066, loss is 0.004592650104314089\n",
      "epoch: 4 step: 1067, loss is 0.05456697195768356\n",
      "epoch: 4 step: 1068, loss is 0.04736432805657387\n",
      "epoch: 4 step: 1069, loss is 0.004172782879322767\n",
      "epoch: 4 step: 1070, loss is 0.012755841948091984\n",
      "epoch: 4 step: 1071, loss is 0.004094547126442194\n",
      "epoch: 4 step: 1072, loss is 0.011186172254383564\n",
      "epoch: 4 step: 1073, loss is 0.0020656054839491844\n",
      "epoch: 4 step: 1074, loss is 0.011436428874731064\n",
      "epoch: 4 step: 1075, loss is 0.10468349605798721\n",
      "epoch: 4 step: 1076, loss is 0.0048340847715735435\n",
      "epoch: 4 step: 1077, loss is 0.0023728813976049423\n",
      "epoch: 4 step: 1078, loss is 0.07750820368528366\n",
      "epoch: 4 step: 1079, loss is 0.06010744720697403\n",
      "epoch: 4 step: 1080, loss is 0.01331144105643034\n",
      "epoch: 4 step: 1081, loss is 0.010998187586665154\n",
      "epoch: 4 step: 1082, loss is 0.08939351886510849\n",
      "epoch: 4 step: 1083, loss is 0.060605354607105255\n",
      "epoch: 4 step: 1084, loss is 0.07557462900876999\n",
      "epoch: 4 step: 1085, loss is 0.001923191244713962\n",
      "epoch: 4 step: 1086, loss is 0.021141959354281425\n",
      "epoch: 4 step: 1087, loss is 0.01778031885623932\n",
      "epoch: 4 step: 1088, loss is 0.02393403835594654\n",
      "epoch: 4 step: 1089, loss is 0.00799407809972763\n",
      "epoch: 4 step: 1090, loss is 0.01832074485719204\n",
      "epoch: 4 step: 1091, loss is 0.002130836481228471\n",
      "epoch: 4 step: 1092, loss is 0.0007857236778363585\n",
      "epoch: 4 step: 1093, loss is 0.0014966204762458801\n",
      "epoch: 4 step: 1094, loss is 0.06471893191337585\n",
      "epoch: 4 step: 1095, loss is 0.023437362164258957\n",
      "epoch: 4 step: 1096, loss is 0.012139175087213516\n",
      "epoch: 4 step: 1097, loss is 0.0016186402644962072\n",
      "epoch: 4 step: 1098, loss is 0.07207992672920227\n",
      "epoch: 4 step: 1099, loss is 0.0016622759867459536\n",
      "epoch: 4 step: 1100, loss is 0.010411795228719711\n",
      "epoch: 4 step: 1101, loss is 0.005295831244438887\n",
      "epoch: 4 step: 1102, loss is 0.03518327325582504\n",
      "epoch: 4 step: 1103, loss is 0.013837714679539204\n",
      "epoch: 4 step: 1104, loss is 0.09889375418424606\n",
      "epoch: 4 step: 1105, loss is 0.03748220577836037\n",
      "epoch: 4 step: 1106, loss is 0.02398272417485714\n",
      "epoch: 4 step: 1107, loss is 0.01850505918264389\n",
      "epoch: 4 step: 1108, loss is 0.0016134718898683786\n",
      "epoch: 4 step: 1109, loss is 0.11876782029867172\n",
      "epoch: 4 step: 1110, loss is 0.12073242664337158\n",
      "epoch: 4 step: 1111, loss is 0.020913030952215195\n",
      "epoch: 4 step: 1112, loss is 0.000944065279327333\n",
      "epoch: 4 step: 1113, loss is 0.0006649703136645257\n",
      "epoch: 4 step: 1114, loss is 0.029261218383908272\n",
      "epoch: 4 step: 1115, loss is 0.00041712867096066475\n",
      "epoch: 4 step: 1116, loss is 0.005066468380391598\n",
      "epoch: 4 step: 1117, loss is 0.0004987127031199634\n",
      "epoch: 4 step: 1118, loss is 0.003440511878579855\n",
      "epoch: 4 step: 1119, loss is 0.009133053943514824\n",
      "epoch: 4 step: 1120, loss is 0.011723903007805347\n",
      "epoch: 4 step: 1121, loss is 0.0030170721001923084\n",
      "epoch: 4 step: 1122, loss is 0.0029852576553821564\n",
      "epoch: 4 step: 1123, loss is 0.0001926069671753794\n",
      "epoch: 4 step: 1124, loss is 0.0005600196891464293\n",
      "epoch: 4 step: 1125, loss is 0.2656219005584717\n",
      "epoch: 4 step: 1126, loss is 0.05512158200144768\n",
      "epoch: 4 step: 1127, loss is 0.010918371379375458\n",
      "epoch: 4 step: 1128, loss is 0.004972234833985567\n",
      "epoch: 4 step: 1129, loss is 0.10233698785305023\n",
      "epoch: 4 step: 1130, loss is 0.044538695365190506\n",
      "epoch: 4 step: 1131, loss is 0.021284297108650208\n",
      "epoch: 4 step: 1132, loss is 0.0046669733710587025\n",
      "epoch: 4 step: 1133, loss is 0.0005081992712803185\n",
      "epoch: 4 step: 1134, loss is 0.046885598450899124\n",
      "epoch: 4 step: 1135, loss is 0.0018973457626998425\n",
      "epoch: 4 step: 1136, loss is 0.19374850392341614\n",
      "epoch: 4 step: 1137, loss is 0.0009249934228137136\n",
      "epoch: 4 step: 1138, loss is 0.04269074276089668\n",
      "epoch: 4 step: 1139, loss is 0.0022355234250426292\n",
      "epoch: 4 step: 1140, loss is 0.003157534170895815\n",
      "epoch: 4 step: 1141, loss is 0.0021283673122525215\n",
      "epoch: 4 step: 1142, loss is 0.0014205154730007052\n",
      "epoch: 4 step: 1143, loss is 0.03201579675078392\n",
      "epoch: 4 step: 1144, loss is 0.061137501150369644\n",
      "epoch: 4 step: 1145, loss is 0.0004848659737035632\n",
      "epoch: 4 step: 1146, loss is 0.003848982509225607\n",
      "epoch: 4 step: 1147, loss is 0.002953300252556801\n",
      "epoch: 4 step: 1148, loss is 0.0020459217485040426\n",
      "epoch: 4 step: 1149, loss is 0.0026873850729316473\n",
      "epoch: 4 step: 1150, loss is 0.14079397916793823\n",
      "epoch: 4 step: 1151, loss is 0.026813847944140434\n",
      "epoch: 4 step: 1152, loss is 0.0937882736325264\n",
      "epoch: 4 step: 1153, loss is 0.004585120361298323\n",
      "epoch: 4 step: 1154, loss is 0.0010234210640192032\n",
      "epoch: 4 step: 1155, loss is 0.212219700217247\n",
      "epoch: 4 step: 1156, loss is 0.08600297570228577\n",
      "epoch: 4 step: 1157, loss is 0.010365000925958157\n",
      "epoch: 4 step: 1158, loss is 0.048930294811725616\n",
      "epoch: 4 step: 1159, loss is 0.008914435282349586\n",
      "epoch: 4 step: 1160, loss is 0.0005968462792225182\n",
      "epoch: 4 step: 1161, loss is 0.005644061136990786\n",
      "epoch: 4 step: 1162, loss is 0.04735102131962776\n",
      "epoch: 4 step: 1163, loss is 0.02062450349330902\n",
      "epoch: 4 step: 1164, loss is 0.00882550049573183\n",
      "epoch: 4 step: 1165, loss is 0.09913108497858047\n",
      "epoch: 4 step: 1166, loss is 0.08505680412054062\n",
      "epoch: 4 step: 1167, loss is 0.061101581901311874\n",
      "epoch: 4 step: 1168, loss is 0.010281712748110294\n",
      "epoch: 4 step: 1169, loss is 0.023353222757577896\n",
      "epoch: 4 step: 1170, loss is 0.08647192269563675\n",
      "epoch: 4 step: 1171, loss is 0.006861456669867039\n",
      "epoch: 4 step: 1172, loss is 0.00841289572417736\n",
      "epoch: 4 step: 1173, loss is 0.13558833301067352\n",
      "epoch: 4 step: 1174, loss is 0.06296762824058533\n",
      "epoch: 4 step: 1175, loss is 0.15736931562423706\n",
      "epoch: 4 step: 1176, loss is 0.20287352800369263\n",
      "epoch: 4 step: 1177, loss is 0.0019605711568146944\n",
      "epoch: 4 step: 1178, loss is 0.0014347370015457273\n",
      "epoch: 4 step: 1179, loss is 0.0020857921335846186\n",
      "epoch: 4 step: 1180, loss is 0.012967055663466454\n",
      "epoch: 4 step: 1181, loss is 0.0014884795527905226\n",
      "epoch: 4 step: 1182, loss is 0.008716719225049019\n",
      "epoch: 4 step: 1183, loss is 0.04509999603033066\n",
      "epoch: 4 step: 1184, loss is 0.008923977613449097\n",
      "epoch: 4 step: 1185, loss is 0.06057920306921005\n",
      "epoch: 4 step: 1186, loss is 0.05977870151400566\n",
      "epoch: 4 step: 1187, loss is 0.33027610182762146\n",
      "epoch: 4 step: 1188, loss is 0.009099003858864307\n",
      "epoch: 4 step: 1189, loss is 0.005846103187650442\n",
      "epoch: 4 step: 1190, loss is 0.13135764002799988\n",
      "epoch: 4 step: 1191, loss is 0.052562374621629715\n",
      "epoch: 4 step: 1192, loss is 0.3116551339626312\n",
      "epoch: 4 step: 1193, loss is 0.01488573756068945\n",
      "epoch: 4 step: 1194, loss is 0.03579171746969223\n",
      "epoch: 4 step: 1195, loss is 0.07917414605617523\n",
      "epoch: 4 step: 1196, loss is 0.014761018566787243\n",
      "epoch: 4 step: 1197, loss is 0.058821648359298706\n",
      "epoch: 4 step: 1198, loss is 0.009894926100969315\n",
      "epoch: 4 step: 1199, loss is 0.0050872983410954475\n",
      "epoch: 4 step: 1200, loss is 0.0021984120830893517\n",
      "epoch: 4 step: 1201, loss is 0.09893146902322769\n",
      "epoch: 4 step: 1202, loss is 0.05892246216535568\n",
      "epoch: 4 step: 1203, loss is 0.0019486090168356895\n",
      "epoch: 4 step: 1204, loss is 0.15856467187404633\n",
      "epoch: 4 step: 1205, loss is 0.011909225024282932\n",
      "epoch: 4 step: 1206, loss is 0.029781388118863106\n",
      "epoch: 4 step: 1207, loss is 0.006520683877170086\n",
      "epoch: 4 step: 1208, loss is 0.04069830849766731\n",
      "epoch: 4 step: 1209, loss is 0.018830327317118645\n",
      "epoch: 4 step: 1210, loss is 0.04828449338674545\n",
      "epoch: 4 step: 1211, loss is 0.0244615375995636\n",
      "epoch: 4 step: 1212, loss is 0.003063467564061284\n",
      "epoch: 4 step: 1213, loss is 0.01756158098578453\n",
      "epoch: 4 step: 1214, loss is 0.0037011713720858097\n",
      "epoch: 4 step: 1215, loss is 0.014141512103378773\n",
      "epoch: 4 step: 1216, loss is 0.009113384410738945\n",
      "epoch: 4 step: 1217, loss is 0.1896788477897644\n",
      "epoch: 4 step: 1218, loss is 0.025569861754775047\n",
      "epoch: 4 step: 1219, loss is 0.005564942955970764\n",
      "epoch: 4 step: 1220, loss is 0.0034664939157664776\n",
      "epoch: 4 step: 1221, loss is 0.006419883109629154\n",
      "epoch: 4 step: 1222, loss is 0.03038506954908371\n",
      "epoch: 4 step: 1223, loss is 0.003456969279795885\n",
      "epoch: 4 step: 1224, loss is 0.004403769038617611\n",
      "epoch: 4 step: 1225, loss is 0.025079650804400444\n",
      "epoch: 4 step: 1226, loss is 0.15523661673069\n",
      "epoch: 4 step: 1227, loss is 0.05801285430788994\n",
      "epoch: 4 step: 1228, loss is 0.03399711847305298\n",
      "epoch: 4 step: 1229, loss is 0.004252202343195677\n",
      "epoch: 4 step: 1230, loss is 0.05341219902038574\n",
      "epoch: 4 step: 1231, loss is 0.01050841435790062\n",
      "epoch: 4 step: 1232, loss is 0.011538599617779255\n",
      "epoch: 4 step: 1233, loss is 0.006182868033647537\n",
      "epoch: 4 step: 1234, loss is 0.0006222660886123776\n",
      "epoch: 4 step: 1235, loss is 0.0049307141453027725\n",
      "epoch: 4 step: 1236, loss is 0.21794266998767853\n",
      "epoch: 4 step: 1237, loss is 0.010019955225288868\n",
      "epoch: 4 step: 1238, loss is 0.0005941265844739974\n",
      "epoch: 4 step: 1239, loss is 0.0018134808633476496\n",
      "epoch: 4 step: 1240, loss is 0.019553396850824356\n",
      "epoch: 4 step: 1241, loss is 0.0006152911810204387\n",
      "epoch: 4 step: 1242, loss is 0.0004794257110916078\n",
      "epoch: 4 step: 1243, loss is 0.007378342095762491\n",
      "epoch: 4 step: 1244, loss is 0.009365218691527843\n",
      "epoch: 4 step: 1245, loss is 0.010351024568080902\n",
      "epoch: 4 step: 1246, loss is 0.022259173914790154\n",
      "epoch: 4 step: 1247, loss is 0.08039657026529312\n",
      "epoch: 4 step: 1248, loss is 0.0010617523221299052\n",
      "epoch: 4 step: 1249, loss is 0.19427835941314697\n",
      "epoch: 4 step: 1250, loss is 0.0042629241943359375\n",
      "epoch: 4 step: 1251, loss is 0.24277447164058685\n",
      "epoch: 4 step: 1252, loss is 0.00419575022533536\n",
      "epoch: 4 step: 1253, loss is 0.003762762062251568\n",
      "epoch: 4 step: 1254, loss is 0.002028572838753462\n",
      "epoch: 4 step: 1255, loss is 0.010323593392968178\n",
      "epoch: 4 step: 1256, loss is 0.01955161988735199\n",
      "epoch: 4 step: 1257, loss is 0.08275141566991806\n",
      "epoch: 4 step: 1258, loss is 0.004974063020199537\n",
      "epoch: 4 step: 1259, loss is 0.38464391231536865\n",
      "epoch: 4 step: 1260, loss is 0.20231309533119202\n",
      "epoch: 4 step: 1261, loss is 0.01641431450843811\n",
      "epoch: 4 step: 1262, loss is 0.01131067331880331\n",
      "epoch: 4 step: 1263, loss is 0.026682229712605476\n",
      "epoch: 4 step: 1264, loss is 0.0009259933140128851\n",
      "epoch: 4 step: 1265, loss is 0.27393922209739685\n",
      "epoch: 4 step: 1266, loss is 0.011244085617363453\n",
      "epoch: 4 step: 1267, loss is 0.030258962884545326\n",
      "epoch: 4 step: 1268, loss is 0.14827291667461395\n",
      "epoch: 4 step: 1269, loss is 0.001817928859964013\n",
      "epoch: 4 step: 1270, loss is 0.00229616928845644\n",
      "epoch: 4 step: 1271, loss is 0.016358396038413048\n",
      "epoch: 4 step: 1272, loss is 0.017953764647245407\n",
      "epoch: 4 step: 1273, loss is 0.0024479173589497805\n",
      "epoch: 4 step: 1274, loss is 0.15619699656963348\n",
      "epoch: 4 step: 1275, loss is 0.006702104117721319\n",
      "epoch: 4 step: 1276, loss is 0.017467819154262543\n",
      "epoch: 4 step: 1277, loss is 0.1853107362985611\n",
      "epoch: 4 step: 1278, loss is 0.05728779733181\n",
      "epoch: 4 step: 1279, loss is 0.01117017213255167\n",
      "epoch: 4 step: 1280, loss is 0.06286285072565079\n",
      "epoch: 4 step: 1281, loss is 0.0906083807349205\n",
      "epoch: 4 step: 1282, loss is 0.0035912750754505396\n",
      "epoch: 4 step: 1283, loss is 0.001288029132410884\n",
      "epoch: 4 step: 1284, loss is 0.00672856206074357\n",
      "epoch: 4 step: 1285, loss is 0.0011337975738570094\n",
      "epoch: 4 step: 1286, loss is 0.012062687426805496\n",
      "epoch: 4 step: 1287, loss is 0.006422777660191059\n",
      "epoch: 4 step: 1288, loss is 0.013661595061421394\n",
      "epoch: 4 step: 1289, loss is 0.003049007151275873\n",
      "epoch: 4 step: 1290, loss is 0.03647196292877197\n",
      "epoch: 4 step: 1291, loss is 0.0833442285656929\n",
      "epoch: 4 step: 1292, loss is 0.04115461930632591\n",
      "epoch: 4 step: 1293, loss is 0.05998097360134125\n",
      "epoch: 4 step: 1294, loss is 0.0009498988511040807\n",
      "epoch: 4 step: 1295, loss is 0.014332383871078491\n",
      "epoch: 4 step: 1296, loss is 0.007090546656399965\n",
      "epoch: 4 step: 1297, loss is 0.0021896190010011196\n",
      "epoch: 4 step: 1298, loss is 0.1013481542468071\n",
      "epoch: 4 step: 1299, loss is 0.003074369626119733\n",
      "epoch: 4 step: 1300, loss is 0.01541835442185402\n",
      "epoch: 4 step: 1301, loss is 0.0054129986092448235\n",
      "epoch: 4 step: 1302, loss is 0.005191779229789972\n",
      "epoch: 4 step: 1303, loss is 0.06505583226680756\n",
      "epoch: 4 step: 1304, loss is 0.011590655893087387\n",
      "epoch: 4 step: 1305, loss is 0.03143032267689705\n",
      "epoch: 4 step: 1306, loss is 0.09554766863584518\n",
      "epoch: 4 step: 1307, loss is 0.019900524988770485\n",
      "epoch: 4 step: 1308, loss is 0.002480695489794016\n",
      "epoch: 4 step: 1309, loss is 0.0003380067937541753\n",
      "epoch: 4 step: 1310, loss is 0.0011964692967012525\n",
      "epoch: 4 step: 1311, loss is 0.0026510125026106834\n",
      "epoch: 4 step: 1312, loss is 0.018794666975736618\n",
      "epoch: 4 step: 1313, loss is 0.07060913741588593\n",
      "epoch: 4 step: 1314, loss is 0.006235094275325537\n",
      "epoch: 4 step: 1315, loss is 0.00022217923833522946\n",
      "epoch: 4 step: 1316, loss is 0.02102208510041237\n",
      "epoch: 4 step: 1317, loss is 0.026928529143333435\n",
      "epoch: 4 step: 1318, loss is 0.047323573380708694\n",
      "epoch: 4 step: 1319, loss is 0.006538147572427988\n",
      "epoch: 4 step: 1320, loss is 0.011693569831550121\n",
      "epoch: 4 step: 1321, loss is 0.01060094777494669\n",
      "epoch: 4 step: 1322, loss is 0.23018045723438263\n",
      "epoch: 4 step: 1323, loss is 0.03610406070947647\n",
      "epoch: 4 step: 1324, loss is 0.001610006671398878\n",
      "epoch: 4 step: 1325, loss is 0.01958257146179676\n",
      "epoch: 4 step: 1326, loss is 0.008558888919651508\n",
      "epoch: 4 step: 1327, loss is 0.018537655472755432\n",
      "epoch: 4 step: 1328, loss is 0.006847927812486887\n",
      "epoch: 4 step: 1329, loss is 0.009141646325588226\n",
      "epoch: 4 step: 1330, loss is 0.00951294880360365\n",
      "epoch: 4 step: 1331, loss is 0.0013483113143593073\n",
      "epoch: 4 step: 1332, loss is 0.05405433103442192\n",
      "epoch: 4 step: 1333, loss is 0.08010443300008774\n",
      "epoch: 4 step: 1334, loss is 0.013875395059585571\n",
      "epoch: 4 step: 1335, loss is 0.001262481790035963\n",
      "epoch: 4 step: 1336, loss is 0.013353509828448296\n",
      "epoch: 4 step: 1337, loss is 0.09454359114170074\n",
      "epoch: 4 step: 1338, loss is 0.01837744750082493\n",
      "epoch: 4 step: 1339, loss is 0.0007447890238836408\n",
      "epoch: 4 step: 1340, loss is 0.023872017860412598\n",
      "epoch: 4 step: 1341, loss is 0.23160041868686676\n",
      "epoch: 4 step: 1342, loss is 0.0075011542066931725\n",
      "epoch: 4 step: 1343, loss is 0.0030371411703526974\n",
      "epoch: 4 step: 1344, loss is 0.04977425932884216\n",
      "epoch: 4 step: 1345, loss is 0.013857469893991947\n",
      "epoch: 4 step: 1346, loss is 0.10029755532741547\n",
      "epoch: 4 step: 1347, loss is 0.03371632844209671\n",
      "epoch: 4 step: 1348, loss is 0.008164853788912296\n",
      "epoch: 4 step: 1349, loss is 0.00807665754109621\n",
      "epoch: 4 step: 1350, loss is 0.011740757152438164\n",
      "epoch: 4 step: 1351, loss is 0.021323326975107193\n",
      "epoch: 4 step: 1352, loss is 0.13513949513435364\n",
      "epoch: 4 step: 1353, loss is 0.11198028177022934\n",
      "epoch: 4 step: 1354, loss is 0.017736777663230896\n",
      "epoch: 4 step: 1355, loss is 0.08339779078960419\n",
      "epoch: 4 step: 1356, loss is 0.08290939778089523\n",
      "epoch: 4 step: 1357, loss is 0.022724531590938568\n",
      "epoch: 4 step: 1358, loss is 0.0591837577521801\n",
      "epoch: 4 step: 1359, loss is 0.0008128158515319228\n",
      "epoch: 4 step: 1360, loss is 0.005981395486742258\n",
      "epoch: 4 step: 1361, loss is 0.05318274348974228\n",
      "epoch: 4 step: 1362, loss is 0.07015708088874817\n",
      "epoch: 4 step: 1363, loss is 0.08311257511377335\n",
      "epoch: 4 step: 1364, loss is 0.02062077820301056\n",
      "epoch: 4 step: 1365, loss is 0.0039600408636033535\n",
      "epoch: 4 step: 1366, loss is 0.031836848706007004\n",
      "epoch: 4 step: 1367, loss is 0.004063831176608801\n",
      "epoch: 4 step: 1368, loss is 0.2028123438358307\n",
      "epoch: 4 step: 1369, loss is 0.012897278182208538\n",
      "epoch: 4 step: 1370, loss is 0.015671079978346825\n",
      "epoch: 4 step: 1371, loss is 0.007353166583925486\n",
      "epoch: 4 step: 1372, loss is 0.07367867231369019\n",
      "epoch: 4 step: 1373, loss is 0.0016446433728560805\n",
      "epoch: 4 step: 1374, loss is 0.03212006762623787\n",
      "epoch: 4 step: 1375, loss is 0.006765642203390598\n",
      "epoch: 4 step: 1376, loss is 0.01620461232960224\n",
      "epoch: 4 step: 1377, loss is 0.03783417120575905\n",
      "epoch: 4 step: 1378, loss is 0.01639688014984131\n",
      "epoch: 4 step: 1379, loss is 0.0013225083239376545\n",
      "epoch: 4 step: 1380, loss is 0.055103663355112076\n",
      "epoch: 4 step: 1381, loss is 0.0011600006837397814\n",
      "epoch: 4 step: 1382, loss is 0.03432707116007805\n",
      "epoch: 4 step: 1383, loss is 0.004711896646767855\n",
      "epoch: 4 step: 1384, loss is 0.033674564212560654\n",
      "epoch: 4 step: 1385, loss is 0.03234013915061951\n",
      "epoch: 4 step: 1386, loss is 0.005509696900844574\n",
      "epoch: 4 step: 1387, loss is 0.05436718463897705\n",
      "epoch: 4 step: 1388, loss is 0.02134508639574051\n",
      "epoch: 4 step: 1389, loss is 0.004639216233044863\n",
      "epoch: 4 step: 1390, loss is 0.10447879880666733\n",
      "epoch: 4 step: 1391, loss is 0.0007919999188743532\n",
      "epoch: 4 step: 1392, loss is 0.07800876349210739\n",
      "epoch: 4 step: 1393, loss is 0.1508455127477646\n",
      "epoch: 4 step: 1394, loss is 0.0008259955211542547\n",
      "epoch: 4 step: 1395, loss is 0.14584742486476898\n",
      "epoch: 4 step: 1396, loss is 0.0029750049579888582\n",
      "epoch: 4 step: 1397, loss is 0.03827841952443123\n",
      "epoch: 4 step: 1398, loss is 0.004497175104916096\n",
      "epoch: 4 step: 1399, loss is 0.0024468579795211554\n",
      "epoch: 4 step: 1400, loss is 0.0030028659384697676\n",
      "epoch: 4 step: 1401, loss is 0.018549250438809395\n",
      "epoch: 4 step: 1402, loss is 0.07273396104574203\n",
      "epoch: 4 step: 1403, loss is 0.025583330541849136\n",
      "epoch: 4 step: 1404, loss is 0.003526876913383603\n",
      "epoch: 4 step: 1405, loss is 0.09155724197626114\n",
      "epoch: 4 step: 1406, loss is 0.0010781249729916453\n",
      "epoch: 4 step: 1407, loss is 0.0014233645051717758\n",
      "epoch: 4 step: 1408, loss is 0.0015566092915832996\n",
      "epoch: 4 step: 1409, loss is 0.0012854024535045028\n",
      "epoch: 4 step: 1410, loss is 0.015901193022727966\n",
      "epoch: 4 step: 1411, loss is 0.013610687106847763\n",
      "epoch: 4 step: 1412, loss is 0.22446317970752716\n",
      "epoch: 4 step: 1413, loss is 0.030339449644088745\n",
      "epoch: 4 step: 1414, loss is 0.001034718006849289\n",
      "epoch: 4 step: 1415, loss is 0.0030055998358875513\n",
      "epoch: 4 step: 1416, loss is 0.001571107772178948\n",
      "epoch: 4 step: 1417, loss is 0.28757327795028687\n",
      "epoch: 4 step: 1418, loss is 0.00048607453936710954\n",
      "epoch: 4 step: 1419, loss is 0.11970238387584686\n",
      "epoch: 4 step: 1420, loss is 0.007963121868669987\n",
      "epoch: 4 step: 1421, loss is 0.0009616187890060246\n",
      "epoch: 4 step: 1422, loss is 0.005812784191220999\n",
      "epoch: 4 step: 1423, loss is 0.0038162681739777327\n",
      "epoch: 4 step: 1424, loss is 0.06295236200094223\n",
      "epoch: 4 step: 1425, loss is 0.0029128536116331816\n",
      "epoch: 4 step: 1426, loss is 0.00270558032207191\n",
      "epoch: 4 step: 1427, loss is 0.003009794745594263\n",
      "epoch: 4 step: 1428, loss is 0.08968018740415573\n",
      "epoch: 4 step: 1429, loss is 0.008016616106033325\n",
      "epoch: 4 step: 1430, loss is 0.006221049930900335\n",
      "epoch: 4 step: 1431, loss is 0.005321725737303495\n",
      "epoch: 4 step: 1432, loss is 0.003196673234924674\n",
      "epoch: 4 step: 1433, loss is 0.006375402677804232\n",
      "epoch: 4 step: 1434, loss is 0.0417807400226593\n",
      "epoch: 4 step: 1435, loss is 0.029129456728696823\n",
      "epoch: 4 step: 1436, loss is 0.0016171967145055532\n",
      "epoch: 4 step: 1437, loss is 0.024433590471744537\n",
      "epoch: 4 step: 1438, loss is 0.01655726134777069\n",
      "epoch: 4 step: 1439, loss is 0.1652294397354126\n",
      "epoch: 4 step: 1440, loss is 0.0006654436001554132\n",
      "epoch: 4 step: 1441, loss is 0.02266894280910492\n",
      "epoch: 4 step: 1442, loss is 0.014963900670409203\n",
      "epoch: 4 step: 1443, loss is 0.010134098120033741\n",
      "epoch: 4 step: 1444, loss is 0.04001636803150177\n",
      "epoch: 4 step: 1445, loss is 0.0015357586089521646\n",
      "epoch: 4 step: 1446, loss is 0.044063206762075424\n",
      "epoch: 4 step: 1447, loss is 0.004528857301920652\n",
      "epoch: 4 step: 1448, loss is 0.030672280117869377\n",
      "epoch: 4 step: 1449, loss is 0.0604240782558918\n",
      "epoch: 4 step: 1450, loss is 0.009886403568089008\n",
      "epoch: 4 step: 1451, loss is 0.12631213665008545\n",
      "epoch: 4 step: 1452, loss is 0.023721689358353615\n",
      "epoch: 4 step: 1453, loss is 0.0027878363616764545\n",
      "epoch: 4 step: 1454, loss is 0.00943419337272644\n",
      "epoch: 4 step: 1455, loss is 0.22151601314544678\n",
      "epoch: 4 step: 1456, loss is 0.25517305731773376\n",
      "epoch: 4 step: 1457, loss is 0.021962027996778488\n",
      "epoch: 4 step: 1458, loss is 0.0007571929018013179\n",
      "epoch: 4 step: 1459, loss is 0.0161857008934021\n",
      "epoch: 4 step: 1460, loss is 4.1146951843984425e-05\n",
      "epoch: 4 step: 1461, loss is 0.27775177359580994\n",
      "epoch: 4 step: 1462, loss is 0.011628425680100918\n",
      "epoch: 4 step: 1463, loss is 0.0018982304027304053\n",
      "epoch: 4 step: 1464, loss is 0.04135284200310707\n",
      "epoch: 4 step: 1465, loss is 0.029305607080459595\n",
      "epoch: 4 step: 1466, loss is 0.0503070205450058\n",
      "epoch: 4 step: 1467, loss is 0.034216102212667465\n",
      "epoch: 4 step: 1468, loss is 0.30526480078697205\n",
      "epoch: 4 step: 1469, loss is 0.046230267733335495\n",
      "epoch: 4 step: 1470, loss is 0.0006850966601632535\n",
      "epoch: 4 step: 1471, loss is 0.003562965663149953\n",
      "epoch: 4 step: 1472, loss is 0.022705236449837685\n",
      "epoch: 4 step: 1473, loss is 0.13254684209823608\n",
      "epoch: 4 step: 1474, loss is 0.0016715124947950244\n",
      "epoch: 4 step: 1475, loss is 0.01145167089998722\n",
      "epoch: 4 step: 1476, loss is 0.009245435707271099\n",
      "epoch: 4 step: 1477, loss is 0.11913919448852539\n",
      "epoch: 4 step: 1478, loss is 0.007476599887013435\n",
      "epoch: 4 step: 1479, loss is 0.0020416050683707\n",
      "epoch: 4 step: 1480, loss is 0.0007596655050292611\n",
      "epoch: 4 step: 1481, loss is 0.04843771830201149\n",
      "epoch: 4 step: 1482, loss is 0.0011857978533953428\n",
      "epoch: 4 step: 1483, loss is 0.0019590030424296856\n",
      "epoch: 4 step: 1484, loss is 0.04188042879104614\n",
      "epoch: 4 step: 1485, loss is 0.014266112819314003\n",
      "epoch: 4 step: 1486, loss is 0.0014478988014161587\n",
      "epoch: 4 step: 1487, loss is 0.003411763347685337\n",
      "epoch: 4 step: 1488, loss is 0.0818868950009346\n",
      "epoch: 4 step: 1489, loss is 0.1410614550113678\n",
      "epoch: 4 step: 1490, loss is 0.08118616789579391\n",
      "epoch: 4 step: 1491, loss is 0.12653905153274536\n",
      "epoch: 4 step: 1492, loss is 0.03498786315321922\n",
      "epoch: 4 step: 1493, loss is 0.1680164635181427\n",
      "epoch: 4 step: 1494, loss is 0.08119433373212814\n",
      "epoch: 4 step: 1495, loss is 0.00732720922678709\n",
      "epoch: 4 step: 1496, loss is 0.17385482788085938\n",
      "epoch: 4 step: 1497, loss is 0.23102200031280518\n",
      "epoch: 4 step: 1498, loss is 0.00029739204910583794\n",
      "epoch: 4 step: 1499, loss is 0.06666355580091476\n",
      "epoch: 4 step: 1500, loss is 0.03003135696053505\n",
      "epoch: 4 step: 1501, loss is 0.028794005513191223\n",
      "epoch: 4 step: 1502, loss is 0.17706626653671265\n",
      "epoch: 4 step: 1503, loss is 0.09851580858230591\n",
      "epoch: 4 step: 1504, loss is 0.0103992884978652\n",
      "epoch: 4 step: 1505, loss is 0.2609560489654541\n",
      "epoch: 4 step: 1506, loss is 0.0029890197329223156\n",
      "epoch: 4 step: 1507, loss is 0.04062286391854286\n",
      "epoch: 4 step: 1508, loss is 0.03606605902314186\n",
      "epoch: 4 step: 1509, loss is 0.003429098753258586\n",
      "epoch: 4 step: 1510, loss is 0.025836797431111336\n",
      "epoch: 4 step: 1511, loss is 0.09542908519506454\n",
      "epoch: 4 step: 1512, loss is 0.0003757135709747672\n",
      "epoch: 4 step: 1513, loss is 0.004152999259531498\n",
      "epoch: 4 step: 1514, loss is 0.005578417796641588\n",
      "epoch: 4 step: 1515, loss is 0.002622661180794239\n",
      "epoch: 4 step: 1516, loss is 0.14311949908733368\n",
      "epoch: 4 step: 1517, loss is 0.006936409045010805\n",
      "epoch: 4 step: 1518, loss is 0.08854133635759354\n",
      "epoch: 4 step: 1519, loss is 0.0005076485103927553\n",
      "epoch: 4 step: 1520, loss is 0.07708108425140381\n",
      "epoch: 4 step: 1521, loss is 0.026137152686715126\n",
      "epoch: 4 step: 1522, loss is 0.17078597843647003\n",
      "epoch: 4 step: 1523, loss is 0.0007991942693479359\n",
      "epoch: 4 step: 1524, loss is 0.030895130708813667\n",
      "epoch: 4 step: 1525, loss is 0.0058617559261620045\n",
      "epoch: 4 step: 1526, loss is 0.01548072136938572\n",
      "epoch: 4 step: 1527, loss is 0.20367524027824402\n",
      "epoch: 4 step: 1528, loss is 0.3333406448364258\n",
      "epoch: 4 step: 1529, loss is 0.006565502379089594\n",
      "epoch: 4 step: 1530, loss is 0.1016693115234375\n",
      "epoch: 4 step: 1531, loss is 0.0879131406545639\n",
      "epoch: 4 step: 1532, loss is 0.011850431561470032\n",
      "epoch: 4 step: 1533, loss is 0.0017558988183736801\n",
      "epoch: 4 step: 1534, loss is 0.37491297721862793\n",
      "epoch: 4 step: 1535, loss is 0.006681118626147509\n",
      "epoch: 4 step: 1536, loss is 0.012405887246131897\n",
      "epoch: 4 step: 1537, loss is 0.011935609392821789\n",
      "epoch: 4 step: 1538, loss is 0.004844070412218571\n",
      "epoch: 4 step: 1539, loss is 0.016420967876911163\n",
      "epoch: 4 step: 1540, loss is 0.04403568059206009\n",
      "epoch: 4 step: 1541, loss is 0.00834985263645649\n",
      "epoch: 4 step: 1542, loss is 0.15043826401233673\n",
      "epoch: 4 step: 1543, loss is 0.15074507892131805\n",
      "epoch: 4 step: 1544, loss is 0.015119491145014763\n",
      "epoch: 4 step: 1545, loss is 0.013597597368061543\n",
      "epoch: 4 step: 1546, loss is 0.009946309030056\n",
      "epoch: 4 step: 1547, loss is 0.08512086421251297\n",
      "epoch: 4 step: 1548, loss is 0.026602309197187424\n",
      "epoch: 4 step: 1549, loss is 0.08980309963226318\n",
      "epoch: 4 step: 1550, loss is 0.02086915448307991\n",
      "epoch: 4 step: 1551, loss is 0.16509351134300232\n",
      "epoch: 4 step: 1552, loss is 0.00176577630918473\n",
      "epoch: 4 step: 1553, loss is 0.010400397703051567\n",
      "epoch: 4 step: 1554, loss is 0.012174584902822971\n",
      "epoch: 4 step: 1555, loss is 0.08467774838209152\n",
      "epoch: 4 step: 1556, loss is 0.04871603474020958\n",
      "epoch: 4 step: 1557, loss is 0.01362911332398653\n",
      "epoch: 4 step: 1558, loss is 0.04899553209543228\n",
      "epoch: 4 step: 1559, loss is 0.0067846947349607944\n",
      "epoch: 4 step: 1560, loss is 0.003980381414294243\n",
      "epoch: 4 step: 1561, loss is 0.0011428911238908768\n",
      "epoch: 4 step: 1562, loss is 0.1002056822180748\n",
      "epoch: 4 step: 1563, loss is 0.009943321347236633\n",
      "epoch: 4 step: 1564, loss is 0.04369308799505234\n",
      "epoch: 4 step: 1565, loss is 0.0019227430457249284\n",
      "epoch: 4 step: 1566, loss is 0.06812215596437454\n",
      "epoch: 4 step: 1567, loss is 0.030526988208293915\n",
      "epoch: 4 step: 1568, loss is 0.12789005041122437\n",
      "epoch: 4 step: 1569, loss is 0.1030593290925026\n",
      "epoch: 4 step: 1570, loss is 0.017206482589244843\n",
      "epoch: 4 step: 1571, loss is 0.004411099944263697\n",
      "epoch: 4 step: 1572, loss is 0.0033143842592835426\n",
      "epoch: 4 step: 1573, loss is 0.09293872117996216\n",
      "epoch: 4 step: 1574, loss is 0.062956802546978\n",
      "epoch: 4 step: 1575, loss is 0.001873835688456893\n",
      "epoch: 4 step: 1576, loss is 0.009911670349538326\n",
      "epoch: 4 step: 1577, loss is 0.004590204451233149\n",
      "epoch: 4 step: 1578, loss is 0.07110083103179932\n",
      "epoch: 4 step: 1579, loss is 0.08380652219057083\n",
      "epoch: 4 step: 1580, loss is 0.025753846392035484\n",
      "epoch: 4 step: 1581, loss is 0.005873039364814758\n",
      "epoch: 4 step: 1582, loss is 0.0027544680051505566\n",
      "epoch: 4 step: 1583, loss is 0.06643557548522949\n",
      "epoch: 4 step: 1584, loss is 0.014939305372536182\n",
      "epoch: 4 step: 1585, loss is 0.06257568299770355\n",
      "epoch: 4 step: 1586, loss is 0.0026616870891302824\n",
      "epoch: 4 step: 1587, loss is 0.002590644173324108\n",
      "epoch: 4 step: 1588, loss is 0.014214170165359974\n",
      "epoch: 4 step: 1589, loss is 0.07682891935110092\n",
      "epoch: 4 step: 1590, loss is 0.0015892568044364452\n",
      "epoch: 4 step: 1591, loss is 0.005817802622914314\n",
      "epoch: 4 step: 1592, loss is 0.010298185050487518\n",
      "epoch: 4 step: 1593, loss is 0.07113282382488251\n",
      "epoch: 4 step: 1594, loss is 0.10789427906274796\n",
      "epoch: 4 step: 1595, loss is 0.005033768247812986\n",
      "epoch: 4 step: 1596, loss is 0.01807844452559948\n",
      "epoch: 4 step: 1597, loss is 0.047983769327402115\n",
      "epoch: 4 step: 1598, loss is 0.00024396234948653728\n",
      "epoch: 4 step: 1599, loss is 0.1890086829662323\n",
      "epoch: 4 step: 1600, loss is 0.04341838136315346\n",
      "epoch: 4 step: 1601, loss is 0.001540003577247262\n",
      "epoch: 4 step: 1602, loss is 0.02693372592329979\n",
      "epoch: 4 step: 1603, loss is 0.001349388505332172\n",
      "epoch: 4 step: 1604, loss is 0.007320134900510311\n",
      "epoch: 4 step: 1605, loss is 0.027812959626317024\n",
      "epoch: 4 step: 1606, loss is 0.20438528060913086\n",
      "epoch: 4 step: 1607, loss is 0.0014054174534976482\n",
      "epoch: 4 step: 1608, loss is 0.016095735132694244\n",
      "epoch: 4 step: 1609, loss is 0.03463787958025932\n",
      "epoch: 4 step: 1610, loss is 0.16367612779140472\n",
      "epoch: 4 step: 1611, loss is 0.00151131022721529\n",
      "epoch: 4 step: 1612, loss is 0.005166851449757814\n",
      "epoch: 4 step: 1613, loss is 0.0003976252628490329\n",
      "epoch: 4 step: 1614, loss is 0.0006440601428039372\n",
      "epoch: 4 step: 1615, loss is 0.004923968575894833\n",
      "epoch: 4 step: 1616, loss is 0.0024971729144454002\n",
      "epoch: 4 step: 1617, loss is 0.0002976895193569362\n",
      "epoch: 4 step: 1618, loss is 0.07229743152856827\n",
      "epoch: 4 step: 1619, loss is 0.011965987272560596\n",
      "epoch: 4 step: 1620, loss is 0.03898174315690994\n",
      "epoch: 4 step: 1621, loss is 0.02815183997154236\n",
      "epoch: 4 step: 1622, loss is 0.008844127878546715\n",
      "epoch: 4 step: 1623, loss is 0.0017907930305227637\n",
      "epoch: 4 step: 1624, loss is 0.048611339181661606\n",
      "epoch: 4 step: 1625, loss is 0.06943611055612564\n",
      "epoch: 4 step: 1626, loss is 0.016789618879556656\n",
      "epoch: 4 step: 1627, loss is 0.03149823099374771\n",
      "epoch: 4 step: 1628, loss is 0.00473990710452199\n",
      "epoch: 4 step: 1629, loss is 0.1522848904132843\n",
      "epoch: 4 step: 1630, loss is 0.012708086520433426\n",
      "epoch: 4 step: 1631, loss is 0.01617243140935898\n",
      "epoch: 4 step: 1632, loss is 0.002812678227201104\n",
      "epoch: 4 step: 1633, loss is 0.001472738804295659\n",
      "epoch: 4 step: 1634, loss is 0.0008515319786965847\n",
      "epoch: 4 step: 1635, loss is 0.0009763515554368496\n",
      "epoch: 4 step: 1636, loss is 0.00305176991969347\n",
      "epoch: 4 step: 1637, loss is 0.17241084575653076\n",
      "epoch: 4 step: 1638, loss is 0.0010499873897060752\n",
      "epoch: 4 step: 1639, loss is 0.09895669668912888\n",
      "epoch: 4 step: 1640, loss is 0.0022529535926878452\n",
      "epoch: 4 step: 1641, loss is 0.010611739940941334\n",
      "epoch: 4 step: 1642, loss is 0.009483803994953632\n",
      "epoch: 4 step: 1643, loss is 0.17661334574222565\n",
      "epoch: 4 step: 1644, loss is 0.01526671927422285\n",
      "epoch: 4 step: 1645, loss is 0.07935325056314468\n",
      "epoch: 4 step: 1646, loss is 0.01309293694794178\n",
      "epoch: 4 step: 1647, loss is 0.005981105845421553\n",
      "epoch: 4 step: 1648, loss is 0.014684011228382587\n",
      "epoch: 4 step: 1649, loss is 0.0003131311386823654\n",
      "epoch: 4 step: 1650, loss is 0.040004175156354904\n",
      "epoch: 4 step: 1651, loss is 0.002013334771618247\n",
      "epoch: 4 step: 1652, loss is 0.16388928890228271\n",
      "epoch: 4 step: 1653, loss is 0.04929618537425995\n",
      "epoch: 4 step: 1654, loss is 0.06709778308868408\n",
      "epoch: 4 step: 1655, loss is 0.0035135948564857244\n",
      "epoch: 4 step: 1656, loss is 0.00847436860203743\n",
      "epoch: 4 step: 1657, loss is 0.002078186720609665\n",
      "epoch: 4 step: 1658, loss is 0.17915353178977966\n",
      "epoch: 4 step: 1659, loss is 0.049425892531871796\n",
      "epoch: 4 step: 1660, loss is 0.004251629114151001\n",
      "epoch: 4 step: 1661, loss is 0.0031103813089430332\n",
      "epoch: 4 step: 1662, loss is 0.04300573840737343\n",
      "epoch: 4 step: 1663, loss is 0.003153963014483452\n",
      "epoch: 4 step: 1664, loss is 0.029731031507253647\n",
      "epoch: 4 step: 1665, loss is 0.0014148458139970899\n",
      "epoch: 4 step: 1666, loss is 0.03516118600964546\n",
      "epoch: 4 step: 1667, loss is 0.004500903654843569\n",
      "epoch: 4 step: 1668, loss is 0.16152070462703705\n",
      "epoch: 4 step: 1669, loss is 0.005050846375524998\n",
      "epoch: 4 step: 1670, loss is 0.004032445140182972\n",
      "epoch: 4 step: 1671, loss is 0.0019800823647528887\n",
      "epoch: 4 step: 1672, loss is 0.017151834443211555\n",
      "epoch: 4 step: 1673, loss is 0.036677345633506775\n",
      "epoch: 4 step: 1674, loss is 0.005293958354741335\n",
      "epoch: 4 step: 1675, loss is 0.24048644304275513\n",
      "epoch: 4 step: 1676, loss is 0.04504498839378357\n",
      "epoch: 4 step: 1677, loss is 0.009626233018934727\n",
      "epoch: 4 step: 1678, loss is 0.0028209590818732977\n",
      "epoch: 4 step: 1679, loss is 0.01402647327631712\n",
      "epoch: 4 step: 1680, loss is 0.000646119995508343\n",
      "epoch: 4 step: 1681, loss is 0.12003570795059204\n",
      "epoch: 4 step: 1682, loss is 0.003110171528533101\n",
      "epoch: 4 step: 1683, loss is 0.01818838156759739\n",
      "epoch: 4 step: 1684, loss is 0.01542938593775034\n",
      "epoch: 4 step: 1685, loss is 0.03106725588440895\n",
      "epoch: 4 step: 1686, loss is 0.09179859608411789\n",
      "epoch: 4 step: 1687, loss is 0.0002226493670605123\n",
      "epoch: 4 step: 1688, loss is 0.0011310186237096786\n",
      "epoch: 4 step: 1689, loss is 0.003743442939594388\n",
      "epoch: 4 step: 1690, loss is 0.0415058396756649\n",
      "epoch: 4 step: 1691, loss is 0.04931657388806343\n",
      "epoch: 4 step: 1692, loss is 0.0791468620300293\n",
      "epoch: 4 step: 1693, loss is 0.08539500087499619\n",
      "epoch: 4 step: 1694, loss is 0.005188283510506153\n",
      "epoch: 4 step: 1695, loss is 0.2556600570678711\n",
      "epoch: 4 step: 1696, loss is 0.00048714649165049195\n",
      "epoch: 4 step: 1697, loss is 0.004763702861964703\n",
      "epoch: 4 step: 1698, loss is 0.015227466821670532\n",
      "epoch: 4 step: 1699, loss is 0.10962186008691788\n",
      "epoch: 4 step: 1700, loss is 0.004523489158600569\n",
      "epoch: 4 step: 1701, loss is 0.0018647233955562115\n",
      "epoch: 4 step: 1702, loss is 0.008434063754975796\n",
      "epoch: 4 step: 1703, loss is 0.09786383807659149\n",
      "epoch: 4 step: 1704, loss is 0.0010406544897705317\n",
      "epoch: 4 step: 1705, loss is 0.007670756429433823\n",
      "epoch: 4 step: 1706, loss is 0.0021175530273467302\n",
      "epoch: 4 step: 1707, loss is 0.03958235681056976\n",
      "epoch: 4 step: 1708, loss is 0.0053568268194794655\n",
      "epoch: 4 step: 1709, loss is 0.07097962498664856\n",
      "epoch: 4 step: 1710, loss is 0.058318525552749634\n",
      "epoch: 4 step: 1711, loss is 0.012722174637019634\n",
      "epoch: 4 step: 1712, loss is 0.0789746642112732\n",
      "epoch: 4 step: 1713, loss is 0.04223613440990448\n",
      "epoch: 4 step: 1714, loss is 0.004566953517496586\n",
      "epoch: 4 step: 1715, loss is 0.03155365213751793\n",
      "epoch: 4 step: 1716, loss is 0.11263108998537064\n",
      "epoch: 4 step: 1717, loss is 0.059411972761154175\n",
      "epoch: 4 step: 1718, loss is 0.06397180259227753\n",
      "epoch: 4 step: 1719, loss is 0.002858074614778161\n",
      "epoch: 4 step: 1720, loss is 0.04664413630962372\n",
      "epoch: 4 step: 1721, loss is 0.07411548495292664\n",
      "epoch: 4 step: 1722, loss is 0.12749777734279633\n",
      "epoch: 4 step: 1723, loss is 0.01847846619784832\n",
      "epoch: 4 step: 1724, loss is 0.02792566642165184\n",
      "epoch: 4 step: 1725, loss is 0.001740234438329935\n",
      "epoch: 4 step: 1726, loss is 0.1234448030591011\n",
      "epoch: 4 step: 1727, loss is 0.0011478506494313478\n",
      "epoch: 4 step: 1728, loss is 0.06551773101091385\n",
      "epoch: 4 step: 1729, loss is 0.0726207047700882\n",
      "epoch: 4 step: 1730, loss is 0.03226553648710251\n",
      "epoch: 4 step: 1731, loss is 0.007684467360377312\n",
      "epoch: 4 step: 1732, loss is 0.07589505612850189\n",
      "epoch: 4 step: 1733, loss is 0.030198141932487488\n",
      "epoch: 4 step: 1734, loss is 0.2225155532360077\n",
      "epoch: 4 step: 1735, loss is 0.0021524683106690645\n",
      "epoch: 4 step: 1736, loss is 0.035207077860832214\n",
      "epoch: 4 step: 1737, loss is 0.00561423460021615\n",
      "epoch: 4 step: 1738, loss is 0.05710505694150925\n",
      "epoch: 4 step: 1739, loss is 0.00036085786996409297\n",
      "epoch: 4 step: 1740, loss is 0.012261963449418545\n",
      "epoch: 4 step: 1741, loss is 0.041961461305618286\n",
      "epoch: 4 step: 1742, loss is 0.024080468341708183\n",
      "epoch: 4 step: 1743, loss is 0.015514716506004333\n",
      "epoch: 4 step: 1744, loss is 0.0006194891757331789\n",
      "epoch: 4 step: 1745, loss is 0.0037383059971034527\n",
      "epoch: 4 step: 1746, loss is 0.0752624049782753\n",
      "epoch: 4 step: 1747, loss is 0.005331117194145918\n",
      "epoch: 4 step: 1748, loss is 0.012076851911842823\n",
      "epoch: 4 step: 1749, loss is 0.015254736877977848\n",
      "epoch: 4 step: 1750, loss is 0.11589983850717545\n",
      "epoch: 4 step: 1751, loss is 0.15258383750915527\n",
      "epoch: 4 step: 1752, loss is 0.0003568690153770149\n",
      "epoch: 4 step: 1753, loss is 0.011780799366533756\n",
      "epoch: 4 step: 1754, loss is 0.1082385778427124\n",
      "epoch: 4 step: 1755, loss is 0.08569387346506119\n",
      "epoch: 4 step: 1756, loss is 0.013662739656865597\n",
      "epoch: 4 step: 1757, loss is 0.0018946914933621883\n",
      "epoch: 4 step: 1758, loss is 0.00028847111389040947\n",
      "epoch: 4 step: 1759, loss is 0.020648790523409843\n",
      "epoch: 4 step: 1760, loss is 0.002409563632681966\n",
      "epoch: 4 step: 1761, loss is 0.08466921001672745\n",
      "epoch: 4 step: 1762, loss is 0.003682010341435671\n",
      "epoch: 4 step: 1763, loss is 0.0003944463678635657\n",
      "epoch: 4 step: 1764, loss is 0.0019159021321684122\n",
      "epoch: 4 step: 1765, loss is 0.0011355230817571282\n",
      "epoch: 4 step: 1766, loss is 0.026731768622994423\n",
      "epoch: 4 step: 1767, loss is 0.007689178455621004\n",
      "epoch: 4 step: 1768, loss is 0.015013353899121284\n",
      "epoch: 4 step: 1769, loss is 0.0026383190415799618\n",
      "epoch: 4 step: 1770, loss is 0.002844597212970257\n",
      "epoch: 4 step: 1771, loss is 0.14716584980487823\n",
      "epoch: 4 step: 1772, loss is 0.07121448963880539\n",
      "epoch: 4 step: 1773, loss is 0.007213907316327095\n",
      "epoch: 4 step: 1774, loss is 0.003154222620651126\n",
      "epoch: 4 step: 1775, loss is 0.07919641584157944\n",
      "epoch: 4 step: 1776, loss is 0.5157932043075562\n",
      "epoch: 4 step: 1777, loss is 0.04205936938524246\n",
      "epoch: 4 step: 1778, loss is 0.004161303862929344\n",
      "epoch: 4 step: 1779, loss is 0.015857312828302383\n",
      "epoch: 4 step: 1780, loss is 0.010635108686983585\n",
      "epoch: 4 step: 1781, loss is 0.020218638703227043\n",
      "epoch: 4 step: 1782, loss is 0.004094838630408049\n",
      "epoch: 4 step: 1783, loss is 0.001725344336591661\n",
      "epoch: 4 step: 1784, loss is 0.020506249740719795\n",
      "epoch: 4 step: 1785, loss is 0.02166488580405712\n",
      "epoch: 4 step: 1786, loss is 0.004282048903405666\n",
      "epoch: 4 step: 1787, loss is 0.0024538724683225155\n",
      "epoch: 4 step: 1788, loss is 0.04982060194015503\n",
      "epoch: 4 step: 1789, loss is 0.0029526138678193092\n",
      "epoch: 4 step: 1790, loss is 0.0019196816720068455\n",
      "epoch: 4 step: 1791, loss is 0.013212655670940876\n",
      "epoch: 4 step: 1792, loss is 0.0013191842008382082\n",
      "epoch: 4 step: 1793, loss is 0.03269525244832039\n",
      "epoch: 4 step: 1794, loss is 0.0013027761597186327\n",
      "epoch: 4 step: 1795, loss is 0.003877564799040556\n",
      "epoch: 4 step: 1796, loss is 0.0014092752244323492\n",
      "epoch: 4 step: 1797, loss is 0.017715169116854668\n",
      "epoch: 4 step: 1798, loss is 0.06404381990432739\n",
      "epoch: 4 step: 1799, loss is 0.01388055831193924\n",
      "epoch: 4 step: 1800, loss is 0.005425515118986368\n",
      "epoch: 4 step: 1801, loss is 0.06506931036710739\n",
      "epoch: 4 step: 1802, loss is 0.039287496358156204\n",
      "epoch: 4 step: 1803, loss is 0.025947989895939827\n",
      "epoch: 4 step: 1804, loss is 0.028383512049913406\n",
      "epoch: 4 step: 1805, loss is 0.14794814586639404\n",
      "epoch: 4 step: 1806, loss is 0.04739445447921753\n",
      "epoch: 4 step: 1807, loss is 0.03719594329595566\n",
      "epoch: 4 step: 1808, loss is 0.021631238982081413\n",
      "epoch: 4 step: 1809, loss is 0.005372982006520033\n",
      "epoch: 4 step: 1810, loss is 0.010169639252126217\n",
      "epoch: 4 step: 1811, loss is 0.02684074640274048\n",
      "epoch: 4 step: 1812, loss is 0.058977894484996796\n",
      "epoch: 4 step: 1813, loss is 0.0017947679152712226\n",
      "epoch: 4 step: 1814, loss is 0.01572311297059059\n",
      "epoch: 4 step: 1815, loss is 0.21091753244400024\n",
      "epoch: 4 step: 1816, loss is 0.015239996835589409\n",
      "epoch: 4 step: 1817, loss is 0.0022243033163249493\n",
      "epoch: 4 step: 1818, loss is 0.006812316365540028\n",
      "epoch: 4 step: 1819, loss is 0.11235317587852478\n",
      "epoch: 4 step: 1820, loss is 0.011661622673273087\n",
      "epoch: 4 step: 1821, loss is 0.005881879013031721\n",
      "epoch: 4 step: 1822, loss is 0.037349313497543335\n",
      "epoch: 4 step: 1823, loss is 0.032894592732191086\n",
      "epoch: 4 step: 1824, loss is 0.012382451444864273\n",
      "epoch: 4 step: 1825, loss is 0.06091081723570824\n",
      "epoch: 4 step: 1826, loss is 0.09152092784643173\n",
      "epoch: 4 step: 1827, loss is 0.00794965773820877\n",
      "epoch: 4 step: 1828, loss is 0.0015071386005729437\n",
      "epoch: 4 step: 1829, loss is 0.18399488925933838\n",
      "epoch: 4 step: 1830, loss is 0.0025925624649971724\n",
      "epoch: 4 step: 1831, loss is 0.01655689626932144\n",
      "epoch: 4 step: 1832, loss is 0.031029589474201202\n",
      "epoch: 4 step: 1833, loss is 0.004276545252650976\n",
      "epoch: 4 step: 1834, loss is 0.06815660744905472\n",
      "epoch: 4 step: 1835, loss is 0.0013303188607096672\n",
      "epoch: 4 step: 1836, loss is 0.0008137405384331942\n",
      "epoch: 4 step: 1837, loss is 0.006698592100292444\n",
      "epoch: 4 step: 1838, loss is 0.053364261984825134\n",
      "epoch: 4 step: 1839, loss is 0.013949820771813393\n",
      "epoch: 4 step: 1840, loss is 0.0021537314169108868\n",
      "epoch: 4 step: 1841, loss is 0.010574926622211933\n",
      "epoch: 4 step: 1842, loss is 0.2788924276828766\n",
      "epoch: 4 step: 1843, loss is 0.0005389301804825664\n",
      "epoch: 4 step: 1844, loss is 0.07303280383348465\n",
      "epoch: 4 step: 1845, loss is 0.0026104659773409367\n",
      "epoch: 4 step: 1846, loss is 0.09735529124736786\n",
      "epoch: 4 step: 1847, loss is 0.02727164328098297\n",
      "epoch: 4 step: 1848, loss is 0.01373905036598444\n",
      "epoch: 4 step: 1849, loss is 0.004335292614996433\n",
      "epoch: 4 step: 1850, loss is 0.004499942529946566\n",
      "epoch: 4 step: 1851, loss is 0.0047872052527964115\n",
      "epoch: 4 step: 1852, loss is 0.02718641236424446\n",
      "epoch: 4 step: 1853, loss is 0.2788662016391754\n",
      "epoch: 4 step: 1854, loss is 0.15524545311927795\n",
      "epoch: 4 step: 1855, loss is 0.022902514785528183\n",
      "epoch: 4 step: 1856, loss is 0.054239727556705475\n",
      "epoch: 4 step: 1857, loss is 0.08609887957572937\n",
      "epoch: 4 step: 1858, loss is 0.00533171696588397\n",
      "epoch: 4 step: 1859, loss is 0.009416545741260052\n",
      "epoch: 4 step: 1860, loss is 0.013671010732650757\n",
      "epoch: 4 step: 1861, loss is 0.0016629641177132726\n",
      "epoch: 4 step: 1862, loss is 0.032794322818517685\n",
      "epoch: 4 step: 1863, loss is 0.014256617985665798\n",
      "epoch: 4 step: 1864, loss is 0.016680043190717697\n",
      "epoch: 4 step: 1865, loss is 0.03139452263712883\n",
      "epoch: 4 step: 1866, loss is 0.0022606677375733852\n",
      "epoch: 4 step: 1867, loss is 0.006163216661661863\n",
      "epoch: 4 step: 1868, loss is 0.04607196897268295\n",
      "epoch: 4 step: 1869, loss is 0.034222327172756195\n",
      "epoch: 4 step: 1870, loss is 0.05381686985492706\n",
      "epoch: 4 step: 1871, loss is 0.17023375630378723\n",
      "epoch: 4 step: 1872, loss is 0.11435356736183167\n",
      "epoch: 4 step: 1873, loss is 0.011112211272120476\n",
      "epoch: 4 step: 1874, loss is 0.004925211425870657\n",
      "epoch: 4 step: 1875, loss is 0.20996776223182678\n",
      "Train epoch time: 11309.249 ms, per step time: 6.032 ms\n",
      "epoch: 5 step: 1, loss is 0.008963129483163357\n",
      "epoch: 5 step: 2, loss is 0.0066888052970170975\n",
      "epoch: 5 step: 3, loss is 0.05574171617627144\n",
      "epoch: 5 step: 4, loss is 0.042607296258211136\n",
      "epoch: 5 step: 5, loss is 0.006506540346890688\n",
      "epoch: 5 step: 6, loss is 0.0035550203174352646\n",
      "epoch: 5 step: 7, loss is 0.010016913525760174\n",
      "epoch: 5 step: 8, loss is 0.0018473390955477953\n",
      "epoch: 5 step: 9, loss is 0.01662050001323223\n",
      "epoch: 5 step: 10, loss is 0.005222247913479805\n",
      "epoch: 5 step: 11, loss is 0.014741873368620872\n",
      "epoch: 5 step: 12, loss is 0.0032516068313270807\n",
      "epoch: 5 step: 13, loss is 0.003388723125681281\n",
      "epoch: 5 step: 14, loss is 0.00032759239547885954\n",
      "epoch: 5 step: 15, loss is 0.003308224258944392\n",
      "epoch: 5 step: 16, loss is 0.001331028644926846\n",
      "epoch: 5 step: 17, loss is 0.11544910073280334\n",
      "epoch: 5 step: 18, loss is 0.02484794706106186\n",
      "epoch: 5 step: 19, loss is 0.00987471267580986\n",
      "epoch: 5 step: 20, loss is 0.002090782392770052\n",
      "epoch: 5 step: 21, loss is 0.0038760812021791935\n",
      "epoch: 5 step: 22, loss is 0.0046647898852825165\n",
      "epoch: 5 step: 23, loss is 0.001273282920010388\n",
      "epoch: 5 step: 24, loss is 0.08989167958498001\n",
      "epoch: 5 step: 25, loss is 0.04202551767230034\n",
      "epoch: 5 step: 26, loss is 0.001410174067132175\n",
      "epoch: 5 step: 27, loss is 0.005226001143455505\n",
      "epoch: 5 step: 28, loss is 0.02211567759513855\n",
      "epoch: 5 step: 29, loss is 0.13509972393512726\n",
      "epoch: 5 step: 30, loss is 0.0033108911011368036\n",
      "epoch: 5 step: 31, loss is 0.0028397545684129\n",
      "epoch: 5 step: 32, loss is 0.07339109480381012\n",
      "epoch: 5 step: 33, loss is 0.09436692297458649\n",
      "epoch: 5 step: 34, loss is 0.010133570991456509\n",
      "epoch: 5 step: 35, loss is 0.0029530043248087168\n",
      "epoch: 5 step: 36, loss is 0.08594413846731186\n",
      "epoch: 5 step: 37, loss is 0.02792457677423954\n",
      "epoch: 5 step: 38, loss is 0.0029181831050664186\n",
      "epoch: 5 step: 39, loss is 0.007320081815123558\n",
      "epoch: 5 step: 40, loss is 0.0043886518105864525\n",
      "epoch: 5 step: 41, loss is 0.021874649450182915\n",
      "epoch: 5 step: 42, loss is 0.0009060778538696468\n",
      "epoch: 5 step: 43, loss is 0.026072576642036438\n",
      "epoch: 5 step: 44, loss is 0.0010050989221781492\n",
      "epoch: 5 step: 45, loss is 0.11621492356061935\n",
      "epoch: 5 step: 46, loss is 0.0006411177455447614\n",
      "epoch: 5 step: 47, loss is 0.011011913418769836\n",
      "epoch: 5 step: 48, loss is 0.007895386777818203\n",
      "epoch: 5 step: 49, loss is 0.005120173096656799\n",
      "epoch: 5 step: 50, loss is 0.035763081163167953\n",
      "epoch: 5 step: 51, loss is 0.0001892990549094975\n",
      "epoch: 5 step: 52, loss is 0.0001506582775618881\n",
      "epoch: 5 step: 53, loss is 0.0013400636380538344\n",
      "epoch: 5 step: 54, loss is 0.02268463931977749\n",
      "epoch: 5 step: 55, loss is 0.004904302768409252\n",
      "epoch: 5 step: 56, loss is 0.0018089524237439036\n",
      "epoch: 5 step: 57, loss is 0.002048188354820013\n",
      "epoch: 5 step: 58, loss is 0.07636892795562744\n",
      "epoch: 5 step: 59, loss is 0.20808643102645874\n",
      "epoch: 5 step: 60, loss is 0.10851912945508957\n",
      "epoch: 5 step: 61, loss is 0.06151435896754265\n",
      "epoch: 5 step: 62, loss is 0.1473141461610794\n",
      "epoch: 5 step: 63, loss is 0.007359092589467764\n",
      "epoch: 5 step: 64, loss is 0.0957932323217392\n",
      "epoch: 5 step: 65, loss is 0.011626663617789745\n",
      "epoch: 5 step: 66, loss is 0.006112929433584213\n",
      "epoch: 5 step: 67, loss is 0.013247674331068993\n",
      "epoch: 5 step: 68, loss is 0.0023815010208636522\n",
      "epoch: 5 step: 69, loss is 0.019389808177947998\n",
      "epoch: 5 step: 70, loss is 0.003313797991722822\n",
      "epoch: 5 step: 71, loss is 0.0023139354307204485\n",
      "epoch: 5 step: 72, loss is 0.0022718943655490875\n",
      "epoch: 5 step: 73, loss is 0.036320678889751434\n",
      "epoch: 5 step: 74, loss is 0.0016875158762559295\n",
      "epoch: 5 step: 75, loss is 0.002610021736472845\n",
      "epoch: 5 step: 76, loss is 0.027543481439352036\n",
      "epoch: 5 step: 77, loss is 0.06248553842306137\n",
      "epoch: 5 step: 78, loss is 0.012822328135371208\n",
      "epoch: 5 step: 79, loss is 0.0009233660530298948\n",
      "epoch: 5 step: 80, loss is 0.018279684707522392\n",
      "epoch: 5 step: 81, loss is 0.000870242714881897\n",
      "epoch: 5 step: 82, loss is 0.001297829090617597\n",
      "epoch: 5 step: 83, loss is 0.0012031157966703176\n",
      "epoch: 5 step: 84, loss is 0.021052777767181396\n",
      "epoch: 5 step: 85, loss is 0.011464308947324753\n",
      "epoch: 5 step: 86, loss is 0.10139928013086319\n",
      "epoch: 5 step: 87, loss is 0.009015057235956192\n",
      "epoch: 5 step: 88, loss is 0.006605036556720734\n",
      "epoch: 5 step: 89, loss is 0.004250592552125454\n",
      "epoch: 5 step: 90, loss is 0.0011136678513139486\n",
      "epoch: 5 step: 91, loss is 0.0019138599745929241\n",
      "epoch: 5 step: 92, loss is 0.048817235976457596\n",
      "epoch: 5 step: 93, loss is 0.008391665294766426\n",
      "epoch: 5 step: 94, loss is 0.0234778244048357\n",
      "epoch: 5 step: 95, loss is 0.1021028384566307\n",
      "epoch: 5 step: 96, loss is 0.029243718832731247\n",
      "epoch: 5 step: 97, loss is 0.0003280838718637824\n",
      "epoch: 5 step: 98, loss is 0.019337477162480354\n",
      "epoch: 5 step: 99, loss is 0.005356988869607449\n",
      "epoch: 5 step: 100, loss is 0.0005418362561613321\n",
      "epoch: 5 step: 101, loss is 0.0022443546913564205\n",
      "epoch: 5 step: 102, loss is 0.013362333178520203\n",
      "epoch: 5 step: 103, loss is 0.0005302060162648559\n",
      "epoch: 5 step: 104, loss is 0.013068622909486294\n",
      "epoch: 5 step: 105, loss is 0.010320939123630524\n",
      "epoch: 5 step: 106, loss is 0.010037495754659176\n",
      "epoch: 5 step: 107, loss is 0.0005660202004946768\n",
      "epoch: 5 step: 108, loss is 0.004081616643816233\n",
      "epoch: 5 step: 109, loss is 0.018511217087507248\n",
      "epoch: 5 step: 110, loss is 0.0240261722356081\n",
      "epoch: 5 step: 111, loss is 0.001629832200706005\n",
      "epoch: 5 step: 112, loss is 0.005546856205910444\n",
      "epoch: 5 step: 113, loss is 0.010287145152688026\n",
      "epoch: 5 step: 114, loss is 0.04155200719833374\n",
      "epoch: 5 step: 115, loss is 0.009032147005200386\n",
      "epoch: 5 step: 116, loss is 0.10100813210010529\n",
      "epoch: 5 step: 117, loss is 0.0003108742239419371\n",
      "epoch: 5 step: 118, loss is 0.06911049783229828\n",
      "epoch: 5 step: 119, loss is 0.0024823881685733795\n",
      "epoch: 5 step: 120, loss is 0.01775636337697506\n",
      "epoch: 5 step: 121, loss is 0.017000358551740646\n",
      "epoch: 5 step: 122, loss is 0.006579274777323008\n",
      "epoch: 5 step: 123, loss is 0.0002374845207668841\n",
      "epoch: 5 step: 124, loss is 0.0006915455451235175\n",
      "epoch: 5 step: 125, loss is 0.0006482555181719363\n",
      "epoch: 5 step: 126, loss is 0.007179461419582367\n",
      "epoch: 5 step: 127, loss is 0.000561392167583108\n",
      "epoch: 5 step: 128, loss is 0.0003890663792844862\n",
      "epoch: 5 step: 129, loss is 0.006728615146130323\n",
      "epoch: 5 step: 130, loss is 0.002135863993316889\n",
      "epoch: 5 step: 131, loss is 0.002888008952140808\n",
      "epoch: 5 step: 132, loss is 0.003630537074059248\n",
      "epoch: 5 step: 133, loss is 0.015164431184530258\n",
      "epoch: 5 step: 134, loss is 0.03011241741478443\n",
      "epoch: 5 step: 135, loss is 0.08467946201562881\n",
      "epoch: 5 step: 136, loss is 0.0006879342254251242\n",
      "epoch: 5 step: 137, loss is 0.12633514404296875\n",
      "epoch: 5 step: 138, loss is 0.004614941775798798\n",
      "epoch: 5 step: 139, loss is 9.422191214980558e-05\n",
      "epoch: 5 step: 140, loss is 0.01799898035824299\n",
      "epoch: 5 step: 141, loss is 0.016988834366202354\n",
      "epoch: 5 step: 142, loss is 0.016861021518707275\n",
      "epoch: 5 step: 143, loss is 0.014657000079751015\n",
      "epoch: 5 step: 144, loss is 0.0068684350699186325\n",
      "epoch: 5 step: 145, loss is 0.0008579966379329562\n",
      "epoch: 5 step: 146, loss is 0.06644637882709503\n",
      "epoch: 5 step: 147, loss is 0.0014938036911189556\n",
      "epoch: 5 step: 148, loss is 0.03454069793224335\n",
      "epoch: 5 step: 149, loss is 0.027025256305933\n",
      "epoch: 5 step: 150, loss is 0.033045560121536255\n",
      "epoch: 5 step: 151, loss is 0.03084992989897728\n",
      "epoch: 5 step: 152, loss is 0.0015996487345546484\n",
      "epoch: 5 step: 153, loss is 0.1333196610212326\n",
      "epoch: 5 step: 154, loss is 0.009041021578013897\n",
      "epoch: 5 step: 155, loss is 0.0010578196961432695\n",
      "epoch: 5 step: 156, loss is 0.003448731265962124\n",
      "epoch: 5 step: 157, loss is 9.878079436020926e-05\n",
      "epoch: 5 step: 158, loss is 0.09966516494750977\n",
      "epoch: 5 step: 159, loss is 0.06355563551187515\n",
      "epoch: 5 step: 160, loss is 0.020431287586688995\n",
      "epoch: 5 step: 161, loss is 0.0011823588283732533\n",
      "epoch: 5 step: 162, loss is 0.16361436247825623\n",
      "epoch: 5 step: 163, loss is 0.009721633046865463\n",
      "epoch: 5 step: 164, loss is 0.0031384644098579884\n",
      "epoch: 5 step: 165, loss is 0.061933014541864395\n",
      "epoch: 5 step: 166, loss is 0.05949200317263603\n",
      "epoch: 5 step: 167, loss is 0.241542249917984\n",
      "epoch: 5 step: 168, loss is 0.0026903359685093164\n",
      "epoch: 5 step: 169, loss is 0.024062229320406914\n",
      "epoch: 5 step: 170, loss is 0.009444584138691425\n",
      "epoch: 5 step: 171, loss is 0.027507806196808815\n",
      "epoch: 5 step: 172, loss is 2.1653479052474722e-05\n",
      "epoch: 5 step: 173, loss is 0.0035913074389100075\n",
      "epoch: 5 step: 174, loss is 0.0006195755559019744\n",
      "epoch: 5 step: 175, loss is 0.08533576875925064\n",
      "epoch: 5 step: 176, loss is 0.0014979790430516005\n",
      "epoch: 5 step: 177, loss is 0.051490020006895065\n",
      "epoch: 5 step: 178, loss is 0.005274937953799963\n",
      "epoch: 5 step: 179, loss is 0.05215555801987648\n",
      "epoch: 5 step: 180, loss is 0.014216453768312931\n",
      "epoch: 5 step: 181, loss is 0.003042167518287897\n",
      "epoch: 5 step: 182, loss is 0.1274772733449936\n",
      "epoch: 5 step: 183, loss is 0.0872185006737709\n",
      "epoch: 5 step: 184, loss is 0.0003625038079917431\n",
      "epoch: 5 step: 185, loss is 0.10962625592947006\n",
      "epoch: 5 step: 186, loss is 0.028395889326930046\n",
      "epoch: 5 step: 187, loss is 0.0009581975173205137\n",
      "epoch: 5 step: 188, loss is 0.012474823743104935\n",
      "epoch: 5 step: 189, loss is 0.0002860821841750294\n",
      "epoch: 5 step: 190, loss is 0.12657798826694489\n",
      "epoch: 5 step: 191, loss is 0.03339909017086029\n",
      "epoch: 5 step: 192, loss is 0.011571990326046944\n",
      "epoch: 5 step: 193, loss is 0.0006598519394174218\n",
      "epoch: 5 step: 194, loss is 0.1198662742972374\n",
      "epoch: 5 step: 195, loss is 0.0027927826158702374\n",
      "epoch: 5 step: 196, loss is 0.019293220713734627\n",
      "epoch: 5 step: 197, loss is 0.08949387818574905\n",
      "epoch: 5 step: 198, loss is 0.0005822545499540865\n",
      "epoch: 5 step: 199, loss is 0.00037927174707874656\n",
      "epoch: 5 step: 200, loss is 0.009891609661281109\n",
      "epoch: 5 step: 201, loss is 0.029535550624132156\n",
      "epoch: 5 step: 202, loss is 0.066199392080307\n",
      "epoch: 5 step: 203, loss is 0.05295656621456146\n",
      "epoch: 5 step: 204, loss is 0.038078952580690384\n",
      "epoch: 5 step: 205, loss is 0.2209264636039734\n",
      "epoch: 5 step: 206, loss is 0.03500302881002426\n",
      "epoch: 5 step: 207, loss is 0.0009943356271833181\n",
      "epoch: 5 step: 208, loss is 0.009286022745072842\n",
      "epoch: 5 step: 209, loss is 0.015122905373573303\n",
      "epoch: 5 step: 210, loss is 0.0032272711396217346\n",
      "epoch: 5 step: 211, loss is 0.006311523262411356\n",
      "epoch: 5 step: 212, loss is 0.017915859818458557\n",
      "epoch: 5 step: 213, loss is 0.0009756750660017133\n",
      "epoch: 5 step: 214, loss is 0.00035467304405756295\n",
      "epoch: 5 step: 215, loss is 0.15035201609134674\n",
      "epoch: 5 step: 216, loss is 0.013452037237584591\n",
      "epoch: 5 step: 217, loss is 0.2469039261341095\n",
      "epoch: 5 step: 218, loss is 0.0009951754473149776\n",
      "epoch: 5 step: 219, loss is 0.031761690974235535\n",
      "epoch: 5 step: 220, loss is 0.002252018079161644\n",
      "epoch: 5 step: 221, loss is 0.002517793793231249\n",
      "epoch: 5 step: 222, loss is 0.012553483247756958\n",
      "epoch: 5 step: 223, loss is 0.05094413086771965\n",
      "epoch: 5 step: 224, loss is 0.18881210684776306\n",
      "epoch: 5 step: 225, loss is 0.03485977649688721\n",
      "epoch: 5 step: 226, loss is 0.0006159390904940665\n",
      "epoch: 5 step: 227, loss is 0.026182517409324646\n",
      "epoch: 5 step: 228, loss is 0.00012201043136883527\n",
      "epoch: 5 step: 229, loss is 0.024126313626766205\n",
      "epoch: 5 step: 230, loss is 0.0008880659006536007\n",
      "epoch: 5 step: 231, loss is 0.0006518508889712393\n",
      "epoch: 5 step: 232, loss is 0.11491477489471436\n",
      "epoch: 5 step: 233, loss is 0.025862140581011772\n",
      "epoch: 5 step: 234, loss is 0.002014241414144635\n",
      "epoch: 5 step: 235, loss is 0.0021636676974594593\n",
      "epoch: 5 step: 236, loss is 0.014914245344698429\n",
      "epoch: 5 step: 237, loss is 0.00902059581130743\n",
      "epoch: 5 step: 238, loss is 0.007022171746939421\n",
      "epoch: 5 step: 239, loss is 0.1077745258808136\n",
      "epoch: 5 step: 240, loss is 0.0005410437588579953\n",
      "epoch: 5 step: 241, loss is 0.004641649778932333\n",
      "epoch: 5 step: 242, loss is 0.01221751794219017\n",
      "epoch: 5 step: 243, loss is 0.06466925144195557\n",
      "epoch: 5 step: 244, loss is 0.0035787669476121664\n",
      "epoch: 5 step: 245, loss is 0.06670452654361725\n",
      "epoch: 5 step: 246, loss is 0.00044148589950054884\n",
      "epoch: 5 step: 247, loss is 0.01677611470222473\n",
      "epoch: 5 step: 248, loss is 0.005337519571185112\n",
      "epoch: 5 step: 249, loss is 0.0005223897169344127\n",
      "epoch: 5 step: 250, loss is 0.003681439906358719\n",
      "epoch: 5 step: 251, loss is 0.0013716354733332992\n",
      "epoch: 5 step: 252, loss is 0.17201924324035645\n",
      "epoch: 5 step: 253, loss is 0.019261883571743965\n",
      "epoch: 5 step: 254, loss is 0.03320268541574478\n",
      "epoch: 5 step: 255, loss is 0.015059626661241055\n",
      "epoch: 5 step: 256, loss is 0.02448084019124508\n",
      "epoch: 5 step: 257, loss is 0.006327806506305933\n",
      "epoch: 5 step: 258, loss is 0.006849472876638174\n",
      "epoch: 5 step: 259, loss is 0.033820364624261856\n",
      "epoch: 5 step: 260, loss is 0.0007674394291825593\n",
      "epoch: 5 step: 261, loss is 0.03545092046260834\n",
      "epoch: 5 step: 262, loss is 0.20765317976474762\n",
      "epoch: 5 step: 263, loss is 0.028859814628958702\n",
      "epoch: 5 step: 264, loss is 0.021677978336811066\n",
      "epoch: 5 step: 265, loss is 0.07073403149843216\n",
      "epoch: 5 step: 266, loss is 0.0050935158506035805\n",
      "epoch: 5 step: 267, loss is 0.00795042049139738\n",
      "epoch: 5 step: 268, loss is 0.002888832241296768\n",
      "epoch: 5 step: 269, loss is 0.001461139414459467\n",
      "epoch: 5 step: 270, loss is 0.028636125847697258\n",
      "epoch: 5 step: 271, loss is 0.0008700667531229556\n",
      "epoch: 5 step: 272, loss is 0.0020022389944642782\n",
      "epoch: 5 step: 273, loss is 0.017408158630132675\n",
      "epoch: 5 step: 274, loss is 0.0014412866439670324\n",
      "epoch: 5 step: 275, loss is 0.005816695746034384\n",
      "epoch: 5 step: 276, loss is 0.000580495223402977\n",
      "epoch: 5 step: 277, loss is 0.10146903991699219\n",
      "epoch: 5 step: 278, loss is 0.012154842726886272\n",
      "epoch: 5 step: 279, loss is 0.009495639242231846\n",
      "epoch: 5 step: 280, loss is 0.002009602030739188\n",
      "epoch: 5 step: 281, loss is 0.009081671014428139\n",
      "epoch: 5 step: 282, loss is 0.0017288833623751998\n",
      "epoch: 5 step: 283, loss is 0.0019974224269390106\n",
      "epoch: 5 step: 284, loss is 0.06350737065076828\n",
      "epoch: 5 step: 285, loss is 0.16763220727443695\n",
      "epoch: 5 step: 286, loss is 0.014441592618823051\n",
      "epoch: 5 step: 287, loss is 0.00204576482065022\n",
      "epoch: 5 step: 288, loss is 0.028247931972146034\n",
      "epoch: 5 step: 289, loss is 0.01022318098694086\n",
      "epoch: 5 step: 290, loss is 0.025231972336769104\n",
      "epoch: 5 step: 291, loss is 0.0855318158864975\n",
      "epoch: 5 step: 292, loss is 0.02420870214700699\n",
      "epoch: 5 step: 293, loss is 0.04757598042488098\n",
      "epoch: 5 step: 294, loss is 0.0016199303790926933\n",
      "epoch: 5 step: 295, loss is 0.0009544327040202916\n",
      "epoch: 5 step: 296, loss is 0.03472277894616127\n",
      "epoch: 5 step: 297, loss is 0.20962733030319214\n",
      "epoch: 5 step: 298, loss is 0.008116955868899822\n",
      "epoch: 5 step: 299, loss is 0.006636249832808971\n",
      "epoch: 5 step: 300, loss is 0.003333861008286476\n",
      "epoch: 5 step: 301, loss is 0.026984604075551033\n",
      "epoch: 5 step: 302, loss is 0.0018137021688744426\n",
      "epoch: 5 step: 303, loss is 0.0011245490750297904\n",
      "epoch: 5 step: 304, loss is 0.1288747489452362\n",
      "epoch: 5 step: 305, loss is 0.0027667563408613205\n",
      "epoch: 5 step: 306, loss is 0.03129849582910538\n",
      "epoch: 5 step: 307, loss is 0.0002146970946341753\n",
      "epoch: 5 step: 308, loss is 0.0013333299430087209\n",
      "epoch: 5 step: 309, loss is 0.013903157785534859\n",
      "epoch: 5 step: 310, loss is 0.0006075004348531365\n",
      "epoch: 5 step: 311, loss is 0.052631039172410965\n",
      "epoch: 5 step: 312, loss is 0.016024764627218246\n",
      "epoch: 5 step: 313, loss is 0.0004942164523527026\n",
      "epoch: 5 step: 314, loss is 0.18382465839385986\n",
      "epoch: 5 step: 315, loss is 0.031030893325805664\n",
      "epoch: 5 step: 316, loss is 0.01890990510582924\n",
      "epoch: 5 step: 317, loss is 0.014744209125638008\n",
      "epoch: 5 step: 318, loss is 0.001583911944180727\n",
      "epoch: 5 step: 319, loss is 0.003289302811026573\n",
      "epoch: 5 step: 320, loss is 0.002892637625336647\n",
      "epoch: 5 step: 321, loss is 0.005920878611505032\n",
      "epoch: 5 step: 322, loss is 0.0008945601875893772\n",
      "epoch: 5 step: 323, loss is 0.03672091290354729\n",
      "epoch: 5 step: 324, loss is 0.11500759422779083\n",
      "epoch: 5 step: 325, loss is 0.0462709479033947\n",
      "epoch: 5 step: 326, loss is 0.0010363594628870487\n",
      "epoch: 5 step: 327, loss is 0.010489532724022865\n",
      "epoch: 5 step: 328, loss is 0.003241913393139839\n",
      "epoch: 5 step: 329, loss is 0.002369156340137124\n",
      "epoch: 5 step: 330, loss is 0.0012052200036123395\n",
      "epoch: 5 step: 331, loss is 0.004239475820213556\n",
      "epoch: 5 step: 332, loss is 0.10461246967315674\n",
      "epoch: 5 step: 333, loss is 0.0014254823327064514\n",
      "epoch: 5 step: 334, loss is 0.00154661585111171\n",
      "epoch: 5 step: 335, loss is 0.006494663655757904\n",
      "epoch: 5 step: 336, loss is 0.08880063891410828\n",
      "epoch: 5 step: 337, loss is 0.0013214105274528265\n",
      "epoch: 5 step: 338, loss is 0.0014619497815147042\n",
      "epoch: 5 step: 339, loss is 0.002706598024815321\n",
      "epoch: 5 step: 340, loss is 0.0004044051456730813\n",
      "epoch: 5 step: 341, loss is 0.019436752423644066\n",
      "epoch: 5 step: 342, loss is 0.08086234331130981\n",
      "epoch: 5 step: 343, loss is 0.03926531597971916\n",
      "epoch: 5 step: 344, loss is 0.018971776589751244\n",
      "epoch: 5 step: 345, loss is 0.11242619901895523\n",
      "epoch: 5 step: 346, loss is 0.002578644547611475\n",
      "epoch: 5 step: 347, loss is 0.009980825707316399\n",
      "epoch: 5 step: 348, loss is 0.007737940642982721\n",
      "epoch: 5 step: 349, loss is 0.024630295112729073\n",
      "epoch: 5 step: 350, loss is 0.0008675080025568604\n",
      "epoch: 5 step: 351, loss is 0.050078947097063065\n",
      "epoch: 5 step: 352, loss is 0.010206351056694984\n",
      "epoch: 5 step: 353, loss is 0.006044116336852312\n",
      "epoch: 5 step: 354, loss is 0.0031368331983685493\n",
      "epoch: 5 step: 355, loss is 0.00075100630056113\n",
      "epoch: 5 step: 356, loss is 0.071672223508358\n",
      "epoch: 5 step: 357, loss is 0.028253044933080673\n",
      "epoch: 5 step: 358, loss is 0.0018217371543869376\n",
      "epoch: 5 step: 359, loss is 0.01573442667722702\n",
      "epoch: 5 step: 360, loss is 0.02482278272509575\n",
      "epoch: 5 step: 361, loss is 0.25510725378990173\n",
      "epoch: 5 step: 362, loss is 0.006890694610774517\n",
      "epoch: 5 step: 363, loss is 0.08306026458740234\n",
      "epoch: 5 step: 364, loss is 0.1863514482975006\n",
      "epoch: 5 step: 365, loss is 0.0292686615139246\n",
      "epoch: 5 step: 366, loss is 0.0022096780594438314\n",
      "epoch: 5 step: 367, loss is 0.013586001470685005\n",
      "epoch: 5 step: 368, loss is 0.004853725433349609\n",
      "epoch: 5 step: 369, loss is 0.016209043562412262\n",
      "epoch: 5 step: 370, loss is 0.0052642785012722015\n",
      "epoch: 5 step: 371, loss is 0.023711277171969414\n",
      "epoch: 5 step: 372, loss is 0.015094545669853687\n",
      "epoch: 5 step: 373, loss is 0.010647442191839218\n",
      "epoch: 5 step: 374, loss is 0.004359847865998745\n",
      "epoch: 5 step: 375, loss is 0.012161921709775925\n",
      "epoch: 5 step: 376, loss is 0.02656116709113121\n",
      "epoch: 5 step: 377, loss is 0.023039856925606728\n",
      "epoch: 5 step: 378, loss is 0.030399754643440247\n",
      "epoch: 5 step: 379, loss is 0.15920981764793396\n",
      "epoch: 5 step: 380, loss is 0.00046410958748310804\n",
      "epoch: 5 step: 381, loss is 0.0013653732603415847\n",
      "epoch: 5 step: 382, loss is 0.005012035835534334\n",
      "epoch: 5 step: 383, loss is 0.01589275524020195\n",
      "epoch: 5 step: 384, loss is 0.00011294677096884698\n",
      "epoch: 5 step: 385, loss is 0.021818088367581367\n",
      "epoch: 5 step: 386, loss is 0.002849526470527053\n",
      "epoch: 5 step: 387, loss is 0.003900476498529315\n",
      "epoch: 5 step: 388, loss is 0.01053926907479763\n",
      "epoch: 5 step: 389, loss is 0.018476713448762894\n",
      "epoch: 5 step: 390, loss is 0.003660657675936818\n",
      "epoch: 5 step: 391, loss is 0.023123901337385178\n",
      "epoch: 5 step: 392, loss is 0.001993092242628336\n",
      "epoch: 5 step: 393, loss is 0.06019114330410957\n",
      "epoch: 5 step: 394, loss is 0.0067482297308743\n",
      "epoch: 5 step: 395, loss is 0.03281546011567116\n",
      "epoch: 5 step: 396, loss is 0.000688629224896431\n",
      "epoch: 5 step: 397, loss is 0.08629978448152542\n",
      "epoch: 5 step: 398, loss is 0.0360318198800087\n",
      "epoch: 5 step: 399, loss is 0.07309286296367645\n",
      "epoch: 5 step: 400, loss is 0.0019412278197705746\n",
      "epoch: 5 step: 401, loss is 0.018409620970487595\n",
      "epoch: 5 step: 402, loss is 0.02996346540749073\n",
      "epoch: 5 step: 403, loss is 0.016186179593205452\n",
      "epoch: 5 step: 404, loss is 0.048024896532297134\n",
      "epoch: 5 step: 405, loss is 0.009060317650437355\n",
      "epoch: 5 step: 406, loss is 0.12581560015678406\n",
      "epoch: 5 step: 407, loss is 0.0023054834455251694\n",
      "epoch: 5 step: 408, loss is 0.21499116718769073\n",
      "epoch: 5 step: 409, loss is 0.047670189291238785\n",
      "epoch: 5 step: 410, loss is 0.0015278999926522374\n",
      "epoch: 5 step: 411, loss is 0.01684483140707016\n",
      "epoch: 5 step: 412, loss is 0.003561273915693164\n",
      "epoch: 5 step: 413, loss is 0.0016570966690778732\n",
      "epoch: 5 step: 414, loss is 0.36020827293395996\n",
      "epoch: 5 step: 415, loss is 0.0336158350110054\n",
      "epoch: 5 step: 416, loss is 0.0009134179563261569\n",
      "epoch: 5 step: 417, loss is 0.0026628393679857254\n",
      "epoch: 5 step: 418, loss is 0.033660128712654114\n",
      "epoch: 5 step: 419, loss is 0.0048149628564715385\n",
      "epoch: 5 step: 420, loss is 0.05727226287126541\n",
      "epoch: 5 step: 421, loss is 0.036693133413791656\n",
      "epoch: 5 step: 422, loss is 0.08196772634983063\n",
      "epoch: 5 step: 423, loss is 0.2178226262331009\n",
      "epoch: 5 step: 424, loss is 0.0012412103824317455\n",
      "epoch: 5 step: 425, loss is 0.00013655549264512956\n",
      "epoch: 5 step: 426, loss is 0.03551851212978363\n",
      "epoch: 5 step: 427, loss is 0.0025229258462786674\n",
      "epoch: 5 step: 428, loss is 0.0038472162559628487\n",
      "epoch: 5 step: 429, loss is 0.02261016145348549\n",
      "epoch: 5 step: 430, loss is 0.0028578960336744785\n",
      "epoch: 5 step: 431, loss is 0.005535210482776165\n",
      "epoch: 5 step: 432, loss is 0.003575513605028391\n",
      "epoch: 5 step: 433, loss is 0.005493250209838152\n",
      "epoch: 5 step: 434, loss is 0.006200460251420736\n",
      "epoch: 5 step: 435, loss is 0.17795296013355255\n",
      "epoch: 5 step: 436, loss is 0.041397642344236374\n",
      "epoch: 5 step: 437, loss is 0.0005453310441225767\n",
      "epoch: 5 step: 438, loss is 0.010669257491827011\n",
      "epoch: 5 step: 439, loss is 0.014578724279999733\n",
      "epoch: 5 step: 440, loss is 0.02599775232374668\n",
      "epoch: 5 step: 441, loss is 0.004067314323037863\n",
      "epoch: 5 step: 442, loss is 0.010641367174685001\n",
      "epoch: 5 step: 443, loss is 0.012103318236768246\n",
      "epoch: 5 step: 444, loss is 0.06188737228512764\n",
      "epoch: 5 step: 445, loss is 0.11115505546331406\n",
      "epoch: 5 step: 446, loss is 0.005551290698349476\n",
      "epoch: 5 step: 447, loss is 0.1522548645734787\n",
      "epoch: 5 step: 448, loss is 0.10325868427753448\n",
      "epoch: 5 step: 449, loss is 0.006397598423063755\n",
      "epoch: 5 step: 450, loss is 0.0011348536936566234\n",
      "epoch: 5 step: 451, loss is 0.0007480211206711829\n",
      "epoch: 5 step: 452, loss is 0.2625218331813812\n",
      "epoch: 5 step: 453, loss is 0.0075577497482299805\n",
      "epoch: 5 step: 454, loss is 0.1184278279542923\n",
      "epoch: 5 step: 455, loss is 0.04751258343458176\n",
      "epoch: 5 step: 456, loss is 0.07324941456317902\n",
      "epoch: 5 step: 457, loss is 0.015464473515748978\n",
      "epoch: 5 step: 458, loss is 0.002180421492084861\n",
      "epoch: 5 step: 459, loss is 0.03450186178088188\n",
      "epoch: 5 step: 460, loss is 0.011878706514835358\n",
      "epoch: 5 step: 461, loss is 0.039583705365657806\n",
      "epoch: 5 step: 462, loss is 0.007490280084311962\n",
      "epoch: 5 step: 463, loss is 0.010662732645869255\n",
      "epoch: 5 step: 464, loss is 0.0015844817971810699\n",
      "epoch: 5 step: 465, loss is 0.0012631833087652922\n",
      "epoch: 5 step: 466, loss is 0.023799125105142593\n",
      "epoch: 5 step: 467, loss is 0.00012597163731697947\n",
      "epoch: 5 step: 468, loss is 0.016764391213655472\n",
      "epoch: 5 step: 469, loss is 0.01086602732539177\n",
      "epoch: 5 step: 470, loss is 0.00829227827489376\n",
      "epoch: 5 step: 471, loss is 0.016183780506253242\n",
      "epoch: 5 step: 472, loss is 0.011416795663535595\n",
      "epoch: 5 step: 473, loss is 0.00782169308513403\n",
      "epoch: 5 step: 474, loss is 0.11046501249074936\n",
      "epoch: 5 step: 475, loss is 0.0051735443994402885\n",
      "epoch: 5 step: 476, loss is 0.010155634954571724\n",
      "epoch: 5 step: 477, loss is 0.0021044875029474497\n",
      "epoch: 5 step: 478, loss is 0.009482010267674923\n",
      "epoch: 5 step: 479, loss is 0.05319475010037422\n",
      "epoch: 5 step: 480, loss is 0.003799635451287031\n",
      "epoch: 5 step: 481, loss is 0.019527828320860863\n",
      "epoch: 5 step: 482, loss is 0.014221844263374805\n",
      "epoch: 5 step: 483, loss is 0.006230028346180916\n",
      "epoch: 5 step: 484, loss is 0.0496165007352829\n",
      "epoch: 5 step: 485, loss is 0.010687582194805145\n",
      "epoch: 5 step: 486, loss is 0.00138500623870641\n",
      "epoch: 5 step: 487, loss is 0.025287916883826256\n",
      "epoch: 5 step: 488, loss is 0.09211612492799759\n",
      "epoch: 5 step: 489, loss is 0.012173508293926716\n",
      "epoch: 5 step: 490, loss is 0.011009065434336662\n",
      "epoch: 5 step: 491, loss is 0.10007672011852264\n",
      "epoch: 5 step: 492, loss is 0.03048856370151043\n",
      "epoch: 5 step: 493, loss is 0.015454773791134357\n",
      "epoch: 5 step: 494, loss is 0.010369299910962582\n",
      "epoch: 5 step: 495, loss is 0.00984429381787777\n",
      "epoch: 5 step: 496, loss is 0.0005123126320540905\n",
      "epoch: 5 step: 497, loss is 0.00782474223524332\n",
      "epoch: 5 step: 498, loss is 0.00958190206438303\n",
      "epoch: 5 step: 499, loss is 0.007269731722772121\n",
      "epoch: 5 step: 500, loss is 0.018677903339266777\n",
      "epoch: 5 step: 501, loss is 0.0007749101496301591\n",
      "epoch: 5 step: 502, loss is 0.006924142129719257\n",
      "epoch: 5 step: 503, loss is 0.0330413356423378\n",
      "epoch: 5 step: 504, loss is 0.02435961551964283\n",
      "epoch: 5 step: 505, loss is 0.0026528295129537582\n",
      "epoch: 5 step: 506, loss is 0.0022770727518945932\n",
      "epoch: 5 step: 507, loss is 0.0888676866889\n",
      "epoch: 5 step: 508, loss is 0.0012598783941939473\n",
      "epoch: 5 step: 509, loss is 0.006510318256914616\n",
      "epoch: 5 step: 510, loss is 0.0021629189141094685\n",
      "epoch: 5 step: 511, loss is 0.0005751783028244972\n",
      "epoch: 5 step: 512, loss is 0.004878913518041372\n",
      "epoch: 5 step: 513, loss is 0.004856544081121683\n",
      "epoch: 5 step: 514, loss is 0.016774462535977364\n",
      "epoch: 5 step: 515, loss is 0.08164359629154205\n",
      "epoch: 5 step: 516, loss is 0.017931073904037476\n",
      "epoch: 5 step: 517, loss is 0.0586850568652153\n",
      "epoch: 5 step: 518, loss is 0.0020768321119248867\n",
      "epoch: 5 step: 519, loss is 0.0032254133839160204\n",
      "epoch: 5 step: 520, loss is 0.061577629297971725\n",
      "epoch: 5 step: 521, loss is 0.058846913278102875\n",
      "epoch: 5 step: 522, loss is 0.002285746857523918\n",
      "epoch: 5 step: 523, loss is 0.00360379577614367\n",
      "epoch: 5 step: 524, loss is 0.03543194755911827\n",
      "epoch: 5 step: 525, loss is 0.002193826250731945\n",
      "epoch: 5 step: 526, loss is 0.05835513770580292\n",
      "epoch: 5 step: 527, loss is 0.008565869182348251\n",
      "epoch: 5 step: 528, loss is 0.04025546833872795\n",
      "epoch: 5 step: 529, loss is 0.004000278189778328\n",
      "epoch: 5 step: 530, loss is 0.029004564508795738\n",
      "epoch: 5 step: 531, loss is 0.0008162337471731007\n",
      "epoch: 5 step: 532, loss is 0.01695365086197853\n",
      "epoch: 5 step: 533, loss is 0.0006068964721634984\n",
      "epoch: 5 step: 534, loss is 0.024116666987538338\n",
      "epoch: 5 step: 535, loss is 0.05967898666858673\n",
      "epoch: 5 step: 536, loss is 0.012743772938847542\n",
      "epoch: 5 step: 537, loss is 0.0014001436065882444\n",
      "epoch: 5 step: 538, loss is 0.005663018673658371\n",
      "epoch: 5 step: 539, loss is 0.012411663308739662\n",
      "epoch: 5 step: 540, loss is 0.011040071956813335\n",
      "epoch: 5 step: 541, loss is 0.002386705484241247\n",
      "epoch: 5 step: 542, loss is 0.0009276101482100785\n",
      "epoch: 5 step: 543, loss is 0.0028035014402121305\n",
      "epoch: 5 step: 544, loss is 0.0009793033823370934\n",
      "epoch: 5 step: 545, loss is 0.0003890935331583023\n",
      "epoch: 5 step: 546, loss is 0.0016923531657084823\n",
      "epoch: 5 step: 547, loss is 0.00038471425068564713\n",
      "epoch: 5 step: 548, loss is 0.05424797907471657\n",
      "epoch: 5 step: 549, loss is 0.0004919317434541881\n",
      "epoch: 5 step: 550, loss is 0.00929890014231205\n",
      "epoch: 5 step: 551, loss is 0.0021863903384655714\n",
      "epoch: 5 step: 552, loss is 0.006187183782458305\n",
      "epoch: 5 step: 553, loss is 0.05972492694854736\n",
      "epoch: 5 step: 554, loss is 0.007850758731365204\n",
      "epoch: 5 step: 555, loss is 0.02651217207312584\n",
      "epoch: 5 step: 556, loss is 0.03233969956636429\n",
      "epoch: 5 step: 557, loss is 0.0033422831911593676\n",
      "epoch: 5 step: 558, loss is 0.00019995225011371076\n",
      "epoch: 5 step: 559, loss is 0.0036919862031936646\n",
      "epoch: 5 step: 560, loss is 0.0012350520119071007\n",
      "epoch: 5 step: 561, loss is 0.016284001991152763\n",
      "epoch: 5 step: 562, loss is 0.0008986180764622986\n",
      "epoch: 5 step: 563, loss is 0.001018005539663136\n",
      "epoch: 5 step: 564, loss is 0.022172842174768448\n",
      "epoch: 5 step: 565, loss is 0.0014689683448523283\n",
      "epoch: 5 step: 566, loss is 0.001124700647778809\n",
      "epoch: 5 step: 567, loss is 0.0077446759678423405\n",
      "epoch: 5 step: 568, loss is 0.08003856241703033\n",
      "epoch: 5 step: 569, loss is 0.0028466361109167337\n",
      "epoch: 5 step: 570, loss is 0.0020046404097229242\n",
      "epoch: 5 step: 571, loss is 0.0002553216472733766\n",
      "epoch: 5 step: 572, loss is 0.0012454730458557606\n",
      "epoch: 5 step: 573, loss is 0.07127226889133453\n",
      "epoch: 5 step: 574, loss is 0.0009173736325465143\n",
      "epoch: 5 step: 575, loss is 0.014160859398543835\n",
      "epoch: 5 step: 576, loss is 0.0005808339337818325\n",
      "epoch: 5 step: 577, loss is 0.032246869057416916\n",
      "epoch: 5 step: 578, loss is 0.0025956027675420046\n",
      "epoch: 5 step: 579, loss is 0.14363573491573334\n",
      "epoch: 5 step: 580, loss is 0.0032304616179317236\n",
      "epoch: 5 step: 581, loss is 6.515495624626055e-05\n",
      "epoch: 5 step: 582, loss is 0.0033076703548431396\n",
      "epoch: 5 step: 583, loss is 0.00661011366173625\n",
      "epoch: 5 step: 584, loss is 0.00040247320430353284\n",
      "epoch: 5 step: 585, loss is 0.0030597285367548466\n",
      "epoch: 5 step: 586, loss is 0.0026768192183226347\n",
      "epoch: 5 step: 587, loss is 0.0004170194733887911\n",
      "epoch: 5 step: 588, loss is 0.02565579116344452\n",
      "epoch: 5 step: 589, loss is 0.0007645149598829448\n",
      "epoch: 5 step: 590, loss is 0.0016849172534421086\n",
      "epoch: 5 step: 591, loss is 0.014054789207875729\n",
      "epoch: 5 step: 592, loss is 0.0006536274449899793\n",
      "epoch: 5 step: 593, loss is 0.0017269442323595285\n",
      "epoch: 5 step: 594, loss is 0.13455086946487427\n",
      "epoch: 5 step: 595, loss is 0.010947630740702152\n",
      "epoch: 5 step: 596, loss is 0.0477188304066658\n",
      "epoch: 5 step: 597, loss is 0.0014924767892807722\n",
      "epoch: 5 step: 598, loss is 0.0022687637247145176\n",
      "epoch: 5 step: 599, loss is 0.0124738784506917\n",
      "epoch: 5 step: 600, loss is 0.005674362648278475\n",
      "epoch: 5 step: 601, loss is 0.0028965279925614595\n",
      "epoch: 5 step: 602, loss is 0.02500113658607006\n",
      "epoch: 5 step: 603, loss is 0.0022088757250458\n",
      "epoch: 5 step: 604, loss is 0.13948914408683777\n",
      "epoch: 5 step: 605, loss is 0.060105085372924805\n",
      "epoch: 5 step: 606, loss is 0.009767062030732632\n",
      "epoch: 5 step: 607, loss is 6.575271982001141e-05\n",
      "epoch: 5 step: 608, loss is 0.04021120443940163\n",
      "epoch: 5 step: 609, loss is 0.07209982722997665\n",
      "epoch: 5 step: 610, loss is 0.0804692879319191\n",
      "epoch: 5 step: 611, loss is 0.0006008914788253605\n",
      "epoch: 5 step: 612, loss is 0.0011190773220732808\n",
      "epoch: 5 step: 613, loss is 0.0008373599266633391\n",
      "epoch: 5 step: 614, loss is 0.0003053275868296623\n",
      "epoch: 5 step: 615, loss is 0.005223090760409832\n",
      "epoch: 5 step: 616, loss is 0.09702131152153015\n",
      "epoch: 5 step: 617, loss is 0.008497238159179688\n",
      "epoch: 5 step: 618, loss is 0.002444162964820862\n",
      "epoch: 5 step: 619, loss is 0.0033363383263349533\n",
      "epoch: 5 step: 620, loss is 0.002054184675216675\n",
      "epoch: 5 step: 621, loss is 0.04803720489144325\n",
      "epoch: 5 step: 622, loss is 5.378326750360429e-05\n",
      "epoch: 5 step: 623, loss is 0.016696957871317863\n",
      "epoch: 5 step: 624, loss is 0.12746284902095795\n",
      "epoch: 5 step: 625, loss is 0.08186294138431549\n",
      "epoch: 5 step: 626, loss is 0.0005166358314454556\n",
      "epoch: 5 step: 627, loss is 0.07755396515130997\n",
      "epoch: 5 step: 628, loss is 0.0028385573532432318\n",
      "epoch: 5 step: 629, loss is 0.002440753858536482\n",
      "epoch: 5 step: 630, loss is 0.028299523517489433\n",
      "epoch: 5 step: 631, loss is 0.025529444217681885\n",
      "epoch: 5 step: 632, loss is 0.00407807994633913\n",
      "epoch: 5 step: 633, loss is 0.07632844150066376\n",
      "epoch: 5 step: 634, loss is 0.026090603321790695\n",
      "epoch: 5 step: 635, loss is 0.0772801861166954\n",
      "epoch: 5 step: 636, loss is 0.02536390721797943\n",
      "epoch: 5 step: 637, loss is 0.0148897310718894\n",
      "epoch: 5 step: 638, loss is 0.00029570524930022657\n",
      "epoch: 5 step: 639, loss is 0.06994738429784775\n",
      "epoch: 5 step: 640, loss is 0.015577703714370728\n",
      "epoch: 5 step: 641, loss is 0.01969914883375168\n",
      "epoch: 5 step: 642, loss is 0.03529765456914902\n",
      "epoch: 5 step: 643, loss is 0.014022531919181347\n",
      "epoch: 5 step: 644, loss is 0.0028783248271793127\n",
      "epoch: 5 step: 645, loss is 0.0020464546978473663\n",
      "epoch: 5 step: 646, loss is 0.023436617106199265\n",
      "epoch: 5 step: 647, loss is 0.041111402213573456\n",
      "epoch: 5 step: 648, loss is 0.001003363635390997\n",
      "epoch: 5 step: 649, loss is 0.00028631475288420916\n",
      "epoch: 5 step: 650, loss is 0.042671822011470795\n",
      "epoch: 5 step: 651, loss is 0.0011284995125606656\n",
      "epoch: 5 step: 652, loss is 0.18330839276313782\n",
      "epoch: 5 step: 653, loss is 0.009333360940217972\n",
      "epoch: 5 step: 654, loss is 0.003119290806353092\n",
      "epoch: 5 step: 655, loss is 0.0010793388355523348\n",
      "epoch: 5 step: 656, loss is 0.022021906450390816\n",
      "epoch: 5 step: 657, loss is 0.0005323940422385931\n",
      "epoch: 5 step: 658, loss is 0.0003133082645945251\n",
      "epoch: 5 step: 659, loss is 0.20568722486495972\n",
      "epoch: 5 step: 660, loss is 0.0005029489984735847\n",
      "epoch: 5 step: 661, loss is 0.02952517196536064\n",
      "epoch: 5 step: 662, loss is 0.02422407642006874\n",
      "epoch: 5 step: 663, loss is 0.07661404460668564\n",
      "epoch: 5 step: 664, loss is 0.0010216091759502888\n",
      "epoch: 5 step: 665, loss is 0.0007447210955433547\n",
      "epoch: 5 step: 666, loss is 0.11103061586618423\n",
      "epoch: 5 step: 667, loss is 0.023281054571270943\n",
      "epoch: 5 step: 668, loss is 0.014088166877627373\n",
      "epoch: 5 step: 669, loss is 0.01800345443189144\n",
      "epoch: 5 step: 670, loss is 0.0014782187063246965\n",
      "epoch: 5 step: 671, loss is 0.005108392797410488\n",
      "epoch: 5 step: 672, loss is 0.00418403884395957\n",
      "epoch: 5 step: 673, loss is 0.01328473724424839\n",
      "epoch: 5 step: 674, loss is 0.0436900295317173\n",
      "epoch: 5 step: 675, loss is 0.00384903559461236\n",
      "epoch: 5 step: 676, loss is 0.0018046495970338583\n",
      "epoch: 5 step: 677, loss is 0.21944646537303925\n",
      "epoch: 5 step: 678, loss is 0.0014650106895714998\n",
      "epoch: 5 step: 679, loss is 0.012335804291069508\n",
      "epoch: 5 step: 680, loss is 0.030463367700576782\n",
      "epoch: 5 step: 681, loss is 0.02493450976908207\n",
      "epoch: 5 step: 682, loss is 0.004227040335536003\n",
      "epoch: 5 step: 683, loss is 0.017888346686959267\n",
      "epoch: 5 step: 684, loss is 0.02118820883333683\n",
      "epoch: 5 step: 685, loss is 0.06109208986163139\n",
      "epoch: 5 step: 686, loss is 0.02102198824286461\n",
      "epoch: 5 step: 687, loss is 0.10761761665344238\n",
      "epoch: 5 step: 688, loss is 0.5117922425270081\n",
      "epoch: 5 step: 689, loss is 0.0003626341058406979\n",
      "epoch: 5 step: 690, loss is 0.0552256777882576\n",
      "epoch: 5 step: 691, loss is 0.0004183866549283266\n",
      "epoch: 5 step: 692, loss is 0.03832647204399109\n",
      "epoch: 5 step: 693, loss is 0.0008502024575136602\n",
      "epoch: 5 step: 694, loss is 0.1448540836572647\n",
      "epoch: 5 step: 695, loss is 0.006572360172867775\n",
      "epoch: 5 step: 696, loss is 0.031147541478276253\n",
      "epoch: 5 step: 697, loss is 0.0003847727202810347\n",
      "epoch: 5 step: 698, loss is 0.002087801694869995\n",
      "epoch: 5 step: 699, loss is 0.0164751298725605\n",
      "epoch: 5 step: 700, loss is 0.0017703297780826688\n",
      "epoch: 5 step: 701, loss is 0.00042568438220769167\n",
      "epoch: 5 step: 702, loss is 0.0663890615105629\n",
      "epoch: 5 step: 703, loss is 0.012580448761582375\n",
      "epoch: 5 step: 704, loss is 0.000572599470615387\n",
      "epoch: 5 step: 705, loss is 0.07641344517469406\n",
      "epoch: 5 step: 706, loss is 0.0017548252362757921\n",
      "epoch: 5 step: 707, loss is 0.04240825027227402\n",
      "epoch: 5 step: 708, loss is 0.17581693828105927\n",
      "epoch: 5 step: 709, loss is 0.0036394281778484583\n",
      "epoch: 5 step: 710, loss is 0.004715617746114731\n",
      "epoch: 5 step: 711, loss is 0.08704578876495361\n",
      "epoch: 5 step: 712, loss is 0.0034677591174840927\n",
      "epoch: 5 step: 713, loss is 0.07944422215223312\n",
      "epoch: 5 step: 714, loss is 0.23405230045318604\n",
      "epoch: 5 step: 715, loss is 0.02527555450797081\n",
      "epoch: 5 step: 716, loss is 0.05138344690203667\n",
      "epoch: 5 step: 717, loss is 0.009887175634503365\n",
      "epoch: 5 step: 718, loss is 0.030396971851587296\n",
      "epoch: 5 step: 719, loss is 0.006873385515064001\n",
      "epoch: 5 step: 720, loss is 0.07823164761066437\n",
      "epoch: 5 step: 721, loss is 0.01031255442649126\n",
      "epoch: 5 step: 722, loss is 0.1934758871793747\n",
      "epoch: 5 step: 723, loss is 0.008727123960852623\n",
      "epoch: 5 step: 724, loss is 0.003947695251554251\n",
      "epoch: 5 step: 725, loss is 0.021005867049098015\n",
      "epoch: 5 step: 726, loss is 0.008590616285800934\n",
      "epoch: 5 step: 727, loss is 0.0011271648108959198\n",
      "epoch: 5 step: 728, loss is 0.014531208202242851\n",
      "epoch: 5 step: 729, loss is 0.005329713225364685\n",
      "epoch: 5 step: 730, loss is 0.003168543567880988\n",
      "epoch: 5 step: 731, loss is 0.004843438975512981\n",
      "epoch: 5 step: 732, loss is 0.15545037388801575\n",
      "epoch: 5 step: 733, loss is 0.0023917662911117077\n",
      "epoch: 5 step: 734, loss is 0.0025735909584909678\n",
      "epoch: 5 step: 735, loss is 0.10033305734395981\n",
      "epoch: 5 step: 736, loss is 0.0009916780982166529\n",
      "epoch: 5 step: 737, loss is 0.10489627718925476\n",
      "epoch: 5 step: 738, loss is 0.04811468720436096\n",
      "epoch: 5 step: 739, loss is 0.008135722950100899\n",
      "epoch: 5 step: 740, loss is 0.023433592170476913\n",
      "epoch: 5 step: 741, loss is 0.010277980007231236\n",
      "epoch: 5 step: 742, loss is 0.26362958550453186\n",
      "epoch: 5 step: 743, loss is 0.0009773958008736372\n",
      "epoch: 5 step: 744, loss is 0.01249063853174448\n",
      "epoch: 5 step: 745, loss is 0.005526443477720022\n",
      "epoch: 5 step: 746, loss is 0.016787292435765266\n",
      "epoch: 5 step: 747, loss is 0.0164849441498518\n",
      "epoch: 5 step: 748, loss is 0.0008936726371757686\n",
      "epoch: 5 step: 749, loss is 0.0020940895192325115\n",
      "epoch: 5 step: 750, loss is 0.0008763640653342009\n",
      "epoch: 5 step: 751, loss is 0.06304574012756348\n",
      "epoch: 5 step: 752, loss is 0.007423849776387215\n",
      "epoch: 5 step: 753, loss is 0.051529787480831146\n",
      "epoch: 5 step: 754, loss is 0.008666298352181911\n",
      "epoch: 5 step: 755, loss is 0.000945306324865669\n",
      "epoch: 5 step: 756, loss is 0.011976400390267372\n",
      "epoch: 5 step: 757, loss is 0.004325582180172205\n",
      "epoch: 5 step: 758, loss is 0.0816994458436966\n",
      "epoch: 5 step: 759, loss is 0.017151543870568275\n",
      "epoch: 5 step: 760, loss is 0.08018703013658524\n",
      "epoch: 5 step: 761, loss is 0.027073943987488747\n",
      "epoch: 5 step: 762, loss is 0.01788249984383583\n",
      "epoch: 5 step: 763, loss is 0.005291599314659834\n",
      "epoch: 5 step: 764, loss is 0.036640465259552\n",
      "epoch: 5 step: 765, loss is 0.01416509598493576\n",
      "epoch: 5 step: 766, loss is 0.039772577583789825\n",
      "epoch: 5 step: 767, loss is 0.0008239583112299442\n",
      "epoch: 5 step: 768, loss is 0.0011396954068914056\n",
      "epoch: 5 step: 769, loss is 0.0031474835705012083\n",
      "epoch: 5 step: 770, loss is 0.002023971639573574\n",
      "epoch: 5 step: 771, loss is 0.04951493442058563\n",
      "epoch: 5 step: 772, loss is 0.01470559649169445\n",
      "epoch: 5 step: 773, loss is 0.034894589334726334\n",
      "epoch: 5 step: 774, loss is 0.013306629844009876\n",
      "epoch: 5 step: 775, loss is 0.0007015648297965527\n",
      "epoch: 5 step: 776, loss is 0.019205646589398384\n",
      "epoch: 5 step: 777, loss is 0.00176580052357167\n",
      "epoch: 5 step: 778, loss is 0.0040200320072472095\n",
      "epoch: 5 step: 779, loss is 0.006919014733284712\n",
      "epoch: 5 step: 780, loss is 0.0171717070043087\n",
      "epoch: 5 step: 781, loss is 0.1062004566192627\n",
      "epoch: 5 step: 782, loss is 0.003561049234122038\n",
      "epoch: 5 step: 783, loss is 0.02704518474638462\n",
      "epoch: 5 step: 784, loss is 0.005380985792726278\n",
      "epoch: 5 step: 785, loss is 0.01962149702012539\n",
      "epoch: 5 step: 786, loss is 0.004787764046341181\n",
      "epoch: 5 step: 787, loss is 0.06539540737867355\n",
      "epoch: 5 step: 788, loss is 0.015423390083014965\n",
      "epoch: 5 step: 789, loss is 0.0070481617003679276\n",
      "epoch: 5 step: 790, loss is 0.008618973195552826\n",
      "epoch: 5 step: 791, loss is 0.04218136519193649\n",
      "epoch: 5 step: 792, loss is 0.09495143592357635\n",
      "epoch: 5 step: 793, loss is 0.07228671759366989\n",
      "epoch: 5 step: 794, loss is 0.007172988262027502\n",
      "epoch: 5 step: 795, loss is 0.004084286279976368\n",
      "epoch: 5 step: 796, loss is 0.02228700742125511\n",
      "epoch: 5 step: 797, loss is 0.001119751832447946\n",
      "epoch: 5 step: 798, loss is 0.0428597666323185\n",
      "epoch: 5 step: 799, loss is 0.023389628157019615\n",
      "epoch: 5 step: 800, loss is 0.0004944276879541576\n",
      "epoch: 5 step: 801, loss is 0.06439173221588135\n",
      "epoch: 5 step: 802, loss is 0.002075567841529846\n",
      "epoch: 5 step: 803, loss is 0.023959379643201828\n",
      "epoch: 5 step: 804, loss is 0.0010149974841624498\n",
      "epoch: 5 step: 805, loss is 0.010803954675793648\n",
      "epoch: 5 step: 806, loss is 0.003421796252951026\n",
      "epoch: 5 step: 807, loss is 0.012233818881213665\n",
      "epoch: 5 step: 808, loss is 0.0005345534882508218\n",
      "epoch: 5 step: 809, loss is 0.0033761539962142706\n",
      "epoch: 5 step: 810, loss is 0.0006637233309447765\n",
      "epoch: 5 step: 811, loss is 0.0036979469005018473\n",
      "epoch: 5 step: 812, loss is 0.0004532028397079557\n",
      "epoch: 5 step: 813, loss is 0.0006660671206191182\n",
      "epoch: 5 step: 814, loss is 0.0019325388129800558\n",
      "epoch: 5 step: 815, loss is 0.0010968425776809454\n",
      "epoch: 5 step: 816, loss is 0.021486470475792885\n",
      "epoch: 5 step: 817, loss is 0.05490664020180702\n",
      "epoch: 5 step: 818, loss is 0.08024407923221588\n",
      "epoch: 5 step: 819, loss is 0.08517821133136749\n",
      "epoch: 5 step: 820, loss is 0.003644209587946534\n",
      "epoch: 5 step: 821, loss is 0.016274303197860718\n",
      "epoch: 5 step: 822, loss is 0.0003161584318149835\n",
      "epoch: 5 step: 823, loss is 0.014164898544549942\n",
      "epoch: 5 step: 824, loss is 0.032278429716825485\n",
      "epoch: 5 step: 825, loss is 0.010850287042558193\n",
      "epoch: 5 step: 826, loss is 0.02710810676217079\n",
      "epoch: 5 step: 827, loss is 0.34493139386177063\n",
      "epoch: 5 step: 828, loss is 0.0045827776193618774\n",
      "epoch: 5 step: 829, loss is 0.02196859009563923\n",
      "epoch: 5 step: 830, loss is 0.019389644265174866\n",
      "epoch: 5 step: 831, loss is 0.00206410838291049\n",
      "epoch: 5 step: 832, loss is 0.03491370379924774\n",
      "epoch: 5 step: 833, loss is 0.005118126980960369\n",
      "epoch: 5 step: 834, loss is 0.02412772923707962\n",
      "epoch: 5 step: 835, loss is 0.012139354832470417\n",
      "epoch: 5 step: 836, loss is 0.006002285052090883\n",
      "epoch: 5 step: 837, loss is 0.024869926273822784\n",
      "epoch: 5 step: 838, loss is 0.0005347750266082585\n",
      "epoch: 5 step: 839, loss is 0.0004726792685687542\n",
      "epoch: 5 step: 840, loss is 0.006068897899240255\n",
      "epoch: 5 step: 841, loss is 0.007979288697242737\n",
      "epoch: 5 step: 842, loss is 0.03752981126308441\n",
      "epoch: 5 step: 843, loss is 0.08471556752920151\n",
      "epoch: 5 step: 844, loss is 0.004642782732844353\n",
      "epoch: 5 step: 845, loss is 0.0021065010223537683\n",
      "epoch: 5 step: 846, loss is 0.0003008376224897802\n",
      "epoch: 5 step: 847, loss is 0.046405211091041565\n",
      "epoch: 5 step: 848, loss is 0.2620323896408081\n",
      "epoch: 5 step: 849, loss is 0.13611824810504913\n",
      "epoch: 5 step: 850, loss is 0.01144497748464346\n",
      "epoch: 5 step: 851, loss is 0.009330114349722862\n",
      "epoch: 5 step: 852, loss is 0.0008395955665037036\n",
      "epoch: 5 step: 853, loss is 0.16743101179599762\n",
      "epoch: 5 step: 854, loss is 0.013185321353375912\n",
      "epoch: 5 step: 855, loss is 0.08233578503131866\n",
      "epoch: 5 step: 856, loss is 0.08873145282268524\n",
      "epoch: 5 step: 857, loss is 0.0869622603058815\n",
      "epoch: 5 step: 858, loss is 0.015293337404727936\n",
      "epoch: 5 step: 859, loss is 0.0013209190219640732\n",
      "epoch: 5 step: 860, loss is 0.01756785437464714\n",
      "epoch: 5 step: 861, loss is 0.18255077302455902\n",
      "epoch: 5 step: 862, loss is 0.0008406396373175085\n",
      "epoch: 5 step: 863, loss is 0.001451502786949277\n",
      "epoch: 5 step: 864, loss is 0.0031195615883916616\n",
      "epoch: 5 step: 865, loss is 0.052696846425533295\n",
      "epoch: 5 step: 866, loss is 0.0016290502389892936\n",
      "epoch: 5 step: 867, loss is 0.06358147412538528\n",
      "epoch: 5 step: 868, loss is 0.017160790041089058\n",
      "epoch: 5 step: 869, loss is 0.022404152899980545\n",
      "epoch: 5 step: 870, loss is 0.01334852259606123\n",
      "epoch: 5 step: 871, loss is 0.1656551957130432\n",
      "epoch: 5 step: 872, loss is 0.002033455530181527\n",
      "epoch: 5 step: 873, loss is 0.002346619265154004\n",
      "epoch: 5 step: 874, loss is 0.053884655237197876\n",
      "epoch: 5 step: 875, loss is 0.011487609706819057\n",
      "epoch: 5 step: 876, loss is 0.0024321337696164846\n",
      "epoch: 5 step: 877, loss is 0.20437312126159668\n",
      "epoch: 5 step: 878, loss is 0.002628264483064413\n",
      "epoch: 5 step: 879, loss is 0.008861358277499676\n",
      "epoch: 5 step: 880, loss is 0.004489323124289513\n",
      "epoch: 5 step: 881, loss is 0.002333562122657895\n",
      "epoch: 5 step: 882, loss is 0.002925681648775935\n",
      "epoch: 5 step: 883, loss is 0.001050804159604013\n",
      "epoch: 5 step: 884, loss is 0.027013875544071198\n",
      "epoch: 5 step: 885, loss is 0.06341186165809631\n",
      "epoch: 5 step: 886, loss is 0.08850544691085815\n",
      "epoch: 5 step: 887, loss is 0.0014055201318114996\n",
      "epoch: 5 step: 888, loss is 0.001306752092204988\n",
      "epoch: 5 step: 889, loss is 0.1519562005996704\n",
      "epoch: 5 step: 890, loss is 0.01840801164507866\n",
      "epoch: 5 step: 891, loss is 0.0014797038165852427\n",
      "epoch: 5 step: 892, loss is 0.010512796230614185\n",
      "epoch: 5 step: 893, loss is 0.0060991887003183365\n",
      "epoch: 5 step: 894, loss is 0.09838354587554932\n",
      "epoch: 5 step: 895, loss is 0.00703106913715601\n",
      "epoch: 5 step: 896, loss is 0.001380382920615375\n",
      "epoch: 5 step: 897, loss is 0.06133013963699341\n",
      "epoch: 5 step: 898, loss is 0.006305688992142677\n",
      "epoch: 5 step: 899, loss is 0.001936382963322103\n",
      "epoch: 5 step: 900, loss is 0.0522426962852478\n",
      "epoch: 5 step: 901, loss is 0.007932940497994423\n",
      "epoch: 5 step: 902, loss is 0.00655503710731864\n",
      "epoch: 5 step: 903, loss is 0.005719115491956472\n",
      "epoch: 5 step: 904, loss is 0.11772528290748596\n",
      "epoch: 5 step: 905, loss is 0.001054485677741468\n",
      "epoch: 5 step: 906, loss is 0.003144947113469243\n",
      "epoch: 5 step: 907, loss is 0.0003475810226518661\n",
      "epoch: 5 step: 908, loss is 0.18149839341640472\n",
      "epoch: 5 step: 909, loss is 0.001806484186090529\n",
      "epoch: 5 step: 910, loss is 0.1262536346912384\n",
      "epoch: 5 step: 911, loss is 0.002328338101506233\n",
      "epoch: 5 step: 912, loss is 0.0034138206392526627\n",
      "epoch: 5 step: 913, loss is 0.011154594831168652\n",
      "epoch: 5 step: 914, loss is 0.012082141824066639\n",
      "epoch: 5 step: 915, loss is 0.17676930129528046\n",
      "epoch: 5 step: 916, loss is 0.013726402074098587\n",
      "epoch: 5 step: 917, loss is 0.2076137810945511\n",
      "epoch: 5 step: 918, loss is 0.0013304079184308648\n",
      "epoch: 5 step: 919, loss is 0.0231996588408947\n",
      "epoch: 5 step: 920, loss is 0.02176130749285221\n",
      "epoch: 5 step: 921, loss is 0.0013919442426413298\n",
      "epoch: 5 step: 922, loss is 0.004772634245455265\n",
      "epoch: 5 step: 923, loss is 0.04411986470222473\n",
      "epoch: 5 step: 924, loss is 0.2053048014640808\n",
      "epoch: 5 step: 925, loss is 0.027790868654847145\n",
      "epoch: 5 step: 926, loss is 0.004939707927405834\n",
      "epoch: 5 step: 927, loss is 0.08393409848213196\n",
      "epoch: 5 step: 928, loss is 0.004245596472173929\n",
      "epoch: 5 step: 929, loss is 0.0017748133977875113\n",
      "epoch: 5 step: 930, loss is 0.0010470584966242313\n",
      "epoch: 5 step: 931, loss is 0.18346168100833893\n",
      "epoch: 5 step: 932, loss is 0.008582085371017456\n",
      "epoch: 5 step: 933, loss is 0.019618798047304153\n",
      "epoch: 5 step: 934, loss is 0.04233996197581291\n",
      "epoch: 5 step: 935, loss is 0.008806820958852768\n",
      "epoch: 5 step: 936, loss is 0.034100476652383804\n",
      "epoch: 5 step: 937, loss is 0.08816071599721909\n",
      "epoch: 5 step: 938, loss is 0.09966734051704407\n",
      "epoch: 5 step: 939, loss is 0.005349963437765837\n",
      "epoch: 5 step: 940, loss is 0.024767307564616203\n",
      "epoch: 5 step: 941, loss is 0.003935523796826601\n",
      "epoch: 5 step: 942, loss is 0.019503135234117508\n",
      "epoch: 5 step: 943, loss is 0.00217231223359704\n",
      "epoch: 5 step: 944, loss is 0.0010115946643054485\n",
      "epoch: 5 step: 945, loss is 0.014520072378218174\n",
      "epoch: 5 step: 946, loss is 0.0007490352145396173\n",
      "epoch: 5 step: 947, loss is 0.006729943677783012\n",
      "epoch: 5 step: 948, loss is 0.0036626923829317093\n",
      "epoch: 5 step: 949, loss is 0.00035804082290269434\n",
      "epoch: 5 step: 950, loss is 0.009865574538707733\n",
      "epoch: 5 step: 951, loss is 0.015546072274446487\n",
      "epoch: 5 step: 952, loss is 0.0012063677422702312\n",
      "epoch: 5 step: 953, loss is 0.0005915145156905055\n",
      "epoch: 5 step: 954, loss is 0.0002871555043384433\n",
      "epoch: 5 step: 955, loss is 0.15899819135665894\n",
      "epoch: 5 step: 956, loss is 0.0012851095525547862\n",
      "epoch: 5 step: 957, loss is 0.03002486750483513\n",
      "epoch: 5 step: 958, loss is 0.0829140767455101\n",
      "epoch: 5 step: 959, loss is 0.02993931621313095\n",
      "epoch: 5 step: 960, loss is 0.004123658873140812\n",
      "epoch: 5 step: 961, loss is 0.07541314512491226\n",
      "epoch: 5 step: 962, loss is 0.001727883704006672\n",
      "epoch: 5 step: 963, loss is 0.0005507364985533059\n",
      "epoch: 5 step: 964, loss is 0.09025365859270096\n",
      "epoch: 5 step: 965, loss is 0.0010756284464150667\n",
      "epoch: 5 step: 966, loss is 0.01270271372050047\n",
      "epoch: 5 step: 967, loss is 0.0005504815489985049\n",
      "epoch: 5 step: 968, loss is 0.0014684072230011225\n",
      "epoch: 5 step: 969, loss is 0.0973694697022438\n",
      "epoch: 5 step: 970, loss is 0.03753773123025894\n",
      "epoch: 5 step: 971, loss is 0.04077456146478653\n",
      "epoch: 5 step: 972, loss is 0.06030098348855972\n",
      "epoch: 5 step: 973, loss is 0.00029614701634272933\n",
      "epoch: 5 step: 974, loss is 0.06443850696086884\n",
      "epoch: 5 step: 975, loss is 0.0019550889264792204\n",
      "epoch: 5 step: 976, loss is 0.1839006245136261\n",
      "epoch: 5 step: 977, loss is 0.023594791069626808\n",
      "epoch: 5 step: 978, loss is 0.06701712310314178\n",
      "epoch: 5 step: 979, loss is 0.0013570957817137241\n",
      "epoch: 5 step: 980, loss is 0.00790149625390768\n",
      "epoch: 5 step: 981, loss is 0.05492144823074341\n",
      "epoch: 5 step: 982, loss is 0.000570391770452261\n",
      "epoch: 5 step: 983, loss is 0.15158043801784515\n",
      "epoch: 5 step: 984, loss is 0.0756470113992691\n",
      "epoch: 5 step: 985, loss is 0.09770973026752472\n",
      "epoch: 5 step: 986, loss is 0.007833638228476048\n",
      "epoch: 5 step: 987, loss is 0.0030555559787899256\n",
      "epoch: 5 step: 988, loss is 0.2463238388299942\n",
      "epoch: 5 step: 989, loss is 0.13720703125\n",
      "epoch: 5 step: 990, loss is 0.0015361460391432047\n",
      "epoch: 5 step: 991, loss is 0.0012245004763826728\n",
      "epoch: 5 step: 992, loss is 0.0031785513274371624\n",
      "epoch: 5 step: 993, loss is 0.005320694297552109\n",
      "epoch: 5 step: 994, loss is 0.004973352886736393\n",
      "epoch: 5 step: 995, loss is 0.0123114800080657\n",
      "epoch: 5 step: 996, loss is 0.014961319044232368\n",
      "epoch: 5 step: 997, loss is 0.03355864807963371\n",
      "epoch: 5 step: 998, loss is 0.0003470415249466896\n",
      "epoch: 5 step: 999, loss is 0.07927380502223969\n",
      "epoch: 5 step: 1000, loss is 0.008548620156943798\n",
      "epoch: 5 step: 1001, loss is 0.000568187446333468\n",
      "epoch: 5 step: 1002, loss is 0.00031928776297718287\n",
      "epoch: 5 step: 1003, loss is 0.0020769641268998384\n",
      "epoch: 5 step: 1004, loss is 0.14662107825279236\n",
      "epoch: 5 step: 1005, loss is 0.00626777857542038\n",
      "epoch: 5 step: 1006, loss is 0.002921441337093711\n",
      "epoch: 5 step: 1007, loss is 0.041508838534355164\n",
      "epoch: 5 step: 1008, loss is 0.0005342348595149815\n",
      "epoch: 5 step: 1009, loss is 0.003155487822368741\n",
      "epoch: 5 step: 1010, loss is 0.024199023842811584\n",
      "epoch: 5 step: 1011, loss is 0.0009225085377693176\n",
      "epoch: 5 step: 1012, loss is 0.010268948040902615\n",
      "epoch: 5 step: 1013, loss is 0.11008187383413315\n",
      "epoch: 5 step: 1014, loss is 0.03715571016073227\n",
      "epoch: 5 step: 1015, loss is 0.01570740155875683\n",
      "epoch: 5 step: 1016, loss is 0.043413374572992325\n",
      "epoch: 5 step: 1017, loss is 0.008815699256956577\n",
      "epoch: 5 step: 1018, loss is 0.07056639343500137\n",
      "epoch: 5 step: 1019, loss is 0.00579356262460351\n",
      "epoch: 5 step: 1020, loss is 0.002060845261439681\n",
      "epoch: 5 step: 1021, loss is 0.0003897623100783676\n",
      "epoch: 5 step: 1022, loss is 0.0005286222440190613\n",
      "epoch: 5 step: 1023, loss is 0.06699187308549881\n",
      "epoch: 5 step: 1024, loss is 0.003184848465025425\n",
      "epoch: 5 step: 1025, loss is 0.1282292753458023\n",
      "epoch: 5 step: 1026, loss is 0.0033560823649168015\n",
      "epoch: 5 step: 1027, loss is 0.01942039094865322\n",
      "epoch: 5 step: 1028, loss is 0.0008728826651349664\n",
      "epoch: 5 step: 1029, loss is 0.00088105583563447\n",
      "epoch: 5 step: 1030, loss is 0.008025243878364563\n",
      "epoch: 5 step: 1031, loss is 0.03658926486968994\n",
      "epoch: 5 step: 1032, loss is 0.007050852756947279\n",
      "epoch: 5 step: 1033, loss is 0.0034760439302772284\n",
      "epoch: 5 step: 1034, loss is 0.022187909111380577\n",
      "epoch: 5 step: 1035, loss is 0.0022465249057859182\n",
      "epoch: 5 step: 1036, loss is 0.007878541946411133\n",
      "epoch: 5 step: 1037, loss is 0.0497908741235733\n",
      "epoch: 5 step: 1038, loss is 0.0015907553024590015\n",
      "epoch: 5 step: 1039, loss is 0.023649197071790695\n",
      "epoch: 5 step: 1040, loss is 0.008091744966804981\n",
      "epoch: 5 step: 1041, loss is 0.0016831550747156143\n",
      "epoch: 5 step: 1042, loss is 0.029627664014697075\n",
      "epoch: 5 step: 1043, loss is 0.000661803933326155\n",
      "epoch: 5 step: 1044, loss is 0.03489864617586136\n",
      "epoch: 5 step: 1045, loss is 0.028434017673134804\n",
      "epoch: 5 step: 1046, loss is 0.05943511053919792\n",
      "epoch: 5 step: 1047, loss is 0.029979463666677475\n",
      "epoch: 5 step: 1048, loss is 0.009842822328209877\n",
      "epoch: 5 step: 1049, loss is 0.17923758924007416\n",
      "epoch: 5 step: 1050, loss is 0.020940978080034256\n",
      "epoch: 5 step: 1051, loss is 9.770980250323191e-05\n",
      "epoch: 5 step: 1052, loss is 0.0011388722341507673\n",
      "epoch: 5 step: 1053, loss is 0.02633904106914997\n",
      "epoch: 5 step: 1054, loss is 0.06600460410118103\n",
      "epoch: 5 step: 1055, loss is 0.0003377151442691684\n",
      "epoch: 5 step: 1056, loss is 0.0014211732195690274\n",
      "epoch: 5 step: 1057, loss is 0.09125647693872452\n",
      "epoch: 5 step: 1058, loss is 0.007445398718118668\n",
      "epoch: 5 step: 1059, loss is 0.009036175906658173\n",
      "epoch: 5 step: 1060, loss is 0.004604920279234648\n",
      "epoch: 5 step: 1061, loss is 0.0008946493035182357\n",
      "epoch: 5 step: 1062, loss is 0.14567269384860992\n",
      "epoch: 5 step: 1063, loss is 0.002678241115063429\n",
      "epoch: 5 step: 1064, loss is 0.04010584205389023\n",
      "epoch: 5 step: 1065, loss is 0.006294652819633484\n",
      "epoch: 5 step: 1066, loss is 0.33417677879333496\n",
      "epoch: 5 step: 1067, loss is 0.03332033008337021\n",
      "epoch: 5 step: 1068, loss is 0.03863469511270523\n",
      "epoch: 5 step: 1069, loss is 0.0035701817832887173\n",
      "epoch: 5 step: 1070, loss is 0.002121353754773736\n",
      "epoch: 5 step: 1071, loss is 0.0005571786896325648\n",
      "epoch: 5 step: 1072, loss is 0.0003405007009860128\n",
      "epoch: 5 step: 1073, loss is 0.005070261191576719\n",
      "epoch: 5 step: 1074, loss is 0.017820963636040688\n",
      "epoch: 5 step: 1075, loss is 0.0013075679307803512\n",
      "epoch: 5 step: 1076, loss is 0.008155480027198792\n",
      "epoch: 5 step: 1077, loss is 0.015657050535082817\n",
      "epoch: 5 step: 1078, loss is 0.03603799641132355\n",
      "epoch: 5 step: 1079, loss is 0.007568618282675743\n",
      "epoch: 5 step: 1080, loss is 0.003547294531017542\n",
      "epoch: 5 step: 1081, loss is 0.0016850451938807964\n",
      "epoch: 5 step: 1082, loss is 0.0023607348557561636\n",
      "epoch: 5 step: 1083, loss is 0.006516769528388977\n",
      "epoch: 5 step: 1084, loss is 0.011891731061041355\n",
      "epoch: 5 step: 1085, loss is 0.008814393542706966\n",
      "epoch: 5 step: 1086, loss is 0.0022681611590087414\n",
      "epoch: 5 step: 1087, loss is 0.00583201739937067\n",
      "epoch: 5 step: 1088, loss is 0.002765194047242403\n",
      "epoch: 5 step: 1089, loss is 0.0013732013758271933\n",
      "epoch: 5 step: 1090, loss is 0.01436573825776577\n",
      "epoch: 5 step: 1091, loss is 0.0017300412291660905\n",
      "epoch: 5 step: 1092, loss is 0.001587944570928812\n",
      "epoch: 5 step: 1093, loss is 0.060103487223386765\n",
      "epoch: 5 step: 1094, loss is 0.0017489774618297815\n",
      "epoch: 5 step: 1095, loss is 0.1438388079404831\n",
      "epoch: 5 step: 1096, loss is 0.0004939496284350753\n",
      "epoch: 5 step: 1097, loss is 0.002295384183526039\n",
      "epoch: 5 step: 1098, loss is 0.0060892836190760136\n",
      "epoch: 5 step: 1099, loss is 0.03034072555601597\n",
      "epoch: 5 step: 1100, loss is 0.036536574363708496\n",
      "epoch: 5 step: 1101, loss is 0.08489696681499481\n",
      "epoch: 5 step: 1102, loss is 0.0014201211743056774\n",
      "epoch: 5 step: 1103, loss is 0.003431217512115836\n",
      "epoch: 5 step: 1104, loss is 0.00438778568059206\n",
      "epoch: 5 step: 1105, loss is 0.00861892756074667\n",
      "epoch: 5 step: 1106, loss is 0.0010911127319559455\n",
      "epoch: 5 step: 1107, loss is 0.003150559961795807\n",
      "epoch: 5 step: 1108, loss is 0.00836322084069252\n",
      "epoch: 5 step: 1109, loss is 0.03675847500562668\n",
      "epoch: 5 step: 1110, loss is 0.0895114466547966\n",
      "epoch: 5 step: 1111, loss is 0.03681384027004242\n",
      "epoch: 5 step: 1112, loss is 0.008766774088144302\n",
      "epoch: 5 step: 1113, loss is 8.048631571000442e-05\n",
      "epoch: 5 step: 1114, loss is 0.012785120867192745\n",
      "epoch: 5 step: 1115, loss is 0.0004522173840086907\n",
      "epoch: 5 step: 1116, loss is 0.03076891414821148\n",
      "epoch: 5 step: 1117, loss is 0.009884145110845566\n",
      "epoch: 5 step: 1118, loss is 0.0018336722860112786\n",
      "epoch: 5 step: 1119, loss is 0.0006283196271397173\n",
      "epoch: 5 step: 1120, loss is 0.013469468802213669\n",
      "epoch: 5 step: 1121, loss is 0.0034396369010210037\n",
      "epoch: 5 step: 1122, loss is 0.004477600567042828\n",
      "epoch: 5 step: 1123, loss is 0.0009595028823241591\n",
      "epoch: 5 step: 1124, loss is 0.0015097978757694364\n",
      "epoch: 5 step: 1125, loss is 0.0002099577832268551\n",
      "epoch: 5 step: 1126, loss is 0.010533737950026989\n",
      "epoch: 5 step: 1127, loss is 0.08723711967468262\n",
      "epoch: 5 step: 1128, loss is 0.002862707246094942\n",
      "epoch: 5 step: 1129, loss is 0.0057471501640975475\n",
      "epoch: 5 step: 1130, loss is 0.0037308409810066223\n",
      "epoch: 5 step: 1131, loss is 0.06542464345693588\n",
      "epoch: 5 step: 1132, loss is 0.004075936041772366\n",
      "epoch: 5 step: 1133, loss is 3.713478508871049e-05\n",
      "epoch: 5 step: 1134, loss is 0.0009998705936595798\n",
      "epoch: 5 step: 1135, loss is 0.025265507400035858\n",
      "epoch: 5 step: 1136, loss is 0.020304881036281586\n",
      "epoch: 5 step: 1137, loss is 0.00883750431239605\n",
      "epoch: 5 step: 1138, loss is 0.02483280934393406\n",
      "epoch: 5 step: 1139, loss is 0.004831276368349791\n",
      "epoch: 5 step: 1140, loss is 0.005095417145639658\n",
      "epoch: 5 step: 1141, loss is 0.009739407338202\n",
      "epoch: 5 step: 1142, loss is 0.00038372018025256693\n",
      "epoch: 5 step: 1143, loss is 0.0008581920992583036\n",
      "epoch: 5 step: 1144, loss is 0.001191840972751379\n",
      "epoch: 5 step: 1145, loss is 0.00019366807828191668\n",
      "epoch: 5 step: 1146, loss is 0.015415895730257034\n",
      "epoch: 5 step: 1147, loss is 0.13799013197422028\n",
      "epoch: 5 step: 1148, loss is 0.003002476878464222\n",
      "epoch: 5 step: 1149, loss is 0.0002256939624203369\n",
      "epoch: 5 step: 1150, loss is 0.00022604817058891058\n",
      "epoch: 5 step: 1151, loss is 0.0016565759433433414\n",
      "epoch: 5 step: 1152, loss is 0.05397556722164154\n",
      "epoch: 5 step: 1153, loss is 0.005280455108731985\n",
      "epoch: 5 step: 1154, loss is 0.0021246224641799927\n",
      "epoch: 5 step: 1155, loss is 0.0008835619082674384\n",
      "epoch: 5 step: 1156, loss is 0.000473761057946831\n",
      "epoch: 5 step: 1157, loss is 0.016671987250447273\n",
      "epoch: 5 step: 1158, loss is 0.001466292655095458\n",
      "epoch: 5 step: 1159, loss is 0.0033955611288547516\n",
      "epoch: 5 step: 1160, loss is 0.0004360402235761285\n",
      "epoch: 5 step: 1161, loss is 0.003235147101804614\n",
      "epoch: 5 step: 1162, loss is 0.011061361990869045\n",
      "epoch: 5 step: 1163, loss is 0.003199182916432619\n",
      "epoch: 5 step: 1164, loss is 0.01697593182325363\n",
      "epoch: 5 step: 1165, loss is 0.1512596309185028\n",
      "epoch: 5 step: 1166, loss is 0.03907305747270584\n",
      "epoch: 5 step: 1167, loss is 0.048285435885190964\n",
      "epoch: 5 step: 1168, loss is 0.2149202972650528\n",
      "epoch: 5 step: 1169, loss is 0.023311253637075424\n",
      "epoch: 5 step: 1170, loss is 0.021011363714933395\n",
      "epoch: 5 step: 1171, loss is 0.0017580091953277588\n",
      "epoch: 5 step: 1172, loss is 0.013383923098444939\n",
      "epoch: 5 step: 1173, loss is 0.010193460620939732\n",
      "epoch: 5 step: 1174, loss is 0.00120197469368577\n",
      "epoch: 5 step: 1175, loss is 0.0009269986185245216\n",
      "epoch: 5 step: 1176, loss is 0.00935415644198656\n",
      "epoch: 5 step: 1177, loss is 0.06789299845695496\n",
      "epoch: 5 step: 1178, loss is 0.0014767111279070377\n",
      "epoch: 5 step: 1179, loss is 0.004462159238755703\n",
      "epoch: 5 step: 1180, loss is 0.025609897449612617\n",
      "epoch: 5 step: 1181, loss is 0.0035681419540196657\n",
      "epoch: 5 step: 1182, loss is 0.015912486240267754\n",
      "epoch: 5 step: 1183, loss is 0.001028944505378604\n",
      "epoch: 5 step: 1184, loss is 0.06489247828722\n",
      "epoch: 5 step: 1185, loss is 0.005277588963508606\n",
      "epoch: 5 step: 1186, loss is 0.0008065697620622814\n",
      "epoch: 5 step: 1187, loss is 0.0004337229474913329\n",
      "epoch: 5 step: 1188, loss is 0.03279877081513405\n",
      "epoch: 5 step: 1189, loss is 0.002737213158980012\n",
      "epoch: 5 step: 1190, loss is 0.0002746670215856284\n",
      "epoch: 5 step: 1191, loss is 0.00437198206782341\n",
      "epoch: 5 step: 1192, loss is 0.0002663679770193994\n",
      "epoch: 5 step: 1193, loss is 0.15300299227237701\n",
      "epoch: 5 step: 1194, loss is 0.03393363952636719\n",
      "epoch: 5 step: 1195, loss is 0.0011303152423352003\n",
      "epoch: 5 step: 1196, loss is 0.009847049601376057\n",
      "epoch: 5 step: 1197, loss is 0.008558364585042\n",
      "epoch: 5 step: 1198, loss is 0.19143331050872803\n",
      "epoch: 5 step: 1199, loss is 0.0004025297239422798\n",
      "epoch: 5 step: 1200, loss is 0.03488435596227646\n",
      "epoch: 5 step: 1201, loss is 0.07346504926681519\n",
      "epoch: 5 step: 1202, loss is 0.02054343745112419\n",
      "epoch: 5 step: 1203, loss is 0.026947852224111557\n",
      "epoch: 5 step: 1204, loss is 0.0008603761671110988\n",
      "epoch: 5 step: 1205, loss is 0.008513015694916248\n",
      "epoch: 5 step: 1206, loss is 0.022819317877292633\n",
      "epoch: 5 step: 1207, loss is 0.008556271903216839\n",
      "epoch: 5 step: 1208, loss is 0.0005183002795092762\n",
      "epoch: 5 step: 1209, loss is 0.0008112310897558928\n",
      "epoch: 5 step: 1210, loss is 0.004178527742624283\n",
      "epoch: 5 step: 1211, loss is 0.0003213892050553113\n",
      "epoch: 5 step: 1212, loss is 0.0009928318904712796\n",
      "epoch: 5 step: 1213, loss is 0.00039437803206965327\n",
      "epoch: 5 step: 1214, loss is 0.02751169353723526\n",
      "epoch: 5 step: 1215, loss is 0.00036540767177939415\n",
      "epoch: 5 step: 1216, loss is 0.00393118429929018\n",
      "epoch: 5 step: 1217, loss is 9.079529263544828e-05\n",
      "epoch: 5 step: 1218, loss is 9.798118117032573e-05\n",
      "epoch: 5 step: 1219, loss is 0.11457187682390213\n",
      "epoch: 5 step: 1220, loss is 0.00040765409357845783\n",
      "epoch: 5 step: 1221, loss is 0.023331549018621445\n",
      "epoch: 5 step: 1222, loss is 0.08657713234424591\n",
      "epoch: 5 step: 1223, loss is 0.000786868273280561\n",
      "epoch: 5 step: 1224, loss is 0.008644013665616512\n",
      "epoch: 5 step: 1225, loss is 0.10905028134584427\n",
      "epoch: 5 step: 1226, loss is 0.0056597767397761345\n",
      "epoch: 5 step: 1227, loss is 0.00220179813914001\n",
      "epoch: 5 step: 1228, loss is 0.0021696786861866713\n",
      "epoch: 5 step: 1229, loss is 0.00212683598510921\n",
      "epoch: 5 step: 1230, loss is 0.0005292273708619177\n",
      "epoch: 5 step: 1231, loss is 0.007183322217315435\n",
      "epoch: 5 step: 1232, loss is 0.0004652079369407147\n",
      "epoch: 5 step: 1233, loss is 0.11760154366493225\n",
      "epoch: 5 step: 1234, loss is 0.06796931475400925\n",
      "epoch: 5 step: 1235, loss is 0.0007495184545405209\n",
      "epoch: 5 step: 1236, loss is 0.002237567910924554\n",
      "epoch: 5 step: 1237, loss is 0.011384566314518452\n",
      "epoch: 5 step: 1238, loss is 0.003036387963220477\n",
      "epoch: 5 step: 1239, loss is 0.06390123069286346\n",
      "epoch: 5 step: 1240, loss is 0.005847288761287928\n",
      "epoch: 5 step: 1241, loss is 0.00605353619903326\n",
      "epoch: 5 step: 1242, loss is 0.0010542331729084253\n",
      "epoch: 5 step: 1243, loss is 4.928445923724212e-05\n",
      "epoch: 5 step: 1244, loss is 0.0068971868604421616\n",
      "epoch: 5 step: 1245, loss is 0.01514635980129242\n",
      "epoch: 5 step: 1246, loss is 0.0011455276980996132\n",
      "epoch: 5 step: 1247, loss is 0.001723908120766282\n",
      "epoch: 5 step: 1248, loss is 0.2309776246547699\n",
      "epoch: 5 step: 1249, loss is 0.0007681502029299736\n",
      "epoch: 5 step: 1250, loss is 0.03588604927062988\n",
      "epoch: 5 step: 1251, loss is 0.0013407403603196144\n",
      "epoch: 5 step: 1252, loss is 0.009753972291946411\n",
      "epoch: 5 step: 1253, loss is 0.0006776488153263927\n",
      "epoch: 5 step: 1254, loss is 0.0293047484010458\n",
      "epoch: 5 step: 1255, loss is 0.012977330945432186\n",
      "epoch: 5 step: 1256, loss is 0.021549291908740997\n",
      "epoch: 5 step: 1257, loss is 0.002806686796247959\n",
      "epoch: 5 step: 1258, loss is 0.00030466372845694423\n",
      "epoch: 5 step: 1259, loss is 0.009141672402620316\n",
      "epoch: 5 step: 1260, loss is 0.0011011194437742233\n",
      "epoch: 5 step: 1261, loss is 0.0027763936668634415\n",
      "epoch: 5 step: 1262, loss is 0.006503765471279621\n",
      "epoch: 5 step: 1263, loss is 6.477878196164966e-05\n",
      "epoch: 5 step: 1264, loss is 0.010249054990708828\n",
      "epoch: 5 step: 1265, loss is 0.03583091124892235\n",
      "epoch: 5 step: 1266, loss is 0.10315217077732086\n",
      "epoch: 5 step: 1267, loss is 0.13054320216178894\n",
      "epoch: 5 step: 1268, loss is 0.15473110973834991\n",
      "epoch: 5 step: 1269, loss is 0.06291750073432922\n",
      "epoch: 5 step: 1270, loss is 0.00029170597554184496\n",
      "epoch: 5 step: 1271, loss is 0.06091602146625519\n",
      "epoch: 5 step: 1272, loss is 0.0191049724817276\n",
      "epoch: 5 step: 1273, loss is 0.007647547405213118\n",
      "epoch: 5 step: 1274, loss is 0.010482619516551495\n",
      "epoch: 5 step: 1275, loss is 0.013365667313337326\n",
      "epoch: 5 step: 1276, loss is 0.0033430829644203186\n",
      "epoch: 5 step: 1277, loss is 0.0048902081325650215\n",
      "epoch: 5 step: 1278, loss is 0.0002905263681896031\n",
      "epoch: 5 step: 1279, loss is 0.004052372649312019\n",
      "epoch: 5 step: 1280, loss is 0.00728720473125577\n",
      "epoch: 5 step: 1281, loss is 0.0010064051020890474\n",
      "epoch: 5 step: 1282, loss is 0.0006735037313774228\n",
      "epoch: 5 step: 1283, loss is 0.07902677357196808\n",
      "epoch: 5 step: 1284, loss is 0.00015804459690116346\n",
      "epoch: 5 step: 1285, loss is 0.0006452118395827711\n",
      "epoch: 5 step: 1286, loss is 0.14650872349739075\n",
      "epoch: 5 step: 1287, loss is 0.05699477344751358\n",
      "epoch: 5 step: 1288, loss is 0.0013967782724648714\n",
      "epoch: 5 step: 1289, loss is 0.0021602348424494267\n",
      "epoch: 5 step: 1290, loss is 0.028170330449938774\n",
      "epoch: 5 step: 1291, loss is 0.016841890290379524\n",
      "epoch: 5 step: 1292, loss is 0.0007753359968774021\n",
      "epoch: 5 step: 1293, loss is 0.0016470494447275996\n",
      "epoch: 5 step: 1294, loss is 0.24707767367362976\n",
      "epoch: 5 step: 1295, loss is 0.0064882440492510796\n",
      "epoch: 5 step: 1296, loss is 0.004442929290235043\n",
      "epoch: 5 step: 1297, loss is 0.0032988362945616245\n",
      "epoch: 5 step: 1298, loss is 0.22118303179740906\n",
      "epoch: 5 step: 1299, loss is 0.004006619565188885\n",
      "epoch: 5 step: 1300, loss is 0.04708544909954071\n",
      "epoch: 5 step: 1301, loss is 0.002726772101595998\n",
      "epoch: 5 step: 1302, loss is 0.0021986498031765223\n",
      "epoch: 5 step: 1303, loss is 0.0016527774278074503\n",
      "epoch: 5 step: 1304, loss is 0.013766895979642868\n",
      "epoch: 5 step: 1305, loss is 0.02232859656214714\n",
      "epoch: 5 step: 1306, loss is 0.10997437685728073\n",
      "epoch: 5 step: 1307, loss is 0.027930764481425285\n",
      "epoch: 5 step: 1308, loss is 0.0007343763136304915\n",
      "epoch: 5 step: 1309, loss is 0.006511915009468794\n",
      "epoch: 5 step: 1310, loss is 0.0005966377211734653\n",
      "epoch: 5 step: 1311, loss is 0.07526230067014694\n",
      "epoch: 5 step: 1312, loss is 0.0006118030287325382\n",
      "epoch: 5 step: 1313, loss is 0.037799183279275894\n",
      "epoch: 5 step: 1314, loss is 0.003109874902293086\n",
      "epoch: 5 step: 1315, loss is 0.06501267850399017\n",
      "epoch: 5 step: 1316, loss is 0.07698448747396469\n",
      "epoch: 5 step: 1317, loss is 0.07820399105548859\n",
      "epoch: 5 step: 1318, loss is 0.003303994657471776\n",
      "epoch: 5 step: 1319, loss is 0.0018412850331515074\n",
      "epoch: 5 step: 1320, loss is 0.09013926982879639\n",
      "epoch: 5 step: 1321, loss is 0.00023781252093613148\n",
      "epoch: 5 step: 1322, loss is 0.01090696919709444\n",
      "epoch: 5 step: 1323, loss is 0.021431613713502884\n",
      "epoch: 5 step: 1324, loss is 0.08328480273485184\n",
      "epoch: 5 step: 1325, loss is 0.00889851339161396\n",
      "epoch: 5 step: 1326, loss is 0.17508374154567719\n",
      "epoch: 5 step: 1327, loss is 0.0017839238280430436\n",
      "epoch: 5 step: 1328, loss is 0.022284232079982758\n",
      "epoch: 5 step: 1329, loss is 0.005593065172433853\n",
      "epoch: 5 step: 1330, loss is 0.0021262483205646276\n",
      "epoch: 5 step: 1331, loss is 0.0016456025186926126\n",
      "epoch: 5 step: 1332, loss is 0.005041440483182669\n",
      "epoch: 5 step: 1333, loss is 0.08083656430244446\n",
      "epoch: 5 step: 1334, loss is 0.017185525968670845\n",
      "epoch: 5 step: 1335, loss is 0.0010830461978912354\n",
      "epoch: 5 step: 1336, loss is 0.00020165086607448757\n",
      "epoch: 5 step: 1337, loss is 0.005731645971536636\n",
      "epoch: 5 step: 1338, loss is 0.01824970357120037\n",
      "epoch: 5 step: 1339, loss is 0.004268439952284098\n",
      "epoch: 5 step: 1340, loss is 0.0023516109213232994\n",
      "epoch: 5 step: 1341, loss is 0.004091532435268164\n",
      "epoch: 5 step: 1342, loss is 0.0002853563637472689\n",
      "epoch: 5 step: 1343, loss is 0.18564455211162567\n",
      "epoch: 5 step: 1344, loss is 0.09910013526678085\n",
      "epoch: 5 step: 1345, loss is 0.057451505213975906\n",
      "epoch: 5 step: 1346, loss is 0.0073692286387085915\n",
      "epoch: 5 step: 1347, loss is 0.10588443279266357\n",
      "epoch: 5 step: 1348, loss is 0.007309593725949526\n",
      "epoch: 5 step: 1349, loss is 0.14756128191947937\n",
      "epoch: 5 step: 1350, loss is 0.0008009454468265176\n",
      "epoch: 5 step: 1351, loss is 0.0023642259184271097\n",
      "epoch: 5 step: 1352, loss is 0.12336426973342896\n",
      "epoch: 5 step: 1353, loss is 0.06377067416906357\n",
      "epoch: 5 step: 1354, loss is 0.00011460809037089348\n",
      "epoch: 5 step: 1355, loss is 0.0032102817203849554\n",
      "epoch: 5 step: 1356, loss is 0.03702385723590851\n",
      "epoch: 5 step: 1357, loss is 0.16350695490837097\n",
      "epoch: 5 step: 1358, loss is 0.01635761186480522\n",
      "epoch: 5 step: 1359, loss is 0.0015037496341392398\n",
      "epoch: 5 step: 1360, loss is 0.00044501415686681867\n",
      "epoch: 5 step: 1361, loss is 0.05500933900475502\n",
      "epoch: 5 step: 1362, loss is 0.0013660731492564082\n",
      "epoch: 5 step: 1363, loss is 0.0028424670454114676\n",
      "epoch: 5 step: 1364, loss is 0.00843799114227295\n",
      "epoch: 5 step: 1365, loss is 0.059871844947338104\n",
      "epoch: 5 step: 1366, loss is 0.003528082277625799\n",
      "epoch: 5 step: 1367, loss is 0.031743958592414856\n",
      "epoch: 5 step: 1368, loss is 0.0007714364328421652\n",
      "epoch: 5 step: 1369, loss is 0.0016368108335882425\n",
      "epoch: 5 step: 1370, loss is 0.036017224192619324\n",
      "epoch: 5 step: 1371, loss is 0.0045678201131522655\n",
      "epoch: 5 step: 1372, loss is 0.00566298421472311\n",
      "epoch: 5 step: 1373, loss is 0.0035743245389312506\n",
      "epoch: 5 step: 1374, loss is 0.024744953960180283\n",
      "epoch: 5 step: 1375, loss is 0.027523953467607498\n",
      "epoch: 5 step: 1376, loss is 0.00030004687141627073\n",
      "epoch: 5 step: 1377, loss is 0.0012356707593426108\n",
      "epoch: 5 step: 1378, loss is 0.040921490639448166\n",
      "epoch: 5 step: 1379, loss is 0.1175089031457901\n",
      "epoch: 5 step: 1380, loss is 0.0002755216555669904\n",
      "epoch: 5 step: 1381, loss is 0.008522482588887215\n",
      "epoch: 5 step: 1382, loss is 0.0075629036873579025\n",
      "epoch: 5 step: 1383, loss is 0.002955408301204443\n",
      "epoch: 5 step: 1384, loss is 0.0023497324436903\n",
      "epoch: 5 step: 1385, loss is 0.03786146640777588\n",
      "epoch: 5 step: 1386, loss is 0.010834277607500553\n",
      "epoch: 5 step: 1387, loss is 0.011402348056435585\n",
      "epoch: 5 step: 1388, loss is 0.023792559280991554\n",
      "epoch: 5 step: 1389, loss is 0.05131971463561058\n",
      "epoch: 5 step: 1390, loss is 0.37839749455451965\n",
      "epoch: 5 step: 1391, loss is 0.01179156918078661\n",
      "epoch: 5 step: 1392, loss is 0.5051404237747192\n",
      "epoch: 5 step: 1393, loss is 0.25059136748313904\n",
      "epoch: 5 step: 1394, loss is 0.03187242150306702\n",
      "epoch: 5 step: 1395, loss is 0.03962092101573944\n",
      "epoch: 5 step: 1396, loss is 0.008437941782176495\n",
      "epoch: 5 step: 1397, loss is 0.0015582176856696606\n",
      "epoch: 5 step: 1398, loss is 0.010327973403036594\n",
      "epoch: 5 step: 1399, loss is 0.003002685261890292\n",
      "epoch: 5 step: 1400, loss is 0.008836123161017895\n",
      "epoch: 5 step: 1401, loss is 0.06290067732334137\n",
      "epoch: 5 step: 1402, loss is 0.07478807866573334\n",
      "epoch: 5 step: 1403, loss is 0.014100081287324429\n",
      "epoch: 5 step: 1404, loss is 0.004231489263474941\n",
      "epoch: 5 step: 1405, loss is 0.002377887722104788\n",
      "epoch: 5 step: 1406, loss is 0.010978637263178825\n",
      "epoch: 5 step: 1407, loss is 0.030964374542236328\n",
      "epoch: 5 step: 1408, loss is 0.08492337167263031\n",
      "epoch: 5 step: 1409, loss is 0.005573679227381945\n",
      "epoch: 5 step: 1410, loss is 0.0017739246832206845\n",
      "epoch: 5 step: 1411, loss is 0.0031410951633006334\n",
      "epoch: 5 step: 1412, loss is 0.00417340686544776\n",
      "epoch: 5 step: 1413, loss is 0.017780136317014694\n",
      "epoch: 5 step: 1414, loss is 0.13843505084514618\n",
      "epoch: 5 step: 1415, loss is 0.10807910561561584\n",
      "epoch: 5 step: 1416, loss is 0.01735050231218338\n",
      "epoch: 5 step: 1417, loss is 0.00047092518070712686\n",
      "epoch: 5 step: 1418, loss is 0.04447924718260765\n",
      "epoch: 5 step: 1419, loss is 0.020838696509599686\n",
      "epoch: 5 step: 1420, loss is 0.0006787172169424593\n",
      "epoch: 5 step: 1421, loss is 0.015780186280608177\n",
      "epoch: 5 step: 1422, loss is 0.0006636805483140051\n",
      "epoch: 5 step: 1423, loss is 0.006421463098376989\n",
      "epoch: 5 step: 1424, loss is 0.0011997546534985304\n",
      "epoch: 5 step: 1425, loss is 0.038863394409418106\n",
      "epoch: 5 step: 1426, loss is 0.06550377607345581\n",
      "epoch: 5 step: 1427, loss is 0.0016829433152452111\n",
      "epoch: 5 step: 1428, loss is 0.0038283085450530052\n",
      "epoch: 5 step: 1429, loss is 0.028953824192285538\n",
      "epoch: 5 step: 1430, loss is 0.0009475481929257512\n",
      "epoch: 5 step: 1431, loss is 0.05949253961443901\n",
      "epoch: 5 step: 1432, loss is 0.0012054764665663242\n",
      "epoch: 5 step: 1433, loss is 0.0031828524079173803\n",
      "epoch: 5 step: 1434, loss is 0.008695544674992561\n",
      "epoch: 5 step: 1435, loss is 0.039912886917591095\n",
      "epoch: 5 step: 1436, loss is 0.008116181008517742\n",
      "epoch: 5 step: 1437, loss is 0.0225759856402874\n",
      "epoch: 5 step: 1438, loss is 0.02823525294661522\n",
      "epoch: 5 step: 1439, loss is 0.0012015134561806917\n",
      "epoch: 5 step: 1440, loss is 0.011161590926349163\n",
      "epoch: 5 step: 1441, loss is 0.005021252203732729\n",
      "epoch: 5 step: 1442, loss is 0.0012994722928851843\n",
      "epoch: 5 step: 1443, loss is 0.0032306101638823748\n",
      "epoch: 5 step: 1444, loss is 0.008512703701853752\n",
      "epoch: 5 step: 1445, loss is 0.045237455517053604\n",
      "epoch: 5 step: 1446, loss is 0.08855470269918442\n",
      "epoch: 5 step: 1447, loss is 0.0034156315959990025\n",
      "epoch: 5 step: 1448, loss is 0.0006738111842423677\n",
      "epoch: 5 step: 1449, loss is 0.0019298763945698738\n",
      "epoch: 5 step: 1450, loss is 0.00653597479686141\n",
      "epoch: 5 step: 1451, loss is 0.008680406957864761\n",
      "epoch: 5 step: 1452, loss is 0.0023402671795338392\n",
      "epoch: 5 step: 1453, loss is 0.0031372320372611284\n",
      "epoch: 5 step: 1454, loss is 0.12133672088384628\n",
      "epoch: 5 step: 1455, loss is 0.0028988365083932877\n",
      "epoch: 5 step: 1456, loss is 0.1948038637638092\n",
      "epoch: 5 step: 1457, loss is 0.004125882871448994\n",
      "epoch: 5 step: 1458, loss is 0.00024258210032712668\n",
      "epoch: 5 step: 1459, loss is 0.001373198814690113\n",
      "epoch: 5 step: 1460, loss is 0.18965566158294678\n",
      "epoch: 5 step: 1461, loss is 0.00030185742070898414\n",
      "epoch: 5 step: 1462, loss is 0.02191656269133091\n",
      "epoch: 5 step: 1463, loss is 0.0006914080586284399\n",
      "epoch: 5 step: 1464, loss is 0.0026093176566064358\n",
      "epoch: 5 step: 1465, loss is 0.003696585074067116\n",
      "epoch: 5 step: 1466, loss is 0.0004542230162769556\n",
      "epoch: 5 step: 1467, loss is 0.0014964177971705794\n",
      "epoch: 5 step: 1468, loss is 0.002889376599341631\n",
      "epoch: 5 step: 1469, loss is 0.014592841267585754\n",
      "epoch: 5 step: 1470, loss is 0.0004939408972859383\n",
      "epoch: 5 step: 1471, loss is 0.10562574863433838\n",
      "epoch: 5 step: 1472, loss is 0.004943133797496557\n",
      "epoch: 5 step: 1473, loss is 0.4730859398841858\n",
      "epoch: 5 step: 1474, loss is 0.061798591166734695\n",
      "epoch: 5 step: 1475, loss is 0.008696185424923897\n",
      "epoch: 5 step: 1476, loss is 0.002233632141724229\n",
      "epoch: 5 step: 1477, loss is 0.012589890509843826\n",
      "epoch: 5 step: 1478, loss is 0.016347285360097885\n",
      "epoch: 5 step: 1479, loss is 0.004098394885659218\n",
      "epoch: 5 step: 1480, loss is 0.005234044510871172\n",
      "epoch: 5 step: 1481, loss is 0.021598894149065018\n",
      "epoch: 5 step: 1482, loss is 0.01697448641061783\n",
      "epoch: 5 step: 1483, loss is 0.0032727832440286875\n",
      "epoch: 5 step: 1484, loss is 0.015482279472053051\n",
      "epoch: 5 step: 1485, loss is 0.0029660596046596766\n",
      "epoch: 5 step: 1486, loss is 0.004531324841082096\n",
      "epoch: 5 step: 1487, loss is 0.009855682030320168\n",
      "epoch: 5 step: 1488, loss is 0.03452219441533089\n",
      "epoch: 5 step: 1489, loss is 0.017608126625418663\n",
      "epoch: 5 step: 1490, loss is 0.003099810564890504\n",
      "epoch: 5 step: 1491, loss is 0.05778393894433975\n",
      "epoch: 5 step: 1492, loss is 0.000514404964633286\n",
      "epoch: 5 step: 1493, loss is 0.006068689748644829\n",
      "epoch: 5 step: 1494, loss is 0.02869429439306259\n",
      "epoch: 5 step: 1495, loss is 0.015841059386730194\n",
      "epoch: 5 step: 1496, loss is 0.000920321443118155\n",
      "epoch: 5 step: 1497, loss is 0.02764083258807659\n",
      "epoch: 5 step: 1498, loss is 0.007785354740917683\n",
      "epoch: 5 step: 1499, loss is 0.010561471804976463\n",
      "epoch: 5 step: 1500, loss is 0.0023929751478135586\n",
      "epoch: 5 step: 1501, loss is 0.01129404827952385\n",
      "epoch: 5 step: 1502, loss is 0.0010088224662467837\n",
      "epoch: 5 step: 1503, loss is 0.0011484137503430247\n",
      "epoch: 5 step: 1504, loss is 0.0009646932012401521\n",
      "epoch: 5 step: 1505, loss is 0.00640858942642808\n",
      "epoch: 5 step: 1506, loss is 0.031083177775144577\n",
      "epoch: 5 step: 1507, loss is 0.00593084841966629\n",
      "epoch: 5 step: 1508, loss is 0.0036410405300557613\n",
      "epoch: 5 step: 1509, loss is 0.04677819088101387\n",
      "epoch: 5 step: 1510, loss is 0.0020959521643817425\n",
      "epoch: 5 step: 1511, loss is 0.006400401704013348\n",
      "epoch: 5 step: 1512, loss is 0.0637228786945343\n",
      "epoch: 5 step: 1513, loss is 0.018489724025130272\n",
      "epoch: 5 step: 1514, loss is 0.00602538650855422\n",
      "epoch: 5 step: 1515, loss is 0.054557472467422485\n",
      "epoch: 5 step: 1516, loss is 0.00010250236664433032\n",
      "epoch: 5 step: 1517, loss is 0.21411161124706268\n",
      "epoch: 5 step: 1518, loss is 0.0019591066520661116\n",
      "epoch: 5 step: 1519, loss is 0.018711496144533157\n",
      "epoch: 5 step: 1520, loss is 0.0018084452021867037\n",
      "epoch: 5 step: 1521, loss is 0.04907190427184105\n",
      "epoch: 5 step: 1522, loss is 0.0007350147352553904\n",
      "epoch: 5 step: 1523, loss is 0.18406566977500916\n",
      "epoch: 5 step: 1524, loss is 0.041510503739118576\n",
      "epoch: 5 step: 1525, loss is 0.1756635159254074\n",
      "epoch: 5 step: 1526, loss is 0.00048098250408656895\n",
      "epoch: 5 step: 1527, loss is 0.0030947341583669186\n",
      "epoch: 5 step: 1528, loss is 0.0015773550840094686\n",
      "epoch: 5 step: 1529, loss is 0.009621110744774342\n",
      "epoch: 5 step: 1530, loss is 0.04377244412899017\n",
      "epoch: 5 step: 1531, loss is 0.04475875198841095\n",
      "epoch: 5 step: 1532, loss is 0.0011443548137322068\n",
      "epoch: 5 step: 1533, loss is 0.0277804434299469\n",
      "epoch: 5 step: 1534, loss is 0.08842577040195465\n",
      "epoch: 5 step: 1535, loss is 0.09125476330518723\n",
      "epoch: 5 step: 1536, loss is 0.005233332514762878\n",
      "epoch: 5 step: 1537, loss is 0.04087406024336815\n",
      "epoch: 5 step: 1538, loss is 0.001249909633770585\n",
      "epoch: 5 step: 1539, loss is 0.0012409089831635356\n",
      "epoch: 5 step: 1540, loss is 0.01959286816418171\n",
      "epoch: 5 step: 1541, loss is 0.0012709629954770207\n",
      "epoch: 5 step: 1542, loss is 0.03419101610779762\n",
      "epoch: 5 step: 1543, loss is 0.00048634351696819067\n",
      "epoch: 5 step: 1544, loss is 0.04779130220413208\n",
      "epoch: 5 step: 1545, loss is 0.0017072157934308052\n",
      "epoch: 5 step: 1546, loss is 0.003490871051326394\n",
      "epoch: 5 step: 1547, loss is 0.0021963028702884912\n",
      "epoch: 5 step: 1548, loss is 0.0007153385085985065\n",
      "epoch: 5 step: 1549, loss is 0.048454031348228455\n",
      "epoch: 5 step: 1550, loss is 0.001402466674335301\n",
      "epoch: 5 step: 1551, loss is 0.04043199494481087\n",
      "epoch: 5 step: 1552, loss is 0.06473935395479202\n",
      "epoch: 5 step: 1553, loss is 0.005388609599322081\n",
      "epoch: 5 step: 1554, loss is 0.013856852427124977\n",
      "epoch: 5 step: 1555, loss is 0.0014573974767699838\n",
      "epoch: 5 step: 1556, loss is 0.0012730240123346448\n",
      "epoch: 5 step: 1557, loss is 0.004198738839477301\n",
      "epoch: 5 step: 1558, loss is 0.0039102546870708466\n",
      "epoch: 5 step: 1559, loss is 0.0007181403343565762\n",
      "epoch: 5 step: 1560, loss is 0.12429514527320862\n",
      "epoch: 5 step: 1561, loss is 0.00806448981165886\n",
      "epoch: 5 step: 1562, loss is 0.020909693092107773\n",
      "epoch: 5 step: 1563, loss is 0.007284743711352348\n",
      "epoch: 5 step: 1564, loss is 0.0010101533262059093\n",
      "epoch: 5 step: 1565, loss is 0.01445567887276411\n",
      "epoch: 5 step: 1566, loss is 0.0013182577677071095\n",
      "epoch: 5 step: 1567, loss is 0.019477631896734238\n",
      "epoch: 5 step: 1568, loss is 0.010726424865424633\n",
      "epoch: 5 step: 1569, loss is 0.0011449843877926469\n",
      "epoch: 5 step: 1570, loss is 0.00033451334456913173\n",
      "epoch: 5 step: 1571, loss is 0.0015053723473101854\n",
      "epoch: 5 step: 1572, loss is 0.00673958333209157\n",
      "epoch: 5 step: 1573, loss is 0.0056287371553480625\n",
      "epoch: 5 step: 1574, loss is 0.01931404136121273\n",
      "epoch: 5 step: 1575, loss is 0.006877618841826916\n",
      "epoch: 5 step: 1576, loss is 0.001073013641871512\n",
      "epoch: 5 step: 1577, loss is 0.0013665090082213283\n",
      "epoch: 5 step: 1578, loss is 0.09285789728164673\n",
      "epoch: 5 step: 1579, loss is 0.0009837541729211807\n",
      "epoch: 5 step: 1580, loss is 0.017396777868270874\n",
      "epoch: 5 step: 1581, loss is 0.021072668954730034\n",
      "epoch: 5 step: 1582, loss is 0.018627077341079712\n",
      "epoch: 5 step: 1583, loss is 0.10359455645084381\n",
      "epoch: 5 step: 1584, loss is 0.004143456928431988\n",
      "epoch: 5 step: 1585, loss is 0.002871112897992134\n",
      "epoch: 5 step: 1586, loss is 0.010803132317960262\n",
      "epoch: 5 step: 1587, loss is 0.0044972714968025684\n",
      "epoch: 5 step: 1588, loss is 0.01440755371004343\n",
      "epoch: 5 step: 1589, loss is 0.005224956665188074\n",
      "epoch: 5 step: 1590, loss is 0.14606693387031555\n",
      "epoch: 5 step: 1591, loss is 0.0012532109394669533\n",
      "epoch: 5 step: 1592, loss is 0.005756195168942213\n",
      "epoch: 5 step: 1593, loss is 0.0121745765209198\n",
      "epoch: 5 step: 1594, loss is 0.00793405994772911\n",
      "epoch: 5 step: 1595, loss is 0.0028009824454784393\n",
      "epoch: 5 step: 1596, loss is 0.02108101360499859\n",
      "epoch: 5 step: 1597, loss is 0.07868567109107971\n",
      "epoch: 5 step: 1598, loss is 0.034495141357183456\n",
      "epoch: 5 step: 1599, loss is 0.15637333691120148\n",
      "epoch: 5 step: 1600, loss is 0.00039279472548514605\n",
      "epoch: 5 step: 1601, loss is 0.0004861968045588583\n",
      "epoch: 5 step: 1602, loss is 0.01882188580930233\n",
      "epoch: 5 step: 1603, loss is 0.00010717009718064219\n",
      "epoch: 5 step: 1604, loss is 0.004163605161011219\n",
      "epoch: 5 step: 1605, loss is 0.012109027244150639\n",
      "epoch: 5 step: 1606, loss is 0.00027426358428783715\n",
      "epoch: 5 step: 1607, loss is 0.03849422186613083\n",
      "epoch: 5 step: 1608, loss is 0.001994781894609332\n",
      "epoch: 5 step: 1609, loss is 0.07162223011255264\n",
      "epoch: 5 step: 1610, loss is 0.007174741476774216\n",
      "epoch: 5 step: 1611, loss is 0.0030451833736151457\n",
      "epoch: 5 step: 1612, loss is 0.029024269431829453\n",
      "epoch: 5 step: 1613, loss is 0.08229465782642365\n",
      "epoch: 5 step: 1614, loss is 0.01865306869149208\n",
      "epoch: 5 step: 1615, loss is 0.02042272686958313\n",
      "epoch: 5 step: 1616, loss is 0.0011896212818101048\n",
      "epoch: 5 step: 1617, loss is 0.06533518433570862\n",
      "epoch: 5 step: 1618, loss is 0.08250027894973755\n",
      "epoch: 5 step: 1619, loss is 0.0012550525134429336\n",
      "epoch: 5 step: 1620, loss is 0.006473153829574585\n",
      "epoch: 5 step: 1621, loss is 0.027938466519117355\n",
      "epoch: 5 step: 1622, loss is 0.06041374430060387\n",
      "epoch: 5 step: 1623, loss is 0.007472169119864702\n",
      "epoch: 5 step: 1624, loss is 0.00035718700382858515\n",
      "epoch: 5 step: 1625, loss is 0.026177801191806793\n",
      "epoch: 5 step: 1626, loss is 0.003596098627895117\n",
      "epoch: 5 step: 1627, loss is 0.10037451982498169\n",
      "epoch: 5 step: 1628, loss is 0.0002449958701618016\n",
      "epoch: 5 step: 1629, loss is 0.12584151327610016\n",
      "epoch: 5 step: 1630, loss is 0.09274736046791077\n",
      "epoch: 5 step: 1631, loss is 0.04297001659870148\n",
      "epoch: 5 step: 1632, loss is 0.15105335414409637\n",
      "epoch: 5 step: 1633, loss is 0.04587647318840027\n",
      "epoch: 5 step: 1634, loss is 0.0013749460922554135\n",
      "epoch: 5 step: 1635, loss is 0.029738664627075195\n",
      "epoch: 5 step: 1636, loss is 0.0546274334192276\n",
      "epoch: 5 step: 1637, loss is 0.00037679984234273434\n",
      "epoch: 5 step: 1638, loss is 0.0008499798714183271\n",
      "epoch: 5 step: 1639, loss is 0.00025842804461717606\n",
      "epoch: 5 step: 1640, loss is 0.016441449522972107\n",
      "epoch: 5 step: 1641, loss is 0.18648210167884827\n",
      "epoch: 5 step: 1642, loss is 0.0023923292756080627\n",
      "epoch: 5 step: 1643, loss is 0.028045201674103737\n",
      "epoch: 5 step: 1644, loss is 0.002326688962057233\n",
      "epoch: 5 step: 1645, loss is 0.0702093094587326\n",
      "epoch: 5 step: 1646, loss is 0.035924963653087616\n",
      "epoch: 5 step: 1647, loss is 0.014546740800142288\n",
      "epoch: 5 step: 1648, loss is 0.02345903590321541\n",
      "epoch: 5 step: 1649, loss is 0.07000963389873505\n",
      "epoch: 5 step: 1650, loss is 0.0006505770143121481\n",
      "epoch: 5 step: 1651, loss is 0.028688333928585052\n",
      "epoch: 5 step: 1652, loss is 0.0007844553329050541\n",
      "epoch: 5 step: 1653, loss is 0.050128065049648285\n",
      "epoch: 5 step: 1654, loss is 0.0008223889744840562\n",
      "epoch: 5 step: 1655, loss is 0.039513375610113144\n",
      "epoch: 5 step: 1656, loss is 0.1257055103778839\n",
      "epoch: 5 step: 1657, loss is 0.0035885523539036512\n",
      "epoch: 5 step: 1658, loss is 0.056779030710458755\n",
      "epoch: 5 step: 1659, loss is 0.015510227531194687\n",
      "epoch: 5 step: 1660, loss is 0.0005591828376054764\n",
      "epoch: 5 step: 1661, loss is 0.0011163046583533287\n",
      "epoch: 5 step: 1662, loss is 0.015667779371142387\n",
      "epoch: 5 step: 1663, loss is 0.0013561738887801766\n",
      "epoch: 5 step: 1664, loss is 0.0022843200713396072\n",
      "epoch: 5 step: 1665, loss is 0.009332829155027866\n",
      "epoch: 5 step: 1666, loss is 0.0028516396414488554\n",
      "epoch: 5 step: 1667, loss is 0.004239588975906372\n",
      "epoch: 5 step: 1668, loss is 0.002526995725929737\n",
      "epoch: 5 step: 1669, loss is 0.0016512994188815355\n",
      "epoch: 5 step: 1670, loss is 0.00040106294909492135\n",
      "epoch: 5 step: 1671, loss is 0.044829100370407104\n",
      "epoch: 5 step: 1672, loss is 0.0011924335267394781\n",
      "epoch: 5 step: 1673, loss is 0.004832645412534475\n",
      "epoch: 5 step: 1674, loss is 0.0009870409267023206\n",
      "epoch: 5 step: 1675, loss is 0.007062804419547319\n",
      "epoch: 5 step: 1676, loss is 0.0024365924764424562\n",
      "epoch: 5 step: 1677, loss is 0.011943697929382324\n",
      "epoch: 5 step: 1678, loss is 0.0011594840325415134\n",
      "epoch: 5 step: 1679, loss is 0.014379513449966908\n",
      "epoch: 5 step: 1680, loss is 0.0049584233202040195\n",
      "epoch: 5 step: 1681, loss is 0.03849625214934349\n",
      "epoch: 5 step: 1682, loss is 0.06441735476255417\n",
      "epoch: 5 step: 1683, loss is 0.006793243810534477\n",
      "epoch: 5 step: 1684, loss is 0.0024857106618583202\n",
      "epoch: 5 step: 1685, loss is 0.0052742986008524895\n",
      "epoch: 5 step: 1686, loss is 0.023742927238345146\n",
      "epoch: 5 step: 1687, loss is 0.0006107923691160977\n",
      "epoch: 5 step: 1688, loss is 0.005188428331166506\n",
      "epoch: 5 step: 1689, loss is 0.0006089780363254249\n",
      "epoch: 5 step: 1690, loss is 0.021235188469290733\n",
      "epoch: 5 step: 1691, loss is 0.11554727703332901\n",
      "epoch: 5 step: 1692, loss is 0.0010715837124735117\n",
      "epoch: 5 step: 1693, loss is 0.06183932349085808\n",
      "epoch: 5 step: 1694, loss is 0.009308181703090668\n",
      "epoch: 5 step: 1695, loss is 0.0030861853156238794\n",
      "epoch: 5 step: 1696, loss is 0.024198276922106743\n",
      "epoch: 5 step: 1697, loss is 0.0005669941892847419\n",
      "epoch: 5 step: 1698, loss is 0.011711956933140755\n",
      "epoch: 5 step: 1699, loss is 0.3338148295879364\n",
      "epoch: 5 step: 1700, loss is 0.00045015811338089406\n",
      "epoch: 5 step: 1701, loss is 0.012153944000601768\n",
      "epoch: 5 step: 1702, loss is 0.0013856677105650306\n",
      "epoch: 5 step: 1703, loss is 0.01863180287182331\n",
      "epoch: 5 step: 1704, loss is 0.0026873708702623844\n",
      "epoch: 5 step: 1705, loss is 0.0006088184891268611\n",
      "epoch: 5 step: 1706, loss is 0.0012344667920842767\n",
      "epoch: 5 step: 1707, loss is 0.0008972360519692302\n",
      "epoch: 5 step: 1708, loss is 0.24466872215270996\n",
      "epoch: 5 step: 1709, loss is 0.0006294269114732742\n",
      "epoch: 5 step: 1710, loss is 0.07967349886894226\n",
      "epoch: 5 step: 1711, loss is 0.3329702317714691\n",
      "epoch: 5 step: 1712, loss is 0.0014680916210636497\n",
      "epoch: 5 step: 1713, loss is 0.14295820891857147\n",
      "epoch: 5 step: 1714, loss is 0.009146318770945072\n",
      "epoch: 5 step: 1715, loss is 0.0042777168564498425\n",
      "epoch: 5 step: 1716, loss is 0.0011082225246354938\n",
      "epoch: 5 step: 1717, loss is 0.0850374698638916\n",
      "epoch: 5 step: 1718, loss is 0.0024069829378277063\n",
      "epoch: 5 step: 1719, loss is 0.014251429587602615\n",
      "epoch: 5 step: 1720, loss is 0.025832073763012886\n",
      "epoch: 5 step: 1721, loss is 0.004482969641685486\n",
      "epoch: 5 step: 1722, loss is 0.008316799998283386\n",
      "epoch: 5 step: 1723, loss is 0.022847268730401993\n",
      "epoch: 5 step: 1724, loss is 0.0011094713117927313\n",
      "epoch: 5 step: 1725, loss is 0.036328475922346115\n",
      "epoch: 5 step: 1726, loss is 0.012077371589839458\n",
      "epoch: 5 step: 1727, loss is 0.008944198489189148\n",
      "epoch: 5 step: 1728, loss is 0.008291353471577168\n",
      "epoch: 5 step: 1729, loss is 0.007485642097890377\n",
      "epoch: 5 step: 1730, loss is 0.008576602675020695\n",
      "epoch: 5 step: 1731, loss is 0.025991128757596016\n",
      "epoch: 5 step: 1732, loss is 0.05160117894411087\n",
      "epoch: 5 step: 1733, loss is 0.11209551990032196\n",
      "epoch: 5 step: 1734, loss is 0.016634587198495865\n",
      "epoch: 5 step: 1735, loss is 0.004628785885870457\n",
      "epoch: 5 step: 1736, loss is 0.0042485701851546764\n",
      "epoch: 5 step: 1737, loss is 0.0017088582972064614\n",
      "epoch: 5 step: 1738, loss is 0.0018551781540736556\n",
      "epoch: 5 step: 1739, loss is 0.01605292782187462\n",
      "epoch: 5 step: 1740, loss is 0.0011952768545597792\n",
      "epoch: 5 step: 1741, loss is 0.0005392952589318156\n",
      "epoch: 5 step: 1742, loss is 0.022501718252897263\n",
      "epoch: 5 step: 1743, loss is 0.02176724560558796\n",
      "epoch: 5 step: 1744, loss is 0.0015210281126201153\n",
      "epoch: 5 step: 1745, loss is 0.004473110660910606\n",
      "epoch: 5 step: 1746, loss is 0.0036901591811329126\n",
      "epoch: 5 step: 1747, loss is 0.018069544807076454\n",
      "epoch: 5 step: 1748, loss is 0.07105123996734619\n",
      "epoch: 5 step: 1749, loss is 0.011385329067707062\n",
      "epoch: 5 step: 1750, loss is 0.003964683506637812\n",
      "epoch: 5 step: 1751, loss is 0.029055511578917503\n",
      "epoch: 5 step: 1752, loss is 0.0056030284613370895\n",
      "epoch: 5 step: 1753, loss is 0.0007562108803540468\n",
      "epoch: 5 step: 1754, loss is 0.24131612479686737\n",
      "epoch: 5 step: 1755, loss is 0.07616166770458221\n",
      "epoch: 5 step: 1756, loss is 0.0002658254816196859\n",
      "epoch: 5 step: 1757, loss is 0.08664795011281967\n",
      "epoch: 5 step: 1758, loss is 0.004387656692415476\n",
      "epoch: 5 step: 1759, loss is 0.026511438190937042\n",
      "epoch: 5 step: 1760, loss is 0.0025468680541962385\n",
      "epoch: 5 step: 1761, loss is 0.0010806992650032043\n",
      "epoch: 5 step: 1762, loss is 0.00483603123575449\n",
      "epoch: 5 step: 1763, loss is 0.006457579787820578\n",
      "epoch: 5 step: 1764, loss is 0.0005678029265254736\n",
      "epoch: 5 step: 1765, loss is 0.002540027257055044\n",
      "epoch: 5 step: 1766, loss is 0.0679343044757843\n",
      "epoch: 5 step: 1767, loss is 0.004689250607043505\n",
      "epoch: 5 step: 1768, loss is 0.014179108664393425\n",
      "epoch: 5 step: 1769, loss is 0.0006252270541153848\n",
      "epoch: 5 step: 1770, loss is 0.0036473264917731285\n",
      "epoch: 5 step: 1771, loss is 0.007303969003260136\n",
      "epoch: 5 step: 1772, loss is 0.0015453347004950047\n",
      "epoch: 5 step: 1773, loss is 0.022431155666708946\n",
      "epoch: 5 step: 1774, loss is 0.20280911028385162\n",
      "epoch: 5 step: 1775, loss is 0.0006914688856340945\n",
      "epoch: 5 step: 1776, loss is 0.02014005184173584\n",
      "epoch: 5 step: 1777, loss is 0.013311542570590973\n",
      "epoch: 5 step: 1778, loss is 0.008912518620491028\n",
      "epoch: 5 step: 1779, loss is 0.020292572677135468\n",
      "epoch: 5 step: 1780, loss is 0.013530710712075233\n",
      "epoch: 5 step: 1781, loss is 0.011063378304243088\n",
      "epoch: 5 step: 1782, loss is 0.17843520641326904\n",
      "epoch: 5 step: 1783, loss is 0.0015682113589718938\n",
      "epoch: 5 step: 1784, loss is 0.026902422308921814\n",
      "epoch: 5 step: 1785, loss is 0.0041981651447713375\n",
      "epoch: 5 step: 1786, loss is 0.023054329678416252\n",
      "epoch: 5 step: 1787, loss is 0.04716921225190163\n",
      "epoch: 5 step: 1788, loss is 0.015564522705972195\n",
      "epoch: 5 step: 1789, loss is 0.02133588306605816\n",
      "epoch: 5 step: 1790, loss is 0.20393304526805878\n",
      "epoch: 5 step: 1791, loss is 0.012243998236954212\n",
      "epoch: 5 step: 1792, loss is 0.010880601592361927\n",
      "epoch: 5 step: 1793, loss is 0.0007369969971477985\n",
      "epoch: 5 step: 1794, loss is 0.0012874568346887827\n",
      "epoch: 5 step: 1795, loss is 0.025556575506925583\n",
      "epoch: 5 step: 1796, loss is 0.018420778214931488\n",
      "epoch: 5 step: 1797, loss is 0.014039863832294941\n",
      "epoch: 5 step: 1798, loss is 0.03845260664820671\n",
      "epoch: 5 step: 1799, loss is 0.009098382666707039\n",
      "epoch: 5 step: 1800, loss is 0.012624302878975868\n",
      "epoch: 5 step: 1801, loss is 0.008942265063524246\n",
      "epoch: 5 step: 1802, loss is 0.00997591856867075\n",
      "epoch: 5 step: 1803, loss is 0.023806340992450714\n",
      "epoch: 5 step: 1804, loss is 0.012436088174581528\n",
      "epoch: 5 step: 1805, loss is 0.01225503534078598\n",
      "epoch: 5 step: 1806, loss is 0.0014184067258611321\n",
      "epoch: 5 step: 1807, loss is 0.014364184811711311\n",
      "epoch: 5 step: 1808, loss is 0.2055489867925644\n",
      "epoch: 5 step: 1809, loss is 0.0004994238843210042\n",
      "epoch: 5 step: 1810, loss is 0.005141366273164749\n",
      "epoch: 5 step: 1811, loss is 0.0011175990803167224\n",
      "epoch: 5 step: 1812, loss is 0.004913042299449444\n",
      "epoch: 5 step: 1813, loss is 0.0011403969256207347\n",
      "epoch: 5 step: 1814, loss is 0.0007666984456591308\n",
      "epoch: 5 step: 1815, loss is 0.00019387229986023158\n",
      "epoch: 5 step: 1816, loss is 0.006174806039780378\n",
      "epoch: 5 step: 1817, loss is 0.017280669882893562\n",
      "epoch: 5 step: 1818, loss is 0.001036696252413094\n",
      "epoch: 5 step: 1819, loss is 0.04218116030097008\n",
      "epoch: 5 step: 1820, loss is 0.00020838304772041738\n",
      "epoch: 5 step: 1821, loss is 0.001953333616256714\n",
      "epoch: 5 step: 1822, loss is 0.07505065202713013\n",
      "epoch: 5 step: 1823, loss is 0.013194815255701542\n",
      "epoch: 5 step: 1824, loss is 0.059807002544403076\n",
      "epoch: 5 step: 1825, loss is 0.08945099264383316\n",
      "epoch: 5 step: 1826, loss is 0.0657593309879303\n",
      "epoch: 5 step: 1827, loss is 0.007985953241586685\n",
      "epoch: 5 step: 1828, loss is 0.0009823533473536372\n",
      "epoch: 5 step: 1829, loss is 0.00870758481323719\n",
      "epoch: 5 step: 1830, loss is 0.003927111625671387\n",
      "epoch: 5 step: 1831, loss is 0.0059094512835145\n",
      "epoch: 5 step: 1832, loss is 0.0028791851364076138\n",
      "epoch: 5 step: 1833, loss is 0.0017834987957030535\n",
      "epoch: 5 step: 1834, loss is 0.0035988339222967625\n",
      "epoch: 5 step: 1835, loss is 0.03433690965175629\n",
      "epoch: 5 step: 1836, loss is 0.016773780807852745\n",
      "epoch: 5 step: 1837, loss is 0.028386320918798447\n",
      "epoch: 5 step: 1838, loss is 0.002950655994936824\n",
      "epoch: 5 step: 1839, loss is 0.001745779998600483\n",
      "epoch: 5 step: 1840, loss is 0.1109895333647728\n",
      "epoch: 5 step: 1841, loss is 0.02181951142847538\n",
      "epoch: 5 step: 1842, loss is 0.004123570863157511\n",
      "epoch: 5 step: 1843, loss is 0.022125761955976486\n",
      "epoch: 5 step: 1844, loss is 0.14794468879699707\n",
      "epoch: 5 step: 1845, loss is 0.018558768555521965\n",
      "epoch: 5 step: 1846, loss is 0.0046021779999136925\n",
      "epoch: 5 step: 1847, loss is 0.04888921231031418\n",
      "epoch: 5 step: 1848, loss is 0.009545237757265568\n",
      "epoch: 5 step: 1849, loss is 0.005343753844499588\n",
      "epoch: 5 step: 1850, loss is 0.043276943266391754\n",
      "epoch: 5 step: 1851, loss is 0.003465726738795638\n",
      "epoch: 5 step: 1852, loss is 0.05494596064090729\n",
      "epoch: 5 step: 1853, loss is 0.003888373728841543\n",
      "epoch: 5 step: 1854, loss is 0.008751408196985722\n",
      "epoch: 5 step: 1855, loss is 0.001922075287438929\n",
      "epoch: 5 step: 1856, loss is 0.00042465978185646236\n",
      "epoch: 5 step: 1857, loss is 0.019029665738344193\n",
      "epoch: 5 step: 1858, loss is 0.0454692505300045\n",
      "epoch: 5 step: 1859, loss is 0.0014327344251796603\n",
      "epoch: 5 step: 1860, loss is 0.0030254414305090904\n",
      "epoch: 5 step: 1861, loss is 3.42510684276931e-05\n",
      "epoch: 5 step: 1862, loss is 0.039738014340400696\n",
      "epoch: 5 step: 1863, loss is 0.0004516995686572045\n",
      "epoch: 5 step: 1864, loss is 0.000895079632755369\n",
      "epoch: 5 step: 1865, loss is 0.20046916604042053\n",
      "epoch: 5 step: 1866, loss is 0.032309092581272125\n",
      "epoch: 5 step: 1867, loss is 0.00028387215570546687\n",
      "epoch: 5 step: 1868, loss is 0.055532101541757584\n",
      "epoch: 5 step: 1869, loss is 0.01313742808997631\n",
      "epoch: 5 step: 1870, loss is 0.030101893469691277\n",
      "epoch: 5 step: 1871, loss is 0.013981563039124012\n",
      "epoch: 5 step: 1872, loss is 0.000510475249029696\n",
      "epoch: 5 step: 1873, loss is 0.046804726123809814\n",
      "epoch: 5 step: 1874, loss is 0.011209430173039436\n",
      "epoch: 5 step: 1875, loss is 0.07797369360923767\n",
      "Train epoch time: 11499.472 ms, per step time: 6.133 ms\n",
      "epoch: 6 step: 1, loss is 0.011745008639991283\n",
      "epoch: 6 step: 2, loss is 5.68038594792597e-05\n",
      "epoch: 6 step: 3, loss is 0.0011243405751883984\n",
      "epoch: 6 step: 4, loss is 0.057214271277189255\n",
      "epoch: 6 step: 5, loss is 0.03171360120177269\n",
      "epoch: 6 step: 6, loss is 0.016000889241695404\n",
      "epoch: 6 step: 7, loss is 0.10615851730108261\n",
      "epoch: 6 step: 8, loss is 0.017789408564567566\n",
      "epoch: 6 step: 9, loss is 0.02624659053981304\n",
      "epoch: 6 step: 10, loss is 0.010416390374302864\n",
      "epoch: 6 step: 11, loss is 0.007992279715836048\n",
      "epoch: 6 step: 12, loss is 0.012528511695563793\n",
      "epoch: 6 step: 13, loss is 0.008487328886985779\n",
      "epoch: 6 step: 14, loss is 0.01167506817728281\n",
      "epoch: 6 step: 15, loss is 0.09797825664281845\n",
      "epoch: 6 step: 16, loss is 0.024292893707752228\n",
      "epoch: 6 step: 17, loss is 0.021845953539013863\n",
      "epoch: 6 step: 18, loss is 0.05661020055413246\n",
      "epoch: 6 step: 19, loss is 0.09545028954744339\n",
      "epoch: 6 step: 20, loss is 0.00025180220836773515\n",
      "epoch: 6 step: 21, loss is 0.1096171960234642\n",
      "epoch: 6 step: 22, loss is 0.09516792744398117\n",
      "epoch: 6 step: 23, loss is 0.026799188926815987\n",
      "epoch: 6 step: 24, loss is 0.00018257126794196665\n",
      "epoch: 6 step: 25, loss is 0.001516219461336732\n",
      "epoch: 6 step: 26, loss is 0.0006774537032470107\n",
      "epoch: 6 step: 27, loss is 0.019742468371987343\n",
      "epoch: 6 step: 28, loss is 0.0005057498347014189\n",
      "epoch: 6 step: 29, loss is 0.009392504580318928\n",
      "epoch: 6 step: 30, loss is 0.0019386813510209322\n",
      "epoch: 6 step: 31, loss is 0.0002627385256346315\n",
      "epoch: 6 step: 32, loss is 0.0009331107139587402\n",
      "epoch: 6 step: 33, loss is 0.11226513236761093\n",
      "epoch: 6 step: 34, loss is 6.153491995064542e-05\n",
      "epoch: 6 step: 35, loss is 0.11207728832960129\n",
      "epoch: 6 step: 36, loss is 0.003985241055488586\n",
      "epoch: 6 step: 37, loss is 0.018235841765999794\n",
      "epoch: 6 step: 38, loss is 0.0005995493265800178\n",
      "epoch: 6 step: 39, loss is 0.008142100647091866\n",
      "epoch: 6 step: 40, loss is 0.0051687839441001415\n",
      "epoch: 6 step: 41, loss is 0.0025038032326847315\n",
      "epoch: 6 step: 42, loss is 0.006456830073148012\n",
      "epoch: 6 step: 43, loss is 0.026569562032818794\n",
      "epoch: 6 step: 44, loss is 0.0017287032678723335\n",
      "epoch: 6 step: 45, loss is 0.005363724194467068\n",
      "epoch: 6 step: 46, loss is 0.0020321232732385397\n",
      "epoch: 6 step: 47, loss is 0.031028881669044495\n",
      "epoch: 6 step: 48, loss is 0.040667615830898285\n",
      "epoch: 6 step: 49, loss is 0.0034524810034781694\n",
      "epoch: 6 step: 50, loss is 0.0036849589087069035\n",
      "epoch: 6 step: 51, loss is 0.008827748708426952\n",
      "epoch: 6 step: 52, loss is 0.008144339546561241\n",
      "epoch: 6 step: 53, loss is 0.00024782659602351487\n",
      "epoch: 6 step: 54, loss is 0.013931717723608017\n",
      "epoch: 6 step: 55, loss is 0.000907604640815407\n",
      "epoch: 6 step: 56, loss is 0.09129494428634644\n",
      "epoch: 6 step: 57, loss is 0.001497133169323206\n",
      "epoch: 6 step: 58, loss is 0.00011053014895878732\n",
      "epoch: 6 step: 59, loss is 0.001820700359530747\n",
      "epoch: 6 step: 60, loss is 0.01194088626652956\n",
      "epoch: 6 step: 61, loss is 0.01195294689387083\n",
      "epoch: 6 step: 62, loss is 0.00034731635241769254\n",
      "epoch: 6 step: 63, loss is 8.167220948962495e-05\n",
      "epoch: 6 step: 64, loss is 5.322316428646445e-05\n",
      "epoch: 6 step: 65, loss is 0.00010367847426095977\n",
      "epoch: 6 step: 66, loss is 0.008425208739936352\n",
      "epoch: 6 step: 67, loss is 0.00035909682628698647\n",
      "epoch: 6 step: 68, loss is 0.019709009677171707\n",
      "epoch: 6 step: 69, loss is 0.006049355026334524\n",
      "epoch: 6 step: 70, loss is 0.002424159087240696\n",
      "epoch: 6 step: 71, loss is 0.013456062413752079\n",
      "epoch: 6 step: 72, loss is 0.0005202085594646633\n",
      "epoch: 6 step: 73, loss is 0.00023860318469814956\n",
      "epoch: 6 step: 74, loss is 0.0001293696986977011\n",
      "epoch: 6 step: 75, loss is 0.01355314627289772\n",
      "epoch: 6 step: 76, loss is 0.00022149464348331094\n",
      "epoch: 6 step: 77, loss is 0.0003860209253616631\n",
      "epoch: 6 step: 78, loss is 0.14974673092365265\n",
      "epoch: 6 step: 79, loss is 0.0026164078153669834\n",
      "epoch: 6 step: 80, loss is 0.00016008839884307235\n",
      "epoch: 6 step: 81, loss is 0.009730264544487\n",
      "epoch: 6 step: 82, loss is 0.003097645938396454\n",
      "epoch: 6 step: 83, loss is 0.00043117799214087427\n",
      "epoch: 6 step: 84, loss is 0.0002400140365352854\n",
      "epoch: 6 step: 85, loss is 0.02046257071197033\n",
      "epoch: 6 step: 86, loss is 0.0012461247388273478\n",
      "epoch: 6 step: 87, loss is 0.02959289774298668\n",
      "epoch: 6 step: 88, loss is 0.0014179748250171542\n",
      "epoch: 6 step: 89, loss is 8.670453826198354e-05\n",
      "epoch: 6 step: 90, loss is 0.031788170337677\n",
      "epoch: 6 step: 91, loss is 0.009278186596930027\n",
      "epoch: 6 step: 92, loss is 0.07180666923522949\n",
      "epoch: 6 step: 93, loss is 0.009899778291583061\n",
      "epoch: 6 step: 94, loss is 0.0004000481858383864\n",
      "epoch: 6 step: 95, loss is 0.002658520359545946\n",
      "epoch: 6 step: 96, loss is 0.006114078685641289\n",
      "epoch: 6 step: 97, loss is 8.505240111844614e-05\n",
      "epoch: 6 step: 98, loss is 0.0027404858265072107\n",
      "epoch: 6 step: 99, loss is 0.08203195780515671\n",
      "epoch: 6 step: 100, loss is 0.16815976798534393\n",
      "epoch: 6 step: 101, loss is 0.000792378094047308\n",
      "epoch: 6 step: 102, loss is 0.0006324081332422793\n",
      "epoch: 6 step: 103, loss is 0.00307675008662045\n",
      "epoch: 6 step: 104, loss is 0.07949232310056686\n",
      "epoch: 6 step: 105, loss is 0.0020476775243878365\n",
      "epoch: 6 step: 106, loss is 0.042624980211257935\n",
      "epoch: 6 step: 107, loss is 0.00031626661075279117\n",
      "epoch: 6 step: 108, loss is 0.007193577475845814\n",
      "epoch: 6 step: 109, loss is 0.0005527566536329687\n",
      "epoch: 6 step: 110, loss is 0.0006533227860927582\n",
      "epoch: 6 step: 111, loss is 0.03718814626336098\n",
      "epoch: 6 step: 112, loss is 0.007720699068158865\n",
      "epoch: 6 step: 113, loss is 0.004164089914411306\n",
      "epoch: 6 step: 114, loss is 0.10757351666688919\n",
      "epoch: 6 step: 115, loss is 0.004566500429064035\n",
      "epoch: 6 step: 116, loss is 0.012294652871787548\n",
      "epoch: 6 step: 117, loss is 0.004995324648916721\n",
      "epoch: 6 step: 118, loss is 0.0006163987563923001\n",
      "epoch: 6 step: 119, loss is 0.021268432959914207\n",
      "epoch: 6 step: 120, loss is 0.06742070615291595\n",
      "epoch: 6 step: 121, loss is 0.02256406843662262\n",
      "epoch: 6 step: 122, loss is 0.005704034119844437\n",
      "epoch: 6 step: 123, loss is 0.1296805441379547\n",
      "epoch: 6 step: 124, loss is 0.2003481686115265\n",
      "epoch: 6 step: 125, loss is 0.1390615999698639\n",
      "epoch: 6 step: 126, loss is 0.010634835809469223\n",
      "epoch: 6 step: 127, loss is 0.0011262423358857632\n",
      "epoch: 6 step: 128, loss is 0.0008660937310196459\n",
      "epoch: 6 step: 129, loss is 0.001144473091699183\n",
      "epoch: 6 step: 130, loss is 0.005071008112281561\n",
      "epoch: 6 step: 131, loss is 0.14370377361774445\n",
      "epoch: 6 step: 132, loss is 0.0022570027504116297\n",
      "epoch: 6 step: 133, loss is 0.0032214082311838865\n",
      "epoch: 6 step: 134, loss is 0.0015304635744541883\n",
      "epoch: 6 step: 135, loss is 0.11466987431049347\n",
      "epoch: 6 step: 136, loss is 0.0008449823944829404\n",
      "epoch: 6 step: 137, loss is 0.1343105435371399\n",
      "epoch: 6 step: 138, loss is 0.0009661468793638051\n",
      "epoch: 6 step: 139, loss is 0.002372405491769314\n",
      "epoch: 6 step: 140, loss is 0.11453079432249069\n",
      "epoch: 6 step: 141, loss is 0.07419228553771973\n",
      "epoch: 6 step: 142, loss is 0.0019396477146074176\n",
      "epoch: 6 step: 143, loss is 0.0014938907697796822\n",
      "epoch: 6 step: 144, loss is 0.004540907219052315\n",
      "epoch: 6 step: 145, loss is 0.0015541112516075373\n",
      "epoch: 6 step: 146, loss is 0.0002065292646875605\n",
      "epoch: 6 step: 147, loss is 0.04053686559200287\n",
      "epoch: 6 step: 148, loss is 0.003183371853083372\n",
      "epoch: 6 step: 149, loss is 0.09642088413238525\n",
      "epoch: 6 step: 150, loss is 0.0025903331115841866\n",
      "epoch: 6 step: 151, loss is 0.0007158705848269165\n",
      "epoch: 6 step: 152, loss is 0.0011280912440270185\n",
      "epoch: 6 step: 153, loss is 0.004845369607210159\n",
      "epoch: 6 step: 154, loss is 0.02803478203713894\n",
      "epoch: 6 step: 155, loss is 0.0007733836537227035\n",
      "epoch: 6 step: 156, loss is 0.002246640156954527\n",
      "epoch: 6 step: 157, loss is 0.009177014231681824\n",
      "epoch: 6 step: 158, loss is 0.002058679237961769\n",
      "epoch: 6 step: 159, loss is 0.017521101981401443\n",
      "epoch: 6 step: 160, loss is 0.0042052436619997025\n",
      "epoch: 6 step: 161, loss is 0.0011183038586750627\n",
      "epoch: 6 step: 162, loss is 0.010508396662771702\n",
      "epoch: 6 step: 163, loss is 0.0245838463306427\n",
      "epoch: 6 step: 164, loss is 0.007515773642808199\n",
      "epoch: 6 step: 165, loss is 0.013220146298408508\n",
      "epoch: 6 step: 166, loss is 0.021037278696894646\n",
      "epoch: 6 step: 167, loss is 0.014043322764337063\n",
      "epoch: 6 step: 168, loss is 0.0021845451556146145\n",
      "epoch: 6 step: 169, loss is 0.012223299592733383\n",
      "epoch: 6 step: 170, loss is 0.19507074356079102\n",
      "epoch: 6 step: 171, loss is 0.0009694209438748658\n",
      "epoch: 6 step: 172, loss is 0.004181282129138708\n",
      "epoch: 6 step: 173, loss is 0.06510691344738007\n",
      "epoch: 6 step: 174, loss is 0.03938157111406326\n",
      "epoch: 6 step: 175, loss is 0.0013811559183523059\n",
      "epoch: 6 step: 176, loss is 0.16663995385169983\n",
      "epoch: 6 step: 177, loss is 0.02953128144145012\n",
      "epoch: 6 step: 178, loss is 0.019531836733222008\n",
      "epoch: 6 step: 179, loss is 0.003744759364053607\n",
      "epoch: 6 step: 180, loss is 0.04512786865234375\n",
      "epoch: 6 step: 181, loss is 0.002184146549552679\n",
      "epoch: 6 step: 182, loss is 0.004269513301551342\n",
      "epoch: 6 step: 183, loss is 0.010252686217427254\n",
      "epoch: 6 step: 184, loss is 0.08939415961503983\n",
      "epoch: 6 step: 185, loss is 0.0018536102725192904\n",
      "epoch: 6 step: 186, loss is 0.013682681135833263\n",
      "epoch: 6 step: 187, loss is 0.0017375426832586527\n",
      "epoch: 6 step: 188, loss is 0.0022860050667077303\n",
      "epoch: 6 step: 189, loss is 0.0007665355806238949\n",
      "epoch: 6 step: 190, loss is 0.004390350077301264\n",
      "epoch: 6 step: 191, loss is 0.012207775376737118\n",
      "epoch: 6 step: 192, loss is 0.005750945769250393\n",
      "epoch: 6 step: 193, loss is 0.014995928853750229\n",
      "epoch: 6 step: 194, loss is 0.007073345128446817\n",
      "epoch: 6 step: 195, loss is 0.0663701668381691\n",
      "epoch: 6 step: 196, loss is 0.0012541580945253372\n",
      "epoch: 6 step: 197, loss is 0.012526454403996468\n",
      "epoch: 6 step: 198, loss is 0.02043263241648674\n",
      "epoch: 6 step: 199, loss is 0.004082209896296263\n",
      "epoch: 6 step: 200, loss is 0.00024388660676777363\n",
      "epoch: 6 step: 201, loss is 0.011925555765628815\n",
      "epoch: 6 step: 202, loss is 0.027030713856220245\n",
      "epoch: 6 step: 203, loss is 0.021446291357278824\n",
      "epoch: 6 step: 204, loss is 0.030203768983483315\n",
      "epoch: 6 step: 205, loss is 0.00827485416084528\n",
      "epoch: 6 step: 206, loss is 0.005719323176890612\n",
      "epoch: 6 step: 207, loss is 0.03173263370990753\n",
      "epoch: 6 step: 208, loss is 0.005434912629425526\n",
      "epoch: 6 step: 209, loss is 0.06377726048231125\n",
      "epoch: 6 step: 210, loss is 0.011613906361162663\n",
      "epoch: 6 step: 211, loss is 0.06923394650220871\n",
      "epoch: 6 step: 212, loss is 0.07717136293649673\n",
      "epoch: 6 step: 213, loss is 0.01276821456849575\n",
      "epoch: 6 step: 214, loss is 0.001018643844872713\n",
      "epoch: 6 step: 215, loss is 0.10647676885128021\n",
      "epoch: 6 step: 216, loss is 0.00032349361572414637\n",
      "epoch: 6 step: 217, loss is 0.00487492186948657\n",
      "epoch: 6 step: 218, loss is 0.007261007092893124\n",
      "epoch: 6 step: 219, loss is 0.0024027121253311634\n",
      "epoch: 6 step: 220, loss is 0.0002487066958565265\n",
      "epoch: 6 step: 221, loss is 0.003225874388590455\n",
      "epoch: 6 step: 222, loss is 0.00033040018752217293\n",
      "epoch: 6 step: 223, loss is 0.0127570116892457\n",
      "epoch: 6 step: 224, loss is 0.000129794716485776\n",
      "epoch: 6 step: 225, loss is 0.019123796373605728\n",
      "epoch: 6 step: 226, loss is 7.738914428045973e-05\n",
      "epoch: 6 step: 227, loss is 0.0025314849335700274\n",
      "epoch: 6 step: 228, loss is 0.008540540933609009\n",
      "epoch: 6 step: 229, loss is 0.007908085361123085\n",
      "epoch: 6 step: 230, loss is 0.0016856216825544834\n",
      "epoch: 6 step: 231, loss is 0.0053329807706177235\n",
      "epoch: 6 step: 232, loss is 0.00685528852045536\n",
      "epoch: 6 step: 233, loss is 0.009501637890934944\n",
      "epoch: 6 step: 234, loss is 0.03176058456301689\n",
      "epoch: 6 step: 235, loss is 0.0002722862409427762\n",
      "epoch: 6 step: 236, loss is 0.0015425814781337976\n",
      "epoch: 6 step: 237, loss is 0.008838074281811714\n",
      "epoch: 6 step: 238, loss is 0.008599333465099335\n",
      "epoch: 6 step: 239, loss is 0.0003879161668010056\n",
      "epoch: 6 step: 240, loss is 0.0009016263065859675\n",
      "epoch: 6 step: 241, loss is 0.013870231807231903\n",
      "epoch: 6 step: 242, loss is 0.015814948827028275\n",
      "epoch: 6 step: 243, loss is 0.011574601754546165\n",
      "epoch: 6 step: 244, loss is 0.13072796165943146\n",
      "epoch: 6 step: 245, loss is 0.0012789244065061212\n",
      "epoch: 6 step: 246, loss is 0.007295602932572365\n",
      "epoch: 6 step: 247, loss is 0.057118482887744904\n",
      "epoch: 6 step: 248, loss is 0.02106686681509018\n",
      "epoch: 6 step: 249, loss is 0.0017148189945146441\n",
      "epoch: 6 step: 250, loss is 0.011190506629645824\n",
      "epoch: 6 step: 251, loss is 0.05113758146762848\n",
      "epoch: 6 step: 252, loss is 0.0025386775378137827\n",
      "epoch: 6 step: 253, loss is 0.0333007276058197\n",
      "epoch: 6 step: 254, loss is 0.0011063226265832782\n",
      "epoch: 6 step: 255, loss is 0.029321307316422462\n",
      "epoch: 6 step: 256, loss is 0.00028520767227746546\n",
      "epoch: 6 step: 257, loss is 0.02113141119480133\n",
      "epoch: 6 step: 258, loss is 0.020026834681630135\n",
      "epoch: 6 step: 259, loss is 0.03561166673898697\n",
      "epoch: 6 step: 260, loss is 0.0013464500661939383\n",
      "epoch: 6 step: 261, loss is 0.10449030995368958\n",
      "epoch: 6 step: 262, loss is 0.042777951806783676\n",
      "epoch: 6 step: 263, loss is 0.18749675154685974\n",
      "epoch: 6 step: 264, loss is 0.0017097819363698363\n",
      "epoch: 6 step: 265, loss is 0.009057434275746346\n",
      "epoch: 6 step: 266, loss is 0.014864116907119751\n",
      "epoch: 6 step: 267, loss is 0.0007843110943213105\n",
      "epoch: 6 step: 268, loss is 0.02923445589840412\n",
      "epoch: 6 step: 269, loss is 0.07570844888687134\n",
      "epoch: 6 step: 270, loss is 0.01986444555222988\n",
      "epoch: 6 step: 271, loss is 0.03012138418853283\n",
      "epoch: 6 step: 272, loss is 5.485676592797972e-05\n",
      "epoch: 6 step: 273, loss is 0.0004030057170893997\n",
      "epoch: 6 step: 274, loss is 0.0006511528044939041\n",
      "epoch: 6 step: 275, loss is 0.00015431983047164977\n",
      "epoch: 6 step: 276, loss is 0.002258649794384837\n",
      "epoch: 6 step: 277, loss is 0.018500283360481262\n",
      "epoch: 6 step: 278, loss is 0.0054557533003389835\n",
      "epoch: 6 step: 279, loss is 0.0009003715822473168\n",
      "epoch: 6 step: 280, loss is 0.01023954525589943\n",
      "epoch: 6 step: 281, loss is 0.10334949940443039\n",
      "epoch: 6 step: 282, loss is 0.0003035072877537459\n",
      "epoch: 6 step: 283, loss is 0.01735837757587433\n",
      "epoch: 6 step: 284, loss is 0.007424358744174242\n",
      "epoch: 6 step: 285, loss is 0.11796989291906357\n",
      "epoch: 6 step: 286, loss is 0.00016328315541613847\n",
      "epoch: 6 step: 287, loss is 6.287072756094858e-05\n",
      "epoch: 6 step: 288, loss is 0.3407500088214874\n",
      "epoch: 6 step: 289, loss is 0.010337799787521362\n",
      "epoch: 6 step: 290, loss is 0.00011953464854741469\n",
      "epoch: 6 step: 291, loss is 0.001325008925050497\n",
      "epoch: 6 step: 292, loss is 0.004569500219076872\n",
      "epoch: 6 step: 293, loss is 0.0044577778317034245\n",
      "epoch: 6 step: 294, loss is 3.834852032014169e-05\n",
      "epoch: 6 step: 295, loss is 0.006630013696849346\n",
      "epoch: 6 step: 296, loss is 0.0021159984171390533\n",
      "epoch: 6 step: 297, loss is 0.00015027099289000034\n",
      "epoch: 6 step: 298, loss is 0.00451783137395978\n",
      "epoch: 6 step: 299, loss is 0.0024494570679962635\n",
      "epoch: 6 step: 300, loss is 0.00040605521644465625\n",
      "epoch: 6 step: 301, loss is 0.10277757793664932\n",
      "epoch: 6 step: 302, loss is 0.03422047197818756\n",
      "epoch: 6 step: 303, loss is 0.02555004134774208\n",
      "epoch: 6 step: 304, loss is 0.07946178317070007\n",
      "epoch: 6 step: 305, loss is 0.0018956720596179366\n",
      "epoch: 6 step: 306, loss is 0.03240153566002846\n",
      "epoch: 6 step: 307, loss is 0.028659358620643616\n",
      "epoch: 6 step: 308, loss is 0.0005705375806428492\n",
      "epoch: 6 step: 309, loss is 0.0046494221314787865\n",
      "epoch: 6 step: 310, loss is 0.0013021950144320726\n",
      "epoch: 6 step: 311, loss is 0.32409560680389404\n",
      "epoch: 6 step: 312, loss is 0.0043691121973097324\n",
      "epoch: 6 step: 313, loss is 0.017494013532996178\n",
      "epoch: 6 step: 314, loss is 0.009143862873315811\n",
      "epoch: 6 step: 315, loss is 0.006009031552821398\n",
      "epoch: 6 step: 316, loss is 0.006380891893059015\n",
      "epoch: 6 step: 317, loss is 0.0013269082410261035\n",
      "epoch: 6 step: 318, loss is 0.03794117271900177\n",
      "epoch: 6 step: 319, loss is 0.008456351235508919\n",
      "epoch: 6 step: 320, loss is 0.0027286498807370663\n",
      "epoch: 6 step: 321, loss is 0.0007326174527406693\n",
      "epoch: 6 step: 322, loss is 0.0027161173056811094\n",
      "epoch: 6 step: 323, loss is 0.11837547272443771\n",
      "epoch: 6 step: 324, loss is 0.0007094710017554462\n",
      "epoch: 6 step: 325, loss is 0.0019130060682073236\n",
      "epoch: 6 step: 326, loss is 0.02338285930454731\n",
      "epoch: 6 step: 327, loss is 0.031157031655311584\n",
      "epoch: 6 step: 328, loss is 0.0005283097852952778\n",
      "epoch: 6 step: 329, loss is 0.01537473127245903\n",
      "epoch: 6 step: 330, loss is 0.002518431982025504\n",
      "epoch: 6 step: 331, loss is 0.05479096993803978\n",
      "epoch: 6 step: 332, loss is 0.14123596251010895\n",
      "epoch: 6 step: 333, loss is 0.061223242431879044\n",
      "epoch: 6 step: 334, loss is 0.0006002765148878098\n",
      "epoch: 6 step: 335, loss is 0.00038887877599336207\n",
      "epoch: 6 step: 336, loss is 0.004003352951258421\n",
      "epoch: 6 step: 337, loss is 0.005976404994726181\n",
      "epoch: 6 step: 338, loss is 0.0006311036413535476\n",
      "epoch: 6 step: 339, loss is 0.0028682039119303226\n",
      "epoch: 6 step: 340, loss is 0.0013960507931187749\n",
      "epoch: 6 step: 341, loss is 0.011444802395999432\n",
      "epoch: 6 step: 342, loss is 0.0007876129820942879\n",
      "epoch: 6 step: 343, loss is 0.03176906704902649\n",
      "epoch: 6 step: 344, loss is 0.008325154893100262\n",
      "epoch: 6 step: 345, loss is 0.045461881905794144\n",
      "epoch: 6 step: 346, loss is 0.010164105333387852\n",
      "epoch: 6 step: 347, loss is 0.02532401867210865\n",
      "epoch: 6 step: 348, loss is 0.00046862036106176674\n",
      "epoch: 6 step: 349, loss is 0.004162798170000315\n",
      "epoch: 6 step: 350, loss is 0.00234620226547122\n",
      "epoch: 6 step: 351, loss is 0.00013014950673095882\n",
      "epoch: 6 step: 352, loss is 0.0003615151799749583\n",
      "epoch: 6 step: 353, loss is 0.025312408804893494\n",
      "epoch: 6 step: 354, loss is 0.030250636860728264\n",
      "epoch: 6 step: 355, loss is 0.0004392473492771387\n",
      "epoch: 6 step: 356, loss is 0.12207654118537903\n",
      "epoch: 6 step: 357, loss is 0.0026186532340943813\n",
      "epoch: 6 step: 358, loss is 0.07045283913612366\n",
      "epoch: 6 step: 359, loss is 0.08557914942502975\n",
      "epoch: 6 step: 360, loss is 0.03797442838549614\n",
      "epoch: 6 step: 361, loss is 0.000838465872220695\n",
      "epoch: 6 step: 362, loss is 0.0018134218407794833\n",
      "epoch: 6 step: 363, loss is 0.001215294818393886\n",
      "epoch: 6 step: 364, loss is 0.13680928945541382\n",
      "epoch: 6 step: 365, loss is 0.043051134794950485\n",
      "epoch: 6 step: 366, loss is 0.0006699010264128447\n",
      "epoch: 6 step: 367, loss is 0.01772192120552063\n",
      "epoch: 6 step: 368, loss is 0.023161008954048157\n",
      "epoch: 6 step: 369, loss is 0.0018973503028973937\n",
      "epoch: 6 step: 370, loss is 0.006005727685987949\n",
      "epoch: 6 step: 371, loss is 0.02159843035042286\n",
      "epoch: 6 step: 372, loss is 0.014357361942529678\n",
      "epoch: 6 step: 373, loss is 0.004238404333591461\n",
      "epoch: 6 step: 374, loss is 0.016290288418531418\n",
      "epoch: 6 step: 375, loss is 0.02086026780307293\n",
      "epoch: 6 step: 376, loss is 0.002003239933401346\n",
      "epoch: 6 step: 377, loss is 0.015296204946935177\n",
      "epoch: 6 step: 378, loss is 0.0020948010496795177\n",
      "epoch: 6 step: 379, loss is 0.0010564506519585848\n",
      "epoch: 6 step: 380, loss is 0.0007231172057799995\n",
      "epoch: 6 step: 381, loss is 0.03148537501692772\n",
      "epoch: 6 step: 382, loss is 0.011642890982329845\n",
      "epoch: 6 step: 383, loss is 0.012367792427539825\n",
      "epoch: 6 step: 384, loss is 0.006523329298943281\n",
      "epoch: 6 step: 385, loss is 0.002012640470638871\n",
      "epoch: 6 step: 386, loss is 0.002259757136926055\n",
      "epoch: 6 step: 387, loss is 0.007483373861759901\n",
      "epoch: 6 step: 388, loss is 0.0030116767156869173\n",
      "epoch: 6 step: 389, loss is 0.12087251991033554\n",
      "epoch: 6 step: 390, loss is 0.003772210096940398\n",
      "epoch: 6 step: 391, loss is 0.0006933619151823223\n",
      "epoch: 6 step: 392, loss is 0.002998458221554756\n",
      "epoch: 6 step: 393, loss is 0.0015252541052177548\n",
      "epoch: 6 step: 394, loss is 0.004306395538151264\n",
      "epoch: 6 step: 395, loss is 0.0011376175098121166\n",
      "epoch: 6 step: 396, loss is 0.02154061384499073\n",
      "epoch: 6 step: 397, loss is 0.06913129985332489\n",
      "epoch: 6 step: 398, loss is 0.008597388863563538\n",
      "epoch: 6 step: 399, loss is 0.007917292416095734\n",
      "epoch: 6 step: 400, loss is 0.00022396771237254143\n",
      "epoch: 6 step: 401, loss is 0.0003384708834346384\n",
      "epoch: 6 step: 402, loss is 0.006643190979957581\n",
      "epoch: 6 step: 403, loss is 0.00042933967779390514\n",
      "epoch: 6 step: 404, loss is 0.015150121413171291\n",
      "epoch: 6 step: 405, loss is 0.002536310814321041\n",
      "epoch: 6 step: 406, loss is 0.06146347522735596\n",
      "epoch: 6 step: 407, loss is 0.11916325241327286\n",
      "epoch: 6 step: 408, loss is 0.001704243361018598\n",
      "epoch: 6 step: 409, loss is 0.015174588188529015\n",
      "epoch: 6 step: 410, loss is 0.06725488603115082\n",
      "epoch: 6 step: 411, loss is 0.004704541526734829\n",
      "epoch: 6 step: 412, loss is 0.006825284566730261\n",
      "epoch: 6 step: 413, loss is 0.00012630672426894307\n",
      "epoch: 6 step: 414, loss is 0.00013092163135297596\n",
      "epoch: 6 step: 415, loss is 0.0015279357321560383\n",
      "epoch: 6 step: 416, loss is 0.1679781973361969\n",
      "epoch: 6 step: 417, loss is 0.0008642947068437934\n",
      "epoch: 6 step: 418, loss is 0.00920907873660326\n",
      "epoch: 6 step: 419, loss is 0.024406105279922485\n",
      "epoch: 6 step: 420, loss is 0.0003095214779023081\n",
      "epoch: 6 step: 421, loss is 0.003940539434552193\n",
      "epoch: 6 step: 422, loss is 0.03744248300790787\n",
      "epoch: 6 step: 423, loss is 0.003151262179017067\n",
      "epoch: 6 step: 424, loss is 0.026345692574977875\n",
      "epoch: 6 step: 425, loss is 0.13392265141010284\n",
      "epoch: 6 step: 426, loss is 0.00816196110099554\n",
      "epoch: 6 step: 427, loss is 0.0028158293571323156\n",
      "epoch: 6 step: 428, loss is 0.0016472296556457877\n",
      "epoch: 6 step: 429, loss is 0.11577998846769333\n",
      "epoch: 6 step: 430, loss is 0.015527324751019478\n",
      "epoch: 6 step: 431, loss is 0.002916779601946473\n",
      "epoch: 6 step: 432, loss is 0.06630004942417145\n",
      "epoch: 6 step: 433, loss is 0.03845426067709923\n",
      "epoch: 6 step: 434, loss is 0.04067578539252281\n",
      "epoch: 6 step: 435, loss is 0.013388709165155888\n",
      "epoch: 6 step: 436, loss is 0.09105736017227173\n",
      "epoch: 6 step: 437, loss is 0.008758369833230972\n",
      "epoch: 6 step: 438, loss is 0.001333547057583928\n",
      "epoch: 6 step: 439, loss is 0.007208376657217741\n",
      "epoch: 6 step: 440, loss is 0.02974528819322586\n",
      "epoch: 6 step: 441, loss is 0.020721757784485817\n",
      "epoch: 6 step: 442, loss is 0.005052139051258564\n",
      "epoch: 6 step: 443, loss is 0.027324197813868523\n",
      "epoch: 6 step: 444, loss is 0.0002407850552117452\n",
      "epoch: 6 step: 445, loss is 0.0005507233436219394\n",
      "epoch: 6 step: 446, loss is 0.00016373011749237776\n",
      "epoch: 6 step: 447, loss is 0.021322021260857582\n",
      "epoch: 6 step: 448, loss is 0.0055517335422337055\n",
      "epoch: 6 step: 449, loss is 0.012603476643562317\n",
      "epoch: 6 step: 450, loss is 0.004629004746675491\n",
      "epoch: 6 step: 451, loss is 0.010490774177014828\n",
      "epoch: 6 step: 452, loss is 0.019326383247971535\n",
      "epoch: 6 step: 453, loss is 0.003746157744899392\n",
      "epoch: 6 step: 454, loss is 0.0036773020401597023\n",
      "epoch: 6 step: 455, loss is 0.0002944926673080772\n",
      "epoch: 6 step: 456, loss is 0.01566583290696144\n",
      "epoch: 6 step: 457, loss is 0.000643488543573767\n",
      "epoch: 6 step: 458, loss is 0.00023715638963039964\n",
      "epoch: 6 step: 459, loss is 0.0006141308695077896\n",
      "epoch: 6 step: 460, loss is 0.09483279287815094\n",
      "epoch: 6 step: 461, loss is 0.0033166406210511923\n",
      "epoch: 6 step: 462, loss is 0.007024479564279318\n",
      "epoch: 6 step: 463, loss is 0.0025079676415771246\n",
      "epoch: 6 step: 464, loss is 0.0005513039650395513\n",
      "epoch: 6 step: 465, loss is 0.01442598644644022\n",
      "epoch: 6 step: 466, loss is 0.011053105816245079\n",
      "epoch: 6 step: 467, loss is 0.013969479128718376\n",
      "epoch: 6 step: 468, loss is 0.0002282106433995068\n",
      "epoch: 6 step: 469, loss is 0.11449127644300461\n",
      "epoch: 6 step: 470, loss is 9.085884812520817e-05\n",
      "epoch: 6 step: 471, loss is 0.007824625819921494\n",
      "epoch: 6 step: 472, loss is 0.007442598696798086\n",
      "epoch: 6 step: 473, loss is 0.0013831533724442124\n",
      "epoch: 6 step: 474, loss is 7.967728743096814e-05\n",
      "epoch: 6 step: 475, loss is 0.007999365217983723\n",
      "epoch: 6 step: 476, loss is 0.0009105759090743959\n",
      "epoch: 6 step: 477, loss is 0.005849544890224934\n",
      "epoch: 6 step: 478, loss is 0.0051303887739777565\n",
      "epoch: 6 step: 479, loss is 0.020371176302433014\n",
      "epoch: 6 step: 480, loss is 0.022254198789596558\n",
      "epoch: 6 step: 481, loss is 0.028111780062317848\n",
      "epoch: 6 step: 482, loss is 0.0027640818152576685\n",
      "epoch: 6 step: 483, loss is 0.0010866365628316998\n",
      "epoch: 6 step: 484, loss is 0.0018573208944872022\n",
      "epoch: 6 step: 485, loss is 0.0016751354560256004\n",
      "epoch: 6 step: 486, loss is 0.0059624467976391315\n",
      "epoch: 6 step: 487, loss is 0.007377357222139835\n",
      "epoch: 6 step: 488, loss is 0.003265222068876028\n",
      "epoch: 6 step: 489, loss is 0.002043355256319046\n",
      "epoch: 6 step: 490, loss is 0.20624367892742157\n",
      "epoch: 6 step: 491, loss is 0.22062990069389343\n",
      "epoch: 6 step: 492, loss is 0.016095083206892014\n",
      "epoch: 6 step: 493, loss is 0.05081368237733841\n",
      "epoch: 6 step: 494, loss is 0.064915731549263\n",
      "epoch: 6 step: 495, loss is 0.05415993556380272\n",
      "epoch: 6 step: 496, loss is 0.0036679720506072044\n",
      "epoch: 6 step: 497, loss is 0.0006005281466059387\n",
      "epoch: 6 step: 498, loss is 0.0005339902709238231\n",
      "epoch: 6 step: 499, loss is 0.018265822902321815\n",
      "epoch: 6 step: 500, loss is 0.05496445298194885\n",
      "epoch: 6 step: 501, loss is 0.001856009941548109\n",
      "epoch: 6 step: 502, loss is 0.00406763656064868\n",
      "epoch: 6 step: 503, loss is 0.04805513843894005\n",
      "epoch: 6 step: 504, loss is 0.055823151022195816\n",
      "epoch: 6 step: 505, loss is 0.0011743693612515926\n",
      "epoch: 6 step: 506, loss is 0.0033860073890537024\n",
      "epoch: 6 step: 507, loss is 0.06706906855106354\n",
      "epoch: 6 step: 508, loss is 0.01201600581407547\n",
      "epoch: 6 step: 509, loss is 0.0024708963464945555\n",
      "epoch: 6 step: 510, loss is 0.0015880813589319587\n",
      "epoch: 6 step: 511, loss is 0.021136965602636337\n",
      "epoch: 6 step: 512, loss is 0.0031978064216673374\n",
      "epoch: 6 step: 513, loss is 0.053184062242507935\n",
      "epoch: 6 step: 514, loss is 0.01725463569164276\n",
      "epoch: 6 step: 515, loss is 0.0003197105834260583\n",
      "epoch: 6 step: 516, loss is 0.004694703500717878\n",
      "epoch: 6 step: 517, loss is 0.019798841327428818\n",
      "epoch: 6 step: 518, loss is 0.004000936634838581\n",
      "epoch: 6 step: 519, loss is 0.05260995402932167\n",
      "epoch: 6 step: 520, loss is 0.0006433172966353595\n",
      "epoch: 6 step: 521, loss is 0.018406540155410767\n",
      "epoch: 6 step: 522, loss is 0.0007151318714022636\n",
      "epoch: 6 step: 523, loss is 0.008201044984161854\n",
      "epoch: 6 step: 524, loss is 0.002011589240282774\n",
      "epoch: 6 step: 525, loss is 0.001916340785101056\n",
      "epoch: 6 step: 526, loss is 0.008165037259459496\n",
      "epoch: 6 step: 527, loss is 0.07399751991033554\n",
      "epoch: 6 step: 528, loss is 0.002406332641839981\n",
      "epoch: 6 step: 529, loss is 0.1032366007566452\n",
      "epoch: 6 step: 530, loss is 0.00024605815997347236\n",
      "epoch: 6 step: 531, loss is 0.025483764708042145\n",
      "epoch: 6 step: 532, loss is 0.002467337530106306\n",
      "epoch: 6 step: 533, loss is 0.0017006879206746817\n",
      "epoch: 6 step: 534, loss is 0.01512089278548956\n",
      "epoch: 6 step: 535, loss is 0.0008104314911179245\n",
      "epoch: 6 step: 536, loss is 0.11628837883472443\n",
      "epoch: 6 step: 537, loss is 0.0013374448753893375\n",
      "epoch: 6 step: 538, loss is 0.016712510958313942\n",
      "epoch: 6 step: 539, loss is 0.0007242631400004029\n",
      "epoch: 6 step: 540, loss is 0.0029627687763422728\n",
      "epoch: 6 step: 541, loss is 0.0002506799646653235\n",
      "epoch: 6 step: 542, loss is 0.0003163808723911643\n",
      "epoch: 6 step: 543, loss is 0.18342314660549164\n",
      "epoch: 6 step: 544, loss is 0.16578160226345062\n",
      "epoch: 6 step: 545, loss is 0.06488734483718872\n",
      "epoch: 6 step: 546, loss is 0.000515677616931498\n",
      "epoch: 6 step: 547, loss is 0.016402574256062508\n",
      "epoch: 6 step: 548, loss is 0.0002855045604519546\n",
      "epoch: 6 step: 549, loss is 0.0864100232720375\n",
      "epoch: 6 step: 550, loss is 0.00029998706304468215\n",
      "epoch: 6 step: 551, loss is 0.05464109033346176\n",
      "epoch: 6 step: 552, loss is 0.0016671199118718505\n",
      "epoch: 6 step: 553, loss is 0.21214842796325684\n",
      "epoch: 6 step: 554, loss is 0.019401514902710915\n",
      "epoch: 6 step: 555, loss is 0.002597062848508358\n",
      "epoch: 6 step: 556, loss is 0.02490762062370777\n",
      "epoch: 6 step: 557, loss is 0.0022090694401413202\n",
      "epoch: 6 step: 558, loss is 0.003871071618050337\n",
      "epoch: 6 step: 559, loss is 0.1045919805765152\n",
      "epoch: 6 step: 560, loss is 0.021361028775572777\n",
      "epoch: 6 step: 561, loss is 0.000564497197046876\n",
      "epoch: 6 step: 562, loss is 0.0974709689617157\n",
      "epoch: 6 step: 563, loss is 0.0009460562723688781\n",
      "epoch: 6 step: 564, loss is 0.051282916218042374\n",
      "epoch: 6 step: 565, loss is 0.2173139452934265\n",
      "epoch: 6 step: 566, loss is 0.0005820223595947027\n",
      "epoch: 6 step: 567, loss is 0.020517796277999878\n",
      "epoch: 6 step: 568, loss is 0.0037920342292636633\n",
      "epoch: 6 step: 569, loss is 0.07434326410293579\n",
      "epoch: 6 step: 570, loss is 0.014525163918733597\n",
      "epoch: 6 step: 571, loss is 0.011407394893467426\n",
      "epoch: 6 step: 572, loss is 0.00393323041498661\n",
      "epoch: 6 step: 573, loss is 0.026234865188598633\n",
      "epoch: 6 step: 574, loss is 0.002048338297754526\n",
      "epoch: 6 step: 575, loss is 0.02525980770587921\n",
      "epoch: 6 step: 576, loss is 0.0017561543500050902\n",
      "epoch: 6 step: 577, loss is 0.0024064539466053247\n",
      "epoch: 6 step: 578, loss is 0.0024709829594939947\n",
      "epoch: 6 step: 579, loss is 0.0027457387186586857\n",
      "epoch: 6 step: 580, loss is 0.0146904606372118\n",
      "epoch: 6 step: 581, loss is 0.021931756287813187\n",
      "epoch: 6 step: 582, loss is 0.0274649728089571\n",
      "epoch: 6 step: 583, loss is 0.0021494398824870586\n",
      "epoch: 6 step: 584, loss is 0.0009649534476920962\n",
      "epoch: 6 step: 585, loss is 0.004640450235456228\n",
      "epoch: 6 step: 586, loss is 0.045150790363550186\n",
      "epoch: 6 step: 587, loss is 0.0001173036071122624\n",
      "epoch: 6 step: 588, loss is 0.006186320912092924\n",
      "epoch: 6 step: 589, loss is 0.040612250566482544\n",
      "epoch: 6 step: 590, loss is 0.010101930238306522\n",
      "epoch: 6 step: 591, loss is 0.0006993943243287504\n",
      "epoch: 6 step: 592, loss is 0.050111107528209686\n",
      "epoch: 6 step: 593, loss is 0.0019132352899760008\n",
      "epoch: 6 step: 594, loss is 0.13561934232711792\n",
      "epoch: 6 step: 595, loss is 0.040636636316776276\n",
      "epoch: 6 step: 596, loss is 0.0856781154870987\n",
      "epoch: 6 step: 597, loss is 0.00046271292376331985\n",
      "epoch: 6 step: 598, loss is 0.010431798174977303\n",
      "epoch: 6 step: 599, loss is 0.000237565633142367\n",
      "epoch: 6 step: 600, loss is 0.009805449284613132\n",
      "epoch: 6 step: 601, loss is 0.001043729716911912\n",
      "epoch: 6 step: 602, loss is 0.02225135825574398\n",
      "epoch: 6 step: 603, loss is 0.00023427412088494748\n",
      "epoch: 6 step: 604, loss is 0.0004378689918667078\n",
      "epoch: 6 step: 605, loss is 0.02681773714721203\n",
      "epoch: 6 step: 606, loss is 0.0007353061228059232\n",
      "epoch: 6 step: 607, loss is 0.02062362991273403\n",
      "epoch: 6 step: 608, loss is 0.008950207382440567\n",
      "epoch: 6 step: 609, loss is 0.08511053770780563\n",
      "epoch: 6 step: 610, loss is 0.0001450626295991242\n",
      "epoch: 6 step: 611, loss is 0.03465893492102623\n",
      "epoch: 6 step: 612, loss is 0.005635057110339403\n",
      "epoch: 6 step: 613, loss is 0.04360121488571167\n",
      "epoch: 6 step: 614, loss is 0.05171992629766464\n",
      "epoch: 6 step: 615, loss is 0.11323513835668564\n",
      "epoch: 6 step: 616, loss is 0.011508148163557053\n",
      "epoch: 6 step: 617, loss is 0.015481948852539062\n",
      "epoch: 6 step: 618, loss is 0.028359824791550636\n",
      "epoch: 6 step: 619, loss is 0.00027840881375595927\n",
      "epoch: 6 step: 620, loss is 0.010658994317054749\n",
      "epoch: 6 step: 621, loss is 0.028723586350679398\n",
      "epoch: 6 step: 622, loss is 0.07525105774402618\n",
      "epoch: 6 step: 623, loss is 0.09627725183963776\n",
      "epoch: 6 step: 624, loss is 0.00026623954181559384\n",
      "epoch: 6 step: 625, loss is 0.032400645315647125\n",
      "epoch: 6 step: 626, loss is 0.10825188457965851\n",
      "epoch: 6 step: 627, loss is 0.00178871164098382\n",
      "epoch: 6 step: 628, loss is 5.497836536960676e-05\n",
      "epoch: 6 step: 629, loss is 0.09434819966554642\n",
      "epoch: 6 step: 630, loss is 0.0321069061756134\n",
      "epoch: 6 step: 631, loss is 0.10384168475866318\n",
      "epoch: 6 step: 632, loss is 0.03753144294023514\n",
      "epoch: 6 step: 633, loss is 0.0002372910239500925\n",
      "epoch: 6 step: 634, loss is 0.002994634909555316\n",
      "epoch: 6 step: 635, loss is 0.012267283163964748\n",
      "epoch: 6 step: 636, loss is 0.015103026293218136\n",
      "epoch: 6 step: 637, loss is 0.00010896981257246807\n",
      "epoch: 6 step: 638, loss is 0.00581440469250083\n",
      "epoch: 6 step: 639, loss is 8.029781747609377e-05\n",
      "epoch: 6 step: 640, loss is 0.0007285099709406495\n",
      "epoch: 6 step: 641, loss is 0.0024215516168624163\n",
      "epoch: 6 step: 642, loss is 0.0018603595672175288\n",
      "epoch: 6 step: 643, loss is 0.01784357987344265\n",
      "epoch: 6 step: 644, loss is 0.0011347176041454077\n",
      "epoch: 6 step: 645, loss is 0.00842788815498352\n",
      "epoch: 6 step: 646, loss is 0.2695762813091278\n",
      "epoch: 6 step: 647, loss is 0.0029080540407449007\n",
      "epoch: 6 step: 648, loss is 0.03922491893172264\n",
      "epoch: 6 step: 649, loss is 0.03227014094591141\n",
      "epoch: 6 step: 650, loss is 0.22368648648262024\n",
      "epoch: 6 step: 651, loss is 0.033855635672807693\n",
      "epoch: 6 step: 652, loss is 0.0018562583718448877\n",
      "epoch: 6 step: 653, loss is 0.0014623471070080996\n",
      "epoch: 6 step: 654, loss is 0.0008795518078841269\n",
      "epoch: 6 step: 655, loss is 0.03302939608693123\n",
      "epoch: 6 step: 656, loss is 0.3228435516357422\n",
      "epoch: 6 step: 657, loss is 0.000406521197874099\n",
      "epoch: 6 step: 658, loss is 0.029017716646194458\n",
      "epoch: 6 step: 659, loss is 0.08622856438159943\n",
      "epoch: 6 step: 660, loss is 0.0056309327483177185\n",
      "epoch: 6 step: 661, loss is 0.04391583055257797\n",
      "epoch: 6 step: 662, loss is 9.963350021280348e-05\n",
      "epoch: 6 step: 663, loss is 0.00010607357398839667\n",
      "epoch: 6 step: 664, loss is 0.07727246731519699\n",
      "epoch: 6 step: 665, loss is 0.19810836017131805\n",
      "epoch: 6 step: 666, loss is 0.11465335637331009\n",
      "epoch: 6 step: 667, loss is 0.007164309732615948\n",
      "epoch: 6 step: 668, loss is 0.008081013336777687\n",
      "epoch: 6 step: 669, loss is 0.002791415434330702\n",
      "epoch: 6 step: 670, loss is 0.00014209351502358913\n",
      "epoch: 6 step: 671, loss is 0.0009270348236896098\n",
      "epoch: 6 step: 672, loss is 0.004308575298637152\n",
      "epoch: 6 step: 673, loss is 0.018065519630908966\n",
      "epoch: 6 step: 674, loss is 0.005054646171629429\n",
      "epoch: 6 step: 675, loss is 0.00035317486617714167\n",
      "epoch: 6 step: 676, loss is 0.015604164451360703\n",
      "epoch: 6 step: 677, loss is 0.0037459793966263533\n",
      "epoch: 6 step: 678, loss is 0.01613350585103035\n",
      "epoch: 6 step: 679, loss is 0.004065291024744511\n",
      "epoch: 6 step: 680, loss is 0.0009480667067691684\n",
      "epoch: 6 step: 681, loss is 0.009836425073444843\n",
      "epoch: 6 step: 682, loss is 0.00035906428820453584\n",
      "epoch: 6 step: 683, loss is 0.0388479121029377\n",
      "epoch: 6 step: 684, loss is 0.005646025761961937\n",
      "epoch: 6 step: 685, loss is 0.006643000524491072\n",
      "epoch: 6 step: 686, loss is 0.0002070956106763333\n",
      "epoch: 6 step: 687, loss is 0.00020174474047962576\n",
      "epoch: 6 step: 688, loss is 0.000621104147285223\n",
      "epoch: 6 step: 689, loss is 0.0009884583996608853\n",
      "epoch: 6 step: 690, loss is 0.0003736327344086021\n",
      "epoch: 6 step: 691, loss is 4.465672463993542e-05\n",
      "epoch: 6 step: 692, loss is 7.270592323038727e-05\n",
      "epoch: 6 step: 693, loss is 0.004512765444815159\n",
      "epoch: 6 step: 694, loss is 0.01854749396443367\n",
      "epoch: 6 step: 695, loss is 0.01256425678730011\n",
      "epoch: 6 step: 696, loss is 0.02055930346250534\n",
      "epoch: 6 step: 697, loss is 0.0006809017504565418\n",
      "epoch: 6 step: 698, loss is 0.05062754452228546\n",
      "epoch: 6 step: 699, loss is 0.0027973761316388845\n",
      "epoch: 6 step: 700, loss is 0.00029027066193521023\n",
      "epoch: 6 step: 701, loss is 0.0008444131235592067\n",
      "epoch: 6 step: 702, loss is 0.0007105203112587333\n",
      "epoch: 6 step: 703, loss is 0.01805035211145878\n",
      "epoch: 6 step: 704, loss is 0.006511889398097992\n",
      "epoch: 6 step: 705, loss is 0.0007995155174285173\n",
      "epoch: 6 step: 706, loss is 0.0018078004941344261\n",
      "epoch: 6 step: 707, loss is 0.011223840527236462\n",
      "epoch: 6 step: 708, loss is 0.10071354359388351\n",
      "epoch: 6 step: 709, loss is 0.00016185805725399405\n",
      "epoch: 6 step: 710, loss is 0.1122020035982132\n",
      "epoch: 6 step: 711, loss is 0.014367477968335152\n",
      "epoch: 6 step: 712, loss is 0.01643279753625393\n",
      "epoch: 6 step: 713, loss is 0.0052040149457752705\n",
      "epoch: 6 step: 714, loss is 0.00870877131819725\n",
      "epoch: 6 step: 715, loss is 0.01236744038760662\n",
      "epoch: 6 step: 716, loss is 0.00021320898667909205\n",
      "epoch: 6 step: 717, loss is 0.005698271561414003\n",
      "epoch: 6 step: 718, loss is 0.08993056416511536\n",
      "epoch: 6 step: 719, loss is 0.013376179151237011\n",
      "epoch: 6 step: 720, loss is 0.06742329150438309\n",
      "epoch: 6 step: 721, loss is 0.006779681891202927\n",
      "epoch: 6 step: 722, loss is 0.00694032059982419\n",
      "epoch: 6 step: 723, loss is 0.017270967364311218\n",
      "epoch: 6 step: 724, loss is 0.00019625123240984976\n",
      "epoch: 6 step: 725, loss is 0.005239151883870363\n",
      "epoch: 6 step: 726, loss is 0.13943900167942047\n",
      "epoch: 6 step: 727, loss is 0.015657151117920876\n",
      "epoch: 6 step: 728, loss is 0.2876323461532593\n",
      "epoch: 6 step: 729, loss is 0.00021214642038103193\n",
      "epoch: 6 step: 730, loss is 0.0002017683000303805\n",
      "epoch: 6 step: 731, loss is 0.08440060168504715\n",
      "epoch: 6 step: 732, loss is 0.0010547200217843056\n",
      "epoch: 6 step: 733, loss is 0.06943278759717941\n",
      "epoch: 6 step: 734, loss is 0.013504819944500923\n",
      "epoch: 6 step: 735, loss is 0.0020288219675421715\n",
      "epoch: 6 step: 736, loss is 0.101224884390831\n",
      "epoch: 6 step: 737, loss is 0.062259215861558914\n",
      "epoch: 6 step: 738, loss is 0.01012420654296875\n",
      "epoch: 6 step: 739, loss is 0.0015633986331522465\n",
      "epoch: 6 step: 740, loss is 0.003227277658879757\n",
      "epoch: 6 step: 741, loss is 0.046364352107048035\n",
      "epoch: 6 step: 742, loss is 0.002241316018626094\n",
      "epoch: 6 step: 743, loss is 0.00046922045294195414\n",
      "epoch: 6 step: 744, loss is 0.08329302072525024\n",
      "epoch: 6 step: 745, loss is 0.00020331054111011326\n",
      "epoch: 6 step: 746, loss is 0.005253991112112999\n",
      "epoch: 6 step: 747, loss is 0.0015851488569751382\n",
      "epoch: 6 step: 748, loss is 0.01404887530952692\n",
      "epoch: 6 step: 749, loss is 0.0007369245286099613\n",
      "epoch: 6 step: 750, loss is 0.17772549390792847\n",
      "epoch: 6 step: 751, loss is 0.002488778904080391\n",
      "epoch: 6 step: 752, loss is 0.00036773073952645063\n",
      "epoch: 6 step: 753, loss is 0.003022356191650033\n",
      "epoch: 6 step: 754, loss is 0.004604490008205175\n",
      "epoch: 6 step: 755, loss is 0.0016508972039446235\n",
      "epoch: 6 step: 756, loss is 0.005232241004705429\n",
      "epoch: 6 step: 757, loss is 0.11349065601825714\n",
      "epoch: 6 step: 758, loss is 0.059120260179042816\n",
      "epoch: 6 step: 759, loss is 0.0020783240906894207\n",
      "epoch: 6 step: 760, loss is 0.011052756570279598\n",
      "epoch: 6 step: 761, loss is 0.041379623115062714\n",
      "epoch: 6 step: 762, loss is 0.05002853646874428\n",
      "epoch: 6 step: 763, loss is 0.06684660911560059\n",
      "epoch: 6 step: 764, loss is 0.40561822056770325\n",
      "epoch: 6 step: 765, loss is 0.09094808995723724\n",
      "epoch: 6 step: 766, loss is 0.017797591164708138\n",
      "epoch: 6 step: 767, loss is 0.050634536892175674\n",
      "epoch: 6 step: 768, loss is 0.044148337095975876\n",
      "epoch: 6 step: 769, loss is 0.000595926190726459\n",
      "epoch: 6 step: 770, loss is 0.0015806644223630428\n",
      "epoch: 6 step: 771, loss is 0.005358981899917126\n",
      "epoch: 6 step: 772, loss is 0.0009692208841443062\n",
      "epoch: 6 step: 773, loss is 0.0007331984234042466\n",
      "epoch: 6 step: 774, loss is 0.00604379503056407\n",
      "epoch: 6 step: 775, loss is 0.0031799424905329943\n",
      "epoch: 6 step: 776, loss is 0.010208125226199627\n",
      "epoch: 6 step: 777, loss is 0.1584097295999527\n",
      "epoch: 6 step: 778, loss is 0.019508061930537224\n",
      "epoch: 6 step: 779, loss is 0.024214286357164383\n",
      "epoch: 6 step: 780, loss is 0.031141692772507668\n",
      "epoch: 6 step: 781, loss is 0.07722381502389908\n",
      "epoch: 6 step: 782, loss is 0.015525078400969505\n",
      "epoch: 6 step: 783, loss is 0.002046475885435939\n",
      "epoch: 6 step: 784, loss is 0.0006070401286706328\n",
      "epoch: 6 step: 785, loss is 0.1305910348892212\n",
      "epoch: 6 step: 786, loss is 0.007712102960795164\n",
      "epoch: 6 step: 787, loss is 0.0019568370189517736\n",
      "epoch: 6 step: 788, loss is 0.0006058553117327392\n",
      "epoch: 6 step: 789, loss is 0.0034861911553889513\n",
      "epoch: 6 step: 790, loss is 0.0029923899564892054\n",
      "epoch: 6 step: 791, loss is 0.0832483321428299\n",
      "epoch: 6 step: 792, loss is 0.012704090215265751\n",
      "epoch: 6 step: 793, loss is 0.10569781064987183\n",
      "epoch: 6 step: 794, loss is 0.005394657142460346\n",
      "epoch: 6 step: 795, loss is 0.09819544851779938\n",
      "epoch: 6 step: 796, loss is 0.000826079398393631\n",
      "epoch: 6 step: 797, loss is 0.001567291677929461\n",
      "epoch: 6 step: 798, loss is 0.009635704569518566\n",
      "epoch: 6 step: 799, loss is 0.010839205235242844\n",
      "epoch: 6 step: 800, loss is 0.0013226037845015526\n",
      "epoch: 6 step: 801, loss is 0.00898528378456831\n",
      "epoch: 6 step: 802, loss is 0.00171477603726089\n",
      "epoch: 6 step: 803, loss is 0.00031769112683832645\n",
      "epoch: 6 step: 804, loss is 0.0717792883515358\n",
      "epoch: 6 step: 805, loss is 0.004209136590361595\n",
      "epoch: 6 step: 806, loss is 8.795671601546928e-05\n",
      "epoch: 6 step: 807, loss is 0.045488979667425156\n",
      "epoch: 6 step: 808, loss is 0.0020647337660193443\n",
      "epoch: 6 step: 809, loss is 0.09537201374769211\n",
      "epoch: 6 step: 810, loss is 0.14870133996009827\n",
      "epoch: 6 step: 811, loss is 0.009120890870690346\n",
      "epoch: 6 step: 812, loss is 0.04331221431493759\n",
      "epoch: 6 step: 813, loss is 0.000614273187238723\n",
      "epoch: 6 step: 814, loss is 9.089663217309862e-05\n",
      "epoch: 6 step: 815, loss is 0.029190493747591972\n",
      "epoch: 6 step: 816, loss is 0.00832231342792511\n",
      "epoch: 6 step: 817, loss is 0.048565447330474854\n",
      "epoch: 6 step: 818, loss is 0.0016033453866839409\n",
      "epoch: 6 step: 819, loss is 0.037386391311883926\n",
      "epoch: 6 step: 820, loss is 0.01660401187837124\n",
      "epoch: 6 step: 821, loss is 0.004659110214561224\n",
      "epoch: 6 step: 822, loss is 0.00048067313036881387\n",
      "epoch: 6 step: 823, loss is 0.060921333730220795\n",
      "epoch: 6 step: 824, loss is 0.02597486414015293\n",
      "epoch: 6 step: 825, loss is 0.04955552518367767\n",
      "epoch: 6 step: 826, loss is 0.00393435126170516\n",
      "epoch: 6 step: 827, loss is 0.012901064939796925\n",
      "epoch: 6 step: 828, loss is 0.001969641540199518\n",
      "epoch: 6 step: 829, loss is 0.0008407251443713903\n",
      "epoch: 6 step: 830, loss is 0.0042366026900708675\n",
      "epoch: 6 step: 831, loss is 0.007302781566977501\n",
      "epoch: 6 step: 832, loss is 0.1392672061920166\n",
      "epoch: 6 step: 833, loss is 0.007573235780000687\n",
      "epoch: 6 step: 834, loss is 0.004463754594326019\n",
      "epoch: 6 step: 835, loss is 0.09449771791696548\n",
      "epoch: 6 step: 836, loss is 0.0043733566999435425\n",
      "epoch: 6 step: 837, loss is 0.034176722168922424\n",
      "epoch: 6 step: 838, loss is 0.001464608940295875\n",
      "epoch: 6 step: 839, loss is 0.023838510736823082\n",
      "epoch: 6 step: 840, loss is 0.0448232963681221\n",
      "epoch: 6 step: 841, loss is 0.0009165877127088606\n",
      "epoch: 6 step: 842, loss is 0.03407672420144081\n",
      "epoch: 6 step: 843, loss is 0.003405672963708639\n",
      "epoch: 6 step: 844, loss is 0.0011616332922130823\n",
      "epoch: 6 step: 845, loss is 0.013994178734719753\n",
      "epoch: 6 step: 846, loss is 0.05891723558306694\n",
      "epoch: 6 step: 847, loss is 0.021112509071826935\n",
      "epoch: 6 step: 848, loss is 0.0009494050755165517\n",
      "epoch: 6 step: 849, loss is 0.003406527219340205\n",
      "epoch: 6 step: 850, loss is 0.003700917586684227\n",
      "epoch: 6 step: 851, loss is 0.004199541173875332\n",
      "epoch: 6 step: 852, loss is 0.019985053688287735\n",
      "epoch: 6 step: 853, loss is 0.0004742420860566199\n",
      "epoch: 6 step: 854, loss is 0.0016820773016661406\n",
      "epoch: 6 step: 855, loss is 0.0007637370144948363\n",
      "epoch: 6 step: 856, loss is 0.07944364845752716\n",
      "epoch: 6 step: 857, loss is 0.004228066187351942\n",
      "epoch: 6 step: 858, loss is 0.002032098826020956\n",
      "epoch: 6 step: 859, loss is 0.0026280381716787815\n",
      "epoch: 6 step: 860, loss is 0.009360830299556255\n",
      "epoch: 6 step: 861, loss is 0.0060471100732684135\n",
      "epoch: 6 step: 862, loss is 0.0027240964118391275\n",
      "epoch: 6 step: 863, loss is 0.0050405231304466724\n",
      "epoch: 6 step: 864, loss is 0.1647792011499405\n",
      "epoch: 6 step: 865, loss is 0.3165340721607208\n",
      "epoch: 6 step: 866, loss is 0.07015173137187958\n",
      "epoch: 6 step: 867, loss is 0.01696634851396084\n",
      "epoch: 6 step: 868, loss is 0.0008370530558750033\n",
      "epoch: 6 step: 869, loss is 0.01564408652484417\n",
      "epoch: 6 step: 870, loss is 0.00098179851192981\n",
      "epoch: 6 step: 871, loss is 0.012824438512325287\n",
      "epoch: 6 step: 872, loss is 0.04260659217834473\n",
      "epoch: 6 step: 873, loss is 0.01943042501807213\n",
      "epoch: 6 step: 874, loss is 0.022434445098042488\n",
      "epoch: 6 step: 875, loss is 0.0007088530110195279\n",
      "epoch: 6 step: 876, loss is 0.0013514983002096415\n",
      "epoch: 6 step: 877, loss is 0.0020250706002116203\n",
      "epoch: 6 step: 878, loss is 0.10385684669017792\n",
      "epoch: 6 step: 879, loss is 0.0003816433309111744\n",
      "epoch: 6 step: 880, loss is 0.01378506701439619\n",
      "epoch: 6 step: 881, loss is 0.011494017206132412\n",
      "epoch: 6 step: 882, loss is 0.03558511286973953\n",
      "epoch: 6 step: 883, loss is 0.007961778901517391\n",
      "epoch: 6 step: 884, loss is 0.01945832371711731\n",
      "epoch: 6 step: 885, loss is 0.03538690134882927\n",
      "epoch: 6 step: 886, loss is 0.002680117031559348\n",
      "epoch: 6 step: 887, loss is 0.0025440717581659555\n",
      "epoch: 6 step: 888, loss is 0.003957986366003752\n",
      "epoch: 6 step: 889, loss is 0.0006408484769053757\n",
      "epoch: 6 step: 890, loss is 0.014622674323618412\n",
      "epoch: 6 step: 891, loss is 0.004778758157044649\n",
      "epoch: 6 step: 892, loss is 0.0013441289775073528\n",
      "epoch: 6 step: 893, loss is 0.00276487460359931\n",
      "epoch: 6 step: 894, loss is 0.00133343820925802\n",
      "epoch: 6 step: 895, loss is 0.02250213921070099\n",
      "epoch: 6 step: 896, loss is 0.02735508419573307\n",
      "epoch: 6 step: 897, loss is 0.00040108710527420044\n",
      "epoch: 6 step: 898, loss is 0.0787864476442337\n",
      "epoch: 6 step: 899, loss is 0.02812355011701584\n",
      "epoch: 6 step: 900, loss is 0.04686331748962402\n",
      "epoch: 6 step: 901, loss is 0.026920150965452194\n",
      "epoch: 6 step: 902, loss is 0.0777047723531723\n",
      "epoch: 6 step: 903, loss is 0.0040990556590259075\n",
      "epoch: 6 step: 904, loss is 0.0004793064435943961\n",
      "epoch: 6 step: 905, loss is 0.03844544291496277\n",
      "epoch: 6 step: 906, loss is 0.082788847386837\n",
      "epoch: 6 step: 907, loss is 0.0014417842030525208\n",
      "epoch: 6 step: 908, loss is 0.028780611231923103\n",
      "epoch: 6 step: 909, loss is 0.00046839952119626105\n",
      "epoch: 6 step: 910, loss is 0.0025164580438286066\n",
      "epoch: 6 step: 911, loss is 0.00039122308953665197\n",
      "epoch: 6 step: 912, loss is 0.000231373225688003\n",
      "epoch: 6 step: 913, loss is 0.01638924516737461\n",
      "epoch: 6 step: 914, loss is 0.0024392087943851948\n",
      "epoch: 6 step: 915, loss is 0.006808031816035509\n",
      "epoch: 6 step: 916, loss is 0.0003368512843735516\n",
      "epoch: 6 step: 917, loss is 0.09984338283538818\n",
      "epoch: 6 step: 918, loss is 0.0035287828650325537\n",
      "epoch: 6 step: 919, loss is 0.01456115860491991\n",
      "epoch: 6 step: 920, loss is 0.0004387717053759843\n",
      "epoch: 6 step: 921, loss is 0.008213994093239307\n",
      "epoch: 6 step: 922, loss is 0.0005226992070674896\n",
      "epoch: 6 step: 923, loss is 0.0015108662191778421\n",
      "epoch: 6 step: 924, loss is 0.02277880348265171\n",
      "epoch: 6 step: 925, loss is 0.004918972030282021\n",
      "epoch: 6 step: 926, loss is 0.0003739662643056363\n",
      "epoch: 6 step: 927, loss is 0.0001880722411442548\n",
      "epoch: 6 step: 928, loss is 0.03569197654724121\n",
      "epoch: 6 step: 929, loss is 0.00027100503211840987\n",
      "epoch: 6 step: 930, loss is 0.0914272665977478\n",
      "epoch: 6 step: 931, loss is 0.006846827920526266\n",
      "epoch: 6 step: 932, loss is 0.0013034989824518561\n",
      "epoch: 6 step: 933, loss is 0.1801517754793167\n",
      "epoch: 6 step: 934, loss is 0.00465103005990386\n",
      "epoch: 6 step: 935, loss is 0.001322746742516756\n",
      "epoch: 6 step: 936, loss is 0.007350703235715628\n",
      "epoch: 6 step: 937, loss is 0.09351620078086853\n",
      "epoch: 6 step: 938, loss is 0.02975369803607464\n",
      "epoch: 6 step: 939, loss is 0.010720283724367619\n",
      "epoch: 6 step: 940, loss is 0.02332058921456337\n",
      "epoch: 6 step: 941, loss is 0.001037038047797978\n",
      "epoch: 6 step: 942, loss is 0.002975176554173231\n",
      "epoch: 6 step: 943, loss is 0.001796357799321413\n",
      "epoch: 6 step: 944, loss is 0.016160206869244576\n",
      "epoch: 6 step: 945, loss is 0.003959096968173981\n",
      "epoch: 6 step: 946, loss is 0.00019289425108581781\n",
      "epoch: 6 step: 947, loss is 0.006212771870195866\n",
      "epoch: 6 step: 948, loss is 0.0047376519069075584\n",
      "epoch: 6 step: 949, loss is 0.004150704015046358\n",
      "epoch: 6 step: 950, loss is 0.0053370241075754166\n",
      "epoch: 6 step: 951, loss is 0.011403806507587433\n",
      "epoch: 6 step: 952, loss is 0.032238077372312546\n",
      "epoch: 6 step: 953, loss is 0.011437197215855122\n",
      "epoch: 6 step: 954, loss is 0.0035198787227272987\n",
      "epoch: 6 step: 955, loss is 0.06036537513136864\n",
      "epoch: 6 step: 956, loss is 0.01069780346006155\n",
      "epoch: 6 step: 957, loss is 0.01711263321340084\n",
      "epoch: 6 step: 958, loss is 0.00019236425578128546\n",
      "epoch: 6 step: 959, loss is 0.0009499675361439586\n",
      "epoch: 6 step: 960, loss is 0.0011510007316246629\n",
      "epoch: 6 step: 961, loss is 0.008297108113765717\n",
      "epoch: 6 step: 962, loss is 0.000126726008602418\n",
      "epoch: 6 step: 963, loss is 0.0026128317695111036\n",
      "epoch: 6 step: 964, loss is 0.030512899160385132\n",
      "epoch: 6 step: 965, loss is 0.021228302270174026\n",
      "epoch: 6 step: 966, loss is 0.03666843846440315\n",
      "epoch: 6 step: 967, loss is 0.004930301569402218\n",
      "epoch: 6 step: 968, loss is 0.02307233400642872\n",
      "epoch: 6 step: 969, loss is 0.012588627636432648\n",
      "epoch: 6 step: 970, loss is 0.0019537783227860928\n",
      "epoch: 6 step: 971, loss is 0.0011682556942105293\n",
      "epoch: 6 step: 972, loss is 0.04422638192772865\n",
      "epoch: 6 step: 973, loss is 0.002266117837280035\n",
      "epoch: 6 step: 974, loss is 0.003214881056919694\n",
      "epoch: 6 step: 975, loss is 0.03148578479886055\n",
      "epoch: 6 step: 976, loss is 0.24299350380897522\n",
      "epoch: 6 step: 977, loss is 0.0024709561839699745\n",
      "epoch: 6 step: 978, loss is 0.004342797677963972\n",
      "epoch: 6 step: 979, loss is 0.20483072102069855\n",
      "epoch: 6 step: 980, loss is 0.0289082620292902\n",
      "epoch: 6 step: 981, loss is 0.0009572748094797134\n",
      "epoch: 6 step: 982, loss is 0.0530620813369751\n",
      "epoch: 6 step: 983, loss is 0.050335172563791275\n",
      "epoch: 6 step: 984, loss is 0.0009873618837445974\n",
      "epoch: 6 step: 985, loss is 0.006533019244670868\n",
      "epoch: 6 step: 986, loss is 0.0006304543348960578\n",
      "epoch: 6 step: 987, loss is 0.005374441854655743\n",
      "epoch: 6 step: 988, loss is 0.0216219462454319\n",
      "epoch: 6 step: 989, loss is 0.014242063276469707\n",
      "epoch: 6 step: 990, loss is 0.0013856415171176195\n",
      "epoch: 6 step: 991, loss is 0.06497801840305328\n",
      "epoch: 6 step: 992, loss is 0.007179702166467905\n",
      "epoch: 6 step: 993, loss is 0.006459432188421488\n",
      "epoch: 6 step: 994, loss is 0.0547257736325264\n",
      "epoch: 6 step: 995, loss is 0.00014750825357623398\n",
      "epoch: 6 step: 996, loss is 0.0023087039589881897\n",
      "epoch: 6 step: 997, loss is 0.0024883935693651438\n",
      "epoch: 6 step: 998, loss is 0.00385881494730711\n",
      "epoch: 6 step: 999, loss is 0.0007169620948843658\n",
      "epoch: 6 step: 1000, loss is 0.010241212323307991\n",
      "epoch: 6 step: 1001, loss is 0.0017458903603255749\n",
      "epoch: 6 step: 1002, loss is 0.07299122959375381\n",
      "epoch: 6 step: 1003, loss is 0.024531004950404167\n",
      "epoch: 6 step: 1004, loss is 0.059980608522892\n",
      "epoch: 6 step: 1005, loss is 0.0017622043378651142\n",
      "epoch: 6 step: 1006, loss is 0.04104248434305191\n",
      "epoch: 6 step: 1007, loss is 0.009314094670116901\n",
      "epoch: 6 step: 1008, loss is 0.08707460016012192\n",
      "epoch: 6 step: 1009, loss is 0.011399623937904835\n",
      "epoch: 6 step: 1010, loss is 0.0322747640311718\n",
      "epoch: 6 step: 1011, loss is 0.02828780747950077\n",
      "epoch: 6 step: 1012, loss is 0.05031409487128258\n",
      "epoch: 6 step: 1013, loss is 0.0016982894157990813\n",
      "epoch: 6 step: 1014, loss is 0.0009810631163418293\n",
      "epoch: 6 step: 1015, loss is 0.050496906042099\n",
      "epoch: 6 step: 1016, loss is 0.0010183460544794798\n",
      "epoch: 6 step: 1017, loss is 0.0071786800399422646\n",
      "epoch: 6 step: 1018, loss is 0.0012834896333515644\n",
      "epoch: 6 step: 1019, loss is 0.007852071896195412\n",
      "epoch: 6 step: 1020, loss is 0.003741901833564043\n",
      "epoch: 6 step: 1021, loss is 0.0019217233639210463\n",
      "epoch: 6 step: 1022, loss is 0.058608584105968475\n",
      "epoch: 6 step: 1023, loss is 5.117351247463375e-05\n",
      "epoch: 6 step: 1024, loss is 0.0001810650574043393\n",
      "epoch: 6 step: 1025, loss is 0.03348960727453232\n",
      "epoch: 6 step: 1026, loss is 0.2056884765625\n",
      "epoch: 6 step: 1027, loss is 0.008761094883084297\n",
      "epoch: 6 step: 1028, loss is 0.004243252798914909\n",
      "epoch: 6 step: 1029, loss is 0.002268159994855523\n",
      "epoch: 6 step: 1030, loss is 0.025456110015511513\n",
      "epoch: 6 step: 1031, loss is 0.001353951869532466\n",
      "epoch: 6 step: 1032, loss is 0.0011463462142273784\n",
      "epoch: 6 step: 1033, loss is 0.05780594050884247\n",
      "epoch: 6 step: 1034, loss is 0.04109520837664604\n",
      "epoch: 6 step: 1035, loss is 1.3269779628899414e-05\n",
      "epoch: 6 step: 1036, loss is 0.00030634505674242973\n",
      "epoch: 6 step: 1037, loss is 0.012035425752401352\n",
      "epoch: 6 step: 1038, loss is 0.08571132272481918\n",
      "epoch: 6 step: 1039, loss is 0.11466926336288452\n",
      "epoch: 6 step: 1040, loss is 0.013649815693497658\n",
      "epoch: 6 step: 1041, loss is 0.06231573596596718\n",
      "epoch: 6 step: 1042, loss is 0.004355672746896744\n",
      "epoch: 6 step: 1043, loss is 0.00047717575216665864\n",
      "epoch: 6 step: 1044, loss is 0.0005012307083234191\n",
      "epoch: 6 step: 1045, loss is 0.01418660394847393\n",
      "epoch: 6 step: 1046, loss is 0.040240149945020676\n",
      "epoch: 6 step: 1047, loss is 0.03670221567153931\n",
      "epoch: 6 step: 1048, loss is 0.11267703771591187\n",
      "epoch: 6 step: 1049, loss is 0.0012301421957090497\n",
      "epoch: 6 step: 1050, loss is 0.001756858779117465\n",
      "epoch: 6 step: 1051, loss is 0.0031675738282501698\n",
      "epoch: 6 step: 1052, loss is 0.005618546158075333\n",
      "epoch: 6 step: 1053, loss is 0.0010632401099428535\n",
      "epoch: 6 step: 1054, loss is 0.0002935561060439795\n",
      "epoch: 6 step: 1055, loss is 0.13030147552490234\n",
      "epoch: 6 step: 1056, loss is 0.020349405705928802\n",
      "epoch: 6 step: 1057, loss is 0.0011475851060822606\n",
      "epoch: 6 step: 1058, loss is 0.038300592452287674\n",
      "epoch: 6 step: 1059, loss is 0.014473704621195793\n",
      "epoch: 6 step: 1060, loss is 0.00010995068441843614\n",
      "epoch: 6 step: 1061, loss is 0.04169490933418274\n",
      "epoch: 6 step: 1062, loss is 0.0074164182879030704\n",
      "epoch: 6 step: 1063, loss is 0.0008374321041628718\n",
      "epoch: 6 step: 1064, loss is 0.028289638459682465\n",
      "epoch: 6 step: 1065, loss is 0.017349690198898315\n",
      "epoch: 6 step: 1066, loss is 0.0005536865792237222\n",
      "epoch: 6 step: 1067, loss is 0.00046860636211931705\n",
      "epoch: 6 step: 1068, loss is 0.0004466924292501062\n",
      "epoch: 6 step: 1069, loss is 0.0731392353773117\n",
      "epoch: 6 step: 1070, loss is 0.00019687038729898632\n",
      "epoch: 6 step: 1071, loss is 0.000887958100065589\n",
      "epoch: 6 step: 1072, loss is 0.0013539076317101717\n",
      "epoch: 6 step: 1073, loss is 0.003243540646508336\n",
      "epoch: 6 step: 1074, loss is 0.0854589194059372\n",
      "epoch: 6 step: 1075, loss is 0.011017439886927605\n",
      "epoch: 6 step: 1076, loss is 0.0012328323209658265\n",
      "epoch: 6 step: 1077, loss is 0.00715005723759532\n",
      "epoch: 6 step: 1078, loss is 0.0008399346261285245\n",
      "epoch: 6 step: 1079, loss is 0.008262344636023045\n",
      "epoch: 6 step: 1080, loss is 0.04068395495414734\n",
      "epoch: 6 step: 1081, loss is 0.1458643674850464\n",
      "epoch: 6 step: 1082, loss is 0.0724240094423294\n",
      "epoch: 6 step: 1083, loss is 0.028134973719716072\n",
      "epoch: 6 step: 1084, loss is 0.0009025008184835315\n",
      "epoch: 6 step: 1085, loss is 0.038089413195848465\n",
      "epoch: 6 step: 1086, loss is 0.0024349787272512913\n",
      "epoch: 6 step: 1087, loss is 0.05872791260480881\n",
      "epoch: 6 step: 1088, loss is 0.02893935889005661\n",
      "epoch: 6 step: 1089, loss is 0.0001418508472852409\n",
      "epoch: 6 step: 1090, loss is 0.00021637743338942528\n",
      "epoch: 6 step: 1091, loss is 0.0010301412548869848\n",
      "epoch: 6 step: 1092, loss is 0.0014124949229881167\n",
      "epoch: 6 step: 1093, loss is 0.09030644595623016\n",
      "epoch: 6 step: 1094, loss is 0.002091215457767248\n",
      "epoch: 6 step: 1095, loss is 0.026812247931957245\n",
      "epoch: 6 step: 1096, loss is 0.00047203543363139033\n",
      "epoch: 6 step: 1097, loss is 0.05752183496952057\n",
      "epoch: 6 step: 1098, loss is 0.007383132353425026\n",
      "epoch: 6 step: 1099, loss is 0.002557340543717146\n",
      "epoch: 6 step: 1100, loss is 0.034168872982263565\n",
      "epoch: 6 step: 1101, loss is 0.0014944631839171052\n",
      "epoch: 6 step: 1102, loss is 0.0057254573330283165\n",
      "epoch: 6 step: 1103, loss is 0.0042591942474246025\n",
      "epoch: 6 step: 1104, loss is 0.022196145728230476\n",
      "epoch: 6 step: 1105, loss is 0.3129904866218567\n",
      "epoch: 6 step: 1106, loss is 0.009120197966694832\n",
      "epoch: 6 step: 1107, loss is 0.003511162241920829\n",
      "epoch: 6 step: 1108, loss is 0.004156538285315037\n",
      "epoch: 6 step: 1109, loss is 0.0004431679262779653\n",
      "epoch: 6 step: 1110, loss is 0.004221695940941572\n",
      "epoch: 6 step: 1111, loss is 0.0007958156638778746\n",
      "epoch: 6 step: 1112, loss is 0.0990634337067604\n",
      "epoch: 6 step: 1113, loss is 0.0007658383110538125\n",
      "epoch: 6 step: 1114, loss is 0.002122265985235572\n",
      "epoch: 6 step: 1115, loss is 0.01760822907090187\n",
      "epoch: 6 step: 1116, loss is 0.001124136964790523\n",
      "epoch: 6 step: 1117, loss is 0.0006084904889576137\n",
      "epoch: 6 step: 1118, loss is 0.0004920570063404739\n",
      "epoch: 6 step: 1119, loss is 0.006275184918195009\n",
      "epoch: 6 step: 1120, loss is 0.000391740701161325\n",
      "epoch: 6 step: 1121, loss is 0.0009903799509629607\n",
      "epoch: 6 step: 1122, loss is 0.0017861351370811462\n",
      "epoch: 6 step: 1123, loss is 0.0007801000610925257\n",
      "epoch: 6 step: 1124, loss is 0.006643483880907297\n",
      "epoch: 6 step: 1125, loss is 0.0028296702075749636\n",
      "epoch: 6 step: 1126, loss is 0.0059578558430075645\n",
      "epoch: 6 step: 1127, loss is 0.012293740175664425\n",
      "epoch: 6 step: 1128, loss is 0.02020011842250824\n",
      "epoch: 6 step: 1129, loss is 0.0007674774969927967\n",
      "epoch: 6 step: 1130, loss is 0.001357529079541564\n",
      "epoch: 6 step: 1131, loss is 0.17128852009773254\n",
      "epoch: 6 step: 1132, loss is 0.003386429976671934\n",
      "epoch: 6 step: 1133, loss is 0.0035447892732918262\n",
      "epoch: 6 step: 1134, loss is 0.006999148055911064\n",
      "epoch: 6 step: 1135, loss is 0.015106630511581898\n",
      "epoch: 6 step: 1136, loss is 0.11206652224063873\n",
      "epoch: 6 step: 1137, loss is 0.0454903282225132\n",
      "epoch: 6 step: 1138, loss is 0.014640364795923233\n",
      "epoch: 6 step: 1139, loss is 0.00032019452191889286\n",
      "epoch: 6 step: 1140, loss is 0.019718710333108902\n",
      "epoch: 6 step: 1141, loss is 0.10146994143724442\n",
      "epoch: 6 step: 1142, loss is 0.056602079421281815\n",
      "epoch: 6 step: 1143, loss is 0.005109655205160379\n",
      "epoch: 6 step: 1144, loss is 0.0028170712757855654\n",
      "epoch: 6 step: 1145, loss is 0.08681681752204895\n",
      "epoch: 6 step: 1146, loss is 0.010065959766507149\n",
      "epoch: 6 step: 1147, loss is 0.01752978377044201\n",
      "epoch: 6 step: 1148, loss is 0.007247776724398136\n",
      "epoch: 6 step: 1149, loss is 0.0020811313297599554\n",
      "epoch: 6 step: 1150, loss is 0.0042635491117835045\n",
      "epoch: 6 step: 1151, loss is 0.005290124099701643\n",
      "epoch: 6 step: 1152, loss is 0.0005266030202619731\n",
      "epoch: 6 step: 1153, loss is 0.00044374927529133856\n",
      "epoch: 6 step: 1154, loss is 0.007893065921962261\n",
      "epoch: 6 step: 1155, loss is 0.011337007395923138\n",
      "epoch: 6 step: 1156, loss is 0.02260987088084221\n",
      "epoch: 6 step: 1157, loss is 0.0059157139621675014\n",
      "epoch: 6 step: 1158, loss is 0.024032335728406906\n",
      "epoch: 6 step: 1159, loss is 0.021820489317178726\n",
      "epoch: 6 step: 1160, loss is 0.0004980755038559437\n",
      "epoch: 6 step: 1161, loss is 0.07792998105287552\n",
      "epoch: 6 step: 1162, loss is 0.0021030395291745663\n",
      "epoch: 6 step: 1163, loss is 0.0003652116283774376\n",
      "epoch: 6 step: 1164, loss is 0.006581214722245932\n",
      "epoch: 6 step: 1165, loss is 0.03401920571923256\n",
      "epoch: 6 step: 1166, loss is 0.0035789755638688803\n",
      "epoch: 6 step: 1167, loss is 0.00646541453897953\n",
      "epoch: 6 step: 1168, loss is 0.12530972063541412\n",
      "epoch: 6 step: 1169, loss is 0.3099866509437561\n",
      "epoch: 6 step: 1170, loss is 0.00017810534336604178\n",
      "epoch: 6 step: 1171, loss is 0.00021170357649680227\n",
      "epoch: 6 step: 1172, loss is 0.0011567458277568221\n",
      "epoch: 6 step: 1173, loss is 0.0014967689057812095\n",
      "epoch: 6 step: 1174, loss is 0.006904018111526966\n",
      "epoch: 6 step: 1175, loss is 8.362274820683524e-05\n",
      "epoch: 6 step: 1176, loss is 0.017552169039845467\n",
      "epoch: 6 step: 1177, loss is 0.02832331694662571\n",
      "epoch: 6 step: 1178, loss is 0.07045003771781921\n",
      "epoch: 6 step: 1179, loss is 0.2083231657743454\n",
      "epoch: 6 step: 1180, loss is 0.0065132202580571175\n",
      "epoch: 6 step: 1181, loss is 0.11149460077285767\n",
      "epoch: 6 step: 1182, loss is 0.001428818330168724\n",
      "epoch: 6 step: 1183, loss is 0.0022484962828457355\n",
      "epoch: 6 step: 1184, loss is 0.0006670494913123548\n",
      "epoch: 6 step: 1185, loss is 0.0016906469827517867\n",
      "epoch: 6 step: 1186, loss is 0.0006126272492110729\n",
      "epoch: 6 step: 1187, loss is 0.000404419464757666\n",
      "epoch: 6 step: 1188, loss is 0.0002885274589061737\n",
      "epoch: 6 step: 1189, loss is 0.10952410846948624\n",
      "epoch: 6 step: 1190, loss is 0.0011996942339465022\n",
      "epoch: 6 step: 1191, loss is 0.032872606068849564\n",
      "epoch: 6 step: 1192, loss is 0.011986717581748962\n",
      "epoch: 6 step: 1193, loss is 0.002708881162106991\n",
      "epoch: 6 step: 1194, loss is 0.07608143240213394\n",
      "epoch: 6 step: 1195, loss is 0.005139218643307686\n",
      "epoch: 6 step: 1196, loss is 0.01614478789269924\n",
      "epoch: 6 step: 1197, loss is 0.06037180498242378\n",
      "epoch: 6 step: 1198, loss is 0.15601003170013428\n",
      "epoch: 6 step: 1199, loss is 0.0019181732786819339\n",
      "epoch: 6 step: 1200, loss is 0.010658195242285728\n",
      "epoch: 6 step: 1201, loss is 0.0003310773754492402\n",
      "epoch: 6 step: 1202, loss is 0.07176211476325989\n",
      "epoch: 6 step: 1203, loss is 0.0007561012171208858\n",
      "epoch: 6 step: 1204, loss is 0.005436304025352001\n",
      "epoch: 6 step: 1205, loss is 0.0007792600663378835\n",
      "epoch: 6 step: 1206, loss is 0.08973177522420883\n",
      "epoch: 6 step: 1207, loss is 0.00014380081847775728\n",
      "epoch: 6 step: 1208, loss is 0.003604662138968706\n",
      "epoch: 6 step: 1209, loss is 0.0009923933539539576\n",
      "epoch: 6 step: 1210, loss is 0.003805063432082534\n",
      "epoch: 6 step: 1211, loss is 0.05032102018594742\n",
      "epoch: 6 step: 1212, loss is 0.0014818782219663262\n",
      "epoch: 6 step: 1213, loss is 0.13339032232761383\n",
      "epoch: 6 step: 1214, loss is 0.0010281394934281707\n",
      "epoch: 6 step: 1215, loss is 0.0016173403710126877\n",
      "epoch: 6 step: 1216, loss is 0.005531466566026211\n",
      "epoch: 6 step: 1217, loss is 0.012785376980900764\n",
      "epoch: 6 step: 1218, loss is 0.01357949897646904\n",
      "epoch: 6 step: 1219, loss is 0.0048557608388364315\n",
      "epoch: 6 step: 1220, loss is 0.020739205181598663\n",
      "epoch: 6 step: 1221, loss is 0.020163485780358315\n",
      "epoch: 6 step: 1222, loss is 0.0010368471266701818\n",
      "epoch: 6 step: 1223, loss is 0.0002282680943608284\n",
      "epoch: 6 step: 1224, loss is 0.003559133969247341\n",
      "epoch: 6 step: 1225, loss is 0.0020626368932425976\n",
      "epoch: 6 step: 1226, loss is 0.0007547430577687919\n",
      "epoch: 6 step: 1227, loss is 0.08680886775255203\n",
      "epoch: 6 step: 1228, loss is 0.012598703615367413\n",
      "epoch: 6 step: 1229, loss is 0.03208022564649582\n",
      "epoch: 6 step: 1230, loss is 0.00781908631324768\n",
      "epoch: 6 step: 1231, loss is 0.29292380809783936\n",
      "epoch: 6 step: 1232, loss is 0.04316488280892372\n",
      "epoch: 6 step: 1233, loss is 0.010671485215425491\n",
      "epoch: 6 step: 1234, loss is 0.06308873742818832\n",
      "epoch: 6 step: 1235, loss is 0.0019205317366868258\n",
      "epoch: 6 step: 1236, loss is 0.007725451607257128\n",
      "epoch: 6 step: 1237, loss is 0.2291381061077118\n",
      "epoch: 6 step: 1238, loss is 0.16204218566417694\n",
      "epoch: 6 step: 1239, loss is 0.00697324750944972\n",
      "epoch: 6 step: 1240, loss is 0.026488464325666428\n",
      "epoch: 6 step: 1241, loss is 0.0012221031356602907\n",
      "epoch: 6 step: 1242, loss is 0.05990852415561676\n",
      "epoch: 6 step: 1243, loss is 0.08987227827310562\n",
      "epoch: 6 step: 1244, loss is 0.17192339897155762\n",
      "epoch: 6 step: 1245, loss is 0.002902548760175705\n",
      "epoch: 6 step: 1246, loss is 0.0013081248616799712\n",
      "epoch: 6 step: 1247, loss is 0.00814781803637743\n",
      "epoch: 6 step: 1248, loss is 0.0016678820829838514\n",
      "epoch: 6 step: 1249, loss is 0.004940351936966181\n",
      "epoch: 6 step: 1250, loss is 0.005872739478945732\n",
      "epoch: 6 step: 1251, loss is 0.009119052439928055\n",
      "epoch: 6 step: 1252, loss is 0.0009814428631216288\n",
      "epoch: 6 step: 1253, loss is 0.0006781579577364028\n",
      "epoch: 6 step: 1254, loss is 0.052173689007759094\n",
      "epoch: 6 step: 1255, loss is 0.001047875382937491\n",
      "epoch: 6 step: 1256, loss is 0.04875640943646431\n",
      "epoch: 6 step: 1257, loss is 0.002811289159581065\n",
      "epoch: 6 step: 1258, loss is 0.19994114339351654\n",
      "epoch: 6 step: 1259, loss is 0.13438798487186432\n",
      "epoch: 6 step: 1260, loss is 0.0027648855466395617\n",
      "epoch: 6 step: 1261, loss is 0.05197809636592865\n",
      "epoch: 6 step: 1262, loss is 0.11272728443145752\n",
      "epoch: 6 step: 1263, loss is 0.00110564602073282\n",
      "epoch: 6 step: 1264, loss is 0.034173015505075455\n",
      "epoch: 6 step: 1265, loss is 0.0012530488893389702\n",
      "epoch: 6 step: 1266, loss is 0.0017791400896385312\n",
      "epoch: 6 step: 1267, loss is 0.032965827733278275\n",
      "epoch: 6 step: 1268, loss is 0.00839871447533369\n",
      "epoch: 6 step: 1269, loss is 0.009511999785900116\n",
      "epoch: 6 step: 1270, loss is 0.0010129237780347466\n",
      "epoch: 6 step: 1271, loss is 0.1244189515709877\n",
      "epoch: 6 step: 1272, loss is 0.001465355046093464\n",
      "epoch: 6 step: 1273, loss is 0.05618558079004288\n",
      "epoch: 6 step: 1274, loss is 0.0006581568741239607\n",
      "epoch: 6 step: 1275, loss is 0.03273918107151985\n",
      "epoch: 6 step: 1276, loss is 0.0048852660693228245\n",
      "epoch: 6 step: 1277, loss is 0.0008278379682451487\n",
      "epoch: 6 step: 1278, loss is 0.015419119969010353\n",
      "epoch: 6 step: 1279, loss is 0.014686721377074718\n",
      "epoch: 6 step: 1280, loss is 0.03265064209699631\n",
      "epoch: 6 step: 1281, loss is 0.02274550125002861\n",
      "epoch: 6 step: 1282, loss is 0.008012216538190842\n",
      "epoch: 6 step: 1283, loss is 0.0018162084743380547\n",
      "epoch: 6 step: 1284, loss is 0.0030400187242776155\n",
      "epoch: 6 step: 1285, loss is 0.0008676369907334447\n",
      "epoch: 6 step: 1286, loss is 0.0004804384952876717\n",
      "epoch: 6 step: 1287, loss is 0.005846932530403137\n",
      "epoch: 6 step: 1288, loss is 0.0017795109888538718\n",
      "epoch: 6 step: 1289, loss is 0.0011767438845708966\n",
      "epoch: 6 step: 1290, loss is 0.0001043829252012074\n",
      "epoch: 6 step: 1291, loss is 0.07280069589614868\n",
      "epoch: 6 step: 1292, loss is 0.026771994307637215\n",
      "epoch: 6 step: 1293, loss is 0.0006254123873077333\n",
      "epoch: 6 step: 1294, loss is 0.0011643304023891687\n",
      "epoch: 6 step: 1295, loss is 0.1864522248506546\n",
      "epoch: 6 step: 1296, loss is 0.021526670083403587\n",
      "epoch: 6 step: 1297, loss is 0.010937909595668316\n",
      "epoch: 6 step: 1298, loss is 0.03130108490586281\n",
      "epoch: 6 step: 1299, loss is 0.009649965912103653\n",
      "epoch: 6 step: 1300, loss is 0.002629221649840474\n",
      "epoch: 6 step: 1301, loss is 0.0014020793605595827\n",
      "epoch: 6 step: 1302, loss is 0.04679782688617706\n",
      "epoch: 6 step: 1303, loss is 0.006378555204719305\n",
      "epoch: 6 step: 1304, loss is 0.0020310436375439167\n",
      "epoch: 6 step: 1305, loss is 0.002939598634839058\n",
      "epoch: 6 step: 1306, loss is 0.002384418621659279\n",
      "epoch: 6 step: 1307, loss is 0.07975839823484421\n",
      "epoch: 6 step: 1308, loss is 0.062039732933044434\n",
      "epoch: 6 step: 1309, loss is 0.005389129277318716\n",
      "epoch: 6 step: 1310, loss is 0.002835647203028202\n",
      "epoch: 6 step: 1311, loss is 0.003365955548360944\n",
      "epoch: 6 step: 1312, loss is 0.022933287546038628\n",
      "epoch: 6 step: 1313, loss is 0.0007033452857285738\n",
      "epoch: 6 step: 1314, loss is 0.0005126133328303695\n",
      "epoch: 6 step: 1315, loss is 0.005292178131639957\n",
      "epoch: 6 step: 1316, loss is 0.019170112907886505\n",
      "epoch: 6 step: 1317, loss is 0.016574332490563393\n",
      "epoch: 6 step: 1318, loss is 0.006095281336456537\n",
      "epoch: 6 step: 1319, loss is 0.02475554868578911\n",
      "epoch: 6 step: 1320, loss is 0.004412366077303886\n",
      "epoch: 6 step: 1321, loss is 0.00031246914295479655\n",
      "epoch: 6 step: 1322, loss is 0.0008425954729318619\n",
      "epoch: 6 step: 1323, loss is 0.0009051344823092222\n",
      "epoch: 6 step: 1324, loss is 0.043628279119729996\n",
      "epoch: 6 step: 1325, loss is 0.003997967578470707\n",
      "epoch: 6 step: 1326, loss is 0.0012562774354591966\n",
      "epoch: 6 step: 1327, loss is 0.03824573755264282\n",
      "epoch: 6 step: 1328, loss is 0.1031278744339943\n",
      "epoch: 6 step: 1329, loss is 0.0013217647792771459\n",
      "epoch: 6 step: 1330, loss is 0.0009801333071663976\n",
      "epoch: 6 step: 1331, loss is 0.03189242631196976\n",
      "epoch: 6 step: 1332, loss is 0.0070170010440051556\n",
      "epoch: 6 step: 1333, loss is 0.005511801689863205\n",
      "epoch: 6 step: 1334, loss is 0.008047598414123058\n",
      "epoch: 6 step: 1335, loss is 0.013325301930308342\n",
      "epoch: 6 step: 1336, loss is 0.02096673846244812\n",
      "epoch: 6 step: 1337, loss is 0.06313039362430573\n",
      "epoch: 6 step: 1338, loss is 0.009362317621707916\n",
      "epoch: 6 step: 1339, loss is 0.0011902995174750686\n",
      "epoch: 6 step: 1340, loss is 0.08333814889192581\n",
      "epoch: 6 step: 1341, loss is 0.03344862163066864\n",
      "epoch: 6 step: 1342, loss is 0.0001540578668937087\n",
      "epoch: 6 step: 1343, loss is 0.03253484144806862\n",
      "epoch: 6 step: 1344, loss is 0.0021194990258663893\n",
      "epoch: 6 step: 1345, loss is 0.18514633178710938\n",
      "epoch: 6 step: 1346, loss is 0.02895989827811718\n",
      "epoch: 6 step: 1347, loss is 0.012008205987513065\n",
      "epoch: 6 step: 1348, loss is 0.114285409450531\n",
      "epoch: 6 step: 1349, loss is 0.012658673338592052\n",
      "epoch: 6 step: 1350, loss is 0.03804527223110199\n",
      "epoch: 6 step: 1351, loss is 0.0005321134813129902\n",
      "epoch: 6 step: 1352, loss is 0.009317632764577866\n",
      "epoch: 6 step: 1353, loss is 0.001765677472576499\n",
      "epoch: 6 step: 1354, loss is 0.008552761748433113\n",
      "epoch: 6 step: 1355, loss is 0.00026493050972931087\n",
      "epoch: 6 step: 1356, loss is 0.0038702921010553837\n",
      "epoch: 6 step: 1357, loss is 0.013156716711819172\n",
      "epoch: 6 step: 1358, loss is 0.15622320771217346\n",
      "epoch: 6 step: 1359, loss is 0.0005184973124414682\n",
      "epoch: 6 step: 1360, loss is 0.04151079058647156\n",
      "epoch: 6 step: 1361, loss is 0.0008864810806699097\n",
      "epoch: 6 step: 1362, loss is 0.0007812047260813415\n",
      "epoch: 6 step: 1363, loss is 0.02603861875832081\n",
      "epoch: 6 step: 1364, loss is 0.007130678277462721\n",
      "epoch: 6 step: 1365, loss is 0.00023661332670599222\n",
      "epoch: 6 step: 1366, loss is 0.005142264999449253\n",
      "epoch: 6 step: 1367, loss is 0.0028116449248045683\n",
      "epoch: 6 step: 1368, loss is 0.0007746710325591266\n",
      "epoch: 6 step: 1369, loss is 0.06588207930326462\n",
      "epoch: 6 step: 1370, loss is 0.0019925376400351524\n",
      "epoch: 6 step: 1371, loss is 0.007923976518213749\n",
      "epoch: 6 step: 1372, loss is 0.01536356471478939\n",
      "epoch: 6 step: 1373, loss is 0.08411596715450287\n",
      "epoch: 6 step: 1374, loss is 0.0042905425652861595\n",
      "epoch: 6 step: 1375, loss is 0.016192059963941574\n",
      "epoch: 6 step: 1376, loss is 0.009570369496941566\n",
      "epoch: 6 step: 1377, loss is 0.001985101029276848\n",
      "epoch: 6 step: 1378, loss is 0.0797368511557579\n",
      "epoch: 6 step: 1379, loss is 0.08995014429092407\n",
      "epoch: 6 step: 1380, loss is 0.0008810258586890996\n",
      "epoch: 6 step: 1381, loss is 0.0009013155940920115\n",
      "epoch: 6 step: 1382, loss is 0.02667521871626377\n",
      "epoch: 6 step: 1383, loss is 0.03708581626415253\n",
      "epoch: 6 step: 1384, loss is 0.0005924221477471292\n",
      "epoch: 6 step: 1385, loss is 0.016663258895277977\n",
      "epoch: 6 step: 1386, loss is 0.002097583841532469\n",
      "epoch: 6 step: 1387, loss is 0.009170128032565117\n",
      "epoch: 6 step: 1388, loss is 0.00010116762132383883\n",
      "epoch: 6 step: 1389, loss is 0.0021857083775103092\n",
      "epoch: 6 step: 1390, loss is 0.009330707602202892\n",
      "epoch: 6 step: 1391, loss is 0.0022162755485624075\n",
      "epoch: 6 step: 1392, loss is 0.0011816387996077538\n",
      "epoch: 6 step: 1393, loss is 0.004442807752639055\n",
      "epoch: 6 step: 1394, loss is 0.0009020922007039189\n",
      "epoch: 6 step: 1395, loss is 0.0010123004904016852\n",
      "epoch: 6 step: 1396, loss is 0.003190279006958008\n",
      "epoch: 6 step: 1397, loss is 0.0005493929493241012\n",
      "epoch: 6 step: 1398, loss is 0.006703083403408527\n",
      "epoch: 6 step: 1399, loss is 0.00020650804799515754\n",
      "epoch: 6 step: 1400, loss is 0.0004612641641870141\n",
      "epoch: 6 step: 1401, loss is 0.0037718177773058414\n",
      "epoch: 6 step: 1402, loss is 0.11154878884553909\n",
      "epoch: 6 step: 1403, loss is 0.024213295429944992\n",
      "epoch: 6 step: 1404, loss is 6.968084198888391e-05\n",
      "epoch: 6 step: 1405, loss is 0.004533831961452961\n",
      "epoch: 6 step: 1406, loss is 0.006190794985741377\n",
      "epoch: 6 step: 1407, loss is 0.006185675971210003\n",
      "epoch: 6 step: 1408, loss is 0.009192821569740772\n",
      "epoch: 6 step: 1409, loss is 0.002780964830890298\n",
      "epoch: 6 step: 1410, loss is 0.13268417119979858\n",
      "epoch: 6 step: 1411, loss is 0.031035544350743294\n",
      "epoch: 6 step: 1412, loss is 0.036622054874897\n",
      "epoch: 6 step: 1413, loss is 0.0020735240541398525\n",
      "epoch: 6 step: 1414, loss is 0.0015012912917882204\n",
      "epoch: 6 step: 1415, loss is 0.03040141984820366\n",
      "epoch: 6 step: 1416, loss is 0.05378999933600426\n",
      "epoch: 6 step: 1417, loss is 0.006008136086165905\n",
      "epoch: 6 step: 1418, loss is 0.03028745763003826\n",
      "epoch: 6 step: 1419, loss is 0.0010555712506175041\n",
      "epoch: 6 step: 1420, loss is 0.028056666254997253\n",
      "epoch: 6 step: 1421, loss is 0.0021391049958765507\n",
      "epoch: 6 step: 1422, loss is 0.021474430337548256\n",
      "epoch: 6 step: 1423, loss is 0.007651104126125574\n",
      "epoch: 6 step: 1424, loss is 0.11011981219053268\n",
      "epoch: 6 step: 1425, loss is 0.0074090976268053055\n",
      "epoch: 6 step: 1426, loss is 0.0006670909351669252\n",
      "epoch: 6 step: 1427, loss is 4.906709000351839e-05\n",
      "epoch: 6 step: 1428, loss is 0.0002426995342830196\n",
      "epoch: 6 step: 1429, loss is 0.009565277025103569\n",
      "epoch: 6 step: 1430, loss is 0.16172407567501068\n",
      "epoch: 6 step: 1431, loss is 0.0032491537276655436\n",
      "epoch: 6 step: 1432, loss is 0.12597937881946564\n",
      "epoch: 6 step: 1433, loss is 0.0001592290645930916\n",
      "epoch: 6 step: 1434, loss is 0.0008413747418671846\n",
      "epoch: 6 step: 1435, loss is 0.0051982770673930645\n",
      "epoch: 6 step: 1436, loss is 0.07394237816333771\n",
      "epoch: 6 step: 1437, loss is 0.0010828619124367833\n",
      "epoch: 6 step: 1438, loss is 0.00043037233990617096\n",
      "epoch: 6 step: 1439, loss is 0.0010637559462338686\n",
      "epoch: 6 step: 1440, loss is 0.008645469322800636\n",
      "epoch: 6 step: 1441, loss is 0.0020412420853972435\n",
      "epoch: 6 step: 1442, loss is 0.01619098149240017\n",
      "epoch: 6 step: 1443, loss is 0.0006761043332517147\n",
      "epoch: 6 step: 1444, loss is 0.005301153752952814\n",
      "epoch: 6 step: 1445, loss is 0.0050880638882517815\n",
      "epoch: 6 step: 1446, loss is 0.0014613744569942355\n",
      "epoch: 6 step: 1447, loss is 0.027005957439541817\n",
      "epoch: 6 step: 1448, loss is 0.0012185990344733\n",
      "epoch: 6 step: 1449, loss is 0.0059472424909472466\n",
      "epoch: 6 step: 1450, loss is 6.272410973906517e-05\n",
      "epoch: 6 step: 1451, loss is 0.0005236620781943202\n",
      "epoch: 6 step: 1452, loss is 0.12610521912574768\n",
      "epoch: 6 step: 1453, loss is 0.00027872351347468793\n",
      "epoch: 6 step: 1454, loss is 0.0013503959635272622\n",
      "epoch: 6 step: 1455, loss is 0.0029363047797232866\n",
      "epoch: 6 step: 1456, loss is 0.027042055502533913\n",
      "epoch: 6 step: 1457, loss is 0.04272167384624481\n",
      "epoch: 6 step: 1458, loss is 0.0019422933692112565\n",
      "epoch: 6 step: 1459, loss is 0.00036964297760277987\n",
      "epoch: 6 step: 1460, loss is 0.001613876549527049\n",
      "epoch: 6 step: 1461, loss is 0.0014532842906191945\n",
      "epoch: 6 step: 1462, loss is 0.00017211397062055767\n",
      "epoch: 6 step: 1463, loss is 6.820785347372293e-05\n",
      "epoch: 6 step: 1464, loss is 0.0033222453203052282\n",
      "epoch: 6 step: 1465, loss is 5.2607603720389307e-05\n",
      "epoch: 6 step: 1466, loss is 0.022850146517157555\n",
      "epoch: 6 step: 1467, loss is 0.00163206085562706\n",
      "epoch: 6 step: 1468, loss is 0.0001902261865325272\n",
      "epoch: 6 step: 1469, loss is 0.009714352898299694\n",
      "epoch: 6 step: 1470, loss is 0.00016344514733646065\n",
      "epoch: 6 step: 1471, loss is 0.0010339056607335806\n",
      "epoch: 6 step: 1472, loss is 0.004482870455831289\n",
      "epoch: 6 step: 1473, loss is 0.019848257303237915\n",
      "epoch: 6 step: 1474, loss is 0.0008555727545171976\n",
      "epoch: 6 step: 1475, loss is 0.0008073809440247715\n",
      "epoch: 6 step: 1476, loss is 0.007015320938080549\n",
      "epoch: 6 step: 1477, loss is 0.016125191003084183\n",
      "epoch: 6 step: 1478, loss is 0.0014991587959229946\n",
      "epoch: 6 step: 1479, loss is 0.005313400644809008\n",
      "epoch: 6 step: 1480, loss is 0.1719438135623932\n",
      "epoch: 6 step: 1481, loss is 0.05085958540439606\n",
      "epoch: 6 step: 1482, loss is 0.0016086053801700473\n",
      "epoch: 6 step: 1483, loss is 0.0006887929630465806\n",
      "epoch: 6 step: 1484, loss is 0.00010123338870471343\n",
      "epoch: 6 step: 1485, loss is 0.0014187064953148365\n",
      "epoch: 6 step: 1486, loss is 0.00024346582358703017\n",
      "epoch: 6 step: 1487, loss is 0.00019992799207102507\n",
      "epoch: 6 step: 1488, loss is 0.17277558147907257\n",
      "epoch: 6 step: 1489, loss is 0.0022316472604870796\n",
      "epoch: 6 step: 1490, loss is 0.056736040860414505\n",
      "epoch: 6 step: 1491, loss is 0.00014719512546434999\n",
      "epoch: 6 step: 1492, loss is 0.0004905656678602099\n",
      "epoch: 6 step: 1493, loss is 0.02124476432800293\n",
      "epoch: 6 step: 1494, loss is 0.0009238963248208165\n",
      "epoch: 6 step: 1495, loss is 0.018125422298908234\n",
      "epoch: 6 step: 1496, loss is 0.0006822273135185242\n",
      "epoch: 6 step: 1497, loss is 0.01695414073765278\n",
      "epoch: 6 step: 1498, loss is 0.0012229541316628456\n",
      "epoch: 6 step: 1499, loss is 0.00013023887004237622\n",
      "epoch: 6 step: 1500, loss is 0.011152299121022224\n",
      "epoch: 6 step: 1501, loss is 0.07445821911096573\n",
      "epoch: 6 step: 1502, loss is 0.018256545066833496\n",
      "epoch: 6 step: 1503, loss is 0.0004354300326667726\n",
      "epoch: 6 step: 1504, loss is 0.020768404006958008\n",
      "epoch: 6 step: 1505, loss is 0.030971350148320198\n",
      "epoch: 6 step: 1506, loss is 8.137492113746703e-05\n",
      "epoch: 6 step: 1507, loss is 0.0007075354224070907\n",
      "epoch: 6 step: 1508, loss is 0.0035057824570685625\n",
      "epoch: 6 step: 1509, loss is 7.458974869223312e-05\n",
      "epoch: 6 step: 1510, loss is 0.016715334728360176\n",
      "epoch: 6 step: 1511, loss is 0.008262005634605885\n",
      "epoch: 6 step: 1512, loss is 0.0013473844155669212\n",
      "epoch: 6 step: 1513, loss is 0.025124188512563705\n",
      "epoch: 6 step: 1514, loss is 0.0074634128250181675\n",
      "epoch: 6 step: 1515, loss is 0.00041928241262212396\n",
      "epoch: 6 step: 1516, loss is 0.004315114114433527\n",
      "epoch: 6 step: 1517, loss is 0.06430010497570038\n",
      "epoch: 6 step: 1518, loss is 0.45476034283638\n",
      "epoch: 6 step: 1519, loss is 0.0005693714483641088\n",
      "epoch: 6 step: 1520, loss is 0.08561526238918304\n",
      "epoch: 6 step: 1521, loss is 0.0007407642551697791\n",
      "epoch: 6 step: 1522, loss is 0.015258502215147018\n",
      "epoch: 6 step: 1523, loss is 0.2190656065940857\n",
      "epoch: 6 step: 1524, loss is 0.09678134322166443\n",
      "epoch: 6 step: 1525, loss is 0.03883203864097595\n",
      "epoch: 6 step: 1526, loss is 0.0035033447202295065\n",
      "epoch: 6 step: 1527, loss is 0.03246400132775307\n",
      "epoch: 6 step: 1528, loss is 0.0017992134671658278\n",
      "epoch: 6 step: 1529, loss is 0.0019642121624201536\n",
      "epoch: 6 step: 1530, loss is 0.004551513586193323\n",
      "epoch: 6 step: 1531, loss is 0.07231920957565308\n",
      "epoch: 6 step: 1532, loss is 0.06544093042612076\n",
      "epoch: 6 step: 1533, loss is 0.05888139829039574\n",
      "epoch: 6 step: 1534, loss is 0.003385052317753434\n",
      "epoch: 6 step: 1535, loss is 0.002957484684884548\n",
      "epoch: 6 step: 1536, loss is 0.005878411699086428\n",
      "epoch: 6 step: 1537, loss is 0.07024539262056351\n",
      "epoch: 6 step: 1538, loss is 0.005176701582968235\n",
      "epoch: 6 step: 1539, loss is 0.0006093650590628386\n",
      "epoch: 6 step: 1540, loss is 0.01950160413980484\n",
      "epoch: 6 step: 1541, loss is 0.0038901595398783684\n",
      "epoch: 6 step: 1542, loss is 0.015810435637831688\n",
      "epoch: 6 step: 1543, loss is 0.043000176548957825\n",
      "epoch: 6 step: 1544, loss is 0.0919477790594101\n",
      "epoch: 6 step: 1545, loss is 0.002248230390250683\n",
      "epoch: 6 step: 1546, loss is 0.02056533470749855\n",
      "epoch: 6 step: 1547, loss is 0.0009550293325446546\n",
      "epoch: 6 step: 1548, loss is 0.0006991425761952996\n",
      "epoch: 6 step: 1549, loss is 0.0035164018627256155\n",
      "epoch: 6 step: 1550, loss is 0.005668323487043381\n",
      "epoch: 6 step: 1551, loss is 0.0025237894151359797\n",
      "epoch: 6 step: 1552, loss is 0.0006812634528614581\n",
      "epoch: 6 step: 1553, loss is 0.0017389621352776885\n",
      "epoch: 6 step: 1554, loss is 0.03837814927101135\n",
      "epoch: 6 step: 1555, loss is 0.000824783171992749\n",
      "epoch: 6 step: 1556, loss is 0.028623398393392563\n",
      "epoch: 6 step: 1557, loss is 0.0014491429319605231\n",
      "epoch: 6 step: 1558, loss is 0.005923966411501169\n",
      "epoch: 6 step: 1559, loss is 0.31481561064720154\n",
      "epoch: 6 step: 1560, loss is 0.0004091963346581906\n",
      "epoch: 6 step: 1561, loss is 0.005263444967567921\n",
      "epoch: 6 step: 1562, loss is 0.21562343835830688\n",
      "epoch: 6 step: 1563, loss is 0.005101928021758795\n",
      "epoch: 6 step: 1564, loss is 0.003263231832534075\n",
      "epoch: 6 step: 1565, loss is 0.0025042793713510036\n",
      "epoch: 6 step: 1566, loss is 0.12386362999677658\n",
      "epoch: 6 step: 1567, loss is 0.001619601040147245\n",
      "epoch: 6 step: 1568, loss is 0.06341028958559036\n",
      "epoch: 6 step: 1569, loss is 0.04108782857656479\n",
      "epoch: 6 step: 1570, loss is 0.00029147532768547535\n",
      "epoch: 6 step: 1571, loss is 0.0028097378090023994\n",
      "epoch: 6 step: 1572, loss is 0.011460614390671253\n",
      "epoch: 6 step: 1573, loss is 0.1398090273141861\n",
      "epoch: 6 step: 1574, loss is 0.0813075378537178\n",
      "epoch: 6 step: 1575, loss is 0.1692970097064972\n",
      "epoch: 6 step: 1576, loss is 0.016126928851008415\n",
      "epoch: 6 step: 1577, loss is 0.003382972674444318\n",
      "epoch: 6 step: 1578, loss is 0.0032616569660604\n",
      "epoch: 6 step: 1579, loss is 0.04777589067816734\n",
      "epoch: 6 step: 1580, loss is 0.0033077860716730356\n",
      "epoch: 6 step: 1581, loss is 0.03669513761997223\n",
      "epoch: 6 step: 1582, loss is 0.3989611566066742\n",
      "epoch: 6 step: 1583, loss is 0.03903161361813545\n",
      "epoch: 6 step: 1584, loss is 0.001911940285935998\n",
      "epoch: 6 step: 1585, loss is 0.00363371754065156\n",
      "epoch: 6 step: 1586, loss is 0.06797145307064056\n",
      "epoch: 6 step: 1587, loss is 0.016761749982833862\n",
      "epoch: 6 step: 1588, loss is 0.0029449935536831617\n",
      "epoch: 6 step: 1589, loss is 0.004138031974434853\n",
      "epoch: 6 step: 1590, loss is 0.007555688265711069\n",
      "epoch: 6 step: 1591, loss is 0.10359138995409012\n",
      "epoch: 6 step: 1592, loss is 0.02307303436100483\n",
      "epoch: 6 step: 1593, loss is 0.007226902525871992\n",
      "epoch: 6 step: 1594, loss is 0.08644964545965195\n",
      "epoch: 6 step: 1595, loss is 0.0197821706533432\n",
      "epoch: 6 step: 1596, loss is 0.04603922367095947\n",
      "epoch: 6 step: 1597, loss is 0.002172475913539529\n",
      "epoch: 6 step: 1598, loss is 0.015644824132323265\n",
      "epoch: 6 step: 1599, loss is 0.014933803118765354\n",
      "epoch: 6 step: 1600, loss is 0.0014121808344498277\n",
      "epoch: 6 step: 1601, loss is 0.000904461950995028\n",
      "epoch: 6 step: 1602, loss is 0.050353650003671646\n",
      "epoch: 6 step: 1603, loss is 0.0349210686981678\n",
      "epoch: 6 step: 1604, loss is 0.022151149809360504\n",
      "epoch: 6 step: 1605, loss is 0.011249306611716747\n",
      "epoch: 6 step: 1606, loss is 0.016363508999347687\n",
      "epoch: 6 step: 1607, loss is 0.023240111768245697\n",
      "epoch: 6 step: 1608, loss is 0.0830698162317276\n",
      "epoch: 6 step: 1609, loss is 0.010225950740277767\n",
      "epoch: 6 step: 1610, loss is 0.07883358001708984\n",
      "epoch: 6 step: 1611, loss is 0.0010332065867260098\n",
      "epoch: 6 step: 1612, loss is 0.004766041878610849\n",
      "epoch: 6 step: 1613, loss is 0.0049764844588935375\n",
      "epoch: 6 step: 1614, loss is 0.14428037405014038\n",
      "epoch: 6 step: 1615, loss is 0.006402940023690462\n",
      "epoch: 6 step: 1616, loss is 0.05941925197839737\n",
      "epoch: 6 step: 1617, loss is 0.007101140450686216\n",
      "epoch: 6 step: 1618, loss is 0.010163411498069763\n",
      "epoch: 6 step: 1619, loss is 0.2162846326828003\n",
      "epoch: 6 step: 1620, loss is 0.002726042876020074\n",
      "epoch: 6 step: 1621, loss is 0.012707652524113655\n",
      "epoch: 6 step: 1622, loss is 0.08497831970453262\n",
      "epoch: 6 step: 1623, loss is 0.026870856061577797\n",
      "epoch: 6 step: 1624, loss is 0.0003479482838883996\n",
      "epoch: 6 step: 1625, loss is 0.0009781275875866413\n",
      "epoch: 6 step: 1626, loss is 0.016406573355197906\n",
      "epoch: 6 step: 1627, loss is 0.012693236581981182\n",
      "epoch: 6 step: 1628, loss is 0.003787105903029442\n",
      "epoch: 6 step: 1629, loss is 0.015205436386168003\n",
      "epoch: 6 step: 1630, loss is 0.020565319806337357\n",
      "epoch: 6 step: 1631, loss is 0.0022835605777800083\n",
      "epoch: 6 step: 1632, loss is 0.026722220703959465\n",
      "epoch: 6 step: 1633, loss is 0.12646806240081787\n",
      "epoch: 6 step: 1634, loss is 0.006945583503693342\n",
      "epoch: 6 step: 1635, loss is 0.010201733559370041\n",
      "epoch: 6 step: 1636, loss is 0.01782234199345112\n",
      "epoch: 6 step: 1637, loss is 0.0006949490634724498\n",
      "epoch: 6 step: 1638, loss is 0.08976932615041733\n",
      "epoch: 6 step: 1639, loss is 0.010672544129192829\n",
      "epoch: 6 step: 1640, loss is 0.03280055522918701\n",
      "epoch: 6 step: 1641, loss is 0.01605290360748768\n",
      "epoch: 6 step: 1642, loss is 0.019146686419844627\n",
      "epoch: 6 step: 1643, loss is 0.0107364971190691\n",
      "epoch: 6 step: 1644, loss is 0.0011711487313732505\n",
      "epoch: 6 step: 1645, loss is 0.14014844596385956\n",
      "epoch: 6 step: 1646, loss is 0.03271415829658508\n",
      "epoch: 6 step: 1647, loss is 0.00023393210722133517\n",
      "epoch: 6 step: 1648, loss is 0.02732367254793644\n",
      "epoch: 6 step: 1649, loss is 0.061124224215745926\n",
      "epoch: 6 step: 1650, loss is 0.009975590743124485\n",
      "epoch: 6 step: 1651, loss is 0.035035643726587296\n",
      "epoch: 6 step: 1652, loss is 0.011552808806300163\n",
      "epoch: 6 step: 1653, loss is 0.0006234003813005984\n",
      "epoch: 6 step: 1654, loss is 0.05788714438676834\n",
      "epoch: 6 step: 1655, loss is 0.005968828219920397\n",
      "epoch: 6 step: 1656, loss is 0.0013834007550030947\n",
      "epoch: 6 step: 1657, loss is 0.05575566738843918\n",
      "epoch: 6 step: 1658, loss is 0.015178252942860126\n",
      "epoch: 6 step: 1659, loss is 0.0004225342709105462\n",
      "epoch: 6 step: 1660, loss is 0.0042427293956279755\n",
      "epoch: 6 step: 1661, loss is 0.0003553484275471419\n",
      "epoch: 6 step: 1662, loss is 0.0008849062724038959\n",
      "epoch: 6 step: 1663, loss is 0.0006755597423762083\n",
      "epoch: 6 step: 1664, loss is 0.006674105301499367\n",
      "epoch: 6 step: 1665, loss is 0.0002430551394354552\n",
      "epoch: 6 step: 1666, loss is 0.012258420698344707\n",
      "epoch: 6 step: 1667, loss is 0.0054394835606217384\n",
      "epoch: 6 step: 1668, loss is 0.0025051457341760397\n",
      "epoch: 6 step: 1669, loss is 0.0008815971668809652\n",
      "epoch: 6 step: 1670, loss is 0.0008155951509252191\n",
      "epoch: 6 step: 1671, loss is 0.014532242901623249\n",
      "epoch: 6 step: 1672, loss is 0.0676349475979805\n",
      "epoch: 6 step: 1673, loss is 0.0012878426350653172\n",
      "epoch: 6 step: 1674, loss is 0.0007168900920078158\n",
      "epoch: 6 step: 1675, loss is 0.005202003754675388\n",
      "epoch: 6 step: 1676, loss is 0.1610604077577591\n",
      "epoch: 6 step: 1677, loss is 0.00658026896417141\n",
      "epoch: 6 step: 1678, loss is 0.0004613538912963122\n",
      "epoch: 6 step: 1679, loss is 0.02392503060400486\n",
      "epoch: 6 step: 1680, loss is 0.0012792195193469524\n",
      "epoch: 6 step: 1681, loss is 0.0007812102558091283\n",
      "epoch: 6 step: 1682, loss is 0.0017112474888563156\n",
      "epoch: 6 step: 1683, loss is 0.0870823785662651\n",
      "epoch: 6 step: 1684, loss is 0.0034494660794734955\n",
      "epoch: 6 step: 1685, loss is 0.014799438416957855\n",
      "epoch: 6 step: 1686, loss is 0.012630286626517773\n",
      "epoch: 6 step: 1687, loss is 0.0008672965341247618\n",
      "epoch: 6 step: 1688, loss is 0.00041430312558077276\n",
      "epoch: 6 step: 1689, loss is 2.254481842101086e-05\n",
      "epoch: 6 step: 1690, loss is 0.08477555215358734\n",
      "epoch: 6 step: 1691, loss is 0.0038794251158833504\n",
      "epoch: 6 step: 1692, loss is 0.0008089103503152728\n",
      "epoch: 6 step: 1693, loss is 0.002159551717340946\n",
      "epoch: 6 step: 1694, loss is 0.007682850584387779\n",
      "epoch: 6 step: 1695, loss is 0.10942581295967102\n",
      "epoch: 6 step: 1696, loss is 0.00019250152399763465\n",
      "epoch: 6 step: 1697, loss is 0.0015386943705379963\n",
      "epoch: 6 step: 1698, loss is 0.004525902681052685\n",
      "epoch: 6 step: 1699, loss is 0.007438840810209513\n",
      "epoch: 6 step: 1700, loss is 0.02438919059932232\n",
      "epoch: 6 step: 1701, loss is 0.0032194433733820915\n",
      "epoch: 6 step: 1702, loss is 0.001086086151190102\n",
      "epoch: 6 step: 1703, loss is 0.014539197087287903\n",
      "epoch: 6 step: 1704, loss is 0.05207002907991409\n",
      "epoch: 6 step: 1705, loss is 0.00016067139222286642\n",
      "epoch: 6 step: 1706, loss is 0.00012869066267739981\n",
      "epoch: 6 step: 1707, loss is 0.0067175826989114285\n",
      "epoch: 6 step: 1708, loss is 0.16206961870193481\n",
      "epoch: 6 step: 1709, loss is 0.0009843020234256983\n",
      "epoch: 6 step: 1710, loss is 0.0006693970644846559\n",
      "epoch: 6 step: 1711, loss is 0.001824079197831452\n",
      "epoch: 6 step: 1712, loss is 0.0008728845859877765\n",
      "epoch: 6 step: 1713, loss is 0.0023581967689096928\n",
      "epoch: 6 step: 1714, loss is 0.018146049231290817\n",
      "epoch: 6 step: 1715, loss is 0.0034814365208148956\n",
      "epoch: 6 step: 1716, loss is 0.05321357399225235\n",
      "epoch: 6 step: 1717, loss is 0.0007007213425822556\n",
      "epoch: 6 step: 1718, loss is 0.0006481550517491996\n",
      "epoch: 6 step: 1719, loss is 0.08303220570087433\n",
      "epoch: 6 step: 1720, loss is 0.0004034250450786203\n",
      "epoch: 6 step: 1721, loss is 0.04252813756465912\n",
      "epoch: 6 step: 1722, loss is 0.002176413545385003\n",
      "epoch: 6 step: 1723, loss is 0.0004671482602134347\n",
      "epoch: 6 step: 1724, loss is 0.0004595874925144017\n",
      "epoch: 6 step: 1725, loss is 0.09519688040018082\n",
      "epoch: 6 step: 1726, loss is 0.0005378914647735655\n",
      "epoch: 6 step: 1727, loss is 0.0007902231300249696\n",
      "epoch: 6 step: 1728, loss is 0.2469043731689453\n",
      "epoch: 6 step: 1729, loss is 0.007748867850750685\n",
      "epoch: 6 step: 1730, loss is 0.00022977648768574\n",
      "epoch: 6 step: 1731, loss is 0.06971313804388046\n",
      "epoch: 6 step: 1732, loss is 0.0012990079121664166\n",
      "epoch: 6 step: 1733, loss is 0.10636478662490845\n",
      "epoch: 6 step: 1734, loss is 0.023116732016205788\n",
      "epoch: 6 step: 1735, loss is 0.07773643732070923\n",
      "epoch: 6 step: 1736, loss is 0.005356060341000557\n",
      "epoch: 6 step: 1737, loss is 0.017596373334527016\n",
      "epoch: 6 step: 1738, loss is 0.002544793300330639\n",
      "epoch: 6 step: 1739, loss is 0.04513930156826973\n",
      "epoch: 6 step: 1740, loss is 0.004715355113148689\n",
      "epoch: 6 step: 1741, loss is 0.011221745051443577\n",
      "epoch: 6 step: 1742, loss is 0.020594749599695206\n",
      "epoch: 6 step: 1743, loss is 0.0027793492190539837\n",
      "epoch: 6 step: 1744, loss is 0.001651278231292963\n",
      "epoch: 6 step: 1745, loss is 0.00020066658908035606\n",
      "epoch: 6 step: 1746, loss is 0.0389615036547184\n",
      "epoch: 6 step: 1747, loss is 0.0003092428669333458\n",
      "epoch: 6 step: 1748, loss is 0.010020149871706963\n",
      "epoch: 6 step: 1749, loss is 0.048236824572086334\n",
      "epoch: 6 step: 1750, loss is 0.04856615141034126\n",
      "epoch: 6 step: 1751, loss is 0.020423850044608116\n",
      "epoch: 6 step: 1752, loss is 0.0012414178345352411\n",
      "epoch: 6 step: 1753, loss is 0.0005020591197535396\n",
      "epoch: 6 step: 1754, loss is 0.00792388804256916\n",
      "epoch: 6 step: 1755, loss is 0.0031388825736939907\n",
      "epoch: 6 step: 1756, loss is 0.0003587001992855221\n",
      "epoch: 6 step: 1757, loss is 0.001268554711714387\n",
      "epoch: 6 step: 1758, loss is 0.018289925530552864\n",
      "epoch: 6 step: 1759, loss is 0.20255960524082184\n",
      "epoch: 6 step: 1760, loss is 0.004106858279556036\n",
      "epoch: 6 step: 1761, loss is 0.004207948222756386\n",
      "epoch: 6 step: 1762, loss is 0.001728520612232387\n",
      "epoch: 6 step: 1763, loss is 0.005315523128956556\n",
      "epoch: 6 step: 1764, loss is 0.002369923982769251\n",
      "epoch: 6 step: 1765, loss is 0.002715383656322956\n",
      "epoch: 6 step: 1766, loss is 0.003836812451481819\n",
      "epoch: 6 step: 1767, loss is 0.0005801868974231184\n",
      "epoch: 6 step: 1768, loss is 0.028751924633979797\n",
      "epoch: 6 step: 1769, loss is 0.0807274803519249\n",
      "epoch: 6 step: 1770, loss is 0.03772147372364998\n",
      "epoch: 6 step: 1771, loss is 0.0222109816968441\n",
      "epoch: 6 step: 1772, loss is 0.043579187244176865\n",
      "epoch: 6 step: 1773, loss is 0.018352234736084938\n",
      "epoch: 6 step: 1774, loss is 0.0006699922378174961\n",
      "epoch: 6 step: 1775, loss is 0.09225744754076004\n",
      "epoch: 6 step: 1776, loss is 0.004094699397683144\n",
      "epoch: 6 step: 1777, loss is 0.001136909006163478\n",
      "epoch: 6 step: 1778, loss is 0.005550680682063103\n",
      "epoch: 6 step: 1779, loss is 0.0013822655891999602\n",
      "epoch: 6 step: 1780, loss is 0.06741160899400711\n",
      "epoch: 6 step: 1781, loss is 0.04048954322934151\n",
      "epoch: 6 step: 1782, loss is 0.0016715193632990122\n",
      "epoch: 6 step: 1783, loss is 0.04952022060751915\n",
      "epoch: 6 step: 1784, loss is 0.0674453154206276\n",
      "epoch: 6 step: 1785, loss is 0.00675535760819912\n",
      "epoch: 6 step: 1786, loss is 0.0008840331574901938\n",
      "epoch: 6 step: 1787, loss is 0.001845437684096396\n",
      "epoch: 6 step: 1788, loss is 0.03534586355090141\n",
      "epoch: 6 step: 1789, loss is 0.05155690759420395\n",
      "epoch: 6 step: 1790, loss is 0.0034766022581607103\n",
      "epoch: 6 step: 1791, loss is 0.02736085280776024\n",
      "epoch: 6 step: 1792, loss is 0.0022312095388770103\n",
      "epoch: 6 step: 1793, loss is 0.011105824261903763\n",
      "epoch: 6 step: 1794, loss is 0.07854728400707245\n",
      "epoch: 6 step: 1795, loss is 0.06598061323165894\n",
      "epoch: 6 step: 1796, loss is 0.023618945851922035\n",
      "epoch: 6 step: 1797, loss is 0.0027894536033272743\n",
      "epoch: 6 step: 1798, loss is 0.002036518882960081\n",
      "epoch: 6 step: 1799, loss is 0.05279652774333954\n",
      "epoch: 6 step: 1800, loss is 0.002282500732690096\n",
      "epoch: 6 step: 1801, loss is 0.07068746536970139\n",
      "epoch: 6 step: 1802, loss is 0.003658181754872203\n",
      "epoch: 6 step: 1803, loss is 0.18200168013572693\n",
      "epoch: 6 step: 1804, loss is 0.0005009475280530751\n",
      "epoch: 6 step: 1805, loss is 0.0007029403932392597\n",
      "epoch: 6 step: 1806, loss is 0.05114121362566948\n",
      "epoch: 6 step: 1807, loss is 0.06769169867038727\n",
      "epoch: 6 step: 1808, loss is 0.05215255916118622\n",
      "epoch: 6 step: 1809, loss is 0.03293988108634949\n",
      "epoch: 6 step: 1810, loss is 0.038423169404268265\n",
      "epoch: 6 step: 1811, loss is 0.000293144112220034\n",
      "epoch: 6 step: 1812, loss is 0.006327386014163494\n",
      "epoch: 6 step: 1813, loss is 0.01332843117415905\n",
      "epoch: 6 step: 1814, loss is 0.013188832439482212\n",
      "epoch: 6 step: 1815, loss is 0.0012133814161643386\n",
      "epoch: 6 step: 1816, loss is 0.006661495193839073\n",
      "epoch: 6 step: 1817, loss is 0.005158522166311741\n",
      "epoch: 6 step: 1818, loss is 0.004686800763010979\n",
      "epoch: 6 step: 1819, loss is 0.022209446877241135\n",
      "epoch: 6 step: 1820, loss is 0.000586889567784965\n",
      "epoch: 6 step: 1821, loss is 0.14221540093421936\n",
      "epoch: 6 step: 1822, loss is 0.018186727538704872\n",
      "epoch: 6 step: 1823, loss is 0.07006616145372391\n",
      "epoch: 6 step: 1824, loss is 0.002655238378793001\n",
      "epoch: 6 step: 1825, loss is 0.021179016679525375\n",
      "epoch: 6 step: 1826, loss is 0.0014781851787120104\n",
      "epoch: 6 step: 1827, loss is 0.0029573985375463963\n",
      "epoch: 6 step: 1828, loss is 0.00838710181415081\n",
      "epoch: 6 step: 1829, loss is 0.011745809577405453\n",
      "epoch: 6 step: 1830, loss is 0.07428522408008575\n",
      "epoch: 6 step: 1831, loss is 0.04534722864627838\n",
      "epoch: 6 step: 1832, loss is 0.0003713585319928825\n",
      "epoch: 6 step: 1833, loss is 0.016235046088695526\n",
      "epoch: 6 step: 1834, loss is 0.010123202577233315\n",
      "epoch: 6 step: 1835, loss is 0.02669229730963707\n",
      "epoch: 6 step: 1836, loss is 0.001462011830881238\n",
      "epoch: 6 step: 1837, loss is 0.00335194100625813\n",
      "epoch: 6 step: 1838, loss is 0.02073703333735466\n",
      "epoch: 6 step: 1839, loss is 0.14830119907855988\n",
      "epoch: 6 step: 1840, loss is 0.001071272068656981\n",
      "epoch: 6 step: 1841, loss is 0.003725806251168251\n",
      "epoch: 6 step: 1842, loss is 0.002585925627499819\n",
      "epoch: 6 step: 1843, loss is 0.0003385758609510958\n",
      "epoch: 6 step: 1844, loss is 0.0030024813022464514\n",
      "epoch: 6 step: 1845, loss is 0.03358306363224983\n",
      "epoch: 6 step: 1846, loss is 0.12863866984844208\n",
      "epoch: 6 step: 1847, loss is 0.001150607829913497\n",
      "epoch: 6 step: 1848, loss is 0.009104805067181587\n",
      "epoch: 6 step: 1849, loss is 0.0011894901981577277\n",
      "epoch: 6 step: 1850, loss is 0.005185846239328384\n",
      "epoch: 6 step: 1851, loss is 0.00018654149607755244\n",
      "epoch: 6 step: 1852, loss is 0.0021035883110016584\n",
      "epoch: 6 step: 1853, loss is 0.0003273335169069469\n",
      "epoch: 6 step: 1854, loss is 0.0017556969542056322\n",
      "epoch: 6 step: 1855, loss is 0.000263167021330446\n",
      "epoch: 6 step: 1856, loss is 0.000977740273810923\n",
      "epoch: 6 step: 1857, loss is 0.005796022713184357\n",
      "epoch: 6 step: 1858, loss is 0.020765719935297966\n",
      "epoch: 6 step: 1859, loss is 0.006247623357921839\n",
      "epoch: 6 step: 1860, loss is 0.0017151596257463098\n",
      "epoch: 6 step: 1861, loss is 0.03539483621716499\n",
      "epoch: 6 step: 1862, loss is 0.054256901144981384\n",
      "epoch: 6 step: 1863, loss is 0.02346367947757244\n",
      "epoch: 6 step: 1864, loss is 0.0034603585954755545\n",
      "epoch: 6 step: 1865, loss is 0.05767986550927162\n",
      "epoch: 6 step: 1866, loss is 0.13076256215572357\n",
      "epoch: 6 step: 1867, loss is 0.0010901018977165222\n",
      "epoch: 6 step: 1868, loss is 0.00160886044614017\n",
      "epoch: 6 step: 1869, loss is 0.0037152341101318598\n",
      "epoch: 6 step: 1870, loss is 0.08496472239494324\n",
      "epoch: 6 step: 1871, loss is 0.161760151386261\n",
      "epoch: 6 step: 1872, loss is 0.001325462944805622\n",
      "epoch: 6 step: 1873, loss is 0.020458104088902473\n",
      "epoch: 6 step: 1874, loss is 0.013049110770225525\n",
      "epoch: 6 step: 1875, loss is 0.016553621739149094\n",
      "Train epoch time: 13448.558 ms, per step time: 7.173 ms\n",
      "epoch: 7 step: 1, loss is 0.043149832636117935\n",
      "epoch: 7 step: 2, loss is 0.08190527558326721\n",
      "epoch: 7 step: 3, loss is 0.11608729511499405\n",
      "epoch: 7 step: 4, loss is 0.015096140094101429\n",
      "epoch: 7 step: 5, loss is 0.003669293597340584\n",
      "epoch: 7 step: 6, loss is 0.010084236040711403\n",
      "epoch: 7 step: 7, loss is 0.027191162109375\n",
      "epoch: 7 step: 8, loss is 0.0004497300542425364\n",
      "epoch: 7 step: 9, loss is 0.0426301471889019\n",
      "epoch: 7 step: 10, loss is 0.0013577948557212949\n",
      "epoch: 7 step: 11, loss is 0.04119616746902466\n",
      "epoch: 7 step: 12, loss is 0.009798240847885609\n",
      "epoch: 7 step: 13, loss is 0.044786542654037476\n",
      "epoch: 7 step: 14, loss is 0.0025972553994506598\n",
      "epoch: 7 step: 15, loss is 0.006328337825834751\n",
      "epoch: 7 step: 16, loss is 0.04206910356879234\n",
      "epoch: 7 step: 17, loss is 0.0034229098819196224\n",
      "epoch: 7 step: 18, loss is 5.859206066816114e-05\n",
      "epoch: 7 step: 19, loss is 0.0037787742912769318\n",
      "epoch: 7 step: 20, loss is 0.000688433472532779\n",
      "epoch: 7 step: 21, loss is 0.00035581187694333494\n",
      "epoch: 7 step: 22, loss is 0.0007230283808894455\n",
      "epoch: 7 step: 23, loss is 0.07234003394842148\n",
      "epoch: 7 step: 24, loss is 0.07311482727527618\n",
      "epoch: 7 step: 25, loss is 0.0010316520929336548\n",
      "epoch: 7 step: 26, loss is 0.006124604027718306\n",
      "epoch: 7 step: 27, loss is 0.028508368879556656\n",
      "epoch: 7 step: 28, loss is 0.003056748304516077\n",
      "epoch: 7 step: 29, loss is 0.0027753470931202173\n",
      "epoch: 7 step: 30, loss is 0.0004611588374245912\n",
      "epoch: 7 step: 31, loss is 0.0003963364870287478\n",
      "epoch: 7 step: 32, loss is 0.023578912019729614\n",
      "epoch: 7 step: 33, loss is 0.05013206601142883\n",
      "epoch: 7 step: 34, loss is 0.01205347292125225\n",
      "epoch: 7 step: 35, loss is 0.0036985266488045454\n",
      "epoch: 7 step: 36, loss is 0.1874619424343109\n",
      "epoch: 7 step: 37, loss is 0.03548364341259003\n",
      "epoch: 7 step: 38, loss is 0.0016006664372980595\n",
      "epoch: 7 step: 39, loss is 0.057353418320417404\n",
      "epoch: 7 step: 40, loss is 0.03554772958159447\n",
      "epoch: 7 step: 41, loss is 0.00029355165315791965\n",
      "epoch: 7 step: 42, loss is 0.0012144221691414714\n",
      "epoch: 7 step: 43, loss is 0.017765117809176445\n",
      "epoch: 7 step: 44, loss is 0.03137929365038872\n",
      "epoch: 7 step: 45, loss is 0.0018606210360303521\n",
      "epoch: 7 step: 46, loss is 0.2583906054496765\n",
      "epoch: 7 step: 47, loss is 0.0005341990618035197\n",
      "epoch: 7 step: 48, loss is 0.1703549027442932\n",
      "epoch: 7 step: 49, loss is 0.021393559873104095\n",
      "epoch: 7 step: 50, loss is 0.0006825347081758082\n",
      "epoch: 7 step: 51, loss is 0.0021801406983286142\n",
      "epoch: 7 step: 52, loss is 0.03652440756559372\n",
      "epoch: 7 step: 53, loss is 0.00013830201351083815\n",
      "epoch: 7 step: 54, loss is 0.06757878512144089\n",
      "epoch: 7 step: 55, loss is 0.005787828471511602\n",
      "epoch: 7 step: 56, loss is 0.012894854880869389\n",
      "epoch: 7 step: 57, loss is 0.02310304343700409\n",
      "epoch: 7 step: 58, loss is 0.0023789978586137295\n",
      "epoch: 7 step: 59, loss is 0.007225126028060913\n",
      "epoch: 7 step: 60, loss is 0.0013699583942070603\n",
      "epoch: 7 step: 61, loss is 0.0015540196327492595\n",
      "epoch: 7 step: 62, loss is 0.00017490683239884675\n",
      "epoch: 7 step: 63, loss is 0.027799515053629875\n",
      "epoch: 7 step: 64, loss is 0.013202476315200329\n",
      "epoch: 7 step: 65, loss is 0.0009182374342344701\n",
      "epoch: 7 step: 66, loss is 0.21071240305900574\n",
      "epoch: 7 step: 67, loss is 0.0017117538955062628\n",
      "epoch: 7 step: 68, loss is 0.07601483166217804\n",
      "epoch: 7 step: 69, loss is 0.08387720584869385\n",
      "epoch: 7 step: 70, loss is 0.002132733818143606\n",
      "epoch: 7 step: 71, loss is 0.0009159923647530377\n",
      "epoch: 7 step: 72, loss is 0.014309228397905827\n",
      "epoch: 7 step: 73, loss is 0.03782285004854202\n",
      "epoch: 7 step: 74, loss is 0.0036810417659580708\n",
      "epoch: 7 step: 75, loss is 0.009405707940459251\n",
      "epoch: 7 step: 76, loss is 0.0016017958987504244\n",
      "epoch: 7 step: 77, loss is 0.026567257940769196\n",
      "epoch: 7 step: 78, loss is 0.0035401838831603527\n",
      "epoch: 7 step: 79, loss is 0.0014878042275086045\n",
      "epoch: 7 step: 80, loss is 0.0006653828313574195\n",
      "epoch: 7 step: 81, loss is 0.011499632149934769\n",
      "epoch: 7 step: 82, loss is 0.05021457374095917\n",
      "epoch: 7 step: 83, loss is 0.0008492977358400822\n",
      "epoch: 7 step: 84, loss is 0.009723165072500706\n",
      "epoch: 7 step: 85, loss is 0.043306827545166016\n",
      "epoch: 7 step: 86, loss is 0.00169448833912611\n",
      "epoch: 7 step: 87, loss is 0.031176861375570297\n",
      "epoch: 7 step: 88, loss is 0.18401241302490234\n",
      "epoch: 7 step: 89, loss is 0.012463216669857502\n",
      "epoch: 7 step: 90, loss is 0.0470430962741375\n",
      "epoch: 7 step: 91, loss is 0.0032364914659410715\n",
      "epoch: 7 step: 92, loss is 0.004798841197043657\n",
      "epoch: 7 step: 93, loss is 0.0004920391365885735\n",
      "epoch: 7 step: 94, loss is 0.00013342090824153274\n",
      "epoch: 7 step: 95, loss is 0.00014012213796377182\n",
      "epoch: 7 step: 96, loss is 0.0013202663976699114\n",
      "epoch: 7 step: 97, loss is 0.0007073781453073025\n",
      "epoch: 7 step: 98, loss is 0.07524719089269638\n",
      "epoch: 7 step: 99, loss is 0.012854574248194695\n",
      "epoch: 7 step: 100, loss is 0.0009893628302961588\n",
      "epoch: 7 step: 101, loss is 0.00014562478463631123\n",
      "epoch: 7 step: 102, loss is 0.0010620064567774534\n",
      "epoch: 7 step: 103, loss is 0.00874412339180708\n",
      "epoch: 7 step: 104, loss is 0.01014347467571497\n",
      "epoch: 7 step: 105, loss is 0.0010030717821791768\n",
      "epoch: 7 step: 106, loss is 0.0035509844310581684\n",
      "epoch: 7 step: 107, loss is 0.001019253279082477\n",
      "epoch: 7 step: 108, loss is 0.0013193127233535051\n",
      "epoch: 7 step: 109, loss is 0.02616959623992443\n",
      "epoch: 7 step: 110, loss is 0.0007103444077074528\n",
      "epoch: 7 step: 111, loss is 0.1589311957359314\n",
      "epoch: 7 step: 112, loss is 0.0005106460303068161\n",
      "epoch: 7 step: 113, loss is 0.0063887969590723515\n",
      "epoch: 7 step: 114, loss is 0.002109138062223792\n",
      "epoch: 7 step: 115, loss is 0.09349071979522705\n",
      "epoch: 7 step: 116, loss is 0.011114483699202538\n",
      "epoch: 7 step: 117, loss is 0.004957711789757013\n",
      "epoch: 7 step: 118, loss is 0.00040424574399366975\n",
      "epoch: 7 step: 119, loss is 7.082920637913048e-05\n",
      "epoch: 7 step: 120, loss is 0.0005245618522167206\n",
      "epoch: 7 step: 121, loss is 0.019640710204839706\n",
      "epoch: 7 step: 122, loss is 0.08828316628932953\n",
      "epoch: 7 step: 123, loss is 0.0019897501915693283\n",
      "epoch: 7 step: 124, loss is 0.025462551042437553\n",
      "epoch: 7 step: 125, loss is 0.0044417353346943855\n",
      "epoch: 7 step: 126, loss is 0.05502483993768692\n",
      "epoch: 7 step: 127, loss is 0.013683075085282326\n",
      "epoch: 7 step: 128, loss is 0.007615785580128431\n",
      "epoch: 7 step: 129, loss is 0.00043499309686012566\n",
      "epoch: 7 step: 130, loss is 0.0012054399121552706\n",
      "epoch: 7 step: 131, loss is 0.03715440258383751\n",
      "epoch: 7 step: 132, loss is 0.004042807035148144\n",
      "epoch: 7 step: 133, loss is 0.0007727110642008483\n",
      "epoch: 7 step: 134, loss is 0.017891336232423782\n",
      "epoch: 7 step: 135, loss is 0.00020156559185124934\n",
      "epoch: 7 step: 136, loss is 0.0025442002806812525\n",
      "epoch: 7 step: 137, loss is 0.005574185401201248\n",
      "epoch: 7 step: 138, loss is 0.00030079405405558646\n",
      "epoch: 7 step: 139, loss is 0.0047495258040726185\n",
      "epoch: 7 step: 140, loss is 0.055565617978572845\n",
      "epoch: 7 step: 141, loss is 0.004641002509742975\n",
      "epoch: 7 step: 142, loss is 0.012851317413151264\n",
      "epoch: 7 step: 143, loss is 0.00042185629718005657\n",
      "epoch: 7 step: 144, loss is 0.0016337513225153089\n",
      "epoch: 7 step: 145, loss is 0.011694391258060932\n",
      "epoch: 7 step: 146, loss is 0.0028387948404997587\n",
      "epoch: 7 step: 147, loss is 0.02550717256963253\n",
      "epoch: 7 step: 148, loss is 0.02478315681219101\n",
      "epoch: 7 step: 149, loss is 0.06295226514339447\n",
      "epoch: 7 step: 150, loss is 0.02277236431837082\n",
      "epoch: 7 step: 151, loss is 0.0012346092844381928\n",
      "epoch: 7 step: 152, loss is 0.0005856668576598167\n",
      "epoch: 7 step: 153, loss is 0.013342254795134068\n",
      "epoch: 7 step: 154, loss is 0.009004308842122555\n",
      "epoch: 7 step: 155, loss is 0.00021072170056868345\n",
      "epoch: 7 step: 156, loss is 0.0014833519235253334\n",
      "epoch: 7 step: 157, loss is 0.010733230970799923\n",
      "epoch: 7 step: 158, loss is 0.011354354210197926\n",
      "epoch: 7 step: 159, loss is 0.009020913392305374\n",
      "epoch: 7 step: 160, loss is 0.0020558801479637623\n",
      "epoch: 7 step: 161, loss is 0.03299807012081146\n",
      "epoch: 7 step: 162, loss is 0.05123296007514\n",
      "epoch: 7 step: 163, loss is 0.001169168739579618\n",
      "epoch: 7 step: 164, loss is 0.011268466711044312\n",
      "epoch: 7 step: 165, loss is 0.002550241770222783\n",
      "epoch: 7 step: 166, loss is 0.0030577541328966618\n",
      "epoch: 7 step: 167, loss is 0.003337844042107463\n",
      "epoch: 7 step: 168, loss is 0.000741169264074415\n",
      "epoch: 7 step: 169, loss is 0.00014035464846529067\n",
      "epoch: 7 step: 170, loss is 0.018962174654006958\n",
      "epoch: 7 step: 171, loss is 0.0034921246115118265\n",
      "epoch: 7 step: 172, loss is 0.0050427671521902084\n",
      "epoch: 7 step: 173, loss is 0.10454350709915161\n",
      "epoch: 7 step: 174, loss is 0.06406521797180176\n",
      "epoch: 7 step: 175, loss is 0.0770644024014473\n",
      "epoch: 7 step: 176, loss is 0.0008262262563221157\n",
      "epoch: 7 step: 177, loss is 0.005084548145532608\n",
      "epoch: 7 step: 178, loss is 0.0008305222727358341\n",
      "epoch: 7 step: 179, loss is 0.03473811224102974\n",
      "epoch: 7 step: 180, loss is 0.0031925877556204796\n",
      "epoch: 7 step: 181, loss is 0.0001119572261814028\n",
      "epoch: 7 step: 182, loss is 7.540586375398561e-05\n",
      "epoch: 7 step: 183, loss is 0.0006439106073230505\n",
      "epoch: 7 step: 184, loss is 0.00036674339207820594\n",
      "epoch: 7 step: 185, loss is 0.00051707006059587\n",
      "epoch: 7 step: 186, loss is 0.0030355150811374187\n",
      "epoch: 7 step: 187, loss is 8.551059727324173e-05\n",
      "epoch: 7 step: 188, loss is 0.007868906483054161\n",
      "epoch: 7 step: 189, loss is 0.00022242615523282439\n",
      "epoch: 7 step: 190, loss is 0.0009573689312674105\n",
      "epoch: 7 step: 191, loss is 0.1140458807349205\n",
      "epoch: 7 step: 192, loss is 0.004337318707257509\n",
      "epoch: 7 step: 193, loss is 0.0022035608999431133\n",
      "epoch: 7 step: 194, loss is 0.0011673191329464316\n",
      "epoch: 7 step: 195, loss is 0.060119353234767914\n",
      "epoch: 7 step: 196, loss is 0.002111539477482438\n",
      "epoch: 7 step: 197, loss is 0.0004813489504158497\n",
      "epoch: 7 step: 198, loss is 0.008891240693628788\n",
      "epoch: 7 step: 199, loss is 0.0010025565279647708\n",
      "epoch: 7 step: 200, loss is 0.41579002141952515\n",
      "epoch: 7 step: 201, loss is 0.0070031797513365746\n",
      "epoch: 7 step: 202, loss is 0.007693341467529535\n",
      "epoch: 7 step: 203, loss is 0.08436223864555359\n",
      "epoch: 7 step: 204, loss is 0.08011198043823242\n",
      "epoch: 7 step: 205, loss is 0.001774129457771778\n",
      "epoch: 7 step: 206, loss is 0.05527736619114876\n",
      "epoch: 7 step: 207, loss is 0.0007303467718884349\n",
      "epoch: 7 step: 208, loss is 0.0028547777328640223\n",
      "epoch: 7 step: 209, loss is 0.011902389116585255\n",
      "epoch: 7 step: 210, loss is 0.047742944210767746\n",
      "epoch: 7 step: 211, loss is 0.009820622391998768\n",
      "epoch: 7 step: 212, loss is 0.0030093975365161896\n",
      "epoch: 7 step: 213, loss is 0.016117926687002182\n",
      "epoch: 7 step: 214, loss is 0.02528860606253147\n",
      "epoch: 7 step: 215, loss is 0.005627106875181198\n",
      "epoch: 7 step: 216, loss is 0.04386701434850693\n",
      "epoch: 7 step: 217, loss is 0.02652197703719139\n",
      "epoch: 7 step: 218, loss is 0.0009969017701223493\n",
      "epoch: 7 step: 219, loss is 0.09431124478578568\n",
      "epoch: 7 step: 220, loss is 0.00032848905539140105\n",
      "epoch: 7 step: 221, loss is 0.010959114879369736\n",
      "epoch: 7 step: 222, loss is 0.008161349222064018\n",
      "epoch: 7 step: 223, loss is 0.0019219140522181988\n",
      "epoch: 7 step: 224, loss is 8.478552626911551e-05\n",
      "epoch: 7 step: 225, loss is 0.15057310461997986\n",
      "epoch: 7 step: 226, loss is 0.042329516261816025\n",
      "epoch: 7 step: 227, loss is 0.15182913839817047\n",
      "epoch: 7 step: 228, loss is 0.00044067567796446383\n",
      "epoch: 7 step: 229, loss is 0.0017686657374724746\n",
      "epoch: 7 step: 230, loss is 0.15137192606925964\n",
      "epoch: 7 step: 231, loss is 0.000850755488499999\n",
      "epoch: 7 step: 232, loss is 0.0017379270866513252\n",
      "epoch: 7 step: 233, loss is 0.08560655266046524\n",
      "epoch: 7 step: 234, loss is 0.0016589981969445944\n",
      "epoch: 7 step: 235, loss is 0.0043644062243402\n",
      "epoch: 7 step: 236, loss is 0.00387598667293787\n",
      "epoch: 7 step: 237, loss is 0.0018940579611808062\n",
      "epoch: 7 step: 238, loss is 0.0015348775777965784\n",
      "epoch: 7 step: 239, loss is 0.18018405139446259\n",
      "epoch: 7 step: 240, loss is 0.020254837349057198\n",
      "epoch: 7 step: 241, loss is 0.01019243523478508\n",
      "epoch: 7 step: 242, loss is 0.026427429169416428\n",
      "epoch: 7 step: 243, loss is 0.02130701206624508\n",
      "epoch: 7 step: 244, loss is 0.00012657039042096585\n",
      "epoch: 7 step: 245, loss is 0.003975329454988241\n",
      "epoch: 7 step: 246, loss is 0.020431295037269592\n",
      "epoch: 7 step: 247, loss is 0.010867513716220856\n",
      "epoch: 7 step: 248, loss is 0.0019940051715821028\n",
      "epoch: 7 step: 249, loss is 0.002157140290364623\n",
      "epoch: 7 step: 250, loss is 0.007929086685180664\n",
      "epoch: 7 step: 251, loss is 0.0005738345789723098\n",
      "epoch: 7 step: 252, loss is 0.0014098366955295205\n",
      "epoch: 7 step: 253, loss is 0.0261007659137249\n",
      "epoch: 7 step: 254, loss is 0.00020576408132910728\n",
      "epoch: 7 step: 255, loss is 0.02772487699985504\n",
      "epoch: 7 step: 256, loss is 0.0016599135706201196\n",
      "epoch: 7 step: 257, loss is 0.0018632856663316488\n",
      "epoch: 7 step: 258, loss is 0.024714116007089615\n",
      "epoch: 7 step: 259, loss is 0.00036922734580002725\n",
      "epoch: 7 step: 260, loss is 0.000608864997047931\n",
      "epoch: 7 step: 261, loss is 0.0481511652469635\n",
      "epoch: 7 step: 262, loss is 0.0006858202395960689\n",
      "epoch: 7 step: 263, loss is 0.006552761420607567\n",
      "epoch: 7 step: 264, loss is 0.00030938294366933405\n",
      "epoch: 7 step: 265, loss is 0.017953187227249146\n",
      "epoch: 7 step: 266, loss is 0.013925006613135338\n",
      "epoch: 7 step: 267, loss is 0.000604508793912828\n",
      "epoch: 7 step: 268, loss is 0.000988597166724503\n",
      "epoch: 7 step: 269, loss is 0.0033492245711386204\n",
      "epoch: 7 step: 270, loss is 0.00017214819672517478\n",
      "epoch: 7 step: 271, loss is 0.000792310805991292\n",
      "epoch: 7 step: 272, loss is 0.00022725790040567517\n",
      "epoch: 7 step: 273, loss is 0.05791053548455238\n",
      "epoch: 7 step: 274, loss is 0.0003104698844254017\n",
      "epoch: 7 step: 275, loss is 0.013044266030192375\n",
      "epoch: 7 step: 276, loss is 0.008929936215281487\n",
      "epoch: 7 step: 277, loss is 0.0001398894819431007\n",
      "epoch: 7 step: 278, loss is 0.001888211234472692\n",
      "epoch: 7 step: 279, loss is 0.011309294030070305\n",
      "epoch: 7 step: 280, loss is 0.003348854137584567\n",
      "epoch: 7 step: 281, loss is 0.0009436345426365733\n",
      "epoch: 7 step: 282, loss is 0.17302978038787842\n",
      "epoch: 7 step: 283, loss is 0.009390048682689667\n",
      "epoch: 7 step: 284, loss is 0.004548402037471533\n",
      "epoch: 7 step: 285, loss is 0.0002970331406686455\n",
      "epoch: 7 step: 286, loss is 0.00855749286711216\n",
      "epoch: 7 step: 287, loss is 0.0026606598403304815\n",
      "epoch: 7 step: 288, loss is 0.009009451605379581\n",
      "epoch: 7 step: 289, loss is 0.0009541742620058358\n",
      "epoch: 7 step: 290, loss is 0.010107900016009808\n",
      "epoch: 7 step: 291, loss is 0.023669399321079254\n",
      "epoch: 7 step: 292, loss is 0.00267383293248713\n",
      "epoch: 7 step: 293, loss is 0.07294231653213501\n",
      "epoch: 7 step: 294, loss is 0.0003618427726905793\n",
      "epoch: 7 step: 295, loss is 0.001430824282579124\n",
      "epoch: 7 step: 296, loss is 0.0009157821768894792\n",
      "epoch: 7 step: 297, loss is 0.0005442693363875151\n",
      "epoch: 7 step: 298, loss is 0.0016140345251187682\n",
      "epoch: 7 step: 299, loss is 0.0010682808933779597\n",
      "epoch: 7 step: 300, loss is 0.0014805602841079235\n",
      "epoch: 7 step: 301, loss is 0.0015956427669152617\n",
      "epoch: 7 step: 302, loss is 0.20764675736427307\n",
      "epoch: 7 step: 303, loss is 0.00035351229598745704\n",
      "epoch: 7 step: 304, loss is 0.005952366162091494\n",
      "epoch: 7 step: 305, loss is 0.0005079461843706667\n",
      "epoch: 7 step: 306, loss is 0.10433542728424072\n",
      "epoch: 7 step: 307, loss is 0.0031173410825431347\n",
      "epoch: 7 step: 308, loss is 0.09000028669834137\n",
      "epoch: 7 step: 309, loss is 0.010328656062483788\n",
      "epoch: 7 step: 310, loss is 0.003419324057176709\n",
      "epoch: 7 step: 311, loss is 0.10912313312292099\n",
      "epoch: 7 step: 312, loss is 0.003506897948682308\n",
      "epoch: 7 step: 313, loss is 0.00044545589480549097\n",
      "epoch: 7 step: 314, loss is 0.0008582633454352617\n",
      "epoch: 7 step: 315, loss is 0.004940638318657875\n",
      "epoch: 7 step: 316, loss is 0.1509469747543335\n",
      "epoch: 7 step: 317, loss is 0.006913596764206886\n",
      "epoch: 7 step: 318, loss is 0.04631068930029869\n",
      "epoch: 7 step: 319, loss is 0.0007252442883327603\n",
      "epoch: 7 step: 320, loss is 0.012718780897557735\n",
      "epoch: 7 step: 321, loss is 0.002838064916431904\n",
      "epoch: 7 step: 322, loss is 0.3161858022212982\n",
      "epoch: 7 step: 323, loss is 0.000798563938587904\n",
      "epoch: 7 step: 324, loss is 0.021404843777418137\n",
      "epoch: 7 step: 325, loss is 0.008848502300679684\n",
      "epoch: 7 step: 326, loss is 0.00015531061217188835\n",
      "epoch: 7 step: 327, loss is 0.08193392306566238\n",
      "epoch: 7 step: 328, loss is 0.0005874320049770176\n",
      "epoch: 7 step: 329, loss is 0.014293322339653969\n",
      "epoch: 7 step: 330, loss is 0.20966115593910217\n",
      "epoch: 7 step: 331, loss is 0.0013934349408373237\n",
      "epoch: 7 step: 332, loss is 0.0007681968272663653\n",
      "epoch: 7 step: 333, loss is 0.0033557615242898464\n",
      "epoch: 7 step: 334, loss is 0.005795336794108152\n",
      "epoch: 7 step: 335, loss is 0.0013980293879285455\n",
      "epoch: 7 step: 336, loss is 0.014533230103552341\n",
      "epoch: 7 step: 337, loss is 7.482868386432528e-05\n",
      "epoch: 7 step: 338, loss is 0.019641900435090065\n",
      "epoch: 7 step: 339, loss is 0.003194257616996765\n",
      "epoch: 7 step: 340, loss is 0.003314337693154812\n",
      "epoch: 7 step: 341, loss is 0.002603396074846387\n",
      "epoch: 7 step: 342, loss is 0.0002135863178409636\n",
      "epoch: 7 step: 343, loss is 0.005530891939997673\n",
      "epoch: 7 step: 344, loss is 0.0026801147032529116\n",
      "epoch: 7 step: 345, loss is 0.051926128566265106\n",
      "epoch: 7 step: 346, loss is 0.002165095880627632\n",
      "epoch: 7 step: 347, loss is 0.012495388276875019\n",
      "epoch: 7 step: 348, loss is 0.009102104231715202\n",
      "epoch: 7 step: 349, loss is 0.005066380836069584\n",
      "epoch: 7 step: 350, loss is 0.00031367846531793475\n",
      "epoch: 7 step: 351, loss is 0.10361238569021225\n",
      "epoch: 7 step: 352, loss is 0.004792577587068081\n",
      "epoch: 7 step: 353, loss is 0.0070899659767746925\n",
      "epoch: 7 step: 354, loss is 0.02973952144384384\n",
      "epoch: 7 step: 355, loss is 0.2296770215034485\n",
      "epoch: 7 step: 356, loss is 0.020969191566109657\n",
      "epoch: 7 step: 357, loss is 0.004055148921906948\n",
      "epoch: 7 step: 358, loss is 0.008276743814349174\n",
      "epoch: 7 step: 359, loss is 0.038233157247304916\n",
      "epoch: 7 step: 360, loss is 0.05945843830704689\n",
      "epoch: 7 step: 361, loss is 0.0020215739496052265\n",
      "epoch: 7 step: 362, loss is 0.005444359499961138\n",
      "epoch: 7 step: 363, loss is 0.1047409176826477\n",
      "epoch: 7 step: 364, loss is 0.0010906817624345422\n",
      "epoch: 7 step: 365, loss is 0.06522884964942932\n",
      "epoch: 7 step: 366, loss is 0.00676173297688365\n",
      "epoch: 7 step: 367, loss is 0.03277707099914551\n",
      "epoch: 7 step: 368, loss is 0.2228284776210785\n",
      "epoch: 7 step: 369, loss is 0.0016313261585310102\n",
      "epoch: 7 step: 370, loss is 0.006475064903497696\n",
      "epoch: 7 step: 371, loss is 0.0005553910741582513\n",
      "epoch: 7 step: 372, loss is 0.050943773239851\n",
      "epoch: 7 step: 373, loss is 0.00032626744359731674\n",
      "epoch: 7 step: 374, loss is 5.507300375029445e-05\n",
      "epoch: 7 step: 375, loss is 0.0038701919838786125\n",
      "epoch: 7 step: 376, loss is 0.014400696381926537\n",
      "epoch: 7 step: 377, loss is 0.010730290785431862\n",
      "epoch: 7 step: 378, loss is 0.0761553943157196\n",
      "epoch: 7 step: 379, loss is 0.00767155084758997\n",
      "epoch: 7 step: 380, loss is 0.0016778655117377639\n",
      "epoch: 7 step: 381, loss is 0.04021570459008217\n",
      "epoch: 7 step: 382, loss is 0.0002378015487920493\n",
      "epoch: 7 step: 383, loss is 0.1329669952392578\n",
      "epoch: 7 step: 384, loss is 0.25511419773101807\n",
      "epoch: 7 step: 385, loss is 0.01492086611688137\n",
      "epoch: 7 step: 386, loss is 0.004410256166011095\n",
      "epoch: 7 step: 387, loss is 0.0017558976542204618\n",
      "epoch: 7 step: 388, loss is 0.010760445147752762\n",
      "epoch: 7 step: 389, loss is 0.019271153956651688\n",
      "epoch: 7 step: 390, loss is 0.017420021817088127\n",
      "epoch: 7 step: 391, loss is 0.00027407248853705823\n",
      "epoch: 7 step: 392, loss is 0.004418684169650078\n",
      "epoch: 7 step: 393, loss is 0.01606213115155697\n",
      "epoch: 7 step: 394, loss is 0.0012690541334450245\n",
      "epoch: 7 step: 395, loss is 0.011551291681826115\n",
      "epoch: 7 step: 396, loss is 0.0016751845832914114\n",
      "epoch: 7 step: 397, loss is 0.0026159498374909163\n",
      "epoch: 7 step: 398, loss is 0.0003687577845994383\n",
      "epoch: 7 step: 399, loss is 8.259932656073943e-05\n",
      "epoch: 7 step: 400, loss is 0.02273283712565899\n",
      "epoch: 7 step: 401, loss is 0.06880329549312592\n",
      "epoch: 7 step: 402, loss is 0.0012524062767624855\n",
      "epoch: 7 step: 403, loss is 0.0006351827178150415\n",
      "epoch: 7 step: 404, loss is 0.013015310280025005\n",
      "epoch: 7 step: 405, loss is 0.007091102190315723\n",
      "epoch: 7 step: 406, loss is 0.10379774123430252\n",
      "epoch: 7 step: 407, loss is 0.00125537917483598\n",
      "epoch: 7 step: 408, loss is 0.0001661973656155169\n",
      "epoch: 7 step: 409, loss is 0.014813902787864208\n",
      "epoch: 7 step: 410, loss is 0.002394294599071145\n",
      "epoch: 7 step: 411, loss is 0.00908181257545948\n",
      "epoch: 7 step: 412, loss is 0.008782727643847466\n",
      "epoch: 7 step: 413, loss is 0.00828869454562664\n",
      "epoch: 7 step: 414, loss is 0.0038109682500362396\n",
      "epoch: 7 step: 415, loss is 0.0031533779110759497\n",
      "epoch: 7 step: 416, loss is 0.0006844211602583528\n",
      "epoch: 7 step: 417, loss is 0.015035927295684814\n",
      "epoch: 7 step: 418, loss is 0.003011186607182026\n",
      "epoch: 7 step: 419, loss is 0.08547389507293701\n",
      "epoch: 7 step: 420, loss is 0.002279361244291067\n",
      "epoch: 7 step: 421, loss is 0.04426528885960579\n",
      "epoch: 7 step: 422, loss is 0.008636237122118473\n",
      "epoch: 7 step: 423, loss is 0.0007592699257656932\n",
      "epoch: 7 step: 424, loss is 0.0018395273946225643\n",
      "epoch: 7 step: 425, loss is 0.0034135894384235144\n",
      "epoch: 7 step: 426, loss is 0.002947245491668582\n",
      "epoch: 7 step: 427, loss is 0.0012916793348267674\n",
      "epoch: 7 step: 428, loss is 0.00150814198423177\n",
      "epoch: 7 step: 429, loss is 0.003346362616866827\n",
      "epoch: 7 step: 430, loss is 0.0006150699337013066\n",
      "epoch: 7 step: 431, loss is 0.04045409709215164\n",
      "epoch: 7 step: 432, loss is 0.006527373101562262\n",
      "epoch: 7 step: 433, loss is 0.05039360001683235\n",
      "epoch: 7 step: 434, loss is 0.009903750382363796\n",
      "epoch: 7 step: 435, loss is 0.07174944132566452\n",
      "epoch: 7 step: 436, loss is 0.010522445663809776\n",
      "epoch: 7 step: 437, loss is 0.005323490127921104\n",
      "epoch: 7 step: 438, loss is 0.03177667409181595\n",
      "epoch: 7 step: 439, loss is 0.006244566757231951\n",
      "epoch: 7 step: 440, loss is 0.00012231549771968275\n",
      "epoch: 7 step: 441, loss is 0.0016937482869252563\n",
      "epoch: 7 step: 442, loss is 0.0371503122150898\n",
      "epoch: 7 step: 443, loss is 0.058818332850933075\n",
      "epoch: 7 step: 444, loss is 0.0017168031772598624\n",
      "epoch: 7 step: 445, loss is 0.03711995482444763\n",
      "epoch: 7 step: 446, loss is 0.0032315209973603487\n",
      "epoch: 7 step: 447, loss is 0.0006131972768343985\n",
      "epoch: 7 step: 448, loss is 0.000864718749653548\n",
      "epoch: 7 step: 449, loss is 0.00010149637091672048\n",
      "epoch: 7 step: 450, loss is 0.003859172109514475\n",
      "epoch: 7 step: 451, loss is 0.003749378491193056\n",
      "epoch: 7 step: 452, loss is 0.005712062586098909\n",
      "epoch: 7 step: 453, loss is 0.0006444062455557287\n",
      "epoch: 7 step: 454, loss is 0.0006514318520203233\n",
      "epoch: 7 step: 455, loss is 0.1357479989528656\n",
      "epoch: 7 step: 456, loss is 0.0009272691677324474\n",
      "epoch: 7 step: 457, loss is 0.0189459640532732\n",
      "epoch: 7 step: 458, loss is 0.0004043367807753384\n",
      "epoch: 7 step: 459, loss is 0.018332088366150856\n",
      "epoch: 7 step: 460, loss is 0.007056448608636856\n",
      "epoch: 7 step: 461, loss is 0.017089704051613808\n",
      "epoch: 7 step: 462, loss is 0.0033269182313233614\n",
      "epoch: 7 step: 463, loss is 0.011423178017139435\n",
      "epoch: 7 step: 464, loss is 0.00022076434106566012\n",
      "epoch: 7 step: 465, loss is 0.009275246411561966\n",
      "epoch: 7 step: 466, loss is 0.0032509304583072662\n",
      "epoch: 7 step: 467, loss is 0.005872633308172226\n",
      "epoch: 7 step: 468, loss is 0.003728029318153858\n",
      "epoch: 7 step: 469, loss is 0.002972894813865423\n",
      "epoch: 7 step: 470, loss is 0.00013263495930004865\n",
      "epoch: 7 step: 471, loss is 0.008143440820276737\n",
      "epoch: 7 step: 472, loss is 0.002204864053055644\n",
      "epoch: 7 step: 473, loss is 0.001100632594898343\n",
      "epoch: 7 step: 474, loss is 0.014993041753768921\n",
      "epoch: 7 step: 475, loss is 0.002305747242644429\n",
      "epoch: 7 step: 476, loss is 0.00019652582705020905\n",
      "epoch: 7 step: 477, loss is 0.002171487780287862\n",
      "epoch: 7 step: 478, loss is 0.02100946195423603\n",
      "epoch: 7 step: 479, loss is 0.025633560493588448\n",
      "epoch: 7 step: 480, loss is 0.12917104363441467\n",
      "epoch: 7 step: 481, loss is 0.02500738576054573\n",
      "epoch: 7 step: 482, loss is 0.009422216564416885\n",
      "epoch: 7 step: 483, loss is 0.27519768476486206\n",
      "epoch: 7 step: 484, loss is 0.004247539676725864\n",
      "epoch: 7 step: 485, loss is 0.007105845492333174\n",
      "epoch: 7 step: 486, loss is 0.017571531236171722\n",
      "epoch: 7 step: 487, loss is 0.0005086827441118658\n",
      "epoch: 7 step: 488, loss is 0.0008778862538747489\n",
      "epoch: 7 step: 489, loss is 0.024115633219480515\n",
      "epoch: 7 step: 490, loss is 0.010864191688597202\n",
      "epoch: 7 step: 491, loss is 0.07575248926877975\n",
      "epoch: 7 step: 492, loss is 0.00017452368047088385\n",
      "epoch: 7 step: 493, loss is 0.05823289230465889\n",
      "epoch: 7 step: 494, loss is 0.005890930537134409\n",
      "epoch: 7 step: 495, loss is 0.0028912294656038284\n",
      "epoch: 7 step: 496, loss is 0.16654455661773682\n",
      "epoch: 7 step: 497, loss is 0.00785026140511036\n",
      "epoch: 7 step: 498, loss is 0.004116571508347988\n",
      "epoch: 7 step: 499, loss is 0.00012203003279864788\n",
      "epoch: 7 step: 500, loss is 0.0006627464317716658\n",
      "epoch: 7 step: 501, loss is 0.0009925618069246411\n",
      "epoch: 7 step: 502, loss is 0.008174318820238113\n",
      "epoch: 7 step: 503, loss is 0.00016874996072147042\n",
      "epoch: 7 step: 504, loss is 0.007736251689493656\n",
      "epoch: 7 step: 505, loss is 0.038973353803157806\n",
      "epoch: 7 step: 506, loss is 0.008364531211555004\n",
      "epoch: 7 step: 507, loss is 0.0012551851104944944\n",
      "epoch: 7 step: 508, loss is 0.005193985067307949\n",
      "epoch: 7 step: 509, loss is 0.0006065298221074045\n",
      "epoch: 7 step: 510, loss is 0.05356611683964729\n",
      "epoch: 7 step: 511, loss is 0.0017696284921839833\n",
      "epoch: 7 step: 512, loss is 0.004789051599800587\n",
      "epoch: 7 step: 513, loss is 0.012565979734063148\n",
      "epoch: 7 step: 514, loss is 0.005209362134337425\n",
      "epoch: 7 step: 515, loss is 0.03187156468629837\n",
      "epoch: 7 step: 516, loss is 0.002505658892914653\n",
      "epoch: 7 step: 517, loss is 0.004190017934888601\n",
      "epoch: 7 step: 518, loss is 0.012148888781666756\n",
      "epoch: 7 step: 519, loss is 0.0006906447815708816\n",
      "epoch: 7 step: 520, loss is 0.07057519257068634\n",
      "epoch: 7 step: 521, loss is 0.0002893107011914253\n",
      "epoch: 7 step: 522, loss is 7.826946239219978e-05\n",
      "epoch: 7 step: 523, loss is 0.0024599628522992134\n",
      "epoch: 7 step: 524, loss is 0.0001918901689350605\n",
      "epoch: 7 step: 525, loss is 0.05103151127696037\n",
      "epoch: 7 step: 526, loss is 0.015292595140635967\n",
      "epoch: 7 step: 527, loss is 5.3932297305436805e-05\n",
      "epoch: 7 step: 528, loss is 0.0007699734997004271\n",
      "epoch: 7 step: 529, loss is 0.005550495348870754\n",
      "epoch: 7 step: 530, loss is 0.001010532840155065\n",
      "epoch: 7 step: 531, loss is 0.0011163249146193266\n",
      "epoch: 7 step: 532, loss is 0.14866583049297333\n",
      "epoch: 7 step: 533, loss is 0.00594509718939662\n",
      "epoch: 7 step: 534, loss is 0.08206531405448914\n",
      "epoch: 7 step: 535, loss is 0.04785803705453873\n",
      "epoch: 7 step: 536, loss is 0.029014162719249725\n",
      "epoch: 7 step: 537, loss is 0.002379696350544691\n",
      "epoch: 7 step: 538, loss is 0.04148303344845772\n",
      "epoch: 7 step: 539, loss is 0.005848947912454605\n",
      "epoch: 7 step: 540, loss is 0.00045810642768628895\n",
      "epoch: 7 step: 541, loss is 0.05669838562607765\n",
      "epoch: 7 step: 542, loss is 0.004469418898224831\n",
      "epoch: 7 step: 543, loss is 0.0002965534513350576\n",
      "epoch: 7 step: 544, loss is 0.0001643734285607934\n",
      "epoch: 7 step: 545, loss is 0.013439550995826721\n",
      "epoch: 7 step: 546, loss is 0.0271591916680336\n",
      "epoch: 7 step: 547, loss is 0.030719997361302376\n",
      "epoch: 7 step: 548, loss is 0.040779564529657364\n",
      "epoch: 7 step: 549, loss is 0.0024146907962858677\n",
      "epoch: 7 step: 550, loss is 0.04055439308285713\n",
      "epoch: 7 step: 551, loss is 0.00022521860955748707\n",
      "epoch: 7 step: 552, loss is 0.09081663191318512\n",
      "epoch: 7 step: 553, loss is 0.058117154985666275\n",
      "epoch: 7 step: 554, loss is 0.0009053401299752295\n",
      "epoch: 7 step: 555, loss is 0.030655428767204285\n",
      "epoch: 7 step: 556, loss is 0.006096181459724903\n",
      "epoch: 7 step: 557, loss is 0.00031048007076606154\n",
      "epoch: 7 step: 558, loss is 0.029876934364438057\n",
      "epoch: 7 step: 559, loss is 0.026328183710575104\n",
      "epoch: 7 step: 560, loss is 0.043187905102968216\n",
      "epoch: 7 step: 561, loss is 0.20548346638679504\n",
      "epoch: 7 step: 562, loss is 0.004177505616098642\n",
      "epoch: 7 step: 563, loss is 0.004066456574946642\n",
      "epoch: 7 step: 564, loss is 0.04193659499287605\n",
      "epoch: 7 step: 565, loss is 0.00032439158530905843\n",
      "epoch: 7 step: 566, loss is 0.00011978847032878548\n",
      "epoch: 7 step: 567, loss is 0.0005531942588277161\n",
      "epoch: 7 step: 568, loss is 0.009667390957474709\n",
      "epoch: 7 step: 569, loss is 0.001337090041488409\n",
      "epoch: 7 step: 570, loss is 0.0002743900113273412\n",
      "epoch: 7 step: 571, loss is 0.0007242712308652699\n",
      "epoch: 7 step: 572, loss is 0.005188979208469391\n",
      "epoch: 7 step: 573, loss is 0.027132423594594002\n",
      "epoch: 7 step: 574, loss is 0.000638944620732218\n",
      "epoch: 7 step: 575, loss is 0.013016613200306892\n",
      "epoch: 7 step: 576, loss is 0.009324848651885986\n",
      "epoch: 7 step: 577, loss is 0.000741460477001965\n",
      "epoch: 7 step: 578, loss is 0.0160483680665493\n",
      "epoch: 7 step: 579, loss is 0.00013544328976422548\n",
      "epoch: 7 step: 580, loss is 0.006763172335922718\n",
      "epoch: 7 step: 581, loss is 0.0050803907215595245\n",
      "epoch: 7 step: 582, loss is 0.23887835443019867\n",
      "epoch: 7 step: 583, loss is 0.00011230720701860264\n",
      "epoch: 7 step: 584, loss is 0.17593473196029663\n",
      "epoch: 7 step: 585, loss is 0.0002555742976255715\n",
      "epoch: 7 step: 586, loss is 0.004916772246360779\n",
      "epoch: 7 step: 587, loss is 0.13334670662879944\n",
      "epoch: 7 step: 588, loss is 0.014215138740837574\n",
      "epoch: 7 step: 589, loss is 0.0004429618129506707\n",
      "epoch: 7 step: 590, loss is 0.0007412449922412634\n",
      "epoch: 7 step: 591, loss is 0.0926712155342102\n",
      "epoch: 7 step: 592, loss is 0.004589098505675793\n",
      "epoch: 7 step: 593, loss is 0.0013951968867331743\n",
      "epoch: 7 step: 594, loss is 0.000874336517881602\n",
      "epoch: 7 step: 595, loss is 0.0004722047597169876\n",
      "epoch: 7 step: 596, loss is 0.00016676122322678566\n",
      "epoch: 7 step: 597, loss is 0.07359331101179123\n",
      "epoch: 7 step: 598, loss is 0.0011956178350374103\n",
      "epoch: 7 step: 599, loss is 0.000644754443783313\n",
      "epoch: 7 step: 600, loss is 0.024319233372807503\n",
      "epoch: 7 step: 601, loss is 0.0017748536774888635\n",
      "epoch: 7 step: 602, loss is 0.0002945303567685187\n",
      "epoch: 7 step: 603, loss is 0.0002562531444709748\n",
      "epoch: 7 step: 604, loss is 0.00417535612359643\n",
      "epoch: 7 step: 605, loss is 0.01496101450175047\n",
      "epoch: 7 step: 606, loss is 0.0016226499574258924\n",
      "epoch: 7 step: 607, loss is 0.01891167089343071\n",
      "epoch: 7 step: 608, loss is 0.005137498024851084\n",
      "epoch: 7 step: 609, loss is 0.002154821529984474\n",
      "epoch: 7 step: 610, loss is 0.0037398829590529203\n",
      "epoch: 7 step: 611, loss is 0.01409801933914423\n",
      "epoch: 7 step: 612, loss is 6.981650221860036e-05\n",
      "epoch: 7 step: 613, loss is 0.023930257186293602\n",
      "epoch: 7 step: 614, loss is 0.002052550669759512\n",
      "epoch: 7 step: 615, loss is 0.12184014171361923\n",
      "epoch: 7 step: 616, loss is 0.0003296661889180541\n",
      "epoch: 7 step: 617, loss is 0.003297459566965699\n",
      "epoch: 7 step: 618, loss is 0.24962900578975677\n",
      "epoch: 7 step: 619, loss is 0.03379932418465614\n",
      "epoch: 7 step: 620, loss is 0.05162554234266281\n",
      "epoch: 7 step: 621, loss is 0.00042348369606770575\n",
      "epoch: 7 step: 622, loss is 0.0032098719384521246\n",
      "epoch: 7 step: 623, loss is 0.002556625520810485\n",
      "epoch: 7 step: 624, loss is 0.0023524826392531395\n",
      "epoch: 7 step: 625, loss is 0.0001888768601929769\n",
      "epoch: 7 step: 626, loss is 0.11645043641328812\n",
      "epoch: 7 step: 627, loss is 0.17758060991764069\n",
      "epoch: 7 step: 628, loss is 0.00737632205709815\n",
      "epoch: 7 step: 629, loss is 0.0044709136709570885\n",
      "epoch: 7 step: 630, loss is 0.0015358658274635673\n",
      "epoch: 7 step: 631, loss is 0.0008061174303293228\n",
      "epoch: 7 step: 632, loss is 0.0006764510180801153\n",
      "epoch: 7 step: 633, loss is 0.00027080628206022084\n",
      "epoch: 7 step: 634, loss is 0.0253224465996027\n",
      "epoch: 7 step: 635, loss is 0.03179284930229187\n",
      "epoch: 7 step: 636, loss is 0.010529388673603535\n",
      "epoch: 7 step: 637, loss is 0.004452001769095659\n",
      "epoch: 7 step: 638, loss is 0.00960050243884325\n",
      "epoch: 7 step: 639, loss is 0.0015547610819339752\n",
      "epoch: 7 step: 640, loss is 0.0003124528157059103\n",
      "epoch: 7 step: 641, loss is 0.002328656380996108\n",
      "epoch: 7 step: 642, loss is 0.044609732925891876\n",
      "epoch: 7 step: 643, loss is 0.011156062595546246\n",
      "epoch: 7 step: 644, loss is 0.004297263920307159\n",
      "epoch: 7 step: 645, loss is 0.1774321049451828\n",
      "epoch: 7 step: 646, loss is 8.267504017567262e-05\n",
      "epoch: 7 step: 647, loss is 0.013751923106610775\n",
      "epoch: 7 step: 648, loss is 0.00659241434186697\n",
      "epoch: 7 step: 649, loss is 0.00130943744443357\n",
      "epoch: 7 step: 650, loss is 0.005801440216600895\n",
      "epoch: 7 step: 651, loss is 0.0009071744862012565\n",
      "epoch: 7 step: 652, loss is 0.16456350684165955\n",
      "epoch: 7 step: 653, loss is 0.13095660507678986\n",
      "epoch: 7 step: 654, loss is 0.07125254720449448\n",
      "epoch: 7 step: 655, loss is 0.008982070721685886\n",
      "epoch: 7 step: 656, loss is 0.06593860685825348\n",
      "epoch: 7 step: 657, loss is 0.0890439972281456\n",
      "epoch: 7 step: 658, loss is 0.011738432571291924\n",
      "epoch: 7 step: 659, loss is 0.007976854220032692\n",
      "epoch: 7 step: 660, loss is 0.014768997207283974\n",
      "epoch: 7 step: 661, loss is 0.04365460202097893\n",
      "epoch: 7 step: 662, loss is 0.0007265735184773803\n",
      "epoch: 7 step: 663, loss is 0.00037917710142210126\n",
      "epoch: 7 step: 664, loss is 0.06223811209201813\n",
      "epoch: 7 step: 665, loss is 0.0031971195712685585\n",
      "epoch: 7 step: 666, loss is 0.019610347226262093\n",
      "epoch: 7 step: 667, loss is 0.0030473556835204363\n",
      "epoch: 7 step: 668, loss is 0.24259832501411438\n",
      "epoch: 7 step: 669, loss is 0.00040506821824237704\n",
      "epoch: 7 step: 670, loss is 0.0013024986255913973\n",
      "epoch: 7 step: 671, loss is 0.03519368916749954\n",
      "epoch: 7 step: 672, loss is 0.10133197903633118\n",
      "epoch: 7 step: 673, loss is 0.008534316904842854\n",
      "epoch: 7 step: 674, loss is 0.00924895890057087\n",
      "epoch: 7 step: 675, loss is 0.17180703580379486\n",
      "epoch: 7 step: 676, loss is 0.0018936885753646493\n",
      "epoch: 7 step: 677, loss is 0.0028296266682446003\n",
      "epoch: 7 step: 678, loss is 0.012638295069336891\n",
      "epoch: 7 step: 679, loss is 0.0027023525908589363\n",
      "epoch: 7 step: 680, loss is 0.015797322615981102\n",
      "epoch: 7 step: 681, loss is 0.0014185890322551131\n",
      "epoch: 7 step: 682, loss is 0.005379451438784599\n",
      "epoch: 7 step: 683, loss is 0.0010658339597284794\n",
      "epoch: 7 step: 684, loss is 0.012057055719196796\n",
      "epoch: 7 step: 685, loss is 0.001284091966226697\n",
      "epoch: 7 step: 686, loss is 0.02347053959965706\n",
      "epoch: 7 step: 687, loss is 0.0007325266487896442\n",
      "epoch: 7 step: 688, loss is 0.004420250188559294\n",
      "epoch: 7 step: 689, loss is 0.015351572073996067\n",
      "epoch: 7 step: 690, loss is 0.0005796548211947083\n",
      "epoch: 7 step: 691, loss is 0.019962463527917862\n",
      "epoch: 7 step: 692, loss is 0.009898552671074867\n",
      "epoch: 7 step: 693, loss is 0.031416334211826324\n",
      "epoch: 7 step: 694, loss is 0.0036187234800308943\n",
      "epoch: 7 step: 695, loss is 0.0002411955938441679\n",
      "epoch: 7 step: 696, loss is 0.04405246675014496\n",
      "epoch: 7 step: 697, loss is 0.018320154398679733\n",
      "epoch: 7 step: 698, loss is 0.006610611919313669\n",
      "epoch: 7 step: 699, loss is 0.010294489562511444\n",
      "epoch: 7 step: 700, loss is 0.0021895994432270527\n",
      "epoch: 7 step: 701, loss is 0.13980869948863983\n",
      "epoch: 7 step: 702, loss is 0.0008682276238687336\n",
      "epoch: 7 step: 703, loss is 0.17239046096801758\n",
      "epoch: 7 step: 704, loss is 0.0014273207634687424\n",
      "epoch: 7 step: 705, loss is 0.015628522261977196\n",
      "epoch: 7 step: 706, loss is 0.016335567459464073\n",
      "epoch: 7 step: 707, loss is 0.00019288212934043258\n",
      "epoch: 7 step: 708, loss is 0.03316899761557579\n",
      "epoch: 7 step: 709, loss is 0.10765853524208069\n",
      "epoch: 7 step: 710, loss is 0.0007851245463825762\n",
      "epoch: 7 step: 711, loss is 0.0031995587050914764\n",
      "epoch: 7 step: 712, loss is 0.0020906804129481316\n",
      "epoch: 7 step: 713, loss is 0.03297676146030426\n",
      "epoch: 7 step: 714, loss is 0.005572208669036627\n",
      "epoch: 7 step: 715, loss is 0.001491023227572441\n",
      "epoch: 7 step: 716, loss is 0.002876079175621271\n",
      "epoch: 7 step: 717, loss is 0.026011629030108452\n",
      "epoch: 7 step: 718, loss is 0.0008912495104596019\n",
      "epoch: 7 step: 719, loss is 0.018972041085362434\n",
      "epoch: 7 step: 720, loss is 0.0871601551771164\n",
      "epoch: 7 step: 721, loss is 0.0467093363404274\n",
      "epoch: 7 step: 722, loss is 0.009828347712755203\n",
      "epoch: 7 step: 723, loss is 0.0011450137244537473\n",
      "epoch: 7 step: 724, loss is 0.015474044717848301\n",
      "epoch: 7 step: 725, loss is 0.0017950307810679078\n",
      "epoch: 7 step: 726, loss is 0.15605925023555756\n",
      "epoch: 7 step: 727, loss is 0.0020396725740283728\n",
      "epoch: 7 step: 728, loss is 0.025180906057357788\n",
      "epoch: 7 step: 729, loss is 0.004904068540781736\n",
      "epoch: 7 step: 730, loss is 0.0007765104528516531\n",
      "epoch: 7 step: 731, loss is 0.014347595162689686\n",
      "epoch: 7 step: 732, loss is 0.0007969058351591229\n",
      "epoch: 7 step: 733, loss is 0.014634723775088787\n",
      "epoch: 7 step: 734, loss is 0.0006580043118447065\n",
      "epoch: 7 step: 735, loss is 0.008226504549384117\n",
      "epoch: 7 step: 736, loss is 0.0009163739159703255\n",
      "epoch: 7 step: 737, loss is 0.0015201752539724112\n",
      "epoch: 7 step: 738, loss is 0.002415563678368926\n",
      "epoch: 7 step: 739, loss is 0.025913842022418976\n",
      "epoch: 7 step: 740, loss is 0.003106531221419573\n",
      "epoch: 7 step: 741, loss is 0.19747313857078552\n",
      "epoch: 7 step: 742, loss is 0.00026655441615730524\n",
      "epoch: 7 step: 743, loss is 0.021233920007944107\n",
      "epoch: 7 step: 744, loss is 0.013939252123236656\n",
      "epoch: 7 step: 745, loss is 0.0003634802997112274\n",
      "epoch: 7 step: 746, loss is 0.01195257157087326\n",
      "epoch: 7 step: 747, loss is 0.0019471178529784083\n",
      "epoch: 7 step: 748, loss is 0.042805735021829605\n",
      "epoch: 7 step: 749, loss is 0.014937532134354115\n",
      "epoch: 7 step: 750, loss is 0.017939060926437378\n",
      "epoch: 7 step: 751, loss is 0.010076386854052544\n",
      "epoch: 7 step: 752, loss is 0.01962992548942566\n",
      "epoch: 7 step: 753, loss is 0.014722933992743492\n",
      "epoch: 7 step: 754, loss is 0.0013720232527703047\n",
      "epoch: 7 step: 755, loss is 0.005576055031269789\n",
      "epoch: 7 step: 756, loss is 0.12127698957920074\n",
      "epoch: 7 step: 757, loss is 0.019669966772198677\n",
      "epoch: 7 step: 758, loss is 0.0713745579123497\n",
      "epoch: 7 step: 759, loss is 0.021478954702615738\n",
      "epoch: 7 step: 760, loss is 0.007844790816307068\n",
      "epoch: 7 step: 761, loss is 0.012008859775960445\n",
      "epoch: 7 step: 762, loss is 0.007614303380250931\n",
      "epoch: 7 step: 763, loss is 0.006860972847789526\n",
      "epoch: 7 step: 764, loss is 0.010928762145340443\n",
      "epoch: 7 step: 765, loss is 0.0010029892437160015\n",
      "epoch: 7 step: 766, loss is 0.017789939418435097\n",
      "epoch: 7 step: 767, loss is 0.0012774817878380418\n",
      "epoch: 7 step: 768, loss is 0.006939797196537256\n",
      "epoch: 7 step: 769, loss is 0.025969700887799263\n",
      "epoch: 7 step: 770, loss is 0.0003110560355708003\n",
      "epoch: 7 step: 771, loss is 0.0008296798332594335\n",
      "epoch: 7 step: 772, loss is 0.0001464688975829631\n",
      "epoch: 7 step: 773, loss is 0.02169080637395382\n",
      "epoch: 7 step: 774, loss is 0.028173137456178665\n",
      "epoch: 7 step: 775, loss is 0.0010014655999839306\n",
      "epoch: 7 step: 776, loss is 0.011538509279489517\n",
      "epoch: 7 step: 777, loss is 0.06448714435100555\n",
      "epoch: 7 step: 778, loss is 0.0008694587158970535\n",
      "epoch: 7 step: 779, loss is 0.00821523368358612\n",
      "epoch: 7 step: 780, loss is 0.010094243101775646\n",
      "epoch: 7 step: 781, loss is 0.0003844409657176584\n",
      "epoch: 7 step: 782, loss is 0.010140922851860523\n",
      "epoch: 7 step: 783, loss is 0.011055064387619495\n",
      "epoch: 7 step: 784, loss is 0.0002914132783189416\n",
      "epoch: 7 step: 785, loss is 0.0855141282081604\n",
      "epoch: 7 step: 786, loss is 0.003196809906512499\n",
      "epoch: 7 step: 787, loss is 0.0007679019472561777\n",
      "epoch: 7 step: 788, loss is 0.15649397671222687\n",
      "epoch: 7 step: 789, loss is 0.0014720855979248881\n",
      "epoch: 7 step: 790, loss is 0.00018509333312977105\n",
      "epoch: 7 step: 791, loss is 0.0016788438661023974\n",
      "epoch: 7 step: 792, loss is 0.00032458006171509624\n",
      "epoch: 7 step: 793, loss is 0.00035635606036521494\n",
      "epoch: 7 step: 794, loss is 0.0016402732580900192\n",
      "epoch: 7 step: 795, loss is 8.834896289044991e-05\n",
      "epoch: 7 step: 796, loss is 0.00045800823136232793\n",
      "epoch: 7 step: 797, loss is 0.0039900303818285465\n",
      "epoch: 7 step: 798, loss is 0.0033158257137984037\n",
      "epoch: 7 step: 799, loss is 0.0001487733970861882\n",
      "epoch: 7 step: 800, loss is 0.005432439036667347\n",
      "epoch: 7 step: 801, loss is 0.0642000213265419\n",
      "epoch: 7 step: 802, loss is 0.005405331961810589\n",
      "epoch: 7 step: 803, loss is 0.08100734651088715\n",
      "epoch: 7 step: 804, loss is 0.01128698606044054\n",
      "epoch: 7 step: 805, loss is 0.0048075788654387\n",
      "epoch: 7 step: 806, loss is 0.00025607855059206486\n",
      "epoch: 7 step: 807, loss is 0.0040525454096496105\n",
      "epoch: 7 step: 808, loss is 0.06173328310251236\n",
      "epoch: 7 step: 809, loss is 0.0006416381802409887\n",
      "epoch: 7 step: 810, loss is 0.000816386251244694\n",
      "epoch: 7 step: 811, loss is 0.004616197198629379\n",
      "epoch: 7 step: 812, loss is 0.0552288219332695\n",
      "epoch: 7 step: 813, loss is 0.0006335329380817711\n",
      "epoch: 7 step: 814, loss is 0.0003104418865405023\n",
      "epoch: 7 step: 815, loss is 0.002204710152000189\n",
      "epoch: 7 step: 816, loss is 0.00016891132690943778\n",
      "epoch: 7 step: 817, loss is 0.0028607358690351248\n",
      "epoch: 7 step: 818, loss is 0.007150986697524786\n",
      "epoch: 7 step: 819, loss is 0.11905267834663391\n",
      "epoch: 7 step: 820, loss is 0.0004091300070285797\n",
      "epoch: 7 step: 821, loss is 0.0013119977666065097\n",
      "epoch: 7 step: 822, loss is 0.0003062424948439002\n",
      "epoch: 7 step: 823, loss is 0.0019792525563389063\n",
      "epoch: 7 step: 824, loss is 0.11531354486942291\n",
      "epoch: 7 step: 825, loss is 0.001964691560715437\n",
      "epoch: 7 step: 826, loss is 0.006417098920792341\n",
      "epoch: 7 step: 827, loss is 0.12040279805660248\n",
      "epoch: 7 step: 828, loss is 0.008057868108153343\n",
      "epoch: 7 step: 829, loss is 0.0013801470631733537\n",
      "epoch: 7 step: 830, loss is 0.06521864980459213\n",
      "epoch: 7 step: 831, loss is 0.0015559643507003784\n",
      "epoch: 7 step: 832, loss is 0.0890917256474495\n",
      "epoch: 7 step: 833, loss is 0.0047885519452393055\n",
      "epoch: 7 step: 834, loss is 0.0018457230180501938\n",
      "epoch: 7 step: 835, loss is 0.007469753734767437\n",
      "epoch: 7 step: 836, loss is 0.002595870289951563\n",
      "epoch: 7 step: 837, loss is 0.0028393601533025503\n",
      "epoch: 7 step: 838, loss is 0.005968981422483921\n",
      "epoch: 7 step: 839, loss is 0.0026192464865744114\n",
      "epoch: 7 step: 840, loss is 0.2111406773328781\n",
      "epoch: 7 step: 841, loss is 0.0025294723454862833\n",
      "epoch: 7 step: 842, loss is 0.15206344425678253\n",
      "epoch: 7 step: 843, loss is 0.03890758752822876\n",
      "epoch: 7 step: 844, loss is 0.004584861919283867\n",
      "epoch: 7 step: 845, loss is 0.0027656732127070427\n",
      "epoch: 7 step: 846, loss is 0.0024761988315731287\n",
      "epoch: 7 step: 847, loss is 0.00219112285412848\n",
      "epoch: 7 step: 848, loss is 0.05431690439581871\n",
      "epoch: 7 step: 849, loss is 0.0024850033223628998\n",
      "epoch: 7 step: 850, loss is 0.0025211137253791094\n",
      "epoch: 7 step: 851, loss is 0.0010141218081116676\n",
      "epoch: 7 step: 852, loss is 0.0009631306165829301\n",
      "epoch: 7 step: 853, loss is 0.0014954437501728535\n",
      "epoch: 7 step: 854, loss is 0.05986528471112251\n",
      "epoch: 7 step: 855, loss is 0.011011169292032719\n",
      "epoch: 7 step: 856, loss is 0.005771508906036615\n",
      "epoch: 7 step: 857, loss is 0.01417902484536171\n",
      "epoch: 7 step: 858, loss is 0.1617041379213333\n",
      "epoch: 7 step: 859, loss is 0.01385203842073679\n",
      "epoch: 7 step: 860, loss is 0.017530521377921104\n",
      "epoch: 7 step: 861, loss is 0.06531474739313126\n",
      "epoch: 7 step: 862, loss is 0.0015609471593052149\n",
      "epoch: 7 step: 863, loss is 0.006999367848038673\n",
      "epoch: 7 step: 864, loss is 0.0033211493864655495\n",
      "epoch: 7 step: 865, loss is 0.006290866062045097\n",
      "epoch: 7 step: 866, loss is 0.00011844769323943183\n",
      "epoch: 7 step: 867, loss is 0.005159325432032347\n",
      "epoch: 7 step: 868, loss is 0.008227751590311527\n",
      "epoch: 7 step: 869, loss is 0.029197391122579575\n",
      "epoch: 7 step: 870, loss is 0.1084115281701088\n",
      "epoch: 7 step: 871, loss is 0.0030564884655177593\n",
      "epoch: 7 step: 872, loss is 0.1726720631122589\n",
      "epoch: 7 step: 873, loss is 0.043338388204574585\n",
      "epoch: 7 step: 874, loss is 0.04843597114086151\n",
      "epoch: 7 step: 875, loss is 0.02544841356575489\n",
      "epoch: 7 step: 876, loss is 0.0627724751830101\n",
      "epoch: 7 step: 877, loss is 0.0016609585145488381\n",
      "epoch: 7 step: 878, loss is 0.002457094844430685\n",
      "epoch: 7 step: 879, loss is 0.0024361959658563137\n",
      "epoch: 7 step: 880, loss is 0.019889071583747864\n",
      "epoch: 7 step: 881, loss is 0.014138259924948215\n",
      "epoch: 7 step: 882, loss is 0.019236018881201744\n",
      "epoch: 7 step: 883, loss is 0.04582669213414192\n",
      "epoch: 7 step: 884, loss is 0.002232446800917387\n",
      "epoch: 7 step: 885, loss is 0.020375143736600876\n",
      "epoch: 7 step: 886, loss is 0.060335833579301834\n",
      "epoch: 7 step: 887, loss is 0.009795738384127617\n",
      "epoch: 7 step: 888, loss is 0.03326632082462311\n",
      "epoch: 7 step: 889, loss is 0.0009061020682565868\n",
      "epoch: 7 step: 890, loss is 0.0017877559876069427\n",
      "epoch: 7 step: 891, loss is 0.017962895333766937\n",
      "epoch: 7 step: 892, loss is 0.029508939012885094\n",
      "epoch: 7 step: 893, loss is 0.010013383813202381\n",
      "epoch: 7 step: 894, loss is 0.00183899886906147\n",
      "epoch: 7 step: 895, loss is 0.05991232022643089\n",
      "epoch: 7 step: 896, loss is 0.0013470727717503905\n",
      "epoch: 7 step: 897, loss is 0.0043012406677007675\n",
      "epoch: 7 step: 898, loss is 0.0033651208505034447\n",
      "epoch: 7 step: 899, loss is 0.03345832601189613\n",
      "epoch: 7 step: 900, loss is 0.005743964575231075\n",
      "epoch: 7 step: 901, loss is 0.0031171636655926704\n",
      "epoch: 7 step: 902, loss is 0.004015016369521618\n",
      "epoch: 7 step: 903, loss is 0.005363209173083305\n",
      "epoch: 7 step: 904, loss is 0.04959716647863388\n",
      "epoch: 7 step: 905, loss is 0.07029535621404648\n",
      "epoch: 7 step: 906, loss is 0.011123008094727993\n",
      "epoch: 7 step: 907, loss is 0.05083189532160759\n",
      "epoch: 7 step: 908, loss is 0.0008213850669562817\n",
      "epoch: 7 step: 909, loss is 0.03491366282105446\n",
      "epoch: 7 step: 910, loss is 0.0027218027971684933\n",
      "epoch: 7 step: 911, loss is 0.0028982709627598524\n",
      "epoch: 7 step: 912, loss is 0.0005605144542641938\n",
      "epoch: 7 step: 913, loss is 0.002075664699077606\n",
      "epoch: 7 step: 914, loss is 0.03902274742722511\n",
      "epoch: 7 step: 915, loss is 0.0002607658097986132\n",
      "epoch: 7 step: 916, loss is 6.0729431424988434e-05\n",
      "epoch: 7 step: 917, loss is 8.0509387771599e-05\n",
      "epoch: 7 step: 918, loss is 0.0073768459260463715\n",
      "epoch: 7 step: 919, loss is 0.13474655151367188\n",
      "epoch: 7 step: 920, loss is 0.0014371993020176888\n",
      "epoch: 7 step: 921, loss is 0.009146938100457191\n",
      "epoch: 7 step: 922, loss is 0.0019843035843223333\n",
      "epoch: 7 step: 923, loss is 0.0031189413275569677\n",
      "epoch: 7 step: 924, loss is 0.0011628002393990755\n",
      "epoch: 7 step: 925, loss is 0.0038922985550016165\n",
      "epoch: 7 step: 926, loss is 0.0009367110906168818\n",
      "epoch: 7 step: 927, loss is 0.0004223187279421836\n",
      "epoch: 7 step: 928, loss is 0.027441272512078285\n",
      "epoch: 7 step: 929, loss is 0.11192624270915985\n",
      "epoch: 7 step: 930, loss is 0.10689093917608261\n",
      "epoch: 7 step: 931, loss is 0.004849969409406185\n",
      "epoch: 7 step: 932, loss is 0.0069017913192510605\n",
      "epoch: 7 step: 933, loss is 0.003280449192970991\n",
      "epoch: 7 step: 934, loss is 0.002180156297981739\n",
      "epoch: 7 step: 935, loss is 0.009684585966169834\n",
      "epoch: 7 step: 936, loss is 0.000405141239752993\n",
      "epoch: 7 step: 937, loss is 0.03732087090611458\n",
      "epoch: 7 step: 938, loss is 0.028860215097665787\n",
      "epoch: 7 step: 939, loss is 0.004814383573830128\n",
      "epoch: 7 step: 940, loss is 0.00975001323968172\n",
      "epoch: 7 step: 941, loss is 0.002849049400538206\n",
      "epoch: 7 step: 942, loss is 0.0033319899812340736\n",
      "epoch: 7 step: 943, loss is 0.00046124786604195833\n",
      "epoch: 7 step: 944, loss is 0.2929801344871521\n",
      "epoch: 7 step: 945, loss is 0.002774552209302783\n",
      "epoch: 7 step: 946, loss is 0.09851666539907455\n",
      "epoch: 7 step: 947, loss is 0.04384208470582962\n",
      "epoch: 7 step: 948, loss is 0.001210440881550312\n",
      "epoch: 7 step: 949, loss is 0.01161800418049097\n",
      "epoch: 7 step: 950, loss is 0.13501855731010437\n",
      "epoch: 7 step: 951, loss is 0.006312749348580837\n",
      "epoch: 7 step: 952, loss is 0.0004232986248098314\n",
      "epoch: 7 step: 953, loss is 0.03666631504893303\n",
      "epoch: 7 step: 954, loss is 0.06359928101301193\n",
      "epoch: 7 step: 955, loss is 0.002762880176305771\n",
      "epoch: 7 step: 956, loss is 0.008620805107057095\n",
      "epoch: 7 step: 957, loss is 0.034160908311605453\n",
      "epoch: 7 step: 958, loss is 0.023309769108891487\n",
      "epoch: 7 step: 959, loss is 0.038413506001234055\n",
      "epoch: 7 step: 960, loss is 0.1886959820985794\n",
      "epoch: 7 step: 961, loss is 0.0029857975896447897\n",
      "epoch: 7 step: 962, loss is 0.0009237262420356274\n",
      "epoch: 7 step: 963, loss is 0.0007350917439907789\n",
      "epoch: 7 step: 964, loss is 0.0031920073088258505\n",
      "epoch: 7 step: 965, loss is 0.0012063324684277177\n",
      "epoch: 7 step: 966, loss is 0.002331711817532778\n",
      "epoch: 7 step: 967, loss is 0.00023472507018595934\n",
      "epoch: 7 step: 968, loss is 0.008670919574797153\n",
      "epoch: 7 step: 969, loss is 0.01058567501604557\n",
      "epoch: 7 step: 970, loss is 0.00044664612505584955\n",
      "epoch: 7 step: 971, loss is 0.002632610034197569\n",
      "epoch: 7 step: 972, loss is 0.00030654476722702384\n",
      "epoch: 7 step: 973, loss is 0.04882631450891495\n",
      "epoch: 7 step: 974, loss is 0.008314350619912148\n",
      "epoch: 7 step: 975, loss is 0.00014609792560804635\n",
      "epoch: 7 step: 976, loss is 0.036124132573604584\n",
      "epoch: 7 step: 977, loss is 0.00048238414456136525\n",
      "epoch: 7 step: 978, loss is 0.14870746433734894\n",
      "epoch: 7 step: 979, loss is 0.007641131989657879\n",
      "epoch: 7 step: 980, loss is 0.010203731246292591\n",
      "epoch: 7 step: 981, loss is 0.00020483223488554358\n",
      "epoch: 7 step: 982, loss is 0.001512062968686223\n",
      "epoch: 7 step: 983, loss is 0.037007223814725876\n",
      "epoch: 7 step: 984, loss is 8.121281280182302e-05\n",
      "epoch: 7 step: 985, loss is 0.0002794157189782709\n",
      "epoch: 7 step: 986, loss is 0.07167358696460724\n",
      "epoch: 7 step: 987, loss is 0.00010663219291018322\n",
      "epoch: 7 step: 988, loss is 0.010562346316874027\n",
      "epoch: 7 step: 989, loss is 0.0003651036531664431\n",
      "epoch: 7 step: 990, loss is 0.010086615569889545\n",
      "epoch: 7 step: 991, loss is 0.024136757478117943\n",
      "epoch: 7 step: 992, loss is 0.002206843812018633\n",
      "epoch: 7 step: 993, loss is 0.0011283480562269688\n",
      "epoch: 7 step: 994, loss is 0.036124859005212784\n",
      "epoch: 7 step: 995, loss is 0.0026931692846119404\n",
      "epoch: 7 step: 996, loss is 0.0035981559194624424\n",
      "epoch: 7 step: 997, loss is 0.00030299820355139673\n",
      "epoch: 7 step: 998, loss is 0.009824976325035095\n",
      "epoch: 7 step: 999, loss is 0.00010253071377519518\n",
      "epoch: 7 step: 1000, loss is 0.022747622802853584\n",
      "epoch: 7 step: 1001, loss is 0.14199122786521912\n",
      "epoch: 7 step: 1002, loss is 0.05315876752138138\n",
      "epoch: 7 step: 1003, loss is 3.6228859244147316e-05\n",
      "epoch: 7 step: 1004, loss is 0.00319031928665936\n",
      "epoch: 7 step: 1005, loss is 0.001973225036635995\n",
      "epoch: 7 step: 1006, loss is 0.00028731071506626904\n",
      "epoch: 7 step: 1007, loss is 0.0012066250201314688\n",
      "epoch: 7 step: 1008, loss is 0.00047611602349206805\n",
      "epoch: 7 step: 1009, loss is 6.533836858579889e-05\n",
      "epoch: 7 step: 1010, loss is 0.0005816282355226576\n",
      "epoch: 7 step: 1011, loss is 0.008999746292829514\n",
      "epoch: 7 step: 1012, loss is 0.029771627858281136\n",
      "epoch: 7 step: 1013, loss is 0.03513694554567337\n",
      "epoch: 7 step: 1014, loss is 0.37642815709114075\n",
      "epoch: 7 step: 1015, loss is 0.047797687351703644\n",
      "epoch: 7 step: 1016, loss is 0.0007544748950749636\n",
      "epoch: 7 step: 1017, loss is 0.018182829022407532\n",
      "epoch: 7 step: 1018, loss is 0.006428385153412819\n",
      "epoch: 7 step: 1019, loss is 0.003501620376482606\n",
      "epoch: 7 step: 1020, loss is 0.00010384478082414716\n",
      "epoch: 7 step: 1021, loss is 0.00019988948770333081\n",
      "epoch: 7 step: 1022, loss is 0.0010496523464098573\n",
      "epoch: 7 step: 1023, loss is 0.0035355144646018744\n",
      "epoch: 7 step: 1024, loss is 0.0028500736225396395\n",
      "epoch: 7 step: 1025, loss is 5.885363134439103e-05\n",
      "epoch: 7 step: 1026, loss is 0.0012078842846676707\n",
      "epoch: 7 step: 1027, loss is 0.38566067814826965\n",
      "epoch: 7 step: 1028, loss is 0.00026387922116555274\n",
      "epoch: 7 step: 1029, loss is 0.0024952800013124943\n",
      "epoch: 7 step: 1030, loss is 0.02818232774734497\n",
      "epoch: 7 step: 1031, loss is 0.0008944675209932029\n",
      "epoch: 7 step: 1032, loss is 0.00014640524750575423\n",
      "epoch: 7 step: 1033, loss is 0.0011807812843471766\n",
      "epoch: 7 step: 1034, loss is 0.001190526527352631\n",
      "epoch: 7 step: 1035, loss is 0.09045663475990295\n",
      "epoch: 7 step: 1036, loss is 0.0021128966473042965\n",
      "epoch: 7 step: 1037, loss is 0.00033646440715529025\n",
      "epoch: 7 step: 1038, loss is 0.0003365878073964268\n",
      "epoch: 7 step: 1039, loss is 0.00021451618522405624\n",
      "epoch: 7 step: 1040, loss is 0.012809502892196178\n",
      "epoch: 7 step: 1041, loss is 0.11662942916154861\n",
      "epoch: 7 step: 1042, loss is 0.007984867319464684\n",
      "epoch: 7 step: 1043, loss is 0.02015078440308571\n",
      "epoch: 7 step: 1044, loss is 0.09534420818090439\n",
      "epoch: 7 step: 1045, loss is 0.06080164760351181\n",
      "epoch: 7 step: 1046, loss is 0.01966048590838909\n",
      "epoch: 7 step: 1047, loss is 0.00040944720967672765\n",
      "epoch: 7 step: 1048, loss is 0.004272505175322294\n",
      "epoch: 7 step: 1049, loss is 0.0014599751448258758\n",
      "epoch: 7 step: 1050, loss is 0.001009965781122446\n",
      "epoch: 7 step: 1051, loss is 0.0022994193714112043\n",
      "epoch: 7 step: 1052, loss is 0.002793713705614209\n",
      "epoch: 7 step: 1053, loss is 0.0006694609182886779\n",
      "epoch: 7 step: 1054, loss is 0.025235673412680626\n",
      "epoch: 7 step: 1055, loss is 0.014658146537840366\n",
      "epoch: 7 step: 1056, loss is 0.0006220167269930243\n",
      "epoch: 7 step: 1057, loss is 0.08425304293632507\n",
      "epoch: 7 step: 1058, loss is 0.041315268725156784\n",
      "epoch: 7 step: 1059, loss is 0.0005661069881170988\n",
      "epoch: 7 step: 1060, loss is 0.027351103723049164\n",
      "epoch: 7 step: 1061, loss is 0.14603015780448914\n",
      "epoch: 7 step: 1062, loss is 0.0005837698699906468\n",
      "epoch: 7 step: 1063, loss is 0.035253800451755524\n",
      "epoch: 7 step: 1064, loss is 0.0023236386477947235\n",
      "epoch: 7 step: 1065, loss is 0.10792026668787003\n",
      "epoch: 7 step: 1066, loss is 0.0017872507451102138\n",
      "epoch: 7 step: 1067, loss is 0.0009691178565844893\n",
      "epoch: 7 step: 1068, loss is 0.011706569232046604\n",
      "epoch: 7 step: 1069, loss is 0.007972639054059982\n",
      "epoch: 7 step: 1070, loss is 0.0004338995204307139\n",
      "epoch: 7 step: 1071, loss is 0.004038069862872362\n",
      "epoch: 7 step: 1072, loss is 0.0020878342911601067\n",
      "epoch: 7 step: 1073, loss is 0.0003881666634697467\n",
      "epoch: 7 step: 1074, loss is 0.0004658260731957853\n",
      "epoch: 7 step: 1075, loss is 0.06046207994222641\n",
      "epoch: 7 step: 1076, loss is 0.002444162033498287\n",
      "epoch: 7 step: 1077, loss is 6.363860302371904e-05\n",
      "epoch: 7 step: 1078, loss is 0.013778799213469028\n",
      "epoch: 7 step: 1079, loss is 0.027676839381456375\n",
      "epoch: 7 step: 1080, loss is 0.0033814962953329086\n",
      "epoch: 7 step: 1081, loss is 0.0009325842838734388\n",
      "epoch: 7 step: 1082, loss is 0.02164616622030735\n",
      "epoch: 7 step: 1083, loss is 0.0041848476976156235\n",
      "epoch: 7 step: 1084, loss is 0.0018283519893884659\n",
      "epoch: 7 step: 1085, loss is 0.00019633847114164382\n",
      "epoch: 7 step: 1086, loss is 0.0006793852662667632\n",
      "epoch: 7 step: 1087, loss is 0.0006096541183069348\n",
      "epoch: 7 step: 1088, loss is 0.026505928486585617\n",
      "epoch: 7 step: 1089, loss is 0.0009750289027579129\n",
      "epoch: 7 step: 1090, loss is 0.0024591637775301933\n",
      "epoch: 7 step: 1091, loss is 0.004343935288488865\n",
      "epoch: 7 step: 1092, loss is 0.00011409113358240575\n",
      "epoch: 7 step: 1093, loss is 0.00013573357136920094\n",
      "epoch: 7 step: 1094, loss is 0.00811691489070654\n",
      "epoch: 7 step: 1095, loss is 0.023567520081996918\n",
      "epoch: 7 step: 1096, loss is 0.0006697085918858647\n",
      "epoch: 7 step: 1097, loss is 0.032548196613788605\n",
      "epoch: 7 step: 1098, loss is 0.0016773719107732177\n",
      "epoch: 7 step: 1099, loss is 0.01837274432182312\n",
      "epoch: 7 step: 1100, loss is 0.0003859910066239536\n",
      "epoch: 7 step: 1101, loss is 0.0010725549655035138\n",
      "epoch: 7 step: 1102, loss is 0.00011251284013269469\n",
      "epoch: 7 step: 1103, loss is 0.0006690719746984541\n",
      "epoch: 7 step: 1104, loss is 0.04300090670585632\n",
      "epoch: 7 step: 1105, loss is 0.00019898022583220154\n",
      "epoch: 7 step: 1106, loss is 0.08590725064277649\n",
      "epoch: 7 step: 1107, loss is 0.012665700167417526\n",
      "epoch: 7 step: 1108, loss is 0.00048107196926139295\n",
      "epoch: 7 step: 1109, loss is 0.004950794391334057\n",
      "epoch: 7 step: 1110, loss is 0.0009163253125734627\n",
      "epoch: 7 step: 1111, loss is 0.0004725118924397975\n",
      "epoch: 7 step: 1112, loss is 0.0017087055603042245\n",
      "epoch: 7 step: 1113, loss is 4.845764487981796e-05\n",
      "epoch: 7 step: 1114, loss is 0.00047629023902118206\n",
      "epoch: 7 step: 1115, loss is 0.006424624938517809\n",
      "epoch: 7 step: 1116, loss is 0.00039664559881202877\n",
      "epoch: 7 step: 1117, loss is 0.05069795995950699\n",
      "epoch: 7 step: 1118, loss is 0.10420441627502441\n",
      "epoch: 7 step: 1119, loss is 0.015949862077832222\n",
      "epoch: 7 step: 1120, loss is 0.0003361313429195434\n",
      "epoch: 7 step: 1121, loss is 0.022265924140810966\n",
      "epoch: 7 step: 1122, loss is 0.011473674327135086\n",
      "epoch: 7 step: 1123, loss is 0.001232846174389124\n",
      "epoch: 7 step: 1124, loss is 0.0025904178619384766\n",
      "epoch: 7 step: 1125, loss is 0.002770784543827176\n",
      "epoch: 7 step: 1126, loss is 7.464634109055623e-05\n",
      "epoch: 7 step: 1127, loss is 0.159995898604393\n",
      "epoch: 7 step: 1128, loss is 0.0006095359567552805\n",
      "epoch: 7 step: 1129, loss is 0.00020393816521391273\n",
      "epoch: 7 step: 1130, loss is 0.003808114677667618\n",
      "epoch: 7 step: 1131, loss is 0.0009876862168312073\n",
      "epoch: 7 step: 1132, loss is 0.03932211175560951\n",
      "epoch: 7 step: 1133, loss is 0.0848746970295906\n",
      "epoch: 7 step: 1134, loss is 0.0034611779265105724\n",
      "epoch: 7 step: 1135, loss is 0.04643061012029648\n",
      "epoch: 7 step: 1136, loss is 0.011961997486650944\n",
      "epoch: 7 step: 1137, loss is 0.0027563143521547318\n",
      "epoch: 7 step: 1138, loss is 0.0011867849389091134\n",
      "epoch: 7 step: 1139, loss is 0.004676398355513811\n",
      "epoch: 7 step: 1140, loss is 0.015240204520523548\n",
      "epoch: 7 step: 1141, loss is 0.001036678091622889\n",
      "epoch: 7 step: 1142, loss is 0.0003517568111419678\n",
      "epoch: 7 step: 1143, loss is 0.00116347661241889\n",
      "epoch: 7 step: 1144, loss is 0.005619721952825785\n",
      "epoch: 7 step: 1145, loss is 0.0036653070710599422\n",
      "epoch: 7 step: 1146, loss is 0.0058910236693918705\n",
      "epoch: 7 step: 1147, loss is 0.0029168229084461927\n",
      "epoch: 7 step: 1148, loss is 0.03033144399523735\n",
      "epoch: 7 step: 1149, loss is 0.0011544824810698628\n",
      "epoch: 7 step: 1150, loss is 0.010862769559025764\n",
      "epoch: 7 step: 1151, loss is 0.0009852302027866244\n",
      "epoch: 7 step: 1152, loss is 0.001050383667461574\n",
      "epoch: 7 step: 1153, loss is 0.014886817894876003\n",
      "epoch: 7 step: 1154, loss is 0.0014578618574887514\n",
      "epoch: 7 step: 1155, loss is 1.925925062096212e-05\n",
      "epoch: 7 step: 1156, loss is 0.0034281774424016476\n",
      "epoch: 7 step: 1157, loss is 0.0652603805065155\n",
      "epoch: 7 step: 1158, loss is 0.03863300010561943\n",
      "epoch: 7 step: 1159, loss is 0.0728987529873848\n",
      "epoch: 7 step: 1160, loss is 0.00558391772210598\n",
      "epoch: 7 step: 1161, loss is 0.0022085397504270077\n",
      "epoch: 7 step: 1162, loss is 0.002673622453585267\n",
      "epoch: 7 step: 1163, loss is 0.0002475449873600155\n",
      "epoch: 7 step: 1164, loss is 0.0020663151517510414\n",
      "epoch: 7 step: 1165, loss is 0.003099132562056184\n",
      "epoch: 7 step: 1166, loss is 0.002766405465081334\n",
      "epoch: 7 step: 1167, loss is 6.80453740642406e-05\n",
      "epoch: 7 step: 1168, loss is 0.005423031747341156\n",
      "epoch: 7 step: 1169, loss is 0.04717694967985153\n",
      "epoch: 7 step: 1170, loss is 0.007593714166432619\n",
      "epoch: 7 step: 1171, loss is 0.008488605730235577\n",
      "epoch: 7 step: 1172, loss is 6.750164175173268e-05\n",
      "epoch: 7 step: 1173, loss is 0.002959859324619174\n",
      "epoch: 7 step: 1174, loss is 0.0028802938759326935\n",
      "epoch: 7 step: 1175, loss is 0.07746079564094543\n",
      "epoch: 7 step: 1176, loss is 0.0015808047028258443\n",
      "epoch: 7 step: 1177, loss is 0.05067215859889984\n",
      "epoch: 7 step: 1178, loss is 0.000524762668646872\n",
      "epoch: 7 step: 1179, loss is 5.9590645832940936e-05\n",
      "epoch: 7 step: 1180, loss is 0.005001019686460495\n",
      "epoch: 7 step: 1181, loss is 0.00015350356989074498\n",
      "epoch: 7 step: 1182, loss is 0.0009501251624897122\n",
      "epoch: 7 step: 1183, loss is 0.031174756586551666\n",
      "epoch: 7 step: 1184, loss is 0.0010661047417670488\n",
      "epoch: 7 step: 1185, loss is 0.0009857476688921452\n",
      "epoch: 7 step: 1186, loss is 0.008533070795238018\n",
      "epoch: 7 step: 1187, loss is 0.012016181834042072\n",
      "epoch: 7 step: 1188, loss is 0.0006322105182334781\n",
      "epoch: 7 step: 1189, loss is 0.12188968807458878\n",
      "epoch: 7 step: 1190, loss is 0.00396095123142004\n",
      "epoch: 7 step: 1191, loss is 0.006215974222868681\n",
      "epoch: 7 step: 1192, loss is 0.02980133518576622\n",
      "epoch: 7 step: 1193, loss is 0.19834236800670624\n",
      "epoch: 7 step: 1194, loss is 0.002080762991681695\n",
      "epoch: 7 step: 1195, loss is 0.009973717853426933\n",
      "epoch: 7 step: 1196, loss is 0.0004193699569441378\n",
      "epoch: 7 step: 1197, loss is 0.0015961250755935907\n",
      "epoch: 7 step: 1198, loss is 3.174399898853153e-05\n",
      "epoch: 7 step: 1199, loss is 0.0037813920062035322\n",
      "epoch: 7 step: 1200, loss is 0.0002897526428569108\n",
      "epoch: 7 step: 1201, loss is 0.00014853216998744756\n",
      "epoch: 7 step: 1202, loss is 0.00031688896706327796\n",
      "epoch: 7 step: 1203, loss is 0.027912963181734085\n",
      "epoch: 7 step: 1204, loss is 0.01596350036561489\n",
      "epoch: 7 step: 1205, loss is 0.06083924323320389\n",
      "epoch: 7 step: 1206, loss is 0.014223720878362656\n",
      "epoch: 7 step: 1207, loss is 0.0015976654831320047\n",
      "epoch: 7 step: 1208, loss is 0.011803184635937214\n",
      "epoch: 7 step: 1209, loss is 0.08489388227462769\n",
      "epoch: 7 step: 1210, loss is 0.008631766773760319\n",
      "epoch: 7 step: 1211, loss is 0.023091817274689674\n",
      "epoch: 7 step: 1212, loss is 0.03318500891327858\n",
      "epoch: 7 step: 1213, loss is 0.006054079160094261\n",
      "epoch: 7 step: 1214, loss is 0.01285658311098814\n",
      "epoch: 7 step: 1215, loss is 0.23553945124149323\n",
      "epoch: 7 step: 1216, loss is 0.004061779007315636\n",
      "epoch: 7 step: 1217, loss is 0.014520028606057167\n",
      "epoch: 7 step: 1218, loss is 0.006359369959682226\n",
      "epoch: 7 step: 1219, loss is 0.0013221056433394551\n",
      "epoch: 7 step: 1220, loss is 0.0011085816659033298\n",
      "epoch: 7 step: 1221, loss is 0.000739210459869355\n",
      "epoch: 7 step: 1222, loss is 0.04393034428358078\n",
      "epoch: 7 step: 1223, loss is 0.006328559014946222\n",
      "epoch: 7 step: 1224, loss is 0.0003934724081773311\n",
      "epoch: 7 step: 1225, loss is 0.004191665910184383\n",
      "epoch: 7 step: 1226, loss is 0.0036485157907009125\n",
      "epoch: 7 step: 1227, loss is 0.007201012223958969\n",
      "epoch: 7 step: 1228, loss is 0.0010060180211439729\n",
      "epoch: 7 step: 1229, loss is 0.009085297584533691\n",
      "epoch: 7 step: 1230, loss is 0.012511838227510452\n",
      "epoch: 7 step: 1231, loss is 0.009628715924918652\n",
      "epoch: 7 step: 1232, loss is 0.017069198191165924\n",
      "epoch: 7 step: 1233, loss is 0.004537454806268215\n",
      "epoch: 7 step: 1234, loss is 0.04032823070883751\n",
      "epoch: 7 step: 1235, loss is 0.0034610864240676165\n",
      "epoch: 7 step: 1236, loss is 0.02178051508963108\n",
      "epoch: 7 step: 1237, loss is 0.01682116463780403\n",
      "epoch: 7 step: 1238, loss is 0.0006186655373312533\n",
      "epoch: 7 step: 1239, loss is 0.22105740010738373\n",
      "epoch: 7 step: 1240, loss is 0.0005251509719528258\n",
      "epoch: 7 step: 1241, loss is 0.00038203629083000124\n",
      "epoch: 7 step: 1242, loss is 8.889745367923751e-05\n",
      "epoch: 7 step: 1243, loss is 0.02218383550643921\n",
      "epoch: 7 step: 1244, loss is 0.00037350921775214374\n",
      "epoch: 7 step: 1245, loss is 0.09737060964107513\n",
      "epoch: 7 step: 1246, loss is 0.004058560356497765\n",
      "epoch: 7 step: 1247, loss is 0.00036906712921336293\n",
      "epoch: 7 step: 1248, loss is 0.0001778485020622611\n",
      "epoch: 7 step: 1249, loss is 0.02823890559375286\n",
      "epoch: 7 step: 1250, loss is 0.020101426169276237\n",
      "epoch: 7 step: 1251, loss is 0.00020319531904533505\n",
      "epoch: 7 step: 1252, loss is 0.016167813912034035\n",
      "epoch: 7 step: 1253, loss is 0.02225017547607422\n",
      "epoch: 7 step: 1254, loss is 0.01668907143175602\n",
      "epoch: 7 step: 1255, loss is 0.21177774667739868\n",
      "epoch: 7 step: 1256, loss is 0.04521006718277931\n",
      "epoch: 7 step: 1257, loss is 0.10264508426189423\n",
      "epoch: 7 step: 1258, loss is 0.00992153212428093\n",
      "epoch: 7 step: 1259, loss is 0.002232286147773266\n",
      "epoch: 7 step: 1260, loss is 0.012066146358847618\n",
      "epoch: 7 step: 1261, loss is 0.001646318705752492\n",
      "epoch: 7 step: 1262, loss is 0.014243925921618938\n",
      "epoch: 7 step: 1263, loss is 0.001696970546618104\n",
      "epoch: 7 step: 1264, loss is 0.011061063036322594\n",
      "epoch: 7 step: 1265, loss is 0.004752490669488907\n",
      "epoch: 7 step: 1266, loss is 0.006094802170991898\n",
      "epoch: 7 step: 1267, loss is 0.06900229305028915\n",
      "epoch: 7 step: 1268, loss is 0.00032169074984267354\n",
      "epoch: 7 step: 1269, loss is 0.01877899467945099\n",
      "epoch: 7 step: 1270, loss is 0.06601914763450623\n",
      "epoch: 7 step: 1271, loss is 0.0012519219890236855\n",
      "epoch: 7 step: 1272, loss is 0.000272493896773085\n",
      "epoch: 7 step: 1273, loss is 0.002311157062649727\n",
      "epoch: 7 step: 1274, loss is 0.006200205534696579\n",
      "epoch: 7 step: 1275, loss is 0.0009565090294927359\n",
      "epoch: 7 step: 1276, loss is 0.002303812652826309\n",
      "epoch: 7 step: 1277, loss is 0.014438055455684662\n",
      "epoch: 7 step: 1278, loss is 0.0017830525757744908\n",
      "epoch: 7 step: 1279, loss is 0.13211683928966522\n",
      "epoch: 7 step: 1280, loss is 0.0011827140115201473\n",
      "epoch: 7 step: 1281, loss is 0.007320105563849211\n",
      "epoch: 7 step: 1282, loss is 0.03855929151177406\n",
      "epoch: 7 step: 1283, loss is 0.0006145268562249839\n",
      "epoch: 7 step: 1284, loss is 0.05165211111307144\n",
      "epoch: 7 step: 1285, loss is 0.0025575768668204546\n",
      "epoch: 7 step: 1286, loss is 0.003000520868226886\n",
      "epoch: 7 step: 1287, loss is 0.0003645861870609224\n",
      "epoch: 7 step: 1288, loss is 0.007116928230971098\n",
      "epoch: 7 step: 1289, loss is 0.0013524574460461736\n",
      "epoch: 7 step: 1290, loss is 0.0004339845909271389\n",
      "epoch: 7 step: 1291, loss is 0.08783871680498123\n",
      "epoch: 7 step: 1292, loss is 0.0030519398860633373\n",
      "epoch: 7 step: 1293, loss is 0.0007415911532007158\n",
      "epoch: 7 step: 1294, loss is 0.0009254585020244122\n",
      "epoch: 7 step: 1295, loss is 0.004358761478215456\n",
      "epoch: 7 step: 1296, loss is 0.0073141371831297874\n",
      "epoch: 7 step: 1297, loss is 0.0006579127511940897\n",
      "epoch: 7 step: 1298, loss is 0.009830141440033913\n",
      "epoch: 7 step: 1299, loss is 0.17167314887046814\n",
      "epoch: 7 step: 1300, loss is 0.23432794213294983\n",
      "epoch: 7 step: 1301, loss is 0.00013766578922513872\n",
      "epoch: 7 step: 1302, loss is 0.0012287154095247388\n",
      "epoch: 7 step: 1303, loss is 0.010440935380756855\n",
      "epoch: 7 step: 1304, loss is 0.0004867230309173465\n",
      "epoch: 7 step: 1305, loss is 0.0052733039483428\n",
      "epoch: 7 step: 1306, loss is 0.0010003119241446257\n",
      "epoch: 7 step: 1307, loss is 0.006995694246143103\n",
      "epoch: 7 step: 1308, loss is 0.0008928518509492278\n",
      "epoch: 7 step: 1309, loss is 0.0023792139254510403\n",
      "epoch: 7 step: 1310, loss is 0.20033422112464905\n",
      "epoch: 7 step: 1311, loss is 0.010540932416915894\n",
      "epoch: 7 step: 1312, loss is 0.1481872946023941\n",
      "epoch: 7 step: 1313, loss is 0.0030143947806209326\n",
      "epoch: 7 step: 1314, loss is 0.14210903644561768\n",
      "epoch: 7 step: 1315, loss is 0.02633058838546276\n",
      "epoch: 7 step: 1316, loss is 0.0037929401732981205\n",
      "epoch: 7 step: 1317, loss is 0.0714152380824089\n",
      "epoch: 7 step: 1318, loss is 0.01818973757326603\n",
      "epoch: 7 step: 1319, loss is 0.006639548111706972\n",
      "epoch: 7 step: 1320, loss is 0.005755920894443989\n",
      "epoch: 7 step: 1321, loss is 0.02267700806260109\n",
      "epoch: 7 step: 1322, loss is 0.007091486360877752\n",
      "epoch: 7 step: 1323, loss is 0.002676640870049596\n",
      "epoch: 7 step: 1324, loss is 0.045782461762428284\n",
      "epoch: 7 step: 1325, loss is 0.023941755294799805\n",
      "epoch: 7 step: 1326, loss is 0.042710527777671814\n",
      "epoch: 7 step: 1327, loss is 0.06880173087120056\n",
      "epoch: 7 step: 1328, loss is 0.005404598079621792\n",
      "epoch: 7 step: 1329, loss is 0.0012745753629133105\n",
      "epoch: 7 step: 1330, loss is 0.03432994335889816\n",
      "epoch: 7 step: 1331, loss is 0.00601934501901269\n",
      "epoch: 7 step: 1332, loss is 0.005606137216091156\n",
      "epoch: 7 step: 1333, loss is 0.014130079187452793\n",
      "epoch: 7 step: 1334, loss is 0.0030876388773322105\n",
      "epoch: 7 step: 1335, loss is 0.048013631254434586\n",
      "epoch: 7 step: 1336, loss is 0.006212904583662748\n",
      "epoch: 7 step: 1337, loss is 0.0797833800315857\n",
      "epoch: 7 step: 1338, loss is 0.10482767969369888\n",
      "epoch: 7 step: 1339, loss is 0.02150135487318039\n",
      "epoch: 7 step: 1340, loss is 0.001655262429267168\n",
      "epoch: 7 step: 1341, loss is 0.024648893624544144\n",
      "epoch: 7 step: 1342, loss is 0.0004359401937108487\n",
      "epoch: 7 step: 1343, loss is 0.003552126232534647\n",
      "epoch: 7 step: 1344, loss is 0.02377377077937126\n",
      "epoch: 7 step: 1345, loss is 0.0005435008206404746\n",
      "epoch: 7 step: 1346, loss is 0.002378280973061919\n",
      "epoch: 7 step: 1347, loss is 0.17459923028945923\n",
      "epoch: 7 step: 1348, loss is 0.005866670981049538\n",
      "epoch: 7 step: 1349, loss is 0.0014845519326627254\n",
      "epoch: 7 step: 1350, loss is 0.007963858544826508\n",
      "epoch: 7 step: 1351, loss is 0.00529162771999836\n",
      "epoch: 7 step: 1352, loss is 0.000893265416380018\n",
      "epoch: 7 step: 1353, loss is 0.014775841496884823\n",
      "epoch: 7 step: 1354, loss is 0.0007927845581434667\n",
      "epoch: 7 step: 1355, loss is 0.057610273361206055\n",
      "epoch: 7 step: 1356, loss is 0.030942833051085472\n",
      "epoch: 7 step: 1357, loss is 0.006433836184442043\n",
      "epoch: 7 step: 1358, loss is 0.0019881254993379116\n",
      "epoch: 7 step: 1359, loss is 0.000957251526415348\n",
      "epoch: 7 step: 1360, loss is 0.11056141555309296\n",
      "epoch: 7 step: 1361, loss is 0.004137355834245682\n",
      "epoch: 7 step: 1362, loss is 0.0019860570318996906\n",
      "epoch: 7 step: 1363, loss is 0.014667827636003494\n",
      "epoch: 7 step: 1364, loss is 0.005390536971390247\n",
      "epoch: 7 step: 1365, loss is 0.04079261049628258\n",
      "epoch: 7 step: 1366, loss is 0.05548802763223648\n",
      "epoch: 7 step: 1367, loss is 0.004980415105819702\n",
      "epoch: 7 step: 1368, loss is 0.03770847246050835\n",
      "epoch: 7 step: 1369, loss is 0.0012324582785367966\n",
      "epoch: 7 step: 1370, loss is 0.009806690737605095\n",
      "epoch: 7 step: 1371, loss is 0.18102946877479553\n",
      "epoch: 7 step: 1372, loss is 0.1918024867773056\n",
      "epoch: 7 step: 1373, loss is 0.05269314721226692\n",
      "epoch: 7 step: 1374, loss is 0.00125247857067734\n",
      "epoch: 7 step: 1375, loss is 0.0093146450817585\n",
      "epoch: 7 step: 1376, loss is 0.0010870184050872922\n",
      "epoch: 7 step: 1377, loss is 0.023755239322781563\n",
      "epoch: 7 step: 1378, loss is 0.01012636162340641\n",
      "epoch: 7 step: 1379, loss is 0.000950002868194133\n",
      "epoch: 7 step: 1380, loss is 0.002071995520964265\n",
      "epoch: 7 step: 1381, loss is 0.2703547179698944\n",
      "epoch: 7 step: 1382, loss is 0.013747213408350945\n",
      "epoch: 7 step: 1383, loss is 0.0034866095520555973\n",
      "epoch: 7 step: 1384, loss is 0.0030831191688776016\n",
      "epoch: 7 step: 1385, loss is 0.03322577103972435\n",
      "epoch: 7 step: 1386, loss is 0.0002887518494389951\n",
      "epoch: 7 step: 1387, loss is 0.026815742254257202\n",
      "epoch: 7 step: 1388, loss is 0.013866787776350975\n",
      "epoch: 7 step: 1389, loss is 0.24294443428516388\n",
      "epoch: 7 step: 1390, loss is 0.0005683913477696478\n",
      "epoch: 7 step: 1391, loss is 0.007153693120926619\n",
      "epoch: 7 step: 1392, loss is 0.0004937335033901036\n",
      "epoch: 7 step: 1393, loss is 0.002119615441188216\n",
      "epoch: 7 step: 1394, loss is 0.0025950754061341286\n",
      "epoch: 7 step: 1395, loss is 0.0163788590580225\n",
      "epoch: 7 step: 1396, loss is 0.03966153785586357\n",
      "epoch: 7 step: 1397, loss is 0.0027124269399791956\n",
      "epoch: 7 step: 1398, loss is 0.0016388961812481284\n",
      "epoch: 7 step: 1399, loss is 0.03781433403491974\n",
      "epoch: 7 step: 1400, loss is 0.0006848830380477011\n",
      "epoch: 7 step: 1401, loss is 0.04134869575500488\n",
      "epoch: 7 step: 1402, loss is 0.0020457932259887457\n",
      "epoch: 7 step: 1403, loss is 0.0009766113944351673\n",
      "epoch: 7 step: 1404, loss is 0.0018237531185150146\n",
      "epoch: 7 step: 1405, loss is 0.0012328169541433454\n",
      "epoch: 7 step: 1406, loss is 0.00027424163999967277\n",
      "epoch: 7 step: 1407, loss is 0.02888534776866436\n",
      "epoch: 7 step: 1408, loss is 0.0026006412226706743\n",
      "epoch: 7 step: 1409, loss is 0.00038050394505262375\n",
      "epoch: 7 step: 1410, loss is 0.006743581499904394\n",
      "epoch: 7 step: 1411, loss is 0.0022541850339621305\n",
      "epoch: 7 step: 1412, loss is 0.00018065093900077045\n",
      "epoch: 7 step: 1413, loss is 0.06501851230859756\n",
      "epoch: 7 step: 1414, loss is 0.0032040083315223455\n",
      "epoch: 7 step: 1415, loss is 0.0002018189406953752\n",
      "epoch: 7 step: 1416, loss is 0.000691190711222589\n",
      "epoch: 7 step: 1417, loss is 0.00023515311477240175\n",
      "epoch: 7 step: 1418, loss is 0.0025923852808773518\n",
      "epoch: 7 step: 1419, loss is 0.03757905587553978\n",
      "epoch: 7 step: 1420, loss is 0.13669249415397644\n",
      "epoch: 7 step: 1421, loss is 0.028255807235836983\n",
      "epoch: 7 step: 1422, loss is 0.0013306193286553025\n",
      "epoch: 7 step: 1423, loss is 0.06785135716199875\n",
      "epoch: 7 step: 1424, loss is 0.006630893796682358\n",
      "epoch: 7 step: 1425, loss is 0.07355667650699615\n",
      "epoch: 7 step: 1426, loss is 0.0007063152152113616\n",
      "epoch: 7 step: 1427, loss is 0.0010024351067841053\n",
      "epoch: 7 step: 1428, loss is 0.11289829760789871\n",
      "epoch: 7 step: 1429, loss is 0.009591687470674515\n",
      "epoch: 7 step: 1430, loss is 0.015284103341400623\n",
      "epoch: 7 step: 1431, loss is 0.011984726414084435\n",
      "epoch: 7 step: 1432, loss is 0.01613093540072441\n",
      "epoch: 7 step: 1433, loss is 0.0062754787504673\n",
      "epoch: 7 step: 1434, loss is 0.0025106787215918303\n",
      "epoch: 7 step: 1435, loss is 0.00013894075527787209\n",
      "epoch: 7 step: 1436, loss is 0.0001161896507255733\n",
      "epoch: 7 step: 1437, loss is 0.04773014038801193\n",
      "epoch: 7 step: 1438, loss is 0.0016527000116184354\n",
      "epoch: 7 step: 1439, loss is 0.0007065239478833973\n",
      "epoch: 7 step: 1440, loss is 0.005021612625569105\n",
      "epoch: 7 step: 1441, loss is 0.03929537162184715\n",
      "epoch: 7 step: 1442, loss is 0.0005441118264570832\n",
      "epoch: 7 step: 1443, loss is 0.09441155195236206\n",
      "epoch: 7 step: 1444, loss is 0.001827487489208579\n",
      "epoch: 7 step: 1445, loss is 0.03502756357192993\n",
      "epoch: 7 step: 1446, loss is 0.004711253568530083\n",
      "epoch: 7 step: 1447, loss is 0.0019947460386902094\n",
      "epoch: 7 step: 1448, loss is 0.009261298924684525\n",
      "epoch: 7 step: 1449, loss is 0.11346277594566345\n",
      "epoch: 7 step: 1450, loss is 0.014767679385840893\n",
      "epoch: 7 step: 1451, loss is 0.0070510366931557655\n",
      "epoch: 7 step: 1452, loss is 0.0028462219052016735\n",
      "epoch: 7 step: 1453, loss is 0.00025479428586550057\n",
      "epoch: 7 step: 1454, loss is 0.0009125604410655797\n",
      "epoch: 7 step: 1455, loss is 0.00447314465418458\n",
      "epoch: 7 step: 1456, loss is 0.00121024576947093\n",
      "epoch: 7 step: 1457, loss is 0.00017753538850229234\n",
      "epoch: 7 step: 1458, loss is 0.11364895850419998\n",
      "epoch: 7 step: 1459, loss is 0.0007347747450694442\n",
      "epoch: 7 step: 1460, loss is 0.000640395563095808\n",
      "epoch: 7 step: 1461, loss is 0.0017299714963883162\n",
      "epoch: 7 step: 1462, loss is 0.00014127619215287268\n",
      "epoch: 7 step: 1463, loss is 0.01398236583918333\n",
      "epoch: 7 step: 1464, loss is 0.09835417568683624\n",
      "epoch: 7 step: 1465, loss is 0.019934875890612602\n",
      "epoch: 7 step: 1466, loss is 0.00013267416215967387\n",
      "epoch: 7 step: 1467, loss is 0.03227159380912781\n",
      "epoch: 7 step: 1468, loss is 0.008005448617041111\n",
      "epoch: 7 step: 1469, loss is 0.015605766326189041\n",
      "epoch: 7 step: 1470, loss is 0.003171271877363324\n",
      "epoch: 7 step: 1471, loss is 0.013164454139769077\n",
      "epoch: 7 step: 1472, loss is 0.00025262843701057136\n",
      "epoch: 7 step: 1473, loss is 0.0023467023856937885\n",
      "epoch: 7 step: 1474, loss is 0.003923617769032717\n",
      "epoch: 7 step: 1475, loss is 0.1495041698217392\n",
      "epoch: 7 step: 1476, loss is 0.029740560799837112\n",
      "epoch: 7 step: 1477, loss is 0.0004353584081400186\n",
      "epoch: 7 step: 1478, loss is 0.06725886464118958\n",
      "epoch: 7 step: 1479, loss is 0.0029136589728295803\n",
      "epoch: 7 step: 1480, loss is 0.0010373693658038974\n",
      "epoch: 7 step: 1481, loss is 0.001134449033997953\n",
      "epoch: 7 step: 1482, loss is 0.009984023869037628\n",
      "epoch: 7 step: 1483, loss is 0.2673285901546478\n",
      "epoch: 7 step: 1484, loss is 0.0075402106158435345\n",
      "epoch: 7 step: 1485, loss is 0.01306101679801941\n",
      "epoch: 7 step: 1486, loss is 0.03280199319124222\n",
      "epoch: 7 step: 1487, loss is 0.0005787340342067182\n",
      "epoch: 7 step: 1488, loss is 0.10342013090848923\n",
      "epoch: 7 step: 1489, loss is 0.003672364866361022\n",
      "epoch: 7 step: 1490, loss is 0.0007459325133822858\n",
      "epoch: 7 step: 1491, loss is 0.0003723722184076905\n",
      "epoch: 7 step: 1492, loss is 0.18728485703468323\n",
      "epoch: 7 step: 1493, loss is 0.00023738466552458704\n",
      "epoch: 7 step: 1494, loss is 0.001499987207353115\n",
      "epoch: 7 step: 1495, loss is 0.0039055724628269672\n",
      "epoch: 7 step: 1496, loss is 0.005565492901951075\n",
      "epoch: 7 step: 1497, loss is 0.0006531228427775204\n",
      "epoch: 7 step: 1498, loss is 0.003986709751188755\n",
      "epoch: 7 step: 1499, loss is 0.012890752404928207\n",
      "epoch: 7 step: 1500, loss is 0.012870825827121735\n",
      "epoch: 7 step: 1501, loss is 0.000906291592400521\n",
      "epoch: 7 step: 1502, loss is 0.031025217846035957\n",
      "epoch: 7 step: 1503, loss is 0.0011834593024104834\n",
      "epoch: 7 step: 1504, loss is 0.011315903626382351\n",
      "epoch: 7 step: 1505, loss is 0.0007873587310314178\n",
      "epoch: 7 step: 1506, loss is 0.04233214631676674\n",
      "epoch: 7 step: 1507, loss is 0.001189582864753902\n",
      "epoch: 7 step: 1508, loss is 0.018488049507141113\n",
      "epoch: 7 step: 1509, loss is 0.018340015783905983\n",
      "epoch: 7 step: 1510, loss is 0.0006519227172248065\n",
      "epoch: 7 step: 1511, loss is 0.00019807452918030322\n",
      "epoch: 7 step: 1512, loss is 0.01177485566586256\n",
      "epoch: 7 step: 1513, loss is 0.0030813131015747786\n",
      "epoch: 7 step: 1514, loss is 0.010261486284434795\n",
      "epoch: 7 step: 1515, loss is 0.0012167098466306925\n",
      "epoch: 7 step: 1516, loss is 0.04234284907579422\n",
      "epoch: 7 step: 1517, loss is 0.10476408898830414\n",
      "epoch: 7 step: 1518, loss is 0.0008664792403578758\n",
      "epoch: 7 step: 1519, loss is 0.00098136393353343\n",
      "epoch: 7 step: 1520, loss is 0.0003875639522448182\n",
      "epoch: 7 step: 1521, loss is 0.007821418344974518\n",
      "epoch: 7 step: 1522, loss is 0.002624188782647252\n",
      "epoch: 7 step: 1523, loss is 7.028174150036648e-05\n",
      "epoch: 7 step: 1524, loss is 0.0031672969926148653\n",
      "epoch: 7 step: 1525, loss is 0.03816443681716919\n",
      "epoch: 7 step: 1526, loss is 0.0011507980525493622\n",
      "epoch: 7 step: 1527, loss is 0.010070494376122952\n",
      "epoch: 7 step: 1528, loss is 0.009353553876280785\n",
      "epoch: 7 step: 1529, loss is 0.0021792780607938766\n",
      "epoch: 7 step: 1530, loss is 0.058763615787029266\n",
      "epoch: 7 step: 1531, loss is 0.007617151830345392\n",
      "epoch: 7 step: 1532, loss is 0.0025521214120090008\n",
      "epoch: 7 step: 1533, loss is 0.00018593863933347166\n",
      "epoch: 7 step: 1534, loss is 0.01485306117683649\n",
      "epoch: 7 step: 1535, loss is 0.004301415756344795\n",
      "epoch: 7 step: 1536, loss is 0.16842049360275269\n",
      "epoch: 7 step: 1537, loss is 2.5655741410446353e-05\n",
      "epoch: 7 step: 1538, loss is 0.04065607115626335\n",
      "epoch: 7 step: 1539, loss is 0.00010180872777709737\n",
      "epoch: 7 step: 1540, loss is 6.078577280277386e-05\n",
      "epoch: 7 step: 1541, loss is 0.0019139040959998965\n",
      "epoch: 7 step: 1542, loss is 0.03856939822435379\n",
      "epoch: 7 step: 1543, loss is 0.02195378579199314\n",
      "epoch: 7 step: 1544, loss is 0.0955110490322113\n",
      "epoch: 7 step: 1545, loss is 0.0044796047732234\n",
      "epoch: 7 step: 1546, loss is 7.572656613774598e-05\n",
      "epoch: 7 step: 1547, loss is 0.002564356429502368\n",
      "epoch: 7 step: 1548, loss is 0.0009377477690577507\n",
      "epoch: 7 step: 1549, loss is 0.0008743411162868142\n",
      "epoch: 7 step: 1550, loss is 0.0013465151423588395\n",
      "epoch: 7 step: 1551, loss is 0.00021100306184962392\n",
      "epoch: 7 step: 1552, loss is 0.002264861250296235\n",
      "epoch: 7 step: 1553, loss is 0.005706042516976595\n",
      "epoch: 7 step: 1554, loss is 0.0036860655527561903\n",
      "epoch: 7 step: 1555, loss is 0.0017774441512301564\n",
      "epoch: 7 step: 1556, loss is 0.1305803805589676\n",
      "epoch: 7 step: 1557, loss is 0.00044764793710783124\n",
      "epoch: 7 step: 1558, loss is 0.19902099668979645\n",
      "epoch: 7 step: 1559, loss is 0.009196154773235321\n",
      "epoch: 7 step: 1560, loss is 0.0010696618119254708\n",
      "epoch: 7 step: 1561, loss is 0.0007427310338243842\n",
      "epoch: 7 step: 1562, loss is 0.09216656535863876\n",
      "epoch: 7 step: 1563, loss is 0.005221102386713028\n",
      "epoch: 7 step: 1564, loss is 0.03308551758527756\n",
      "epoch: 7 step: 1565, loss is 0.0035853167064487934\n",
      "epoch: 7 step: 1566, loss is 0.0010272749932482839\n",
      "epoch: 7 step: 1567, loss is 0.001831331173889339\n",
      "epoch: 7 step: 1568, loss is 0.000366777996532619\n",
      "epoch: 7 step: 1569, loss is 0.08966390043497086\n",
      "epoch: 7 step: 1570, loss is 0.002004951238632202\n",
      "epoch: 7 step: 1571, loss is 0.0036291307769715786\n",
      "epoch: 7 step: 1572, loss is 0.0028447876684367657\n",
      "epoch: 7 step: 1573, loss is 0.07409338653087616\n",
      "epoch: 7 step: 1574, loss is 7.977413770277053e-05\n",
      "epoch: 7 step: 1575, loss is 0.020105773583054543\n",
      "epoch: 7 step: 1576, loss is 2.3820484784664586e-05\n",
      "epoch: 7 step: 1577, loss is 0.001280076801776886\n",
      "epoch: 7 step: 1578, loss is 0.0033063769806176424\n",
      "epoch: 7 step: 1579, loss is 0.0028584408573806286\n",
      "epoch: 7 step: 1580, loss is 0.0024467825423926115\n",
      "epoch: 7 step: 1581, loss is 0.012354148551821709\n",
      "epoch: 7 step: 1582, loss is 0.0013943149242550135\n",
      "epoch: 7 step: 1583, loss is 0.033979251980781555\n",
      "epoch: 7 step: 1584, loss is 0.14542829990386963\n",
      "epoch: 7 step: 1585, loss is 0.002836296334862709\n",
      "epoch: 7 step: 1586, loss is 0.00017699120508041233\n",
      "epoch: 7 step: 1587, loss is 0.002249994780868292\n",
      "epoch: 7 step: 1588, loss is 0.00784273911267519\n",
      "epoch: 7 step: 1589, loss is 0.003428854513913393\n",
      "epoch: 7 step: 1590, loss is 0.0005311337299644947\n",
      "epoch: 7 step: 1591, loss is 0.0018817740492522717\n",
      "epoch: 7 step: 1592, loss is 0.004133906215429306\n",
      "epoch: 7 step: 1593, loss is 0.005734758917242289\n",
      "epoch: 7 step: 1594, loss is 0.0003067445068154484\n",
      "epoch: 7 step: 1595, loss is 0.005064216908067465\n",
      "epoch: 7 step: 1596, loss is 0.0009585728985257447\n",
      "epoch: 7 step: 1597, loss is 0.0006256606429815292\n",
      "epoch: 7 step: 1598, loss is 0.09597522765398026\n",
      "epoch: 7 step: 1599, loss is 0.016515467315912247\n",
      "epoch: 7 step: 1600, loss is 0.008874181658029556\n",
      "epoch: 7 step: 1601, loss is 0.0008021802641451359\n",
      "epoch: 7 step: 1602, loss is 0.0029843118973076344\n",
      "epoch: 7 step: 1603, loss is 0.01210801675915718\n",
      "epoch: 7 step: 1604, loss is 0.00025270640617236495\n",
      "epoch: 7 step: 1605, loss is 0.0006080949679017067\n",
      "epoch: 7 step: 1606, loss is 0.00018727486894931644\n",
      "epoch: 7 step: 1607, loss is 0.0003471854142844677\n",
      "epoch: 7 step: 1608, loss is 0.0006882024463266134\n",
      "epoch: 7 step: 1609, loss is 0.0008189364452846348\n",
      "epoch: 7 step: 1610, loss is 0.20505033433437347\n",
      "epoch: 7 step: 1611, loss is 0.0007461220375262201\n",
      "epoch: 7 step: 1612, loss is 0.10086408257484436\n",
      "epoch: 7 step: 1613, loss is 0.005914607550948858\n",
      "epoch: 7 step: 1614, loss is 0.0009525831555947661\n",
      "epoch: 7 step: 1615, loss is 0.00467379204928875\n",
      "epoch: 7 step: 1616, loss is 0.056860487908124924\n",
      "epoch: 7 step: 1617, loss is 0.017167111858725548\n",
      "epoch: 7 step: 1618, loss is 0.019623689353466034\n",
      "epoch: 7 step: 1619, loss is 0.005133811384439468\n",
      "epoch: 7 step: 1620, loss is 0.004426923580467701\n",
      "epoch: 7 step: 1621, loss is 0.02003048174083233\n",
      "epoch: 7 step: 1622, loss is 0.0007732603698968887\n",
      "epoch: 7 step: 1623, loss is 0.21777187287807465\n",
      "epoch: 7 step: 1624, loss is 0.0006532695842906833\n",
      "epoch: 7 step: 1625, loss is 0.0035330592654645443\n",
      "epoch: 7 step: 1626, loss is 0.005515228025615215\n",
      "epoch: 7 step: 1627, loss is 0.0037474818527698517\n",
      "epoch: 7 step: 1628, loss is 0.0008824188844300807\n",
      "epoch: 7 step: 1629, loss is 0.00206664577126503\n",
      "epoch: 7 step: 1630, loss is 0.06905611604452133\n",
      "epoch: 7 step: 1631, loss is 0.12376470118761063\n",
      "epoch: 7 step: 1632, loss is 0.10125209391117096\n",
      "epoch: 7 step: 1633, loss is 0.0017478856025263667\n",
      "epoch: 7 step: 1634, loss is 0.004475475288927555\n",
      "epoch: 7 step: 1635, loss is 0.0015773829072713852\n",
      "epoch: 7 step: 1636, loss is 0.07270466536283493\n",
      "epoch: 7 step: 1637, loss is 0.0009743291884660721\n",
      "epoch: 7 step: 1638, loss is 0.00039292138535529375\n",
      "epoch: 7 step: 1639, loss is 0.016118034720420837\n",
      "epoch: 7 step: 1640, loss is 0.019620584324002266\n",
      "epoch: 7 step: 1641, loss is 0.05877633020281792\n",
      "epoch: 7 step: 1642, loss is 0.00011529827315825969\n",
      "epoch: 7 step: 1643, loss is 0.0016505520325154066\n",
      "epoch: 7 step: 1644, loss is 0.054437633603811264\n",
      "epoch: 7 step: 1645, loss is 0.09274622797966003\n",
      "epoch: 7 step: 1646, loss is 0.029330644756555557\n",
      "epoch: 7 step: 1647, loss is 0.007257773540914059\n",
      "epoch: 7 step: 1648, loss is 0.00048487973981536925\n",
      "epoch: 7 step: 1649, loss is 0.01911824941635132\n",
      "epoch: 7 step: 1650, loss is 0.002348796697333455\n",
      "epoch: 7 step: 1651, loss is 0.0006853624363429844\n",
      "epoch: 7 step: 1652, loss is 0.0005001977551728487\n",
      "epoch: 7 step: 1653, loss is 0.0009635653113946319\n",
      "epoch: 7 step: 1654, loss is 0.0007197069935500622\n",
      "epoch: 7 step: 1655, loss is 0.0038197606336325407\n",
      "epoch: 7 step: 1656, loss is 0.010726259090006351\n",
      "epoch: 7 step: 1657, loss is 0.15934878587722778\n",
      "epoch: 7 step: 1658, loss is 0.026380930095911026\n",
      "epoch: 7 step: 1659, loss is 0.001230719848535955\n",
      "epoch: 7 step: 1660, loss is 0.11188250780105591\n",
      "epoch: 7 step: 1661, loss is 0.18629257380962372\n",
      "epoch: 7 step: 1662, loss is 0.0013127288548275828\n",
      "epoch: 7 step: 1663, loss is 0.13250812888145447\n",
      "epoch: 7 step: 1664, loss is 0.00023725656501483172\n",
      "epoch: 7 step: 1665, loss is 7.078897033352405e-05\n",
      "epoch: 7 step: 1666, loss is 0.0005237088189460337\n",
      "epoch: 7 step: 1667, loss is 0.00027009399491362274\n",
      "epoch: 7 step: 1668, loss is 0.05772586539387703\n",
      "epoch: 7 step: 1669, loss is 0.001591764041222632\n",
      "epoch: 7 step: 1670, loss is 0.022891132161021233\n",
      "epoch: 7 step: 1671, loss is 0.006353140342980623\n",
      "epoch: 7 step: 1672, loss is 0.004770278465002775\n",
      "epoch: 7 step: 1673, loss is 0.0025046057999134064\n",
      "epoch: 7 step: 1674, loss is 0.0008979566628113389\n",
      "epoch: 7 step: 1675, loss is 0.001365321921184659\n",
      "epoch: 7 step: 1676, loss is 0.12149493396282196\n",
      "epoch: 7 step: 1677, loss is 0.017794378101825714\n",
      "epoch: 7 step: 1678, loss is 0.049370910972356796\n",
      "epoch: 7 step: 1679, loss is 0.007801645435392857\n",
      "epoch: 7 step: 1680, loss is 0.000210678277653642\n",
      "epoch: 7 step: 1681, loss is 0.060113102197647095\n",
      "epoch: 7 step: 1682, loss is 0.0029374174773693085\n",
      "epoch: 7 step: 1683, loss is 0.006591617129743099\n",
      "epoch: 7 step: 1684, loss is 0.008474943228065968\n",
      "epoch: 7 step: 1685, loss is 0.038624107837677\n",
      "epoch: 7 step: 1686, loss is 0.028163515031337738\n",
      "epoch: 7 step: 1687, loss is 0.03800354152917862\n",
      "epoch: 7 step: 1688, loss is 0.003714323975145817\n",
      "epoch: 7 step: 1689, loss is 0.147601917386055\n",
      "epoch: 7 step: 1690, loss is 0.03141438588500023\n",
      "epoch: 7 step: 1691, loss is 0.000851278833579272\n",
      "epoch: 7 step: 1692, loss is 0.0004534695472102612\n",
      "epoch: 7 step: 1693, loss is 0.0024048646446317434\n",
      "epoch: 7 step: 1694, loss is 0.003065620781853795\n",
      "epoch: 7 step: 1695, loss is 0.024381211027503014\n",
      "epoch: 7 step: 1696, loss is 0.05219331011176109\n",
      "epoch: 7 step: 1697, loss is 0.009649540297687054\n",
      "epoch: 7 step: 1698, loss is 0.0038015025202184916\n",
      "epoch: 7 step: 1699, loss is 0.045333296060562134\n",
      "epoch: 7 step: 1700, loss is 0.003152337158098817\n",
      "epoch: 7 step: 1701, loss is 0.00920501071959734\n",
      "epoch: 7 step: 1702, loss is 0.0017266430659219623\n",
      "epoch: 7 step: 1703, loss is 0.16968917846679688\n",
      "epoch: 7 step: 1704, loss is 0.05352140963077545\n",
      "epoch: 7 step: 1705, loss is 0.01756836473941803\n",
      "epoch: 7 step: 1706, loss is 0.013619679026305676\n",
      "epoch: 7 step: 1707, loss is 0.004028514493256807\n",
      "epoch: 7 step: 1708, loss is 0.07162276655435562\n",
      "epoch: 7 step: 1709, loss is 0.004805855918675661\n",
      "epoch: 7 step: 1710, loss is 0.00039811257738620043\n",
      "epoch: 7 step: 1711, loss is 0.04094918817281723\n",
      "epoch: 7 step: 1712, loss is 0.0016478569013997912\n",
      "epoch: 7 step: 1713, loss is 0.02416883036494255\n",
      "epoch: 7 step: 1714, loss is 0.001791817368939519\n",
      "epoch: 7 step: 1715, loss is 0.0653003603219986\n",
      "epoch: 7 step: 1716, loss is 0.0007863183273002505\n",
      "epoch: 7 step: 1717, loss is 0.027059633284807205\n",
      "epoch: 7 step: 1718, loss is 0.005611928645521402\n",
      "epoch: 7 step: 1719, loss is 0.0022147877607494593\n",
      "epoch: 7 step: 1720, loss is 0.00037791667273268104\n",
      "epoch: 7 step: 1721, loss is 0.00025584278046153486\n",
      "epoch: 7 step: 1722, loss is 0.013911590911448002\n",
      "epoch: 7 step: 1723, loss is 0.0014977878890931606\n",
      "epoch: 7 step: 1724, loss is 0.012528796680271626\n",
      "epoch: 7 step: 1725, loss is 0.0016075610183179379\n",
      "epoch: 7 step: 1726, loss is 0.017782006412744522\n",
      "epoch: 7 step: 1727, loss is 0.01257693488150835\n",
      "epoch: 7 step: 1728, loss is 0.0001341391762252897\n",
      "epoch: 7 step: 1729, loss is 0.00011910309694940224\n",
      "epoch: 7 step: 1730, loss is 0.006459117867052555\n",
      "epoch: 7 step: 1731, loss is 0.0004151924804318696\n",
      "epoch: 7 step: 1732, loss is 0.011458745226264\n",
      "epoch: 7 step: 1733, loss is 0.00026163310394622386\n",
      "epoch: 7 step: 1734, loss is 0.1917453408241272\n",
      "epoch: 7 step: 1735, loss is 0.008648729883134365\n",
      "epoch: 7 step: 1736, loss is 0.001534793060272932\n",
      "epoch: 7 step: 1737, loss is 0.0026452145539224148\n",
      "epoch: 7 step: 1738, loss is 0.00045021981350146234\n",
      "epoch: 7 step: 1739, loss is 0.0003187294350937009\n",
      "epoch: 7 step: 1740, loss is 0.0013020607875660062\n",
      "epoch: 7 step: 1741, loss is 0.015858866274356842\n",
      "epoch: 7 step: 1742, loss is 0.0005027836305089295\n",
      "epoch: 7 step: 1743, loss is 0.0013172510080039501\n",
      "epoch: 7 step: 1744, loss is 0.0008845504489727318\n",
      "epoch: 7 step: 1745, loss is 0.0007584510603919625\n",
      "epoch: 7 step: 1746, loss is 0.00044323853217065334\n",
      "epoch: 7 step: 1747, loss is 0.0010372756514698267\n",
      "epoch: 7 step: 1748, loss is 0.008747787214815617\n",
      "epoch: 7 step: 1749, loss is 0.07810398936271667\n",
      "epoch: 7 step: 1750, loss is 0.22809866070747375\n",
      "epoch: 7 step: 1751, loss is 0.00014318700414150953\n",
      "epoch: 7 step: 1752, loss is 0.0015532224206253886\n",
      "epoch: 7 step: 1753, loss is 0.00031941934139467776\n",
      "epoch: 7 step: 1754, loss is 0.0007823453051969409\n",
      "epoch: 7 step: 1755, loss is 0.019732890650629997\n",
      "epoch: 7 step: 1756, loss is 0.00027948638307861984\n",
      "epoch: 7 step: 1757, loss is 0.025470014661550522\n",
      "epoch: 7 step: 1758, loss is 0.0006524853524751961\n",
      "epoch: 7 step: 1759, loss is 0.0008993701776489615\n",
      "epoch: 7 step: 1760, loss is 0.029495682567358017\n",
      "epoch: 7 step: 1761, loss is 0.0004447622341103852\n",
      "epoch: 7 step: 1762, loss is 0.033793430775403976\n",
      "epoch: 7 step: 1763, loss is 0.0012857483234256506\n",
      "epoch: 7 step: 1764, loss is 0.003040627110749483\n",
      "epoch: 7 step: 1765, loss is 7.554228068329394e-05\n",
      "epoch: 7 step: 1766, loss is 0.027154384180903435\n",
      "epoch: 7 step: 1767, loss is 0.0019318584818392992\n",
      "epoch: 7 step: 1768, loss is 0.04816712811589241\n",
      "epoch: 7 step: 1769, loss is 0.0831976905465126\n",
      "epoch: 7 step: 1770, loss is 0.0018485791515558958\n",
      "epoch: 7 step: 1771, loss is 0.004180348012596369\n",
      "epoch: 7 step: 1772, loss is 0.12411381304264069\n",
      "epoch: 7 step: 1773, loss is 0.004333493299782276\n",
      "epoch: 7 step: 1774, loss is 0.001403719186782837\n",
      "epoch: 7 step: 1775, loss is 0.03138411045074463\n",
      "epoch: 7 step: 1776, loss is 0.0003223394160158932\n",
      "epoch: 7 step: 1777, loss is 0.0006461383891291916\n",
      "epoch: 7 step: 1778, loss is 0.002095206640660763\n",
      "epoch: 7 step: 1779, loss is 0.00036086540785618126\n",
      "epoch: 7 step: 1780, loss is 0.004087310284376144\n",
      "epoch: 7 step: 1781, loss is 0.0016989267896860838\n",
      "epoch: 7 step: 1782, loss is 0.01483908575028181\n",
      "epoch: 7 step: 1783, loss is 0.015177403576672077\n",
      "epoch: 7 step: 1784, loss is 0.03590497374534607\n",
      "epoch: 7 step: 1785, loss is 0.08896174281835556\n",
      "epoch: 7 step: 1786, loss is 0.004546660464257002\n",
      "epoch: 7 step: 1787, loss is 0.0338575541973114\n",
      "epoch: 7 step: 1788, loss is 0.008436535485088825\n",
      "epoch: 7 step: 1789, loss is 0.0026491726748645306\n",
      "epoch: 7 step: 1790, loss is 0.05298436060547829\n",
      "epoch: 7 step: 1791, loss is 0.0001315839763265103\n",
      "epoch: 7 step: 1792, loss is 0.0072568245232105255\n",
      "epoch: 7 step: 1793, loss is 0.0011936589144170284\n",
      "epoch: 7 step: 1794, loss is 0.0002905253495555371\n",
      "epoch: 7 step: 1795, loss is 0.34391269087791443\n",
      "epoch: 7 step: 1796, loss is 0.04261169210076332\n",
      "epoch: 7 step: 1797, loss is 0.04128618538379669\n",
      "epoch: 7 step: 1798, loss is 0.00022391938546206802\n",
      "epoch: 7 step: 1799, loss is 0.0015219151973724365\n",
      "epoch: 7 step: 1800, loss is 0.10612229257822037\n",
      "epoch: 7 step: 1801, loss is 0.01661161705851555\n",
      "epoch: 7 step: 1802, loss is 0.0011629195651039481\n",
      "epoch: 7 step: 1803, loss is 0.0019850924145430326\n",
      "epoch: 7 step: 1804, loss is 0.0005836822674609721\n",
      "epoch: 7 step: 1805, loss is 0.006152251269668341\n",
      "epoch: 7 step: 1806, loss is 0.086947500705719\n",
      "epoch: 7 step: 1807, loss is 0.0038211015053093433\n",
      "epoch: 7 step: 1808, loss is 0.01803559996187687\n",
      "epoch: 7 step: 1809, loss is 0.02057662233710289\n",
      "epoch: 7 step: 1810, loss is 0.000600681290961802\n",
      "epoch: 7 step: 1811, loss is 0.00818815641105175\n",
      "epoch: 7 step: 1812, loss is 0.06955869495868683\n",
      "epoch: 7 step: 1813, loss is 0.0015603338833898306\n",
      "epoch: 7 step: 1814, loss is 0.0007797033758834004\n",
      "epoch: 7 step: 1815, loss is 0.03423234447836876\n",
      "epoch: 7 step: 1816, loss is 0.017938239499926567\n",
      "epoch: 7 step: 1817, loss is 0.0007329609361477196\n",
      "epoch: 7 step: 1818, loss is 0.0006350376643240452\n",
      "epoch: 7 step: 1819, loss is 0.01412560697644949\n",
      "epoch: 7 step: 1820, loss is 0.006720185279846191\n",
      "epoch: 7 step: 1821, loss is 0.003926838282495737\n",
      "epoch: 7 step: 1822, loss is 0.0029873144812881947\n",
      "epoch: 7 step: 1823, loss is 0.006316767074167728\n",
      "epoch: 7 step: 1824, loss is 0.01679241470992565\n",
      "epoch: 7 step: 1825, loss is 0.000249534408794716\n",
      "epoch: 7 step: 1826, loss is 0.0007766916533000767\n",
      "epoch: 7 step: 1827, loss is 0.0006207748083397746\n",
      "epoch: 7 step: 1828, loss is 3.084277705056593e-05\n",
      "epoch: 7 step: 1829, loss is 3.6072910006623715e-05\n",
      "epoch: 7 step: 1830, loss is 0.002232388826087117\n",
      "epoch: 7 step: 1831, loss is 0.0647280216217041\n",
      "epoch: 7 step: 1832, loss is 0.009995993226766586\n",
      "epoch: 7 step: 1833, loss is 0.0004241288988851011\n",
      "epoch: 7 step: 1834, loss is 0.0021039156708866358\n",
      "epoch: 7 step: 1835, loss is 0.016430342569947243\n",
      "epoch: 7 step: 1836, loss is 0.00041625165613368154\n",
      "epoch: 7 step: 1837, loss is 0.008945595473051071\n",
      "epoch: 7 step: 1838, loss is 0.0023823254741728306\n",
      "epoch: 7 step: 1839, loss is 0.0006475383415818214\n",
      "epoch: 7 step: 1840, loss is 0.0009350809850730002\n",
      "epoch: 7 step: 1841, loss is 0.00043772600474767387\n",
      "epoch: 7 step: 1842, loss is 0.013644464313983917\n",
      "epoch: 7 step: 1843, loss is 0.0024457520339637995\n",
      "epoch: 7 step: 1844, loss is 0.0019229949684813619\n",
      "epoch: 7 step: 1845, loss is 0.0024674730375409126\n",
      "epoch: 7 step: 1846, loss is 0.0015043648891150951\n",
      "epoch: 7 step: 1847, loss is 0.0005739539628848433\n",
      "epoch: 7 step: 1848, loss is 0.0005887188599444926\n",
      "epoch: 7 step: 1849, loss is 0.007791772484779358\n",
      "epoch: 7 step: 1850, loss is 0.006199916359037161\n",
      "epoch: 7 step: 1851, loss is 0.00016348798817489296\n",
      "epoch: 7 step: 1852, loss is 0.0002650897658895701\n",
      "epoch: 7 step: 1853, loss is 0.0008230293751694262\n",
      "epoch: 7 step: 1854, loss is 4.793515108758584e-05\n",
      "epoch: 7 step: 1855, loss is 0.09874004870653152\n",
      "epoch: 7 step: 1856, loss is 0.019777076318860054\n",
      "epoch: 7 step: 1857, loss is 0.0022283378057181835\n",
      "epoch: 7 step: 1858, loss is 0.0009704531403258443\n",
      "epoch: 7 step: 1859, loss is 0.00023722651530988514\n",
      "epoch: 7 step: 1860, loss is 0.04245271906256676\n",
      "epoch: 7 step: 1861, loss is 0.014946026727557182\n",
      "epoch: 7 step: 1862, loss is 3.653358362498693e-05\n",
      "epoch: 7 step: 1863, loss is 0.002602719934657216\n",
      "epoch: 7 step: 1864, loss is 0.0011659516021609306\n",
      "epoch: 7 step: 1865, loss is 0.036650605499744415\n",
      "epoch: 7 step: 1866, loss is 0.03572114184498787\n",
      "epoch: 7 step: 1867, loss is 0.0008081422420218587\n",
      "epoch: 7 step: 1868, loss is 0.0019005162175744772\n",
      "epoch: 7 step: 1869, loss is 0.03677113354206085\n",
      "epoch: 7 step: 1870, loss is 0.00018947984790429473\n",
      "epoch: 7 step: 1871, loss is 6.939285958651453e-05\n",
      "epoch: 7 step: 1872, loss is 0.0014796685427427292\n",
      "epoch: 7 step: 1873, loss is 6.355257937684655e-05\n",
      "epoch: 7 step: 1874, loss is 0.00024067045887932181\n",
      "epoch: 7 step: 1875, loss is 0.01553067285567522\n",
      "Train epoch time: 12958.362 ms, per step time: 6.911 ms\n",
      "epoch: 8 step: 1, loss is 0.00011085795995313674\n",
      "epoch: 8 step: 2, loss is 3.9090871723601595e-05\n",
      "epoch: 8 step: 3, loss is 0.0036344127729535103\n",
      "epoch: 8 step: 4, loss is 0.0036224706564098597\n",
      "epoch: 8 step: 5, loss is 0.0037906852085143328\n",
      "epoch: 8 step: 6, loss is 0.01598241738975048\n",
      "epoch: 8 step: 7, loss is 0.0028541460633277893\n",
      "epoch: 8 step: 8, loss is 0.1478375643491745\n",
      "epoch: 8 step: 9, loss is 0.03776940703392029\n",
      "epoch: 8 step: 10, loss is 0.1158982664346695\n",
      "epoch: 8 step: 11, loss is 0.0003161673084832728\n",
      "epoch: 8 step: 12, loss is 0.004734519869089127\n",
      "epoch: 8 step: 13, loss is 0.00016209890600293875\n",
      "epoch: 8 step: 14, loss is 0.0020062027033418417\n",
      "epoch: 8 step: 15, loss is 0.009219746105372906\n",
      "epoch: 8 step: 16, loss is 2.43669801420765e-05\n",
      "epoch: 8 step: 17, loss is 0.22981032729148865\n",
      "epoch: 8 step: 18, loss is 0.009738275781273842\n",
      "epoch: 8 step: 19, loss is 0.0006679495563730597\n",
      "epoch: 8 step: 20, loss is 0.008529500104486942\n",
      "epoch: 8 step: 21, loss is 0.004301989451050758\n",
      "epoch: 8 step: 22, loss is 0.009276109747588634\n",
      "epoch: 8 step: 23, loss is 0.02366955392062664\n",
      "epoch: 8 step: 24, loss is 0.0047134580090641975\n",
      "epoch: 8 step: 25, loss is 2.7407526431488805e-05\n",
      "epoch: 8 step: 26, loss is 0.00015941058518365026\n",
      "epoch: 8 step: 27, loss is 0.004009093623608351\n",
      "epoch: 8 step: 28, loss is 0.01778349094092846\n",
      "epoch: 8 step: 29, loss is 0.01109817624092102\n",
      "epoch: 8 step: 30, loss is 2.8780692446161993e-05\n",
      "epoch: 8 step: 31, loss is 0.0028224377892911434\n",
      "epoch: 8 step: 32, loss is 0.0013509608106687665\n",
      "epoch: 8 step: 33, loss is 0.001209593960084021\n",
      "epoch: 8 step: 34, loss is 0.0072913565672934055\n",
      "epoch: 8 step: 35, loss is 0.0016723619773983955\n",
      "epoch: 8 step: 36, loss is 0.0009965338977053761\n",
      "epoch: 8 step: 37, loss is 0.0006844117306172848\n",
      "epoch: 8 step: 38, loss is 0.004254684317857027\n",
      "epoch: 8 step: 39, loss is 0.003208599518984556\n",
      "epoch: 8 step: 40, loss is 0.0012745081912726164\n",
      "epoch: 8 step: 41, loss is 0.0037790252827107906\n",
      "epoch: 8 step: 42, loss is 0.08861304074525833\n",
      "epoch: 8 step: 43, loss is 0.008448423817753792\n",
      "epoch: 8 step: 44, loss is 0.004356421064585447\n",
      "epoch: 8 step: 45, loss is 0.00022880559845361859\n",
      "epoch: 8 step: 46, loss is 0.00010231258784187958\n",
      "epoch: 8 step: 47, loss is 0.0019056072924286127\n",
      "epoch: 8 step: 48, loss is 0.0007701144786551595\n",
      "epoch: 8 step: 49, loss is 8.320886990986764e-05\n",
      "epoch: 8 step: 50, loss is 0.006412587594240904\n",
      "epoch: 8 step: 51, loss is 0.038993943482637405\n",
      "epoch: 8 step: 52, loss is 8.354176679858938e-05\n",
      "epoch: 8 step: 53, loss is 0.0002691465779207647\n",
      "epoch: 8 step: 54, loss is 0.0012791622430086136\n",
      "epoch: 8 step: 55, loss is 9.62636768235825e-05\n",
      "epoch: 8 step: 56, loss is 0.018824268132448196\n",
      "epoch: 8 step: 57, loss is 0.008491097018122673\n",
      "epoch: 8 step: 58, loss is 0.0005991398356854916\n",
      "epoch: 8 step: 59, loss is 0.009654439985752106\n",
      "epoch: 8 step: 60, loss is 0.016486870124936104\n",
      "epoch: 8 step: 61, loss is 0.06680891662836075\n",
      "epoch: 8 step: 62, loss is 0.00018094363622367382\n",
      "epoch: 8 step: 63, loss is 0.00016658962704241276\n",
      "epoch: 8 step: 64, loss is 9.812680946197361e-05\n",
      "epoch: 8 step: 65, loss is 0.00026897824136540294\n",
      "epoch: 8 step: 66, loss is 0.001150674419477582\n",
      "epoch: 8 step: 67, loss is 0.0014521739212796092\n",
      "epoch: 8 step: 68, loss is 0.009075667709112167\n",
      "epoch: 8 step: 69, loss is 0.00020548685279209167\n",
      "epoch: 8 step: 70, loss is 0.013356871902942657\n",
      "epoch: 8 step: 71, loss is 0.001845259452238679\n",
      "epoch: 8 step: 72, loss is 8.167830674210563e-05\n",
      "epoch: 8 step: 73, loss is 0.002076218370348215\n",
      "epoch: 8 step: 74, loss is 0.009637147188186646\n",
      "epoch: 8 step: 75, loss is 0.09546659886837006\n",
      "epoch: 8 step: 76, loss is 0.00021851944620721042\n",
      "epoch: 8 step: 77, loss is 0.009839565493166447\n",
      "epoch: 8 step: 78, loss is 8.365101530216634e-05\n",
      "epoch: 8 step: 79, loss is 0.00011533380165928975\n",
      "epoch: 8 step: 80, loss is 0.00770928431302309\n",
      "epoch: 8 step: 81, loss is 0.0006212251610122621\n",
      "epoch: 8 step: 82, loss is 0.003266192739829421\n",
      "epoch: 8 step: 83, loss is 0.00011144982272526249\n",
      "epoch: 8 step: 84, loss is 0.0012450579088181257\n",
      "epoch: 8 step: 85, loss is 0.016384301707148552\n",
      "epoch: 8 step: 86, loss is 0.00016607253928668797\n",
      "epoch: 8 step: 87, loss is 0.02162625826895237\n",
      "epoch: 8 step: 88, loss is 0.0006671333103440702\n",
      "epoch: 8 step: 89, loss is 4.001214256277308e-05\n",
      "epoch: 8 step: 90, loss is 0.0006290574092417955\n",
      "epoch: 8 step: 91, loss is 0.005090587306767702\n",
      "epoch: 8 step: 92, loss is 0.0023975130170583725\n",
      "epoch: 8 step: 93, loss is 0.0003308926825411618\n",
      "epoch: 8 step: 94, loss is 0.00040219392394647\n",
      "epoch: 8 step: 95, loss is 0.0010976451449096203\n",
      "epoch: 8 step: 96, loss is 0.002691974164918065\n",
      "epoch: 8 step: 97, loss is 0.007473488338291645\n",
      "epoch: 8 step: 98, loss is 0.10226113349199295\n",
      "epoch: 8 step: 99, loss is 0.0020100295078009367\n",
      "epoch: 8 step: 100, loss is 0.00027488780324347317\n",
      "epoch: 8 step: 101, loss is 2.7127623980049975e-05\n",
      "epoch: 8 step: 102, loss is 0.0020573867950588465\n",
      "epoch: 8 step: 103, loss is 0.0008660767925903201\n",
      "epoch: 8 step: 104, loss is 0.08936757594347\n",
      "epoch: 8 step: 105, loss is 0.010790172964334488\n",
      "epoch: 8 step: 106, loss is 0.059088826179504395\n",
      "epoch: 8 step: 107, loss is 0.0001718834973871708\n",
      "epoch: 8 step: 108, loss is 0.00013169224257580936\n",
      "epoch: 8 step: 109, loss is 0.0010056901955977082\n",
      "epoch: 8 step: 110, loss is 0.00013345031766220927\n",
      "epoch: 8 step: 111, loss is 8.327499381266534e-05\n",
      "epoch: 8 step: 112, loss is 0.0010025894735008478\n",
      "epoch: 8 step: 113, loss is 0.007116451393812895\n",
      "epoch: 8 step: 114, loss is 0.00047091470332816243\n",
      "epoch: 8 step: 115, loss is 0.020536206662654877\n",
      "epoch: 8 step: 116, loss is 0.002142145298421383\n",
      "epoch: 8 step: 117, loss is 0.020210646092891693\n",
      "epoch: 8 step: 118, loss is 0.15083274245262146\n",
      "epoch: 8 step: 119, loss is 0.056748200207948685\n",
      "epoch: 8 step: 120, loss is 0.002739009214565158\n",
      "epoch: 8 step: 121, loss is 0.014547313563525677\n",
      "epoch: 8 step: 122, loss is 0.011364101432263851\n",
      "epoch: 8 step: 123, loss is 0.0006737234070897102\n",
      "epoch: 8 step: 124, loss is 7.339020612562308e-06\n",
      "epoch: 8 step: 125, loss is 0.00010781368473544717\n",
      "epoch: 8 step: 126, loss is 0.0009145968942902982\n",
      "epoch: 8 step: 127, loss is 0.008640449494123459\n",
      "epoch: 8 step: 128, loss is 0.0011972136562690139\n",
      "epoch: 8 step: 129, loss is 0.0011333003640174866\n",
      "epoch: 8 step: 130, loss is 0.09477096796035767\n",
      "epoch: 8 step: 131, loss is 0.06894859671592712\n",
      "epoch: 8 step: 132, loss is 0.0004541939706541598\n",
      "epoch: 8 step: 133, loss is 0.0024156204890459776\n",
      "epoch: 8 step: 134, loss is 0.029388703405857086\n",
      "epoch: 8 step: 135, loss is 0.0003574047877918929\n",
      "epoch: 8 step: 136, loss is 0.04586648941040039\n",
      "epoch: 8 step: 137, loss is 0.071865975856781\n",
      "epoch: 8 step: 138, loss is 0.18926911056041718\n",
      "epoch: 8 step: 139, loss is 0.16932740807533264\n",
      "epoch: 8 step: 140, loss is 0.0006869960343465209\n",
      "epoch: 8 step: 141, loss is 0.09510296583175659\n",
      "epoch: 8 step: 142, loss is 4.126953353988938e-05\n",
      "epoch: 8 step: 143, loss is 0.01136760413646698\n",
      "epoch: 8 step: 144, loss is 0.0006099320016801357\n",
      "epoch: 8 step: 145, loss is 0.00023291238176170737\n",
      "epoch: 8 step: 146, loss is 6.823231524322182e-05\n",
      "epoch: 8 step: 147, loss is 0.04130027815699577\n",
      "epoch: 8 step: 148, loss is 0.002233384642750025\n",
      "epoch: 8 step: 149, loss is 0.0019837599247694016\n",
      "epoch: 8 step: 150, loss is 0.0011597394477576017\n",
      "epoch: 8 step: 151, loss is 0.0061968229711055756\n",
      "epoch: 8 step: 152, loss is 0.0026707726065069437\n",
      "epoch: 8 step: 153, loss is 0.07662837952375412\n",
      "epoch: 8 step: 154, loss is 0.09088277816772461\n",
      "epoch: 8 step: 155, loss is 0.06253886967897415\n",
      "epoch: 8 step: 156, loss is 0.004807079676538706\n",
      "epoch: 8 step: 157, loss is 0.01762991026043892\n",
      "epoch: 8 step: 158, loss is 0.012575090862810612\n",
      "epoch: 8 step: 159, loss is 0.011099444702267647\n",
      "epoch: 8 step: 160, loss is 0.00027790525928139687\n",
      "epoch: 8 step: 161, loss is 0.0002426907594781369\n",
      "epoch: 8 step: 162, loss is 0.0012280758237466216\n",
      "epoch: 8 step: 163, loss is 0.02170010469853878\n",
      "epoch: 8 step: 164, loss is 0.00033045554300770164\n",
      "epoch: 8 step: 165, loss is 0.0047879815101623535\n",
      "epoch: 8 step: 166, loss is 0.00029803544748574495\n",
      "epoch: 8 step: 167, loss is 0.0004634417127817869\n",
      "epoch: 8 step: 168, loss is 0.005475067533552647\n",
      "epoch: 8 step: 169, loss is 0.0005060650291852653\n",
      "epoch: 8 step: 170, loss is 0.00028884701896458864\n",
      "epoch: 8 step: 171, loss is 0.013292353600263596\n",
      "epoch: 8 step: 172, loss is 0.0025855775456875563\n",
      "epoch: 8 step: 173, loss is 0.0035011880099773407\n",
      "epoch: 8 step: 174, loss is 0.01824805699288845\n",
      "epoch: 8 step: 175, loss is 0.004940452519804239\n",
      "epoch: 8 step: 176, loss is 0.00025414294213987887\n",
      "epoch: 8 step: 177, loss is 0.0008572885999456048\n",
      "epoch: 8 step: 178, loss is 0.002937074052169919\n",
      "epoch: 8 step: 179, loss is 0.0019050132250413299\n",
      "epoch: 8 step: 180, loss is 0.00037836661795154214\n",
      "epoch: 8 step: 181, loss is 0.0014433126198127866\n",
      "epoch: 8 step: 182, loss is 0.0007912727305665612\n",
      "epoch: 8 step: 183, loss is 0.0011804201640188694\n",
      "epoch: 8 step: 184, loss is 0.06727919727563858\n",
      "epoch: 8 step: 185, loss is 0.07802734524011612\n",
      "epoch: 8 step: 186, loss is 0.007095019798725843\n",
      "epoch: 8 step: 187, loss is 0.0018673206213861704\n",
      "epoch: 8 step: 188, loss is 0.000831842771731317\n",
      "epoch: 8 step: 189, loss is 0.017317645251750946\n",
      "epoch: 8 step: 190, loss is 0.00016585405683144927\n",
      "epoch: 8 step: 191, loss is 0.165573388338089\n",
      "epoch: 8 step: 192, loss is 0.05509962514042854\n",
      "epoch: 8 step: 193, loss is 0.009482376277446747\n",
      "epoch: 8 step: 194, loss is 1.815557334339246e-05\n",
      "epoch: 8 step: 195, loss is 0.0004106080159544945\n",
      "epoch: 8 step: 196, loss is 0.05645659193396568\n",
      "epoch: 8 step: 197, loss is 0.007310961838811636\n",
      "epoch: 8 step: 198, loss is 0.043613988906145096\n",
      "epoch: 8 step: 199, loss is 0.0026153712533414364\n",
      "epoch: 8 step: 200, loss is 6.84475526213646e-05\n",
      "epoch: 8 step: 201, loss is 0.00016257243987638503\n",
      "epoch: 8 step: 202, loss is 0.05539684742689133\n",
      "epoch: 8 step: 203, loss is 9.767102164914832e-05\n",
      "epoch: 8 step: 204, loss is 0.0021775043569505215\n",
      "epoch: 8 step: 205, loss is 0.00028492105775512755\n",
      "epoch: 8 step: 206, loss is 0.004754432011395693\n",
      "epoch: 8 step: 207, loss is 0.00046477021533064544\n",
      "epoch: 8 step: 208, loss is 0.0009083532495424151\n",
      "epoch: 8 step: 209, loss is 0.0003604724770411849\n",
      "epoch: 8 step: 210, loss is 0.003726933617144823\n",
      "epoch: 8 step: 211, loss is 0.004788885824382305\n",
      "epoch: 8 step: 212, loss is 0.013559851795434952\n",
      "epoch: 8 step: 213, loss is 0.005483105778694153\n",
      "epoch: 8 step: 214, loss is 0.005449228920042515\n",
      "epoch: 8 step: 215, loss is 0.01850995048880577\n",
      "epoch: 8 step: 216, loss is 2.8566964829224162e-05\n",
      "epoch: 8 step: 217, loss is 0.04031003639101982\n",
      "epoch: 8 step: 218, loss is 0.0003006082260981202\n",
      "epoch: 8 step: 219, loss is 0.0008688066736795008\n",
      "epoch: 8 step: 220, loss is 0.009020848199725151\n",
      "epoch: 8 step: 221, loss is 0.0002702134079299867\n",
      "epoch: 8 step: 222, loss is 5.713254722650163e-05\n",
      "epoch: 8 step: 223, loss is 0.038667917251586914\n",
      "epoch: 8 step: 224, loss is 0.02390066348016262\n",
      "epoch: 8 step: 225, loss is 5.766443791799247e-05\n",
      "epoch: 8 step: 226, loss is 0.0017566756578162313\n",
      "epoch: 8 step: 227, loss is 0.0004196729860268533\n",
      "epoch: 8 step: 228, loss is 0.04521820694208145\n",
      "epoch: 8 step: 229, loss is 0.013454982079565525\n",
      "epoch: 8 step: 230, loss is 0.0008891028701327741\n",
      "epoch: 8 step: 231, loss is 0.0004845887888222933\n",
      "epoch: 8 step: 232, loss is 0.0022687288001179695\n",
      "epoch: 8 step: 233, loss is 0.002644140971824527\n",
      "epoch: 8 step: 234, loss is 0.0035165271256119013\n",
      "epoch: 8 step: 235, loss is 0.026069464161992073\n",
      "epoch: 8 step: 236, loss is 0.07669023424386978\n",
      "epoch: 8 step: 237, loss is 0.000745365337934345\n",
      "epoch: 8 step: 238, loss is 0.0005213285330682993\n",
      "epoch: 8 step: 239, loss is 0.0008514165529049933\n",
      "epoch: 8 step: 240, loss is 0.0009726400021463633\n",
      "epoch: 8 step: 241, loss is 0.0009509831434115767\n",
      "epoch: 8 step: 242, loss is 0.000999139854684472\n",
      "epoch: 8 step: 243, loss is 0.0007975968183018267\n",
      "epoch: 8 step: 244, loss is 0.0033649089746177197\n",
      "epoch: 8 step: 245, loss is 0.0005337463808245957\n",
      "epoch: 8 step: 246, loss is 0.0011217165738344193\n",
      "epoch: 8 step: 247, loss is 0.010712210088968277\n",
      "epoch: 8 step: 248, loss is 1.0437236596771982e-05\n",
      "epoch: 8 step: 249, loss is 0.00024721588124521077\n",
      "epoch: 8 step: 250, loss is 0.00040854746475815773\n",
      "epoch: 8 step: 251, loss is 0.0027934585232287645\n",
      "epoch: 8 step: 252, loss is 0.0037141626235097647\n",
      "epoch: 8 step: 253, loss is 0.0002272725832881406\n",
      "epoch: 8 step: 254, loss is 0.018607089295983315\n",
      "epoch: 8 step: 255, loss is 1.1677741895255167e-05\n",
      "epoch: 8 step: 256, loss is 0.003217078512534499\n",
      "epoch: 8 step: 257, loss is 0.1214105412364006\n",
      "epoch: 8 step: 258, loss is 0.05388785898685455\n",
      "epoch: 8 step: 259, loss is 0.0059584868140518665\n",
      "epoch: 8 step: 260, loss is 0.0008897503139451146\n",
      "epoch: 8 step: 261, loss is 0.0012068053474649787\n",
      "epoch: 8 step: 262, loss is 0.0002413422189420089\n",
      "epoch: 8 step: 263, loss is 0.013291980139911175\n",
      "epoch: 8 step: 264, loss is 0.00019583448010962456\n",
      "epoch: 8 step: 265, loss is 0.0023356142919510603\n",
      "epoch: 8 step: 266, loss is 0.0013820388121530414\n",
      "epoch: 8 step: 267, loss is 0.04803440347313881\n",
      "epoch: 8 step: 268, loss is 0.0004144915728829801\n",
      "epoch: 8 step: 269, loss is 0.011448652483522892\n",
      "epoch: 8 step: 270, loss is 0.0025740533601492643\n",
      "epoch: 8 step: 271, loss is 0.07350842654705048\n",
      "epoch: 8 step: 272, loss is 0.0005851333844475448\n",
      "epoch: 8 step: 273, loss is 0.0007780015002936125\n",
      "epoch: 8 step: 274, loss is 0.00036735390312969685\n",
      "epoch: 8 step: 275, loss is 0.003250963520258665\n",
      "epoch: 8 step: 276, loss is 0.005589389242231846\n",
      "epoch: 8 step: 277, loss is 0.0019994277972728014\n",
      "epoch: 8 step: 278, loss is 0.1138756200671196\n",
      "epoch: 8 step: 279, loss is 0.006545295938849449\n",
      "epoch: 8 step: 280, loss is 0.003308270126581192\n",
      "epoch: 8 step: 281, loss is 0.0003802224528044462\n",
      "epoch: 8 step: 282, loss is 0.010294265113770962\n",
      "epoch: 8 step: 283, loss is 0.03826243057847023\n",
      "epoch: 8 step: 284, loss is 0.019239427521824837\n",
      "epoch: 8 step: 285, loss is 7.280107820406556e-05\n",
      "epoch: 8 step: 286, loss is 9.780578693607822e-05\n",
      "epoch: 8 step: 287, loss is 0.0013796990970149636\n",
      "epoch: 8 step: 288, loss is 0.00024539040168747306\n",
      "epoch: 8 step: 289, loss is 0.0003808248438872397\n",
      "epoch: 8 step: 290, loss is 0.053740307688713074\n",
      "epoch: 8 step: 291, loss is 0.0014195325784385204\n",
      "epoch: 8 step: 292, loss is 0.02063720114529133\n",
      "epoch: 8 step: 293, loss is 0.002399388700723648\n",
      "epoch: 8 step: 294, loss is 8.994019299279898e-05\n",
      "epoch: 8 step: 295, loss is 0.0023739966563880444\n",
      "epoch: 8 step: 296, loss is 0.19606910645961761\n",
      "epoch: 8 step: 297, loss is 0.01902272179722786\n",
      "epoch: 8 step: 298, loss is 0.00014031652244739234\n",
      "epoch: 8 step: 299, loss is 0.024470530450344086\n",
      "epoch: 8 step: 300, loss is 0.29977357387542725\n",
      "epoch: 8 step: 301, loss is 0.07282567769289017\n",
      "epoch: 8 step: 302, loss is 0.023133354261517525\n",
      "epoch: 8 step: 303, loss is 0.0002075701195280999\n",
      "epoch: 8 step: 304, loss is 0.0010092707816511393\n",
      "epoch: 8 step: 305, loss is 0.0025617617648094893\n",
      "epoch: 8 step: 306, loss is 0.00015733878535684198\n",
      "epoch: 8 step: 307, loss is 0.00782990362495184\n",
      "epoch: 8 step: 308, loss is 0.01625615730881691\n",
      "epoch: 8 step: 309, loss is 0.00017924248822964728\n",
      "epoch: 8 step: 310, loss is 0.00020362100622151047\n",
      "epoch: 8 step: 311, loss is 0.0005193157703615725\n",
      "epoch: 8 step: 312, loss is 0.000444537028670311\n",
      "epoch: 8 step: 313, loss is 0.0015639866469427943\n",
      "epoch: 8 step: 314, loss is 0.00932688731700182\n",
      "epoch: 8 step: 315, loss is 0.025599021464586258\n",
      "epoch: 8 step: 316, loss is 0.004689771682024002\n",
      "epoch: 8 step: 317, loss is 0.05033598095178604\n",
      "epoch: 8 step: 318, loss is 0.04345577210187912\n",
      "epoch: 8 step: 319, loss is 0.0029840769711881876\n",
      "epoch: 8 step: 320, loss is 0.0021747970022261143\n",
      "epoch: 8 step: 321, loss is 0.11416077613830566\n",
      "epoch: 8 step: 322, loss is 0.00017290980031248182\n",
      "epoch: 8 step: 323, loss is 0.0012105682399123907\n",
      "epoch: 8 step: 324, loss is 6.463920726673678e-05\n",
      "epoch: 8 step: 325, loss is 0.0008184522157534957\n",
      "epoch: 8 step: 326, loss is 0.0009311175090260804\n",
      "epoch: 8 step: 327, loss is 0.0015473259845748544\n",
      "epoch: 8 step: 328, loss is 0.0008723206119611859\n",
      "epoch: 8 step: 329, loss is 0.00013458730245474726\n",
      "epoch: 8 step: 330, loss is 0.00031576654873788357\n",
      "epoch: 8 step: 331, loss is 0.008257318288087845\n",
      "epoch: 8 step: 332, loss is 0.0001122551184380427\n",
      "epoch: 8 step: 333, loss is 0.005317086819559336\n",
      "epoch: 8 step: 334, loss is 0.002249167999252677\n",
      "epoch: 8 step: 335, loss is 0.00018699115025810897\n",
      "epoch: 8 step: 336, loss is 0.005303164944052696\n",
      "epoch: 8 step: 337, loss is 0.00231579365208745\n",
      "epoch: 8 step: 338, loss is 0.0024161641485989094\n",
      "epoch: 8 step: 339, loss is 0.0807030126452446\n",
      "epoch: 8 step: 340, loss is 0.08225879073143005\n",
      "epoch: 8 step: 341, loss is 0.0008493344066664577\n",
      "epoch: 8 step: 342, loss is 0.03373613581061363\n",
      "epoch: 8 step: 343, loss is 0.009397548623383045\n",
      "epoch: 8 step: 344, loss is 0.11415417492389679\n",
      "epoch: 8 step: 345, loss is 0.0006136732990853488\n",
      "epoch: 8 step: 346, loss is 0.0011496488004922867\n",
      "epoch: 8 step: 347, loss is 0.004037363920360804\n",
      "epoch: 8 step: 348, loss is 0.041164711117744446\n",
      "epoch: 8 step: 349, loss is 0.012288636527955532\n",
      "epoch: 8 step: 350, loss is 0.0006840713904239237\n",
      "epoch: 8 step: 351, loss is 0.002421353477984667\n",
      "epoch: 8 step: 352, loss is 0.026193734258413315\n",
      "epoch: 8 step: 353, loss is 0.07538431137800217\n",
      "epoch: 8 step: 354, loss is 0.01869831047952175\n",
      "epoch: 8 step: 355, loss is 0.0003238061035517603\n",
      "epoch: 8 step: 356, loss is 0.06667280197143555\n",
      "epoch: 8 step: 357, loss is 0.031772784888744354\n",
      "epoch: 8 step: 358, loss is 0.0028019545134156942\n",
      "epoch: 8 step: 359, loss is 0.004128975793719292\n",
      "epoch: 8 step: 360, loss is 0.0005527280154637992\n",
      "epoch: 8 step: 361, loss is 0.0008595181279815733\n",
      "epoch: 8 step: 362, loss is 0.010229037143290043\n",
      "epoch: 8 step: 363, loss is 0.000278887280728668\n",
      "epoch: 8 step: 364, loss is 0.14133359491825104\n",
      "epoch: 8 step: 365, loss is 0.0005554741364903748\n",
      "epoch: 8 step: 366, loss is 0.003948270343244076\n",
      "epoch: 8 step: 367, loss is 0.012723594903945923\n",
      "epoch: 8 step: 368, loss is 0.015862511470913887\n",
      "epoch: 8 step: 369, loss is 0.02259649708867073\n",
      "epoch: 8 step: 370, loss is 0.0010838425951078534\n",
      "epoch: 8 step: 371, loss is 0.0004769325314555317\n",
      "epoch: 8 step: 372, loss is 0.03688659518957138\n",
      "epoch: 8 step: 373, loss is 0.0018896309193223715\n",
      "epoch: 8 step: 374, loss is 0.006704461295157671\n",
      "epoch: 8 step: 375, loss is 0.01722632721066475\n",
      "epoch: 8 step: 376, loss is 0.0006442391313612461\n",
      "epoch: 8 step: 377, loss is 0.026782233268022537\n",
      "epoch: 8 step: 378, loss is 0.07141949981451035\n",
      "epoch: 8 step: 379, loss is 0.001697162282653153\n",
      "epoch: 8 step: 380, loss is 0.03187979385256767\n",
      "epoch: 8 step: 381, loss is 0.004970418754965067\n",
      "epoch: 8 step: 382, loss is 0.003307023784145713\n",
      "epoch: 8 step: 383, loss is 0.0003044969344045967\n",
      "epoch: 8 step: 384, loss is 3.958061643061228e-05\n",
      "epoch: 8 step: 385, loss is 0.0002846623829100281\n",
      "epoch: 8 step: 386, loss is 0.0007078405469655991\n",
      "epoch: 8 step: 387, loss is 0.0036604199558496475\n",
      "epoch: 8 step: 388, loss is 0.001958561362698674\n",
      "epoch: 8 step: 389, loss is 0.001950268866494298\n",
      "epoch: 8 step: 390, loss is 0.0006994704017415643\n",
      "epoch: 8 step: 391, loss is 0.00027754114125855267\n",
      "epoch: 8 step: 392, loss is 0.0005542756989598274\n",
      "epoch: 8 step: 393, loss is 0.01625179685652256\n",
      "epoch: 8 step: 394, loss is 0.00045265519293025136\n",
      "epoch: 8 step: 395, loss is 0.03226541727781296\n",
      "epoch: 8 step: 396, loss is 0.0013765387702733278\n",
      "epoch: 8 step: 397, loss is 0.01636706478893757\n",
      "epoch: 8 step: 398, loss is 0.0005670116515830159\n",
      "epoch: 8 step: 399, loss is 0.0007929553394205868\n",
      "epoch: 8 step: 400, loss is 0.0017282214248552918\n",
      "epoch: 8 step: 401, loss is 0.1454288810491562\n",
      "epoch: 8 step: 402, loss is 0.004587037954479456\n",
      "epoch: 8 step: 403, loss is 0.0010625245049595833\n",
      "epoch: 8 step: 404, loss is 0.0013807510258629918\n",
      "epoch: 8 step: 405, loss is 0.004959991201758385\n",
      "epoch: 8 step: 406, loss is 0.001846801838837564\n",
      "epoch: 8 step: 407, loss is 0.015012888237833977\n",
      "epoch: 8 step: 408, loss is 0.00020416235201992095\n",
      "epoch: 8 step: 409, loss is 0.00037539805634878576\n",
      "epoch: 8 step: 410, loss is 0.004847681149840355\n",
      "epoch: 8 step: 411, loss is 0.0009760137181729078\n",
      "epoch: 8 step: 412, loss is 0.10171713680028915\n",
      "epoch: 8 step: 413, loss is 0.01780606247484684\n",
      "epoch: 8 step: 414, loss is 0.024087991565465927\n",
      "epoch: 8 step: 415, loss is 0.0019354862160980701\n",
      "epoch: 8 step: 416, loss is 0.0010529960272833705\n",
      "epoch: 8 step: 417, loss is 0.00503912940621376\n",
      "epoch: 8 step: 418, loss is 0.008039506152272224\n",
      "epoch: 8 step: 419, loss is 0.0027707647532224655\n",
      "epoch: 8 step: 420, loss is 0.00043089239625260234\n",
      "epoch: 8 step: 421, loss is 0.002447446109727025\n",
      "epoch: 8 step: 422, loss is 0.000131397187942639\n",
      "epoch: 8 step: 423, loss is 0.0034729267936199903\n",
      "epoch: 8 step: 424, loss is 0.00016535504255443811\n",
      "epoch: 8 step: 425, loss is 0.14884403347969055\n",
      "epoch: 8 step: 426, loss is 0.0003999740001745522\n",
      "epoch: 8 step: 427, loss is 0.015315775759518147\n",
      "epoch: 8 step: 428, loss is 0.021488862112164497\n",
      "epoch: 8 step: 429, loss is 0.0020342026837170124\n",
      "epoch: 8 step: 430, loss is 0.003052646294236183\n",
      "epoch: 8 step: 431, loss is 3.4236989449709654e-05\n",
      "epoch: 8 step: 432, loss is 0.022227246314287186\n",
      "epoch: 8 step: 433, loss is 0.10531788319349289\n",
      "epoch: 8 step: 434, loss is 0.00023118488024920225\n",
      "epoch: 8 step: 435, loss is 0.00027806201251223683\n",
      "epoch: 8 step: 436, loss is 0.09574635326862335\n",
      "epoch: 8 step: 437, loss is 0.00015694071771577\n",
      "epoch: 8 step: 438, loss is 0.010170175693929195\n",
      "epoch: 8 step: 439, loss is 0.0074556018225848675\n",
      "epoch: 8 step: 440, loss is 0.00034288151073269546\n",
      "epoch: 8 step: 441, loss is 0.00041589149623177946\n",
      "epoch: 8 step: 442, loss is 0.034872278571128845\n",
      "epoch: 8 step: 443, loss is 0.0009390321793034673\n",
      "epoch: 8 step: 444, loss is 0.020135611295700073\n",
      "epoch: 8 step: 445, loss is 0.0013406312791630626\n",
      "epoch: 8 step: 446, loss is 0.0009756543440744281\n",
      "epoch: 8 step: 447, loss is 0.016307838261127472\n",
      "epoch: 8 step: 448, loss is 0.0028617794159799814\n",
      "epoch: 8 step: 449, loss is 6.948563532205299e-05\n",
      "epoch: 8 step: 450, loss is 0.0008166884654201567\n",
      "epoch: 8 step: 451, loss is 2.9882130547775887e-05\n",
      "epoch: 8 step: 452, loss is 0.000629861606284976\n",
      "epoch: 8 step: 453, loss is 0.002311138901859522\n",
      "epoch: 8 step: 454, loss is 0.00040026570786722004\n",
      "epoch: 8 step: 455, loss is 0.003509773639962077\n",
      "epoch: 8 step: 456, loss is 0.2742123305797577\n",
      "epoch: 8 step: 457, loss is 0.0024435746017843485\n",
      "epoch: 8 step: 458, loss is 0.05727929621934891\n",
      "epoch: 8 step: 459, loss is 0.0022545114625245333\n",
      "epoch: 8 step: 460, loss is 0.01194080151617527\n",
      "epoch: 8 step: 461, loss is 0.00014180665311869234\n",
      "epoch: 8 step: 462, loss is 0.023090554401278496\n",
      "epoch: 8 step: 463, loss is 0.001352946856059134\n",
      "epoch: 8 step: 464, loss is 0.0008139092242345214\n",
      "epoch: 8 step: 465, loss is 0.03621295839548111\n",
      "epoch: 8 step: 466, loss is 0.0002552882069721818\n",
      "epoch: 8 step: 467, loss is 0.00044539448572322726\n",
      "epoch: 8 step: 468, loss is 0.006046427879482508\n",
      "epoch: 8 step: 469, loss is 0.005386268254369497\n",
      "epoch: 8 step: 470, loss is 0.0006576047744601965\n",
      "epoch: 8 step: 471, loss is 0.009814035147428513\n",
      "epoch: 8 step: 472, loss is 0.0003387885808479041\n",
      "epoch: 8 step: 473, loss is 0.00039697616011835635\n",
      "epoch: 8 step: 474, loss is 0.00012084695481462404\n",
      "epoch: 8 step: 475, loss is 0.0041891722939908504\n",
      "epoch: 8 step: 476, loss is 0.023778287693858147\n",
      "epoch: 8 step: 477, loss is 0.000952039728872478\n",
      "epoch: 8 step: 478, loss is 4.930731302010827e-05\n",
      "epoch: 8 step: 479, loss is 0.003938686568289995\n",
      "epoch: 8 step: 480, loss is 0.0038214835803955793\n",
      "epoch: 8 step: 481, loss is 0.12253084033727646\n",
      "epoch: 8 step: 482, loss is 0.015439222566783428\n",
      "epoch: 8 step: 483, loss is 0.022508377209305763\n",
      "epoch: 8 step: 484, loss is 0.014805653132498264\n",
      "epoch: 8 step: 485, loss is 0.013580298982560635\n",
      "epoch: 8 step: 486, loss is 0.002372288377955556\n",
      "epoch: 8 step: 487, loss is 0.13888026773929596\n",
      "epoch: 8 step: 488, loss is 0.0022900209296494722\n",
      "epoch: 8 step: 489, loss is 0.004179834853857756\n",
      "epoch: 8 step: 490, loss is 0.001554611837491393\n",
      "epoch: 8 step: 491, loss is 0.0028541020583361387\n",
      "epoch: 8 step: 492, loss is 0.0016385959461331367\n",
      "epoch: 8 step: 493, loss is 0.015067785046994686\n",
      "epoch: 8 step: 494, loss is 0.0004259086854290217\n",
      "epoch: 8 step: 495, loss is 0.03492013365030289\n",
      "epoch: 8 step: 496, loss is 0.07782551646232605\n",
      "epoch: 8 step: 497, loss is 0.0003770303737837821\n",
      "epoch: 8 step: 498, loss is 0.008591925725340843\n",
      "epoch: 8 step: 499, loss is 0.00025965285021811724\n",
      "epoch: 8 step: 500, loss is 0.0003414769598748535\n",
      "epoch: 8 step: 501, loss is 0.0074661667458713055\n",
      "epoch: 8 step: 502, loss is 0.023538930341601372\n",
      "epoch: 8 step: 503, loss is 0.07266368716955185\n",
      "epoch: 8 step: 504, loss is 0.017213119193911552\n",
      "epoch: 8 step: 505, loss is 0.0073048705235123634\n",
      "epoch: 8 step: 506, loss is 0.0004572748439386487\n",
      "epoch: 8 step: 507, loss is 0.0014524280559271574\n",
      "epoch: 8 step: 508, loss is 0.028849085792899132\n",
      "epoch: 8 step: 509, loss is 0.00017971535271499306\n",
      "epoch: 8 step: 510, loss is 0.00024027566541917622\n",
      "epoch: 8 step: 511, loss is 0.12407699227333069\n",
      "epoch: 8 step: 512, loss is 0.004824906121939421\n",
      "epoch: 8 step: 513, loss is 0.09422241151332855\n",
      "epoch: 8 step: 514, loss is 0.01342965941876173\n",
      "epoch: 8 step: 515, loss is 0.0013188502052798867\n",
      "epoch: 8 step: 516, loss is 0.00019015940779354423\n",
      "epoch: 8 step: 517, loss is 0.006390766706317663\n",
      "epoch: 8 step: 518, loss is 0.003032926470041275\n",
      "epoch: 8 step: 519, loss is 0.07105914503335953\n",
      "epoch: 8 step: 520, loss is 0.0001431117270840332\n",
      "epoch: 8 step: 521, loss is 0.01813419908285141\n",
      "epoch: 8 step: 522, loss is 0.00012514319678302854\n",
      "epoch: 8 step: 523, loss is 0.00014821231889072806\n",
      "epoch: 8 step: 524, loss is 0.0009394572698511183\n",
      "epoch: 8 step: 525, loss is 0.0020320245530456305\n",
      "epoch: 8 step: 526, loss is 0.02816544473171234\n",
      "epoch: 8 step: 527, loss is 1.8181221093982458e-05\n",
      "epoch: 8 step: 528, loss is 0.004829582292586565\n",
      "epoch: 8 step: 529, loss is 0.044948164373636246\n",
      "epoch: 8 step: 530, loss is 0.020659010857343674\n",
      "epoch: 8 step: 531, loss is 2.9701690436922945e-05\n",
      "epoch: 8 step: 532, loss is 0.05706238001585007\n",
      "epoch: 8 step: 533, loss is 0.018040230497717857\n",
      "epoch: 8 step: 534, loss is 0.0002966435276903212\n",
      "epoch: 8 step: 535, loss is 5.896863876841962e-05\n",
      "epoch: 8 step: 536, loss is 0.005517866462469101\n",
      "epoch: 8 step: 537, loss is 0.00029357612947933376\n",
      "epoch: 8 step: 538, loss is 0.00029852695297449827\n",
      "epoch: 8 step: 539, loss is 0.015161099843680859\n",
      "epoch: 8 step: 540, loss is 0.0008644141489639878\n",
      "epoch: 8 step: 541, loss is 0.00755830854177475\n",
      "epoch: 8 step: 542, loss is 0.00023902338580228388\n",
      "epoch: 8 step: 543, loss is 0.0012170844711363316\n",
      "epoch: 8 step: 544, loss is 0.004259450361132622\n",
      "epoch: 8 step: 545, loss is 0.04564346373081207\n",
      "epoch: 8 step: 546, loss is 0.0009458505664952099\n",
      "epoch: 8 step: 547, loss is 6.242927338462323e-05\n",
      "epoch: 8 step: 548, loss is 0.0001355921704089269\n",
      "epoch: 8 step: 549, loss is 0.000255617022048682\n",
      "epoch: 8 step: 550, loss is 0.0027321726083755493\n",
      "epoch: 8 step: 551, loss is 0.021826721727848053\n",
      "epoch: 8 step: 552, loss is 0.0017392768058925867\n",
      "epoch: 8 step: 553, loss is 0.008275233209133148\n",
      "epoch: 8 step: 554, loss is 0.069009929895401\n",
      "epoch: 8 step: 555, loss is 0.008037940599024296\n",
      "epoch: 8 step: 556, loss is 9.315385977970436e-05\n",
      "epoch: 8 step: 557, loss is 0.008256908506155014\n",
      "epoch: 8 step: 558, loss is 0.0006846590549685061\n",
      "epoch: 8 step: 559, loss is 0.00019088099361397326\n",
      "epoch: 8 step: 560, loss is 0.005967601668089628\n",
      "epoch: 8 step: 561, loss is 0.008369332179427147\n",
      "epoch: 8 step: 562, loss is 0.0011848387075588107\n",
      "epoch: 8 step: 563, loss is 0.0003192395670339465\n",
      "epoch: 8 step: 564, loss is 0.029060864821076393\n",
      "epoch: 8 step: 565, loss is 0.007699848152697086\n",
      "epoch: 8 step: 566, loss is 0.005712646991014481\n",
      "epoch: 8 step: 567, loss is 0.0041363718919456005\n",
      "epoch: 8 step: 568, loss is 0.00024478614795953035\n",
      "epoch: 8 step: 569, loss is 0.020507683977484703\n",
      "epoch: 8 step: 570, loss is 0.03681021183729172\n",
      "epoch: 8 step: 571, loss is 0.008605778217315674\n",
      "epoch: 8 step: 572, loss is 0.0090665677562356\n",
      "epoch: 8 step: 573, loss is 0.0049255588091909885\n",
      "epoch: 8 step: 574, loss is 0.001139870728366077\n",
      "epoch: 8 step: 575, loss is 0.00159682787489146\n",
      "epoch: 8 step: 576, loss is 0.0004271639045327902\n",
      "epoch: 8 step: 577, loss is 0.00013467272219713777\n",
      "epoch: 8 step: 578, loss is 0.00014168480993248522\n",
      "epoch: 8 step: 579, loss is 0.0010218668030574918\n",
      "epoch: 8 step: 580, loss is 0.0011852736352011561\n",
      "epoch: 8 step: 581, loss is 0.001266941661015153\n",
      "epoch: 8 step: 582, loss is 0.0002166519989259541\n",
      "epoch: 8 step: 583, loss is 0.012875855900347233\n",
      "epoch: 8 step: 584, loss is 0.00044229940976947546\n",
      "epoch: 8 step: 585, loss is 0.16846907138824463\n",
      "epoch: 8 step: 586, loss is 0.01044783741235733\n",
      "epoch: 8 step: 587, loss is 0.059130191802978516\n",
      "epoch: 8 step: 588, loss is 0.0002663100603967905\n",
      "epoch: 8 step: 589, loss is 0.006552996579557657\n",
      "epoch: 8 step: 590, loss is 0.0273850429803133\n",
      "epoch: 8 step: 591, loss is 0.001809947658330202\n",
      "epoch: 8 step: 592, loss is 0.0053787073120474815\n",
      "epoch: 8 step: 593, loss is 0.004739855881780386\n",
      "epoch: 8 step: 594, loss is 4.733941750600934e-05\n",
      "epoch: 8 step: 595, loss is 0.006425922270864248\n",
      "epoch: 8 step: 596, loss is 0.045139942318201065\n",
      "epoch: 8 step: 597, loss is 0.0006152450223453343\n",
      "epoch: 8 step: 598, loss is 0.0016047017415985465\n",
      "epoch: 8 step: 599, loss is 0.0032645745668560266\n",
      "epoch: 8 step: 600, loss is 0.0002386882551945746\n",
      "epoch: 8 step: 601, loss is 0.10500559210777283\n",
      "epoch: 8 step: 602, loss is 0.053119413554668427\n",
      "epoch: 8 step: 603, loss is 0.004955742508172989\n",
      "epoch: 8 step: 604, loss is 0.00016350897203665227\n",
      "epoch: 8 step: 605, loss is 0.001447281101718545\n",
      "epoch: 8 step: 606, loss is 0.009655993431806564\n",
      "epoch: 8 step: 607, loss is 0.003704156493768096\n",
      "epoch: 8 step: 608, loss is 0.0005054797511547804\n",
      "epoch: 8 step: 609, loss is 0.010549533180892467\n",
      "epoch: 8 step: 610, loss is 0.056022800505161285\n",
      "epoch: 8 step: 611, loss is 0.0252817552536726\n",
      "epoch: 8 step: 612, loss is 0.004428464453667402\n",
      "epoch: 8 step: 613, loss is 0.10137131065130234\n",
      "epoch: 8 step: 614, loss is 0.004172979388386011\n",
      "epoch: 8 step: 615, loss is 8.57482009450905e-05\n",
      "epoch: 8 step: 616, loss is 0.00015475107647944242\n",
      "epoch: 8 step: 617, loss is 0.0007059021736495197\n",
      "epoch: 8 step: 618, loss is 0.0026089386083185673\n",
      "epoch: 8 step: 619, loss is 0.0002694498107302934\n",
      "epoch: 8 step: 620, loss is 0.00016030202095862478\n",
      "epoch: 8 step: 621, loss is 0.001419348525814712\n",
      "epoch: 8 step: 622, loss is 0.0003205298271495849\n",
      "epoch: 8 step: 623, loss is 0.08595573902130127\n",
      "epoch: 8 step: 624, loss is 4.338487269706093e-05\n",
      "epoch: 8 step: 625, loss is 0.000774663407355547\n",
      "epoch: 8 step: 626, loss is 0.007136929780244827\n",
      "epoch: 8 step: 627, loss is 0.0004097849887330085\n",
      "epoch: 8 step: 628, loss is 0.013208555057644844\n",
      "epoch: 8 step: 629, loss is 0.00019885695655830204\n",
      "epoch: 8 step: 630, loss is 0.000293019344098866\n",
      "epoch: 8 step: 631, loss is 0.005413604900240898\n",
      "epoch: 8 step: 632, loss is 2.568570926086977e-05\n",
      "epoch: 8 step: 633, loss is 0.15731175243854523\n",
      "epoch: 8 step: 634, loss is 0.002946800086647272\n",
      "epoch: 8 step: 635, loss is 0.0002312852011527866\n",
      "epoch: 8 step: 636, loss is 0.0740065798163414\n",
      "epoch: 8 step: 637, loss is 0.0031669437885284424\n",
      "epoch: 8 step: 638, loss is 0.049261800944805145\n",
      "epoch: 8 step: 639, loss is 0.008951300755143166\n",
      "epoch: 8 step: 640, loss is 0.004635696765035391\n",
      "epoch: 8 step: 641, loss is 0.0011015261989086866\n",
      "epoch: 8 step: 642, loss is 0.01228414848446846\n",
      "epoch: 8 step: 643, loss is 0.000243171991314739\n",
      "epoch: 8 step: 644, loss is 0.0013713816879317164\n",
      "epoch: 8 step: 645, loss is 0.104719378054142\n",
      "epoch: 8 step: 646, loss is 0.0028984835371375084\n",
      "epoch: 8 step: 647, loss is 0.002702604979276657\n",
      "epoch: 8 step: 648, loss is 0.00031227199360728264\n",
      "epoch: 8 step: 649, loss is 0.00015506221097894013\n",
      "epoch: 8 step: 650, loss is 0.12133857607841492\n",
      "epoch: 8 step: 651, loss is 0.00044748932123184204\n",
      "epoch: 8 step: 652, loss is 0.0003591986314859241\n",
      "epoch: 8 step: 653, loss is 0.001758469152264297\n",
      "epoch: 8 step: 654, loss is 0.00013861324987374246\n",
      "epoch: 8 step: 655, loss is 0.0030166879296302795\n",
      "epoch: 8 step: 656, loss is 0.0008236733265221119\n",
      "epoch: 8 step: 657, loss is 0.0014824462123215199\n",
      "epoch: 8 step: 658, loss is 0.11648280173540115\n",
      "epoch: 8 step: 659, loss is 0.014329779893159866\n",
      "epoch: 8 step: 660, loss is 0.030635256320238113\n",
      "epoch: 8 step: 661, loss is 0.000379251956474036\n",
      "epoch: 8 step: 662, loss is 0.024044495075941086\n",
      "epoch: 8 step: 663, loss is 0.0005257706507109106\n",
      "epoch: 8 step: 664, loss is 0.00026326323859393597\n",
      "epoch: 8 step: 665, loss is 0.005059007555246353\n",
      "epoch: 8 step: 666, loss is 6.404246232705191e-05\n",
      "epoch: 8 step: 667, loss is 0.004174876492470503\n",
      "epoch: 8 step: 668, loss is 0.03472963348031044\n",
      "epoch: 8 step: 669, loss is 0.0010939316125586629\n",
      "epoch: 8 step: 670, loss is 0.0003148776013404131\n",
      "epoch: 8 step: 671, loss is 0.001456068828701973\n",
      "epoch: 8 step: 672, loss is 0.07172144949436188\n",
      "epoch: 8 step: 673, loss is 0.0012287390418350697\n",
      "epoch: 8 step: 674, loss is 0.018157638609409332\n",
      "epoch: 8 step: 675, loss is 0.010894359089434147\n",
      "epoch: 8 step: 676, loss is 0.001640200731344521\n",
      "epoch: 8 step: 677, loss is 0.025434494018554688\n",
      "epoch: 8 step: 678, loss is 0.00021668679255526513\n",
      "epoch: 8 step: 679, loss is 0.1046735867857933\n",
      "epoch: 8 step: 680, loss is 0.00693677319213748\n",
      "epoch: 8 step: 681, loss is 0.1816338449716568\n",
      "epoch: 8 step: 682, loss is 0.0033185044303536415\n",
      "epoch: 8 step: 683, loss is 0.0002847883151844144\n",
      "epoch: 8 step: 684, loss is 0.0013695616507902741\n",
      "epoch: 8 step: 685, loss is 0.06898669898509979\n",
      "epoch: 8 step: 686, loss is 0.03203793615102768\n",
      "epoch: 8 step: 687, loss is 0.20051857829093933\n",
      "epoch: 8 step: 688, loss is 0.00035761756589636207\n",
      "epoch: 8 step: 689, loss is 0.0002776639012154192\n",
      "epoch: 8 step: 690, loss is 0.0016184006817638874\n",
      "epoch: 8 step: 691, loss is 0.0018277856288477778\n",
      "epoch: 8 step: 692, loss is 0.003988613840192556\n",
      "epoch: 8 step: 693, loss is 0.004381817765533924\n",
      "epoch: 8 step: 694, loss is 0.0012173430295661092\n",
      "epoch: 8 step: 695, loss is 0.0035763161722570658\n",
      "epoch: 8 step: 696, loss is 0.0032642267178744078\n",
      "epoch: 8 step: 697, loss is 0.0001915627799462527\n",
      "epoch: 8 step: 698, loss is 0.0048425630666315556\n",
      "epoch: 8 step: 699, loss is 0.0532585084438324\n",
      "epoch: 8 step: 700, loss is 0.0032501257956027985\n",
      "epoch: 8 step: 701, loss is 0.0003365581505931914\n",
      "epoch: 8 step: 702, loss is 0.1604781150817871\n",
      "epoch: 8 step: 703, loss is 2.8742053473251872e-05\n",
      "epoch: 8 step: 704, loss is 0.01165885291993618\n",
      "epoch: 8 step: 705, loss is 0.00020019078510813415\n",
      "epoch: 8 step: 706, loss is 0.0004049331764690578\n",
      "epoch: 8 step: 707, loss is 0.00040196976624429226\n",
      "epoch: 8 step: 708, loss is 0.001413025427609682\n",
      "epoch: 8 step: 709, loss is 0.0048795598559081554\n",
      "epoch: 8 step: 710, loss is 0.0024585004430264235\n",
      "epoch: 8 step: 711, loss is 0.0008084595901891589\n",
      "epoch: 8 step: 712, loss is 0.0004542239475995302\n",
      "epoch: 8 step: 713, loss is 0.00022079760674387217\n",
      "epoch: 8 step: 714, loss is 0.005390720441937447\n",
      "epoch: 8 step: 715, loss is 0.050693802535533905\n",
      "epoch: 8 step: 716, loss is 0.00027069260249845684\n",
      "epoch: 8 step: 717, loss is 0.00025394681142643094\n",
      "epoch: 8 step: 718, loss is 0.004293343983590603\n",
      "epoch: 8 step: 719, loss is 0.016726944595575333\n",
      "epoch: 8 step: 720, loss is 0.008638041093945503\n",
      "epoch: 8 step: 721, loss is 0.004611629992723465\n",
      "epoch: 8 step: 722, loss is 0.014078449457883835\n",
      "epoch: 8 step: 723, loss is 0.0036829793825745583\n",
      "epoch: 8 step: 724, loss is 0.005274901166558266\n",
      "epoch: 8 step: 725, loss is 0.0005189779913052917\n",
      "epoch: 8 step: 726, loss is 0.0024500368162989616\n",
      "epoch: 8 step: 727, loss is 0.003876877250149846\n",
      "epoch: 8 step: 728, loss is 0.0008139189449138939\n",
      "epoch: 8 step: 729, loss is 0.003462828928604722\n",
      "epoch: 8 step: 730, loss is 0.0005201745079830289\n",
      "epoch: 8 step: 731, loss is 0.008408534340560436\n",
      "epoch: 8 step: 732, loss is 0.019342318177223206\n",
      "epoch: 8 step: 733, loss is 0.0011032692855224013\n",
      "epoch: 8 step: 734, loss is 7.711781654506922e-05\n",
      "epoch: 8 step: 735, loss is 0.02855062671005726\n",
      "epoch: 8 step: 736, loss is 0.0002623137552291155\n",
      "epoch: 8 step: 737, loss is 0.0009086196077987552\n",
      "epoch: 8 step: 738, loss is 0.001001683296635747\n",
      "epoch: 8 step: 739, loss is 0.0070803044363856316\n",
      "epoch: 8 step: 740, loss is 0.00011733804421965033\n",
      "epoch: 8 step: 741, loss is 0.02129012532532215\n",
      "epoch: 8 step: 742, loss is 0.011941423639655113\n",
      "epoch: 8 step: 743, loss is 0.0022647730074822903\n",
      "epoch: 8 step: 744, loss is 0.002290855161845684\n",
      "epoch: 8 step: 745, loss is 0.09645409137010574\n",
      "epoch: 8 step: 746, loss is 0.00023508987214881927\n",
      "epoch: 8 step: 747, loss is 0.00020415305334609002\n",
      "epoch: 8 step: 748, loss is 0.0010514103341847658\n",
      "epoch: 8 step: 749, loss is 0.00026340747717767954\n",
      "epoch: 8 step: 750, loss is 0.009397114627063274\n",
      "epoch: 8 step: 751, loss is 0.001064997399225831\n",
      "epoch: 8 step: 752, loss is 0.00020665144256781787\n",
      "epoch: 8 step: 753, loss is 0.0018626147648319602\n",
      "epoch: 8 step: 754, loss is 0.00953925121575594\n",
      "epoch: 8 step: 755, loss is 0.0019270197954028845\n",
      "epoch: 8 step: 756, loss is 9.000145655591041e-05\n",
      "epoch: 8 step: 757, loss is 0.02422761544585228\n",
      "epoch: 8 step: 758, loss is 0.24752096831798553\n",
      "epoch: 8 step: 759, loss is 0.0009103093179874122\n",
      "epoch: 8 step: 760, loss is 0.0002546778123360127\n",
      "epoch: 8 step: 761, loss is 0.00668594753369689\n",
      "epoch: 8 step: 762, loss is 0.00014970431220717728\n",
      "epoch: 8 step: 763, loss is 0.0011565153254196048\n",
      "epoch: 8 step: 764, loss is 0.005024453159421682\n",
      "epoch: 8 step: 765, loss is 2.622915417305194e-05\n",
      "epoch: 8 step: 766, loss is 0.013702396303415298\n",
      "epoch: 8 step: 767, loss is 0.002282906323671341\n",
      "epoch: 8 step: 768, loss is 0.024290436878800392\n",
      "epoch: 8 step: 769, loss is 0.023456871509552002\n",
      "epoch: 8 step: 770, loss is 2.9119635655661114e-05\n",
      "epoch: 8 step: 771, loss is 1.1983609510934912e-05\n",
      "epoch: 8 step: 772, loss is 0.00045010875328443944\n",
      "epoch: 8 step: 773, loss is 0.054042648524045944\n",
      "epoch: 8 step: 774, loss is 0.0001705280301393941\n",
      "epoch: 8 step: 775, loss is 0.0004214883374515921\n",
      "epoch: 8 step: 776, loss is 0.010082215070724487\n",
      "epoch: 8 step: 777, loss is 3.069380545639433e-05\n",
      "epoch: 8 step: 778, loss is 0.00010981291416101158\n",
      "epoch: 8 step: 779, loss is 0.004263532813638449\n",
      "epoch: 8 step: 780, loss is 0.028274372220039368\n",
      "epoch: 8 step: 781, loss is 0.002480735769495368\n",
      "epoch: 8 step: 782, loss is 0.21346376836299896\n",
      "epoch: 8 step: 783, loss is 0.03342258185148239\n",
      "epoch: 8 step: 784, loss is 3.6552668461808935e-05\n",
      "epoch: 8 step: 785, loss is 3.716464198078029e-05\n",
      "epoch: 8 step: 786, loss is 0.004738399293273687\n",
      "epoch: 8 step: 787, loss is 0.009964647702872753\n",
      "epoch: 8 step: 788, loss is 0.0017684530466794968\n",
      "epoch: 8 step: 789, loss is 0.0039046918973326683\n",
      "epoch: 8 step: 790, loss is 0.0020985817536711693\n",
      "epoch: 8 step: 791, loss is 0.0006273973267525434\n",
      "epoch: 8 step: 792, loss is 0.032630644738674164\n",
      "epoch: 8 step: 793, loss is 0.0015476010739803314\n",
      "epoch: 8 step: 794, loss is 2.997247975145001e-05\n",
      "epoch: 8 step: 795, loss is 0.00047059988719411194\n",
      "epoch: 8 step: 796, loss is 0.0001377331791445613\n",
      "epoch: 8 step: 797, loss is 0.00549857085570693\n",
      "epoch: 8 step: 798, loss is 0.003032413776963949\n",
      "epoch: 8 step: 799, loss is 0.009811007417738438\n",
      "epoch: 8 step: 800, loss is 0.010216430760920048\n",
      "epoch: 8 step: 801, loss is 0.00010020145418820903\n",
      "epoch: 8 step: 802, loss is 0.0005678404122591019\n",
      "epoch: 8 step: 803, loss is 0.0001709486241452396\n",
      "epoch: 8 step: 804, loss is 0.00014912172628100961\n",
      "epoch: 8 step: 805, loss is 0.0012349496828392148\n",
      "epoch: 8 step: 806, loss is 0.00247745169326663\n",
      "epoch: 8 step: 807, loss is 0.000401585188228637\n",
      "epoch: 8 step: 808, loss is 0.08288896828889847\n",
      "epoch: 8 step: 809, loss is 0.0005067763850092888\n",
      "epoch: 8 step: 810, loss is 0.012722041457891464\n",
      "epoch: 8 step: 811, loss is 5.6568871514173225e-05\n",
      "epoch: 8 step: 812, loss is 0.0010062078945338726\n",
      "epoch: 8 step: 813, loss is 0.00044960863306187093\n",
      "epoch: 8 step: 814, loss is 0.04516507685184479\n",
      "epoch: 8 step: 815, loss is 0.11589442938566208\n",
      "epoch: 8 step: 816, loss is 0.10902857780456543\n",
      "epoch: 8 step: 817, loss is 0.00545486668124795\n",
      "epoch: 8 step: 818, loss is 0.00013216269144322723\n",
      "epoch: 8 step: 819, loss is 0.03389912098646164\n",
      "epoch: 8 step: 820, loss is 0.0005225230706855655\n",
      "epoch: 8 step: 821, loss is 0.00033633134444244206\n",
      "epoch: 8 step: 822, loss is 0.27724096179008484\n",
      "epoch: 8 step: 823, loss is 0.004421379417181015\n",
      "epoch: 8 step: 824, loss is 0.018385589122772217\n",
      "epoch: 8 step: 825, loss is 0.0002051607152679935\n",
      "epoch: 8 step: 826, loss is 0.0019941097125411034\n",
      "epoch: 8 step: 827, loss is 0.0023111493792384863\n",
      "epoch: 8 step: 828, loss is 0.0007117674103938043\n",
      "epoch: 8 step: 829, loss is 0.015757309272885323\n",
      "epoch: 8 step: 830, loss is 0.0024356029462069273\n",
      "epoch: 8 step: 831, loss is 0.004969482775777578\n",
      "epoch: 8 step: 832, loss is 0.00026939675444737077\n",
      "epoch: 8 step: 833, loss is 0.022220876067876816\n",
      "epoch: 8 step: 834, loss is 0.02926797606050968\n",
      "epoch: 8 step: 835, loss is 0.03489813953638077\n",
      "epoch: 8 step: 836, loss is 0.00036888133035972714\n",
      "epoch: 8 step: 837, loss is 0.00040547747630625963\n",
      "epoch: 8 step: 838, loss is 0.005398893728852272\n",
      "epoch: 8 step: 839, loss is 0.0006928028305992484\n",
      "epoch: 8 step: 840, loss is 0.0006633688462898135\n",
      "epoch: 8 step: 841, loss is 0.007986556738615036\n",
      "epoch: 8 step: 842, loss is 0.020799163728952408\n",
      "epoch: 8 step: 843, loss is 0.01734655350446701\n",
      "epoch: 8 step: 844, loss is 0.008057007566094398\n",
      "epoch: 8 step: 845, loss is 0.08264106512069702\n",
      "epoch: 8 step: 846, loss is 0.0005243251216597855\n",
      "epoch: 8 step: 847, loss is 0.00014749789261259139\n",
      "epoch: 8 step: 848, loss is 0.00025443703634664416\n",
      "epoch: 8 step: 849, loss is 0.0022757898550480604\n",
      "epoch: 8 step: 850, loss is 0.00018340293900109828\n",
      "epoch: 8 step: 851, loss is 0.0006000102148391306\n",
      "epoch: 8 step: 852, loss is 0.025855904445052147\n",
      "epoch: 8 step: 853, loss is 0.004937899764627218\n",
      "epoch: 8 step: 854, loss is 0.0017129110638052225\n",
      "epoch: 8 step: 855, loss is 0.00127007148694247\n",
      "epoch: 8 step: 856, loss is 0.002697434974834323\n",
      "epoch: 8 step: 857, loss is 0.026556069031357765\n",
      "epoch: 8 step: 858, loss is 0.0008021321846172214\n",
      "epoch: 8 step: 859, loss is 0.08461634069681168\n",
      "epoch: 8 step: 860, loss is 0.05502268671989441\n",
      "epoch: 8 step: 861, loss is 0.0006163897924125195\n",
      "epoch: 8 step: 862, loss is 0.008344819769263268\n",
      "epoch: 8 step: 863, loss is 0.010797757655382156\n",
      "epoch: 8 step: 864, loss is 0.000412879599025473\n",
      "epoch: 8 step: 865, loss is 0.15295588970184326\n",
      "epoch: 8 step: 866, loss is 0.025266394019126892\n",
      "epoch: 8 step: 867, loss is 0.04847429320216179\n",
      "epoch: 8 step: 868, loss is 0.004312498494982719\n",
      "epoch: 8 step: 869, loss is 0.006769936066120863\n",
      "epoch: 8 step: 870, loss is 0.00408174516633153\n",
      "epoch: 8 step: 871, loss is 0.00045065340236760676\n",
      "epoch: 8 step: 872, loss is 0.007759151514619589\n",
      "epoch: 8 step: 873, loss is 0.003132488578557968\n",
      "epoch: 8 step: 874, loss is 0.00024695685715414584\n",
      "epoch: 8 step: 875, loss is 0.0026195687241852283\n",
      "epoch: 8 step: 876, loss is 0.024418573826551437\n",
      "epoch: 8 step: 877, loss is 0.00010143331019207835\n",
      "epoch: 8 step: 878, loss is 0.0003855204558931291\n",
      "epoch: 8 step: 879, loss is 0.0038702357560396194\n",
      "epoch: 8 step: 880, loss is 0.0006943594198673964\n",
      "epoch: 8 step: 881, loss is 0.0011559054255485535\n",
      "epoch: 8 step: 882, loss is 0.010986237786710262\n",
      "epoch: 8 step: 883, loss is 0.0007466948009096086\n",
      "epoch: 8 step: 884, loss is 0.05744602158665657\n",
      "epoch: 8 step: 885, loss is 0.008310227654874325\n",
      "epoch: 8 step: 886, loss is 0.0006306780269369483\n",
      "epoch: 8 step: 887, loss is 0.02145698480308056\n",
      "epoch: 8 step: 888, loss is 0.001848036190494895\n",
      "epoch: 8 step: 889, loss is 0.0014259733725339174\n",
      "epoch: 8 step: 890, loss is 0.04096817597746849\n",
      "epoch: 8 step: 891, loss is 5.428021904663183e-05\n",
      "epoch: 8 step: 892, loss is 3.5703655157703906e-05\n",
      "epoch: 8 step: 893, loss is 0.0016082730144262314\n",
      "epoch: 8 step: 894, loss is 0.0021681503858417273\n",
      "epoch: 8 step: 895, loss is 3.6040099075762555e-05\n",
      "epoch: 8 step: 896, loss is 0.00010863893112400547\n",
      "epoch: 8 step: 897, loss is 0.009887817315757275\n",
      "epoch: 8 step: 898, loss is 0.005794546566903591\n",
      "epoch: 8 step: 899, loss is 0.008376667276024818\n",
      "epoch: 8 step: 900, loss is 0.00019211658218409866\n",
      "epoch: 8 step: 901, loss is 0.023427635431289673\n",
      "epoch: 8 step: 902, loss is 0.00010273355292156339\n",
      "epoch: 8 step: 903, loss is 0.021288681775331497\n",
      "epoch: 8 step: 904, loss is 0.0008129039197228849\n",
      "epoch: 8 step: 905, loss is 0.0003752129850909114\n",
      "epoch: 8 step: 906, loss is 0.0005812296876683831\n",
      "epoch: 8 step: 907, loss is 0.00016077519103419036\n",
      "epoch: 8 step: 908, loss is 0.057457368820905685\n",
      "epoch: 8 step: 909, loss is 0.00049473816761747\n",
      "epoch: 8 step: 910, loss is 0.0003674293984659016\n",
      "epoch: 8 step: 911, loss is 0.00053312600357458\n",
      "epoch: 8 step: 912, loss is 0.06845322996377945\n",
      "epoch: 8 step: 913, loss is 0.002763282973319292\n",
      "epoch: 8 step: 914, loss is 0.0023056156933307648\n",
      "epoch: 8 step: 915, loss is 0.010286317206919193\n",
      "epoch: 8 step: 916, loss is 0.0030289418064057827\n",
      "epoch: 8 step: 917, loss is 0.012483736500144005\n",
      "epoch: 8 step: 918, loss is 0.15601949393749237\n",
      "epoch: 8 step: 919, loss is 7.359371375059709e-05\n",
      "epoch: 8 step: 920, loss is 0.04525422677397728\n",
      "epoch: 8 step: 921, loss is 6.665452383458614e-05\n",
      "epoch: 8 step: 922, loss is 0.00261310045607388\n",
      "epoch: 8 step: 923, loss is 0.002067243680357933\n",
      "epoch: 8 step: 924, loss is 0.011061742901802063\n",
      "epoch: 8 step: 925, loss is 0.009885545819997787\n",
      "epoch: 8 step: 926, loss is 0.012834501452744007\n",
      "epoch: 8 step: 927, loss is 0.05781007930636406\n",
      "epoch: 8 step: 928, loss is 0.0022437532898038626\n",
      "epoch: 8 step: 929, loss is 0.008607450872659683\n",
      "epoch: 8 step: 930, loss is 0.001202335231937468\n",
      "epoch: 8 step: 931, loss is 0.00016561166557949036\n",
      "epoch: 8 step: 932, loss is 0.00016717155813239515\n",
      "epoch: 8 step: 933, loss is 4.2227991798426956e-05\n",
      "epoch: 8 step: 934, loss is 0.08244800567626953\n",
      "epoch: 8 step: 935, loss is 0.0035743757616728544\n",
      "epoch: 8 step: 936, loss is 0.015338796190917492\n",
      "epoch: 8 step: 937, loss is 0.0002973760128952563\n",
      "epoch: 8 step: 938, loss is 0.008002686314284801\n",
      "epoch: 8 step: 939, loss is 0.00196103029884398\n",
      "epoch: 8 step: 940, loss is 0.06499337404966354\n",
      "epoch: 8 step: 941, loss is 0.07434528321027756\n",
      "epoch: 8 step: 942, loss is 0.1600532829761505\n",
      "epoch: 8 step: 943, loss is 6.107598164817318e-05\n",
      "epoch: 8 step: 944, loss is 0.08285568654537201\n",
      "epoch: 8 step: 945, loss is 0.04608289152383804\n",
      "epoch: 8 step: 946, loss is 0.0015257078921422362\n",
      "epoch: 8 step: 947, loss is 0.0026542118284851313\n",
      "epoch: 8 step: 948, loss is 0.00015430366329383105\n",
      "epoch: 8 step: 949, loss is 0.00022285632439889014\n",
      "epoch: 8 step: 950, loss is 0.001454541110433638\n",
      "epoch: 8 step: 951, loss is 0.009654726833105087\n",
      "epoch: 8 step: 952, loss is 0.013586420565843582\n",
      "epoch: 8 step: 953, loss is 0.0018176352605223656\n",
      "epoch: 8 step: 954, loss is 0.0020824407692998648\n",
      "epoch: 8 step: 955, loss is 0.007746358402073383\n",
      "epoch: 8 step: 956, loss is 0.006927174981683493\n",
      "epoch: 8 step: 957, loss is 0.0030949667561799288\n",
      "epoch: 8 step: 958, loss is 0.09449918568134308\n",
      "epoch: 8 step: 959, loss is 0.0002843315014615655\n",
      "epoch: 8 step: 960, loss is 0.0007136150379665196\n",
      "epoch: 8 step: 961, loss is 0.005984529387205839\n",
      "epoch: 8 step: 962, loss is 0.003968165721744299\n",
      "epoch: 8 step: 963, loss is 0.0005876350332982838\n",
      "epoch: 8 step: 964, loss is 0.003151657758280635\n",
      "epoch: 8 step: 965, loss is 4.9558686441741884e-05\n",
      "epoch: 8 step: 966, loss is 0.004080767743289471\n",
      "epoch: 8 step: 967, loss is 0.0009129321551881731\n",
      "epoch: 8 step: 968, loss is 0.036627963185310364\n",
      "epoch: 8 step: 969, loss is 0.00033708009868860245\n",
      "epoch: 8 step: 970, loss is 0.02061774954199791\n",
      "epoch: 8 step: 971, loss is 0.24034345149993896\n",
      "epoch: 8 step: 972, loss is 0.0011678412556648254\n",
      "epoch: 8 step: 973, loss is 0.00010516529437154531\n",
      "epoch: 8 step: 974, loss is 0.00033146023633889854\n",
      "epoch: 8 step: 975, loss is 0.047651082277297974\n",
      "epoch: 8 step: 976, loss is 0.004391510039567947\n",
      "epoch: 8 step: 977, loss is 0.2233434021472931\n",
      "epoch: 8 step: 978, loss is 0.001612930209375918\n",
      "epoch: 8 step: 979, loss is 0.00015209335833787918\n",
      "epoch: 8 step: 980, loss is 0.00012577962479554117\n",
      "epoch: 8 step: 981, loss is 0.001293647801503539\n",
      "epoch: 8 step: 982, loss is 0.00025205695419572294\n",
      "epoch: 8 step: 983, loss is 0.0002694590948522091\n",
      "epoch: 8 step: 984, loss is 0.0804997906088829\n",
      "epoch: 8 step: 985, loss is 0.1835387498140335\n",
      "epoch: 8 step: 986, loss is 0.005505899898707867\n",
      "epoch: 8 step: 987, loss is 0.003031414235010743\n",
      "epoch: 8 step: 988, loss is 0.10461536049842834\n",
      "epoch: 8 step: 989, loss is 0.01839660294353962\n",
      "epoch: 8 step: 990, loss is 0.014697364531457424\n",
      "epoch: 8 step: 991, loss is 0.0033688030671328306\n",
      "epoch: 8 step: 992, loss is 0.0003675684565678239\n",
      "epoch: 8 step: 993, loss is 0.028769476339221\n",
      "epoch: 8 step: 994, loss is 0.02418692782521248\n",
      "epoch: 8 step: 995, loss is 0.14810827374458313\n",
      "epoch: 8 step: 996, loss is 0.01544931996613741\n",
      "epoch: 8 step: 997, loss is 0.0008341131615452468\n",
      "epoch: 8 step: 998, loss is 0.11439881473779678\n",
      "epoch: 8 step: 999, loss is 0.00832099374383688\n",
      "epoch: 8 step: 1000, loss is 0.004556847270578146\n",
      "epoch: 8 step: 1001, loss is 0.1778789907693863\n",
      "epoch: 8 step: 1002, loss is 0.0004415649746078998\n",
      "epoch: 8 step: 1003, loss is 0.00017776858294382691\n",
      "epoch: 8 step: 1004, loss is 0.011222907342016697\n",
      "epoch: 8 step: 1005, loss is 0.010426880791783333\n",
      "epoch: 8 step: 1006, loss is 0.0023566754534840584\n",
      "epoch: 8 step: 1007, loss is 0.03687051311135292\n",
      "epoch: 8 step: 1008, loss is 0.057133834809064865\n",
      "epoch: 8 step: 1009, loss is 0.00015370875189546496\n",
      "epoch: 8 step: 1010, loss is 0.004289384000003338\n",
      "epoch: 8 step: 1011, loss is 0.027675766497850418\n",
      "epoch: 8 step: 1012, loss is 0.004293292295187712\n",
      "epoch: 8 step: 1013, loss is 0.00201869523152709\n",
      "epoch: 8 step: 1014, loss is 0.0015969532541930676\n",
      "epoch: 8 step: 1015, loss is 0.0014190495712682605\n",
      "epoch: 8 step: 1016, loss is 0.0008569392957724631\n",
      "epoch: 8 step: 1017, loss is 0.0022980342619121075\n",
      "epoch: 8 step: 1018, loss is 0.0011902268743142486\n",
      "epoch: 8 step: 1019, loss is 0.001721930573694408\n",
      "epoch: 8 step: 1020, loss is 0.0016744264867156744\n",
      "epoch: 8 step: 1021, loss is 0.02022506482899189\n",
      "epoch: 8 step: 1022, loss is 0.08117712289094925\n",
      "epoch: 8 step: 1023, loss is 0.0033483027946203947\n",
      "epoch: 8 step: 1024, loss is 0.004430334083735943\n",
      "epoch: 8 step: 1025, loss is 0.006783852819353342\n",
      "epoch: 8 step: 1026, loss is 0.00302328378893435\n",
      "epoch: 8 step: 1027, loss is 0.0001696375838946551\n",
      "epoch: 8 step: 1028, loss is 0.02028978429734707\n",
      "epoch: 8 step: 1029, loss is 0.0021958774887025356\n",
      "epoch: 8 step: 1030, loss is 0.0032709171064198017\n",
      "epoch: 8 step: 1031, loss is 0.014341307803988457\n",
      "epoch: 8 step: 1032, loss is 0.0011874628253281116\n",
      "epoch: 8 step: 1033, loss is 0.000826313451398164\n",
      "epoch: 8 step: 1034, loss is 0.043585896492004395\n",
      "epoch: 8 step: 1035, loss is 0.004588061943650246\n",
      "epoch: 8 step: 1036, loss is 0.0034638303332030773\n",
      "epoch: 8 step: 1037, loss is 0.0019845927599817514\n",
      "epoch: 8 step: 1038, loss is 0.027888696640729904\n",
      "epoch: 8 step: 1039, loss is 0.005659543909132481\n",
      "epoch: 8 step: 1040, loss is 0.00041989944293163717\n",
      "epoch: 8 step: 1041, loss is 0.017407001927495003\n",
      "epoch: 8 step: 1042, loss is 0.036984071135520935\n",
      "epoch: 8 step: 1043, loss is 0.005934840068221092\n",
      "epoch: 8 step: 1044, loss is 0.0021692938171327114\n",
      "epoch: 8 step: 1045, loss is 0.0010649278992787004\n",
      "epoch: 8 step: 1046, loss is 0.00022865491337142885\n",
      "epoch: 8 step: 1047, loss is 0.008382929489016533\n",
      "epoch: 8 step: 1048, loss is 0.00011453657498350367\n",
      "epoch: 8 step: 1049, loss is 0.0018912620143964887\n",
      "epoch: 8 step: 1050, loss is 0.010122066363692284\n",
      "epoch: 8 step: 1051, loss is 6.838810804765671e-05\n",
      "epoch: 8 step: 1052, loss is 0.0006808117614127696\n",
      "epoch: 8 step: 1053, loss is 0.017461173236370087\n",
      "epoch: 8 step: 1054, loss is 0.0007153807091526687\n",
      "epoch: 8 step: 1055, loss is 0.0037636165507137775\n",
      "epoch: 8 step: 1056, loss is 0.019800331443548203\n",
      "epoch: 8 step: 1057, loss is 1.885311576188542e-05\n",
      "epoch: 8 step: 1058, loss is 0.0012510564411059022\n",
      "epoch: 8 step: 1059, loss is 0.0059817577712237835\n",
      "epoch: 8 step: 1060, loss is 0.0010552388848736882\n",
      "epoch: 8 step: 1061, loss is 0.03699703514575958\n",
      "epoch: 8 step: 1062, loss is 0.005260437726974487\n",
      "epoch: 8 step: 1063, loss is 0.15589402616024017\n",
      "epoch: 8 step: 1064, loss is 0.0012379145482555032\n",
      "epoch: 8 step: 1065, loss is 0.004244301002472639\n",
      "epoch: 8 step: 1066, loss is 0.02552860789000988\n",
      "epoch: 8 step: 1067, loss is 0.03395755961537361\n",
      "epoch: 8 step: 1068, loss is 0.001583254779689014\n",
      "epoch: 8 step: 1069, loss is 0.26049280166625977\n",
      "epoch: 8 step: 1070, loss is 0.0074334521777927876\n",
      "epoch: 8 step: 1071, loss is 5.175238402443938e-05\n",
      "epoch: 8 step: 1072, loss is 0.00032483102404512465\n",
      "epoch: 8 step: 1073, loss is 0.018858643248677254\n",
      "epoch: 8 step: 1074, loss is 0.00042484261211939156\n",
      "epoch: 8 step: 1075, loss is 0.04128843545913696\n",
      "epoch: 8 step: 1076, loss is 0.015531756915152073\n",
      "epoch: 8 step: 1077, loss is 0.0008331256685778499\n",
      "epoch: 8 step: 1078, loss is 0.1397506296634674\n",
      "epoch: 8 step: 1079, loss is 0.0027963987085968256\n",
      "epoch: 8 step: 1080, loss is 0.113735131919384\n",
      "epoch: 8 step: 1081, loss is 0.0007573487237095833\n",
      "epoch: 8 step: 1082, loss is 0.00740359490737319\n",
      "epoch: 8 step: 1083, loss is 0.0010158508084714413\n",
      "epoch: 8 step: 1084, loss is 0.0017802546499297023\n",
      "epoch: 8 step: 1085, loss is 0.005669475067406893\n",
      "epoch: 8 step: 1086, loss is 0.0025309124030172825\n",
      "epoch: 8 step: 1087, loss is 0.025307560339570045\n",
      "epoch: 8 step: 1088, loss is 0.008926196955144405\n",
      "epoch: 8 step: 1089, loss is 0.0025095243472605944\n",
      "epoch: 8 step: 1090, loss is 0.0005942736752331257\n",
      "epoch: 8 step: 1091, loss is 0.04706234112381935\n",
      "epoch: 8 step: 1092, loss is 0.003622903721407056\n",
      "epoch: 8 step: 1093, loss is 0.0010728499619290233\n",
      "epoch: 8 step: 1094, loss is 0.04846035689115524\n",
      "epoch: 8 step: 1095, loss is 0.0020198484417051077\n",
      "epoch: 8 step: 1096, loss is 0.0005090574850328267\n",
      "epoch: 8 step: 1097, loss is 0.008013395592570305\n",
      "epoch: 8 step: 1098, loss is 0.12471956759691238\n",
      "epoch: 8 step: 1099, loss is 0.1328812837600708\n",
      "epoch: 8 step: 1100, loss is 0.07265748828649521\n",
      "epoch: 8 step: 1101, loss is 0.0047118570655584335\n",
      "epoch: 8 step: 1102, loss is 0.0021375473588705063\n",
      "epoch: 8 step: 1103, loss is 0.0021655482705682516\n",
      "epoch: 8 step: 1104, loss is 0.0010598982917144895\n",
      "epoch: 8 step: 1105, loss is 0.00013499311171472073\n",
      "epoch: 8 step: 1106, loss is 0.003744359128177166\n",
      "epoch: 8 step: 1107, loss is 0.0050402176566421986\n",
      "epoch: 8 step: 1108, loss is 0.0026491121388971806\n",
      "epoch: 8 step: 1109, loss is 0.009845851920545101\n",
      "epoch: 8 step: 1110, loss is 0.20502398908138275\n",
      "epoch: 8 step: 1111, loss is 0.035731613636016846\n",
      "epoch: 8 step: 1112, loss is 0.006843038834631443\n",
      "epoch: 8 step: 1113, loss is 0.005903348326683044\n",
      "epoch: 8 step: 1114, loss is 0.0007081719813868403\n",
      "epoch: 8 step: 1115, loss is 4.1418672481086105e-05\n",
      "epoch: 8 step: 1116, loss is 0.11893957853317261\n",
      "epoch: 8 step: 1117, loss is 0.17062583565711975\n",
      "epoch: 8 step: 1118, loss is 0.00039256527088582516\n",
      "epoch: 8 step: 1119, loss is 0.00032908248249441385\n",
      "epoch: 8 step: 1120, loss is 0.029437974095344543\n",
      "epoch: 8 step: 1121, loss is 9.411647624801844e-05\n",
      "epoch: 8 step: 1122, loss is 0.0970807671546936\n",
      "epoch: 8 step: 1123, loss is 0.003380104433745146\n",
      "epoch: 8 step: 1124, loss is 0.01618097722530365\n",
      "epoch: 8 step: 1125, loss is 0.016975589096546173\n",
      "epoch: 8 step: 1126, loss is 0.027467483654618263\n",
      "epoch: 8 step: 1127, loss is 0.010205215774476528\n",
      "epoch: 8 step: 1128, loss is 0.0028508028481155634\n",
      "epoch: 8 step: 1129, loss is 0.0241837278008461\n",
      "epoch: 8 step: 1130, loss is 0.0014997731195762753\n",
      "epoch: 8 step: 1131, loss is 0.0016622187104076147\n",
      "epoch: 8 step: 1132, loss is 0.029432374984025955\n",
      "epoch: 8 step: 1133, loss is 0.0004919820348732173\n",
      "epoch: 8 step: 1134, loss is 4.5169923396315426e-05\n",
      "epoch: 8 step: 1135, loss is 0.004970425274223089\n",
      "epoch: 8 step: 1136, loss is 0.0013830596581101418\n",
      "epoch: 8 step: 1137, loss is 0.00725925900042057\n",
      "epoch: 8 step: 1138, loss is 0.0002732227148953825\n",
      "epoch: 8 step: 1139, loss is 0.03178706765174866\n",
      "epoch: 8 step: 1140, loss is 0.00017559222760610282\n",
      "epoch: 8 step: 1141, loss is 0.010412929579615593\n",
      "epoch: 8 step: 1142, loss is 0.0006221833173185587\n",
      "epoch: 8 step: 1143, loss is 0.00044115600758232176\n",
      "epoch: 8 step: 1144, loss is 0.20483022928237915\n",
      "epoch: 8 step: 1145, loss is 0.0010082641383633018\n",
      "epoch: 8 step: 1146, loss is 0.06383156776428223\n",
      "epoch: 8 step: 1147, loss is 0.0009613967267796397\n",
      "epoch: 8 step: 1148, loss is 0.0660429298877716\n",
      "epoch: 8 step: 1149, loss is 0.002508946228772402\n",
      "epoch: 8 step: 1150, loss is 0.00031724749715067446\n",
      "epoch: 8 step: 1151, loss is 0.0033783460967242718\n",
      "epoch: 8 step: 1152, loss is 0.0010694751981645823\n",
      "epoch: 8 step: 1153, loss is 0.0021110461093485355\n",
      "epoch: 8 step: 1154, loss is 0.007074754685163498\n",
      "epoch: 8 step: 1155, loss is 0.0008479937678202987\n",
      "epoch: 8 step: 1156, loss is 0.0002969218185171485\n",
      "epoch: 8 step: 1157, loss is 0.00117593037430197\n",
      "epoch: 8 step: 1158, loss is 0.019413596019148827\n",
      "epoch: 8 step: 1159, loss is 0.00022788993373978883\n",
      "epoch: 8 step: 1160, loss is 6.208426202647388e-05\n",
      "epoch: 8 step: 1161, loss is 0.004377889912575483\n",
      "epoch: 8 step: 1162, loss is 3.571102206478827e-05\n",
      "epoch: 8 step: 1163, loss is 8.269734826171771e-05\n",
      "epoch: 8 step: 1164, loss is 0.0006488732760772109\n",
      "epoch: 8 step: 1165, loss is 0.0001013372020679526\n",
      "epoch: 8 step: 1166, loss is 0.3456767797470093\n",
      "epoch: 8 step: 1167, loss is 0.015579089522361755\n",
      "epoch: 8 step: 1168, loss is 0.001969396835193038\n",
      "epoch: 8 step: 1169, loss is 0.00018280092626810074\n",
      "epoch: 8 step: 1170, loss is 0.0016485126689076424\n",
      "epoch: 8 step: 1171, loss is 0.0003388507175259292\n",
      "epoch: 8 step: 1172, loss is 0.002275450387969613\n",
      "epoch: 8 step: 1173, loss is 0.15911367535591125\n",
      "epoch: 8 step: 1174, loss is 0.056056082248687744\n",
      "epoch: 8 step: 1175, loss is 0.0007513739401474595\n",
      "epoch: 8 step: 1176, loss is 9.19664598768577e-05\n",
      "epoch: 8 step: 1177, loss is 0.12459775060415268\n",
      "epoch: 8 step: 1178, loss is 0.00010506067337701097\n",
      "epoch: 8 step: 1179, loss is 0.009532332420349121\n",
      "epoch: 8 step: 1180, loss is 0.0030273909214884043\n",
      "epoch: 8 step: 1181, loss is 0.0028462621849030256\n",
      "epoch: 8 step: 1182, loss is 0.00017516646767035127\n",
      "epoch: 8 step: 1183, loss is 0.03968100622296333\n",
      "epoch: 8 step: 1184, loss is 0.00830816850066185\n",
      "epoch: 8 step: 1185, loss is 0.001465367735363543\n",
      "epoch: 8 step: 1186, loss is 0.010207048617303371\n",
      "epoch: 8 step: 1187, loss is 0.00023520499235019088\n",
      "epoch: 8 step: 1188, loss is 0.21141450107097626\n",
      "epoch: 8 step: 1189, loss is 0.0017316766316071153\n",
      "epoch: 8 step: 1190, loss is 0.019150711596012115\n",
      "epoch: 8 step: 1191, loss is 0.03286108374595642\n",
      "epoch: 8 step: 1192, loss is 0.0029679948929697275\n",
      "epoch: 8 step: 1193, loss is 0.0014723638305440545\n",
      "epoch: 8 step: 1194, loss is 0.0005217679427005351\n",
      "epoch: 8 step: 1195, loss is 0.0013168558944016695\n",
      "epoch: 8 step: 1196, loss is 0.002032478107139468\n",
      "epoch: 8 step: 1197, loss is 0.004436796996742487\n",
      "epoch: 8 step: 1198, loss is 0.008828709833323956\n",
      "epoch: 8 step: 1199, loss is 0.002203955315053463\n",
      "epoch: 8 step: 1200, loss is 0.0023107421584427357\n",
      "epoch: 8 step: 1201, loss is 0.00040471769170835614\n",
      "epoch: 8 step: 1202, loss is 0.01183913741260767\n",
      "epoch: 8 step: 1203, loss is 0.0018932631937786937\n",
      "epoch: 8 step: 1204, loss is 0.0016624832060188055\n",
      "epoch: 8 step: 1205, loss is 0.006309741176664829\n",
      "epoch: 8 step: 1206, loss is 0.006054614670574665\n",
      "epoch: 8 step: 1207, loss is 4.689707566285506e-05\n",
      "epoch: 8 step: 1208, loss is 0.001794768963009119\n",
      "epoch: 8 step: 1209, loss is 4.286280818632804e-05\n",
      "epoch: 8 step: 1210, loss is 0.0002655592979863286\n",
      "epoch: 8 step: 1211, loss is 0.003842053236439824\n",
      "epoch: 8 step: 1212, loss is 0.002786586992442608\n",
      "epoch: 8 step: 1213, loss is 0.024480454623699188\n",
      "epoch: 8 step: 1214, loss is 0.01443394273519516\n",
      "epoch: 8 step: 1215, loss is 0.07554646581411362\n",
      "epoch: 8 step: 1216, loss is 0.0005407279240898788\n",
      "epoch: 8 step: 1217, loss is 0.0012497968273237348\n",
      "epoch: 8 step: 1218, loss is 0.0008859348017722368\n",
      "epoch: 8 step: 1219, loss is 0.12083065509796143\n",
      "epoch: 8 step: 1220, loss is 0.00015966870705597103\n",
      "epoch: 8 step: 1221, loss is 4.002941932412796e-05\n",
      "epoch: 8 step: 1222, loss is 0.049893543124198914\n",
      "epoch: 8 step: 1223, loss is 0.00020561213023029268\n",
      "epoch: 8 step: 1224, loss is 0.03930637612938881\n",
      "epoch: 8 step: 1225, loss is 0.0004921967629343271\n",
      "epoch: 8 step: 1226, loss is 0.0005034858477301896\n",
      "epoch: 8 step: 1227, loss is 0.0026302810292690992\n",
      "epoch: 8 step: 1228, loss is 0.00961056724190712\n",
      "epoch: 8 step: 1229, loss is 0.0004016578895971179\n",
      "epoch: 8 step: 1230, loss is 0.03525157645344734\n",
      "epoch: 8 step: 1231, loss is 0.0016451372066512704\n",
      "epoch: 8 step: 1232, loss is 0.08032896369695663\n",
      "epoch: 8 step: 1233, loss is 0.004581233486533165\n",
      "epoch: 8 step: 1234, loss is 0.018114281818270683\n",
      "epoch: 8 step: 1235, loss is 0.0031285476870834827\n",
      "epoch: 8 step: 1236, loss is 0.000985120888799429\n",
      "epoch: 8 step: 1237, loss is 0.18201439082622528\n",
      "epoch: 8 step: 1238, loss is 0.003794704331085086\n",
      "epoch: 8 step: 1239, loss is 0.006045954301953316\n",
      "epoch: 8 step: 1240, loss is 0.0011501533444970846\n",
      "epoch: 8 step: 1241, loss is 0.0038354303687810898\n",
      "epoch: 8 step: 1242, loss is 0.14649136364459991\n",
      "epoch: 8 step: 1243, loss is 8.928903844207525e-05\n",
      "epoch: 8 step: 1244, loss is 0.007648420054465532\n",
      "epoch: 8 step: 1245, loss is 0.003559584729373455\n",
      "epoch: 8 step: 1246, loss is 0.0003601877542678267\n",
      "epoch: 8 step: 1247, loss is 0.003977019339799881\n",
      "epoch: 8 step: 1248, loss is 0.01418250985443592\n",
      "epoch: 8 step: 1249, loss is 0.15756623446941376\n",
      "epoch: 8 step: 1250, loss is 0.0003303520497865975\n",
      "epoch: 8 step: 1251, loss is 0.010070300661027431\n",
      "epoch: 8 step: 1252, loss is 0.0015788486925885081\n",
      "epoch: 8 step: 1253, loss is 0.0023318384774029255\n",
      "epoch: 8 step: 1254, loss is 0.0007852237904444337\n",
      "epoch: 8 step: 1255, loss is 0.001556849223561585\n",
      "epoch: 8 step: 1256, loss is 0.0004748917417600751\n",
      "epoch: 8 step: 1257, loss is 0.10278283059597015\n",
      "epoch: 8 step: 1258, loss is 0.0024506200570613146\n",
      "epoch: 8 step: 1259, loss is 0.0013183035189285874\n",
      "epoch: 8 step: 1260, loss is 0.007434532977640629\n",
      "epoch: 8 step: 1261, loss is 0.030795447528362274\n",
      "epoch: 8 step: 1262, loss is 0.004598094616085291\n",
      "epoch: 8 step: 1263, loss is 0.0019360646838322282\n",
      "epoch: 8 step: 1264, loss is 0.005175628233700991\n",
      "epoch: 8 step: 1265, loss is 0.22299030423164368\n",
      "epoch: 8 step: 1266, loss is 0.007189362775534391\n",
      "epoch: 8 step: 1267, loss is 0.0006685394328087568\n",
      "epoch: 8 step: 1268, loss is 0.03571562096476555\n",
      "epoch: 8 step: 1269, loss is 0.0015986848156899214\n",
      "epoch: 8 step: 1270, loss is 0.0009479018626734614\n",
      "epoch: 8 step: 1271, loss is 0.010605573654174805\n",
      "epoch: 8 step: 1272, loss is 0.00955244991928339\n",
      "epoch: 8 step: 1273, loss is 0.005290638189762831\n",
      "epoch: 8 step: 1274, loss is 0.00821410957723856\n",
      "epoch: 8 step: 1275, loss is 0.04898065701127052\n",
      "epoch: 8 step: 1276, loss is 0.01425511296838522\n",
      "epoch: 8 step: 1277, loss is 0.0082260612398386\n",
      "epoch: 8 step: 1278, loss is 0.0028962467331439257\n",
      "epoch: 8 step: 1279, loss is 0.09213815629482269\n",
      "epoch: 8 step: 1280, loss is 0.0006172481807880104\n",
      "epoch: 8 step: 1281, loss is 0.006107668858021498\n",
      "epoch: 8 step: 1282, loss is 0.0008976344834081829\n",
      "epoch: 8 step: 1283, loss is 0.0870138481259346\n",
      "epoch: 8 step: 1284, loss is 0.03575330600142479\n",
      "epoch: 8 step: 1285, loss is 0.07221606373786926\n",
      "epoch: 8 step: 1286, loss is 0.0005500228144228458\n",
      "epoch: 8 step: 1287, loss is 0.00018691855075303465\n",
      "epoch: 8 step: 1288, loss is 0.004837460350245237\n",
      "epoch: 8 step: 1289, loss is 0.16907310485839844\n",
      "epoch: 8 step: 1290, loss is 0.005260178819298744\n",
      "epoch: 8 step: 1291, loss is 0.06434237211942673\n",
      "epoch: 8 step: 1292, loss is 0.00041103007970377803\n",
      "epoch: 8 step: 1293, loss is 0.026108326390385628\n",
      "epoch: 8 step: 1294, loss is 0.006581211928278208\n",
      "epoch: 8 step: 1295, loss is 0.01807415671646595\n",
      "epoch: 8 step: 1296, loss is 0.0002225790813099593\n",
      "epoch: 8 step: 1297, loss is 0.0010069874115288258\n",
      "epoch: 8 step: 1298, loss is 0.004494465421885252\n",
      "epoch: 8 step: 1299, loss is 0.08052939176559448\n",
      "epoch: 8 step: 1300, loss is 0.028247665613889694\n",
      "epoch: 8 step: 1301, loss is 0.004399391356855631\n",
      "epoch: 8 step: 1302, loss is 0.0007762889727018774\n",
      "epoch: 8 step: 1303, loss is 0.018805811181664467\n",
      "epoch: 8 step: 1304, loss is 0.0059823524206876755\n",
      "epoch: 8 step: 1305, loss is 0.00047921150689944625\n",
      "epoch: 8 step: 1306, loss is 0.009941507130861282\n",
      "epoch: 8 step: 1307, loss is 0.02432492934167385\n",
      "epoch: 8 step: 1308, loss is 0.016810346394777298\n",
      "epoch: 8 step: 1309, loss is 0.006810779217630625\n",
      "epoch: 8 step: 1310, loss is 0.00012903158494737\n",
      "epoch: 8 step: 1311, loss is 0.0001487038389313966\n",
      "epoch: 8 step: 1312, loss is 0.0006283954717218876\n",
      "epoch: 8 step: 1313, loss is 0.0039537521079182625\n",
      "epoch: 8 step: 1314, loss is 0.005951589904725552\n",
      "epoch: 8 step: 1315, loss is 0.0001759432489052415\n",
      "epoch: 8 step: 1316, loss is 0.0003704617265611887\n",
      "epoch: 8 step: 1317, loss is 0.004337075632065535\n",
      "epoch: 8 step: 1318, loss is 0.004118747543543577\n",
      "epoch: 8 step: 1319, loss is 0.21723297238349915\n",
      "epoch: 8 step: 1320, loss is 0.003406324191018939\n",
      "epoch: 8 step: 1321, loss is 0.0010877393651753664\n",
      "epoch: 8 step: 1322, loss is 0.0005295047303661704\n",
      "epoch: 8 step: 1323, loss is 0.00012420571874827147\n",
      "epoch: 8 step: 1324, loss is 0.0001119339358410798\n",
      "epoch: 8 step: 1325, loss is 0.08181733638048172\n",
      "epoch: 8 step: 1326, loss is 0.014811030589044094\n",
      "epoch: 8 step: 1327, loss is 1.0993681826221291e-05\n",
      "epoch: 8 step: 1328, loss is 0.0041895052418112755\n",
      "epoch: 8 step: 1329, loss is 0.01843126118183136\n",
      "epoch: 8 step: 1330, loss is 9.945455531124026e-05\n",
      "epoch: 8 step: 1331, loss is 0.0002249381795991212\n",
      "epoch: 8 step: 1332, loss is 0.0014120180858299136\n",
      "epoch: 8 step: 1333, loss is 0.0017130376072600484\n",
      "epoch: 8 step: 1334, loss is 0.00019129019347019494\n",
      "epoch: 8 step: 1335, loss is 0.001535092364065349\n",
      "epoch: 8 step: 1336, loss is 0.0008328062249347568\n",
      "epoch: 8 step: 1337, loss is 0.025293882936239243\n",
      "epoch: 8 step: 1338, loss is 0.0011693632695823908\n",
      "epoch: 8 step: 1339, loss is 0.005800020415335894\n",
      "epoch: 8 step: 1340, loss is 0.20767730474472046\n",
      "epoch: 8 step: 1341, loss is 0.05473807081580162\n",
      "epoch: 8 step: 1342, loss is 0.0002856847713701427\n",
      "epoch: 8 step: 1343, loss is 0.00024211624986492097\n",
      "epoch: 8 step: 1344, loss is 0.016342759132385254\n",
      "epoch: 8 step: 1345, loss is 0.0005786239635199308\n",
      "epoch: 8 step: 1346, loss is 1.5398396499222144e-05\n",
      "epoch: 8 step: 1347, loss is 0.0013733403757214546\n",
      "epoch: 8 step: 1348, loss is 0.00035179860424250364\n",
      "epoch: 8 step: 1349, loss is 0.0009513586410321295\n",
      "epoch: 8 step: 1350, loss is 0.009412366896867752\n",
      "epoch: 8 step: 1351, loss is 0.003634725697338581\n",
      "epoch: 8 step: 1352, loss is 0.0043652476742863655\n",
      "epoch: 8 step: 1353, loss is 0.00018294446635991335\n",
      "epoch: 8 step: 1354, loss is 0.0002232337574241683\n",
      "epoch: 8 step: 1355, loss is 0.01805715262889862\n",
      "epoch: 8 step: 1356, loss is 0.03947841748595238\n",
      "epoch: 8 step: 1357, loss is 0.005105048883706331\n",
      "epoch: 8 step: 1358, loss is 0.09909102320671082\n",
      "epoch: 8 step: 1359, loss is 0.0003353436477482319\n",
      "epoch: 8 step: 1360, loss is 0.00029290362726897\n",
      "epoch: 8 step: 1361, loss is 0.0006777804810553789\n",
      "epoch: 8 step: 1362, loss is 0.002112992573529482\n",
      "epoch: 8 step: 1363, loss is 0.0009933393448591232\n",
      "epoch: 8 step: 1364, loss is 0.10493475198745728\n",
      "epoch: 8 step: 1365, loss is 0.008218875154852867\n",
      "epoch: 8 step: 1366, loss is 0.0504554808139801\n",
      "epoch: 8 step: 1367, loss is 0.014332905411720276\n",
      "epoch: 8 step: 1368, loss is 2.3557227905257605e-05\n",
      "epoch: 8 step: 1369, loss is 0.09179123491048813\n",
      "epoch: 8 step: 1370, loss is 0.011479543522000313\n",
      "epoch: 8 step: 1371, loss is 0.0033370491582900286\n",
      "epoch: 8 step: 1372, loss is 0.004711676388978958\n",
      "epoch: 8 step: 1373, loss is 7.31030959286727e-05\n",
      "epoch: 8 step: 1374, loss is 0.035785213112831116\n",
      "epoch: 8 step: 1375, loss is 0.1163024976849556\n",
      "epoch: 8 step: 1376, loss is 0.011567403562366962\n",
      "epoch: 8 step: 1377, loss is 0.07698038965463638\n",
      "epoch: 8 step: 1378, loss is 0.02477448433637619\n",
      "epoch: 8 step: 1379, loss is 0.0018282034434378147\n",
      "epoch: 8 step: 1380, loss is 0.0008967750472947955\n",
      "epoch: 8 step: 1381, loss is 5.869465530849993e-05\n",
      "epoch: 8 step: 1382, loss is 6.633956945734099e-05\n",
      "epoch: 8 step: 1383, loss is 3.1519080948783085e-05\n",
      "epoch: 8 step: 1384, loss is 0.0036513395607471466\n",
      "epoch: 8 step: 1385, loss is 0.29478615522384644\n",
      "epoch: 8 step: 1386, loss is 0.00011528302275110036\n",
      "epoch: 8 step: 1387, loss is 0.0023745696526020765\n",
      "epoch: 8 step: 1388, loss is 0.00023053769837133586\n",
      "epoch: 8 step: 1389, loss is 0.04127705097198486\n",
      "epoch: 8 step: 1390, loss is 0.017967820167541504\n",
      "epoch: 8 step: 1391, loss is 0.0007458360632881522\n",
      "epoch: 8 step: 1392, loss is 0.008409368805587292\n",
      "epoch: 8 step: 1393, loss is 0.001430497388355434\n",
      "epoch: 8 step: 1394, loss is 0.02996455505490303\n",
      "epoch: 8 step: 1395, loss is 0.010371577925980091\n",
      "epoch: 8 step: 1396, loss is 0.0034393263049423695\n",
      "epoch: 8 step: 1397, loss is 0.00038599257823079824\n",
      "epoch: 8 step: 1398, loss is 0.14966943860054016\n",
      "epoch: 8 step: 1399, loss is 0.0036745734978467226\n",
      "epoch: 8 step: 1400, loss is 0.00977057684212923\n",
      "epoch: 8 step: 1401, loss is 0.00251420633867383\n",
      "epoch: 8 step: 1402, loss is 0.026939215138554573\n",
      "epoch: 8 step: 1403, loss is 0.00011483018170110881\n",
      "epoch: 8 step: 1404, loss is 0.014386073686182499\n",
      "epoch: 8 step: 1405, loss is 0.025521013885736465\n",
      "epoch: 8 step: 1406, loss is 0.14331866800785065\n",
      "epoch: 8 step: 1407, loss is 0.019549880176782608\n",
      "epoch: 8 step: 1408, loss is 0.00020296141155995429\n",
      "epoch: 8 step: 1409, loss is 0.013418789021670818\n",
      "epoch: 8 step: 1410, loss is 0.002447654725983739\n",
      "epoch: 8 step: 1411, loss is 0.0008600535802543163\n",
      "epoch: 8 step: 1412, loss is 0.03963374346494675\n",
      "epoch: 8 step: 1413, loss is 0.0008733308059163392\n",
      "epoch: 8 step: 1414, loss is 0.0010752896778285503\n",
      "epoch: 8 step: 1415, loss is 0.010615254752337933\n",
      "epoch: 8 step: 1416, loss is 0.019526107236742973\n",
      "epoch: 8 step: 1417, loss is 0.006542664486914873\n",
      "epoch: 8 step: 1418, loss is 0.0007965555414557457\n",
      "epoch: 8 step: 1419, loss is 0.0006967950612306595\n",
      "epoch: 8 step: 1420, loss is 0.0009123813360929489\n",
      "epoch: 8 step: 1421, loss is 0.0008958348771557212\n",
      "epoch: 8 step: 1422, loss is 0.00446720328181982\n",
      "epoch: 8 step: 1423, loss is 0.0009020254947245121\n",
      "epoch: 8 step: 1424, loss is 0.00043112310231663287\n",
      "epoch: 8 step: 1425, loss is 0.0014313013525679708\n",
      "epoch: 8 step: 1426, loss is 0.036580268293619156\n",
      "epoch: 8 step: 1427, loss is 0.11544175446033478\n",
      "epoch: 8 step: 1428, loss is 0.0011089120525866747\n",
      "epoch: 8 step: 1429, loss is 0.001364119932986796\n",
      "epoch: 8 step: 1430, loss is 0.0013736641267314553\n",
      "epoch: 8 step: 1431, loss is 0.0016319751739501953\n",
      "epoch: 8 step: 1432, loss is 0.001346664852462709\n",
      "epoch: 8 step: 1433, loss is 0.02979891188442707\n",
      "epoch: 8 step: 1434, loss is 0.018007447943091393\n",
      "epoch: 8 step: 1435, loss is 0.00034236267674714327\n",
      "epoch: 8 step: 1436, loss is 0.2933630049228668\n",
      "epoch: 8 step: 1437, loss is 0.0029033212922513485\n",
      "epoch: 8 step: 1438, loss is 0.0010885123629122972\n",
      "epoch: 8 step: 1439, loss is 0.0005315429298207164\n",
      "epoch: 8 step: 1440, loss is 0.0018288468709215522\n",
      "epoch: 8 step: 1441, loss is 0.0003735218197107315\n",
      "epoch: 8 step: 1442, loss is 0.0022595892660319805\n",
      "epoch: 8 step: 1443, loss is 0.07968225330114365\n",
      "epoch: 8 step: 1444, loss is 0.009559758938848972\n",
      "epoch: 8 step: 1445, loss is 0.00039198814192786813\n",
      "epoch: 8 step: 1446, loss is 0.00020402221707627177\n",
      "epoch: 8 step: 1447, loss is 0.00018932216335088015\n",
      "epoch: 8 step: 1448, loss is 0.0007576309726573527\n",
      "epoch: 8 step: 1449, loss is 5.7193719840142876e-05\n",
      "epoch: 8 step: 1450, loss is 0.001560669275932014\n",
      "epoch: 8 step: 1451, loss is 0.0426107682287693\n",
      "epoch: 8 step: 1452, loss is 0.004668582696467638\n",
      "epoch: 8 step: 1453, loss is 0.0012094472767785192\n",
      "epoch: 8 step: 1454, loss is 0.019488174468278885\n",
      "epoch: 8 step: 1455, loss is 0.020515205338597298\n",
      "epoch: 8 step: 1456, loss is 0.0004224229196552187\n",
      "epoch: 8 step: 1457, loss is 0.004409165121614933\n",
      "epoch: 8 step: 1458, loss is 0.0002700013283174485\n",
      "epoch: 8 step: 1459, loss is 0.002648433670401573\n",
      "epoch: 8 step: 1460, loss is 0.005232453811913729\n",
      "epoch: 8 step: 1461, loss is 0.016212837770581245\n",
      "epoch: 8 step: 1462, loss is 0.0010433841962367296\n",
      "epoch: 8 step: 1463, loss is 0.0662769228219986\n",
      "epoch: 8 step: 1464, loss is 0.024858374148607254\n",
      "epoch: 8 step: 1465, loss is 0.0009554956341162324\n",
      "epoch: 8 step: 1466, loss is 0.07544326037168503\n",
      "epoch: 8 step: 1467, loss is 0.015446201898157597\n",
      "epoch: 8 step: 1468, loss is 0.0009381031268276274\n",
      "epoch: 8 step: 1469, loss is 0.0010910193668678403\n",
      "epoch: 8 step: 1470, loss is 0.05643901601433754\n",
      "epoch: 8 step: 1471, loss is 0.0012078346917405725\n",
      "epoch: 8 step: 1472, loss is 0.005133556202054024\n",
      "epoch: 8 step: 1473, loss is 0.0009571160771884024\n",
      "epoch: 8 step: 1474, loss is 0.006583414506167173\n",
      "epoch: 8 step: 1475, loss is 0.00997089222073555\n",
      "epoch: 8 step: 1476, loss is 0.002262991387397051\n",
      "epoch: 8 step: 1477, loss is 0.000494038627948612\n",
      "epoch: 8 step: 1478, loss is 0.002259552478790283\n",
      "epoch: 8 step: 1479, loss is 0.0011473047779873013\n",
      "epoch: 8 step: 1480, loss is 0.018938308581709862\n",
      "epoch: 8 step: 1481, loss is 0.0009808981558308005\n",
      "epoch: 8 step: 1482, loss is 0.002138914540410042\n",
      "epoch: 8 step: 1483, loss is 0.00011344517406541854\n",
      "epoch: 8 step: 1484, loss is 0.0013112947344779968\n",
      "epoch: 8 step: 1485, loss is 8.526428428012878e-05\n",
      "epoch: 8 step: 1486, loss is 0.07570377737283707\n",
      "epoch: 8 step: 1487, loss is 0.0009149328107014298\n",
      "epoch: 8 step: 1488, loss is 0.0008200582815334201\n",
      "epoch: 8 step: 1489, loss is 0.005820263177156448\n",
      "epoch: 8 step: 1490, loss is 0.0038258966524153948\n",
      "epoch: 8 step: 1491, loss is 0.023123569786548615\n",
      "epoch: 8 step: 1492, loss is 0.0026958726812154055\n",
      "epoch: 8 step: 1493, loss is 0.0009834691882133484\n",
      "epoch: 8 step: 1494, loss is 0.008624675683677197\n",
      "epoch: 8 step: 1495, loss is 0.005566863343119621\n",
      "epoch: 8 step: 1496, loss is 9.244755347026512e-05\n",
      "epoch: 8 step: 1497, loss is 0.0008756935712881386\n",
      "epoch: 8 step: 1498, loss is 0.0009826187742874026\n",
      "epoch: 8 step: 1499, loss is 0.0004016435705125332\n",
      "epoch: 8 step: 1500, loss is 0.003995187114924192\n",
      "epoch: 8 step: 1501, loss is 0.00023260919260792434\n",
      "epoch: 8 step: 1502, loss is 0.004941979888826609\n",
      "epoch: 8 step: 1503, loss is 0.008955822326242924\n",
      "epoch: 8 step: 1504, loss is 0.00345106259919703\n",
      "epoch: 8 step: 1505, loss is 0.00021984420891385525\n",
      "epoch: 8 step: 1506, loss is 0.009680142626166344\n",
      "epoch: 8 step: 1507, loss is 0.003572546411305666\n",
      "epoch: 8 step: 1508, loss is 0.05803867056965828\n",
      "epoch: 8 step: 1509, loss is 5.946918099652976e-05\n",
      "epoch: 8 step: 1510, loss is 0.17453119158744812\n",
      "epoch: 8 step: 1511, loss is 0.005137973465025425\n",
      "epoch: 8 step: 1512, loss is 0.0015434182714670897\n",
      "epoch: 8 step: 1513, loss is 0.0014807337429374456\n",
      "epoch: 8 step: 1514, loss is 0.07014121860265732\n",
      "epoch: 8 step: 1515, loss is 0.004352944903075695\n",
      "epoch: 8 step: 1516, loss is 0.007391244173049927\n",
      "epoch: 8 step: 1517, loss is 0.001247632666490972\n",
      "epoch: 8 step: 1518, loss is 0.0014575350796803832\n",
      "epoch: 8 step: 1519, loss is 0.0641832947731018\n",
      "epoch: 8 step: 1520, loss is 0.05296676605939865\n",
      "epoch: 8 step: 1521, loss is 0.003709985874593258\n",
      "epoch: 8 step: 1522, loss is 0.06809278577566147\n",
      "epoch: 8 step: 1523, loss is 0.004478545859456062\n",
      "epoch: 8 step: 1524, loss is 0.0017629109788686037\n",
      "epoch: 8 step: 1525, loss is 0.0082975123077631\n",
      "epoch: 8 step: 1526, loss is 0.03620976582169533\n",
      "epoch: 8 step: 1527, loss is 0.00011674197594402358\n",
      "epoch: 8 step: 1528, loss is 0.006719638127833605\n",
      "epoch: 8 step: 1529, loss is 0.007847215980291367\n",
      "epoch: 8 step: 1530, loss is 0.0001854039146564901\n",
      "epoch: 8 step: 1531, loss is 0.0005192229873500764\n",
      "epoch: 8 step: 1532, loss is 0.1161772683262825\n",
      "epoch: 8 step: 1533, loss is 0.001462402637116611\n",
      "epoch: 8 step: 1534, loss is 0.024898696690797806\n",
      "epoch: 8 step: 1535, loss is 0.0005392222083173692\n",
      "epoch: 8 step: 1536, loss is 0.002537431661039591\n",
      "epoch: 8 step: 1537, loss is 0.019527900964021683\n",
      "epoch: 8 step: 1538, loss is 0.0048834639601409435\n",
      "epoch: 8 step: 1539, loss is 0.0015557610895484686\n",
      "epoch: 8 step: 1540, loss is 0.0023149342741817236\n",
      "epoch: 8 step: 1541, loss is 0.029207726940512657\n",
      "epoch: 8 step: 1542, loss is 0.004262538161128759\n",
      "epoch: 8 step: 1543, loss is 0.04136047884821892\n",
      "epoch: 8 step: 1544, loss is 0.06282183527946472\n",
      "epoch: 8 step: 1545, loss is 0.05806276202201843\n",
      "epoch: 8 step: 1546, loss is 0.00010917034524027258\n",
      "epoch: 8 step: 1547, loss is 0.0005540063139051199\n",
      "epoch: 8 step: 1548, loss is 0.00020270643290132284\n",
      "epoch: 8 step: 1549, loss is 0.013510176911950111\n",
      "epoch: 8 step: 1550, loss is 0.0014507302548736334\n",
      "epoch: 8 step: 1551, loss is 0.00043892889516428113\n",
      "epoch: 8 step: 1552, loss is 0.0018604209180921316\n",
      "epoch: 8 step: 1553, loss is 0.00803131889551878\n",
      "epoch: 8 step: 1554, loss is 0.0020514039788395166\n",
      "epoch: 8 step: 1555, loss is 0.007761217653751373\n",
      "epoch: 8 step: 1556, loss is 0.0009113508858717978\n",
      "epoch: 8 step: 1557, loss is 0.0010701598366722465\n",
      "epoch: 8 step: 1558, loss is 0.07597405463457108\n",
      "epoch: 8 step: 1559, loss is 0.03126850724220276\n",
      "epoch: 8 step: 1560, loss is 0.051481109112501144\n",
      "epoch: 8 step: 1561, loss is 0.0001220850826939568\n",
      "epoch: 8 step: 1562, loss is 0.0028027084190398455\n",
      "epoch: 8 step: 1563, loss is 0.002371251117438078\n",
      "epoch: 8 step: 1564, loss is 0.10293588787317276\n",
      "epoch: 8 step: 1565, loss is 0.0082818903028965\n",
      "epoch: 8 step: 1566, loss is 0.000462174677522853\n",
      "epoch: 8 step: 1567, loss is 0.0012499559670686722\n",
      "epoch: 8 step: 1568, loss is 0.023227401077747345\n",
      "epoch: 8 step: 1569, loss is 0.0022868714295327663\n",
      "epoch: 8 step: 1570, loss is 0.027533112093806267\n",
      "epoch: 8 step: 1571, loss is 0.0007531404262408614\n",
      "epoch: 8 step: 1572, loss is 0.10922044515609741\n",
      "epoch: 8 step: 1573, loss is 0.025356484577059746\n",
      "epoch: 8 step: 1574, loss is 6.924938497832045e-05\n",
      "epoch: 8 step: 1575, loss is 0.01954566314816475\n",
      "epoch: 8 step: 1576, loss is 0.004756290931254625\n",
      "epoch: 8 step: 1577, loss is 0.0716545581817627\n",
      "epoch: 8 step: 1578, loss is 0.0017329157562926412\n",
      "epoch: 8 step: 1579, loss is 0.005056597758084536\n",
      "epoch: 8 step: 1580, loss is 0.012131339870393276\n",
      "epoch: 8 step: 1581, loss is 0.008372868411242962\n",
      "epoch: 8 step: 1582, loss is 0.00013350018707569689\n",
      "epoch: 8 step: 1583, loss is 0.0012893438106402755\n",
      "epoch: 8 step: 1584, loss is 0.00019343034364283085\n",
      "epoch: 8 step: 1585, loss is 0.004538979846984148\n",
      "epoch: 8 step: 1586, loss is 0.0008469060412608087\n",
      "epoch: 8 step: 1587, loss is 0.0008902826812118292\n",
      "epoch: 8 step: 1588, loss is 0.0008503814460709691\n",
      "epoch: 8 step: 1589, loss is 0.0052264477126300335\n",
      "epoch: 8 step: 1590, loss is 0.01335712056607008\n",
      "epoch: 8 step: 1591, loss is 0.02799507975578308\n",
      "epoch: 8 step: 1592, loss is 0.0005496108788065612\n",
      "epoch: 8 step: 1593, loss is 0.0010923495283350348\n",
      "epoch: 8 step: 1594, loss is 0.0009273405885323882\n",
      "epoch: 8 step: 1595, loss is 0.005569713655859232\n",
      "epoch: 8 step: 1596, loss is 0.00029744295170530677\n",
      "epoch: 8 step: 1597, loss is 0.0009451420046389103\n",
      "epoch: 8 step: 1598, loss is 0.000713443907443434\n",
      "epoch: 8 step: 1599, loss is 0.033160626888275146\n",
      "epoch: 8 step: 1600, loss is 0.49297061562538147\n",
      "epoch: 8 step: 1601, loss is 0.00020000108634121716\n",
      "epoch: 8 step: 1602, loss is 0.0007933664019219577\n",
      "epoch: 8 step: 1603, loss is 0.04930616170167923\n",
      "epoch: 8 step: 1604, loss is 0.0007405126234516501\n",
      "epoch: 8 step: 1605, loss is 0.036786921322345734\n",
      "epoch: 8 step: 1606, loss is 0.01950187422335148\n",
      "epoch: 8 step: 1607, loss is 0.003270668676123023\n",
      "epoch: 8 step: 1608, loss is 0.0002571020449977368\n",
      "epoch: 8 step: 1609, loss is 0.014839704148471355\n",
      "epoch: 8 step: 1610, loss is 0.0018610396655276418\n",
      "epoch: 8 step: 1611, loss is 0.0013704245211556554\n",
      "epoch: 8 step: 1612, loss is 0.0055131153203547\n",
      "epoch: 8 step: 1613, loss is 0.0005431585013866425\n",
      "epoch: 8 step: 1614, loss is 0.08420490473508835\n",
      "epoch: 8 step: 1615, loss is 0.10064977407455444\n",
      "epoch: 8 step: 1616, loss is 0.013677886687219143\n",
      "epoch: 8 step: 1617, loss is 0.06287690997123718\n",
      "epoch: 8 step: 1618, loss is 0.00041142530972138047\n",
      "epoch: 8 step: 1619, loss is 0.0007381706964224577\n",
      "epoch: 8 step: 1620, loss is 0.028067173436284065\n",
      "epoch: 8 step: 1621, loss is 0.001871528453193605\n",
      "epoch: 8 step: 1622, loss is 0.0030978030990809202\n",
      "epoch: 8 step: 1623, loss is 0.015438416972756386\n",
      "epoch: 8 step: 1624, loss is 0.0029251223895698786\n",
      "epoch: 8 step: 1625, loss is 5.58297942916397e-05\n",
      "epoch: 8 step: 1626, loss is 0.053353145718574524\n",
      "epoch: 8 step: 1627, loss is 0.003497024765238166\n",
      "epoch: 8 step: 1628, loss is 2.4648552425787784e-05\n",
      "epoch: 8 step: 1629, loss is 0.0013933023437857628\n",
      "epoch: 8 step: 1630, loss is 0.02207319624722004\n",
      "epoch: 8 step: 1631, loss is 0.0007655014633201063\n",
      "epoch: 8 step: 1632, loss is 0.001892934087663889\n",
      "epoch: 8 step: 1633, loss is 0.0135696642100811\n",
      "epoch: 8 step: 1634, loss is 0.0013149089645594358\n",
      "epoch: 8 step: 1635, loss is 0.0705769807100296\n",
      "epoch: 8 step: 1636, loss is 8.483008423354477e-05\n",
      "epoch: 8 step: 1637, loss is 0.00045206217328086495\n",
      "epoch: 8 step: 1638, loss is 0.0008027677540667355\n",
      "epoch: 8 step: 1639, loss is 0.05425158888101578\n",
      "epoch: 8 step: 1640, loss is 0.0009956463472917676\n",
      "epoch: 8 step: 1641, loss is 0.004418790340423584\n",
      "epoch: 8 step: 1642, loss is 0.00651609105989337\n",
      "epoch: 8 step: 1643, loss is 0.000586270063649863\n",
      "epoch: 8 step: 1644, loss is 0.0014878641813993454\n",
      "epoch: 8 step: 1645, loss is 0.006872151978313923\n",
      "epoch: 8 step: 1646, loss is 0.0006289149168878794\n",
      "epoch: 8 step: 1647, loss is 0.04541497305035591\n",
      "epoch: 8 step: 1648, loss is 0.005374079570174217\n",
      "epoch: 8 step: 1649, loss is 3.442719025770202e-05\n",
      "epoch: 8 step: 1650, loss is 0.0027796681970357895\n",
      "epoch: 8 step: 1651, loss is 0.0006892734090797603\n",
      "epoch: 8 step: 1652, loss is 0.0005673609557561576\n",
      "epoch: 8 step: 1653, loss is 0.011715492233633995\n",
      "epoch: 8 step: 1654, loss is 0.00018473973614163697\n",
      "epoch: 8 step: 1655, loss is 0.017033088952302933\n",
      "epoch: 8 step: 1656, loss is 0.17689605057239532\n",
      "epoch: 8 step: 1657, loss is 0.12242560088634491\n",
      "epoch: 8 step: 1658, loss is 0.0029994689393788576\n",
      "epoch: 8 step: 1659, loss is 0.00026437785709276795\n",
      "epoch: 8 step: 1660, loss is 0.00016884761862456799\n",
      "epoch: 8 step: 1661, loss is 0.00905906967818737\n",
      "epoch: 8 step: 1662, loss is 0.02261733077466488\n",
      "epoch: 8 step: 1663, loss is 0.0028152763843536377\n",
      "epoch: 8 step: 1664, loss is 0.0002633560507092625\n",
      "epoch: 8 step: 1665, loss is 5.3100760851521045e-05\n",
      "epoch: 8 step: 1666, loss is 0.10742664337158203\n",
      "epoch: 8 step: 1667, loss is 0.00023760565090924501\n",
      "epoch: 8 step: 1668, loss is 0.005271032452583313\n",
      "epoch: 8 step: 1669, loss is 0.0009168915566988289\n",
      "epoch: 8 step: 1670, loss is 0.0011748508550226688\n",
      "epoch: 8 step: 1671, loss is 0.0038373065181076527\n",
      "epoch: 8 step: 1672, loss is 0.022116024047136307\n",
      "epoch: 8 step: 1673, loss is 0.04570629820227623\n",
      "epoch: 8 step: 1674, loss is 0.011008835397660732\n",
      "epoch: 8 step: 1675, loss is 0.001968598924577236\n",
      "epoch: 8 step: 1676, loss is 0.0001672218641033396\n",
      "epoch: 8 step: 1677, loss is 0.006178944371640682\n",
      "epoch: 8 step: 1678, loss is 0.10996239632368088\n",
      "epoch: 8 step: 1679, loss is 0.00018063466995954514\n",
      "epoch: 8 step: 1680, loss is 0.14290252327919006\n",
      "epoch: 8 step: 1681, loss is 0.0011388425482437015\n",
      "epoch: 8 step: 1682, loss is 0.00025343973538838327\n",
      "epoch: 8 step: 1683, loss is 0.009900085628032684\n",
      "epoch: 8 step: 1684, loss is 0.0006280239904299378\n",
      "epoch: 8 step: 1685, loss is 0.0002653142437338829\n",
      "epoch: 8 step: 1686, loss is 0.0003644211683422327\n",
      "epoch: 8 step: 1687, loss is 0.0013375022681429982\n",
      "epoch: 8 step: 1688, loss is 0.0010639640968292952\n",
      "epoch: 8 step: 1689, loss is 0.003109973855316639\n",
      "epoch: 8 step: 1690, loss is 0.00466306833550334\n",
      "epoch: 8 step: 1691, loss is 0.009641832672059536\n",
      "epoch: 8 step: 1692, loss is 0.01430255826562643\n",
      "epoch: 8 step: 1693, loss is 0.0006231203442439437\n",
      "epoch: 8 step: 1694, loss is 0.04313654080033302\n",
      "epoch: 8 step: 1695, loss is 0.09760841727256775\n",
      "epoch: 8 step: 1696, loss is 0.0010836648289114237\n",
      "epoch: 8 step: 1697, loss is 0.003178252140060067\n",
      "epoch: 8 step: 1698, loss is 0.0335463285446167\n",
      "epoch: 8 step: 1699, loss is 0.031133612617850304\n",
      "epoch: 8 step: 1700, loss is 0.0016162121901288629\n",
      "epoch: 8 step: 1701, loss is 0.001014283043332398\n",
      "epoch: 8 step: 1702, loss is 0.0012966811191290617\n",
      "epoch: 8 step: 1703, loss is 0.0009264777763746679\n",
      "epoch: 8 step: 1704, loss is 0.007089605554938316\n",
      "epoch: 8 step: 1705, loss is 0.0037795298267155886\n",
      "epoch: 8 step: 1706, loss is 0.0005743349902331829\n",
      "epoch: 8 step: 1707, loss is 0.0023825939279049635\n",
      "epoch: 8 step: 1708, loss is 0.004376760218292475\n",
      "epoch: 8 step: 1709, loss is 0.000679739925544709\n",
      "epoch: 8 step: 1710, loss is 0.003731618169695139\n",
      "epoch: 8 step: 1711, loss is 0.026149539276957512\n",
      "epoch: 8 step: 1712, loss is 0.0009122178889811039\n",
      "epoch: 8 step: 1713, loss is 0.00015995153808034956\n",
      "epoch: 8 step: 1714, loss is 0.0013139676302671432\n",
      "epoch: 8 step: 1715, loss is 0.146332785487175\n",
      "epoch: 8 step: 1716, loss is 0.02839094214141369\n",
      "epoch: 8 step: 1717, loss is 0.004130459390580654\n",
      "epoch: 8 step: 1718, loss is 0.018753118813037872\n",
      "epoch: 8 step: 1719, loss is 0.0013550930889323354\n",
      "epoch: 8 step: 1720, loss is 0.0010453802533447742\n",
      "epoch: 8 step: 1721, loss is 0.009718921035528183\n",
      "epoch: 8 step: 1722, loss is 0.0034798523411154747\n",
      "epoch: 8 step: 1723, loss is 0.030694464221596718\n",
      "epoch: 8 step: 1724, loss is 0.0002821986854542047\n",
      "epoch: 8 step: 1725, loss is 0.06498870998620987\n",
      "epoch: 8 step: 1726, loss is 0.0018143821507692337\n",
      "epoch: 8 step: 1727, loss is 0.0007422829512506723\n",
      "epoch: 8 step: 1728, loss is 0.002428126987069845\n",
      "epoch: 8 step: 1729, loss is 0.07076074928045273\n",
      "epoch: 8 step: 1730, loss is 0.0005301206838339567\n",
      "epoch: 8 step: 1731, loss is 0.18518513441085815\n",
      "epoch: 8 step: 1732, loss is 0.00026458807406015694\n",
      "epoch: 8 step: 1733, loss is 0.01383209228515625\n",
      "epoch: 8 step: 1734, loss is 0.004031553864479065\n",
      "epoch: 8 step: 1735, loss is 0.038041528314352036\n",
      "epoch: 8 step: 1736, loss is 0.0003495425917208195\n",
      "epoch: 8 step: 1737, loss is 0.03135841712355614\n",
      "epoch: 8 step: 1738, loss is 0.1085047796368599\n",
      "epoch: 8 step: 1739, loss is 0.0007269636262208223\n",
      "epoch: 8 step: 1740, loss is 0.00038332221447490156\n",
      "epoch: 8 step: 1741, loss is 0.0014894126215949655\n",
      "epoch: 8 step: 1742, loss is 0.0016647479496896267\n",
      "epoch: 8 step: 1743, loss is 0.01468859426677227\n",
      "epoch: 8 step: 1744, loss is 0.0015311655588448048\n",
      "epoch: 8 step: 1745, loss is 0.0005546099855564535\n",
      "epoch: 8 step: 1746, loss is 8.796250040177256e-05\n",
      "epoch: 8 step: 1747, loss is 0.036045707762241364\n",
      "epoch: 8 step: 1748, loss is 0.002614194992929697\n",
      "epoch: 8 step: 1749, loss is 0.000645291293039918\n",
      "epoch: 8 step: 1750, loss is 0.06944432854652405\n",
      "epoch: 8 step: 1751, loss is 0.0012651399010792375\n",
      "epoch: 8 step: 1752, loss is 0.0028793695382773876\n",
      "epoch: 8 step: 1753, loss is 0.0028972295112907887\n",
      "epoch: 8 step: 1754, loss is 0.00024946185294538736\n",
      "epoch: 8 step: 1755, loss is 0.15360045433044434\n",
      "epoch: 8 step: 1756, loss is 0.0012732489267364144\n",
      "epoch: 8 step: 1757, loss is 0.1061820387840271\n",
      "epoch: 8 step: 1758, loss is 0.01633848436176777\n",
      "epoch: 8 step: 1759, loss is 0.005277586169540882\n",
      "epoch: 8 step: 1760, loss is 0.0031122940126806498\n",
      "epoch: 8 step: 1761, loss is 0.0018486432963982224\n",
      "epoch: 8 step: 1762, loss is 0.0011865021660923958\n",
      "epoch: 8 step: 1763, loss is 0.09564059972763062\n",
      "epoch: 8 step: 1764, loss is 0.0035095917992293835\n",
      "epoch: 8 step: 1765, loss is 0.0003527600783854723\n",
      "epoch: 8 step: 1766, loss is 0.00017042465333361179\n",
      "epoch: 8 step: 1767, loss is 0.01837781071662903\n",
      "epoch: 8 step: 1768, loss is 0.0008695957949385047\n",
      "epoch: 8 step: 1769, loss is 0.0016129373107105494\n",
      "epoch: 8 step: 1770, loss is 0.0007068749400787055\n",
      "epoch: 8 step: 1771, loss is 0.013282621279358864\n",
      "epoch: 8 step: 1772, loss is 0.009568434208631516\n",
      "epoch: 8 step: 1773, loss is 0.0003666699049063027\n",
      "epoch: 8 step: 1774, loss is 0.0006480702431872487\n",
      "epoch: 8 step: 1775, loss is 0.004743556492030621\n",
      "epoch: 8 step: 1776, loss is 0.0008604472386650741\n",
      "epoch: 8 step: 1777, loss is 0.14274495840072632\n",
      "epoch: 8 step: 1778, loss is 0.03328382223844528\n",
      "epoch: 8 step: 1779, loss is 0.005923595745116472\n",
      "epoch: 8 step: 1780, loss is 0.0023767880629748106\n",
      "epoch: 8 step: 1781, loss is 0.00012871526996605098\n",
      "epoch: 8 step: 1782, loss is 0.013651532121002674\n",
      "epoch: 8 step: 1783, loss is 0.00037100512417964637\n",
      "epoch: 8 step: 1784, loss is 0.007459240034222603\n",
      "epoch: 8 step: 1785, loss is 0.0027088848873972893\n",
      "epoch: 8 step: 1786, loss is 0.07175315171480179\n",
      "epoch: 8 step: 1787, loss is 0.0002536032407078892\n",
      "epoch: 8 step: 1788, loss is 0.03842601180076599\n",
      "epoch: 8 step: 1789, loss is 0.015780095010995865\n",
      "epoch: 8 step: 1790, loss is 0.12401627749204636\n",
      "epoch: 8 step: 1791, loss is 0.016496816650032997\n",
      "epoch: 8 step: 1792, loss is 0.00919125135987997\n",
      "epoch: 8 step: 1793, loss is 0.006861894391477108\n",
      "epoch: 8 step: 1794, loss is 0.003914838191121817\n",
      "epoch: 8 step: 1795, loss is 0.014137896709144115\n",
      "epoch: 8 step: 1796, loss is 0.0010801843600347638\n",
      "epoch: 8 step: 1797, loss is 0.032684218138456345\n",
      "epoch: 8 step: 1798, loss is 0.005625233519822359\n",
      "epoch: 8 step: 1799, loss is 0.0013562454842031002\n",
      "epoch: 8 step: 1800, loss is 0.006444334518164396\n",
      "epoch: 8 step: 1801, loss is 0.025181684643030167\n",
      "epoch: 8 step: 1802, loss is 0.00020502529514487833\n",
      "epoch: 8 step: 1803, loss is 0.02703874558210373\n",
      "epoch: 8 step: 1804, loss is 0.01924397237598896\n",
      "epoch: 8 step: 1805, loss is 0.004454651847481728\n",
      "epoch: 8 step: 1806, loss is 0.0001424646470695734\n",
      "epoch: 8 step: 1807, loss is 0.09408339112997055\n",
      "epoch: 8 step: 1808, loss is 0.0009288262226618826\n",
      "epoch: 8 step: 1809, loss is 0.002475517801940441\n",
      "epoch: 8 step: 1810, loss is 0.01914052665233612\n",
      "epoch: 8 step: 1811, loss is 0.00035857586772181094\n",
      "epoch: 8 step: 1812, loss is 0.0004915882018394768\n",
      "epoch: 8 step: 1813, loss is 0.001093285740353167\n",
      "epoch: 8 step: 1814, loss is 0.006741099525243044\n",
      "epoch: 8 step: 1815, loss is 0.005090360064059496\n",
      "epoch: 8 step: 1816, loss is 0.0013149414444342256\n",
      "epoch: 8 step: 1817, loss is 0.00015472812810912728\n",
      "epoch: 8 step: 1818, loss is 0.0007180586108006537\n",
      "epoch: 8 step: 1819, loss is 0.1075151190161705\n",
      "epoch: 8 step: 1820, loss is 0.00850879866629839\n",
      "epoch: 8 step: 1821, loss is 0.0002909568138420582\n",
      "epoch: 8 step: 1822, loss is 0.00020200789731461555\n",
      "epoch: 8 step: 1823, loss is 0.002044245833531022\n",
      "epoch: 8 step: 1824, loss is 9.417827641300391e-06\n",
      "epoch: 8 step: 1825, loss is 0.049804747104644775\n",
      "epoch: 8 step: 1826, loss is 0.00027354754274711013\n",
      "epoch: 8 step: 1827, loss is 0.00017186049080919474\n",
      "epoch: 8 step: 1828, loss is 3.367579120094888e-05\n",
      "epoch: 8 step: 1829, loss is 0.0008427843567915261\n",
      "epoch: 8 step: 1830, loss is 0.06714358180761337\n",
      "epoch: 8 step: 1831, loss is 0.0036122389137744904\n",
      "epoch: 8 step: 1832, loss is 4.7328529035439715e-05\n",
      "epoch: 8 step: 1833, loss is 0.011897922493517399\n",
      "epoch: 8 step: 1834, loss is 0.005736869759857655\n",
      "epoch: 8 step: 1835, loss is 2.4950297301984392e-05\n",
      "epoch: 8 step: 1836, loss is 0.0038459482602775097\n",
      "epoch: 8 step: 1837, loss is 0.004881998058408499\n",
      "epoch: 8 step: 1838, loss is 0.0070542399771511555\n",
      "epoch: 8 step: 1839, loss is 0.0021742000244557858\n",
      "epoch: 8 step: 1840, loss is 7.526695117121562e-05\n",
      "epoch: 8 step: 1841, loss is 0.03400582820177078\n",
      "epoch: 8 step: 1842, loss is 0.0079019945114851\n",
      "epoch: 8 step: 1843, loss is 0.002916464116424322\n",
      "epoch: 8 step: 1844, loss is 0.0008011693716980517\n",
      "epoch: 8 step: 1845, loss is 7.286678010132164e-05\n",
      "epoch: 8 step: 1846, loss is 0.05199799686670303\n",
      "epoch: 8 step: 1847, loss is 0.011735175736248493\n",
      "epoch: 8 step: 1848, loss is 0.0002750053827185184\n",
      "epoch: 8 step: 1849, loss is 0.0016626172000542283\n",
      "epoch: 8 step: 1850, loss is 0.0002855277562048286\n",
      "epoch: 8 step: 1851, loss is 0.17368309199810028\n",
      "epoch: 8 step: 1852, loss is 0.0009968229569494724\n",
      "epoch: 8 step: 1853, loss is 0.00021524867042899132\n",
      "epoch: 8 step: 1854, loss is 0.0004079208301845938\n",
      "epoch: 8 step: 1855, loss is 0.003137135412544012\n",
      "epoch: 8 step: 1856, loss is 0.060045335441827774\n",
      "epoch: 8 step: 1857, loss is 0.032629746943712234\n",
      "epoch: 8 step: 1858, loss is 0.08345779776573181\n",
      "epoch: 8 step: 1859, loss is 0.04213787615299225\n",
      "epoch: 8 step: 1860, loss is 0.0013013567076995969\n",
      "epoch: 8 step: 1861, loss is 0.09977231919765472\n",
      "epoch: 8 step: 1862, loss is 0.0011946543818339705\n",
      "epoch: 8 step: 1863, loss is 0.015478525310754776\n",
      "epoch: 8 step: 1864, loss is 0.0016338881105184555\n",
      "epoch: 8 step: 1865, loss is 0.00488556083291769\n",
      "epoch: 8 step: 1866, loss is 0.0435163751244545\n",
      "epoch: 8 step: 1867, loss is 0.0025961496867239475\n",
      "epoch: 8 step: 1868, loss is 0.000610035378485918\n",
      "epoch: 8 step: 1869, loss is 0.0005769575946033001\n",
      "epoch: 8 step: 1870, loss is 0.02345556579530239\n",
      "epoch: 8 step: 1871, loss is 0.0008202516473829746\n",
      "epoch: 8 step: 1872, loss is 0.004536616615951061\n",
      "epoch: 8 step: 1873, loss is 0.001725648296996951\n",
      "epoch: 8 step: 1874, loss is 0.00010925630340352654\n",
      "epoch: 8 step: 1875, loss is 0.0015891680959612131\n",
      "Train epoch time: 13578.135 ms, per step time: 7.242 ms\n",
      "epoch: 9 step: 1, loss is 0.004641556181013584\n",
      "epoch: 9 step: 2, loss is 0.0010035161394625902\n",
      "epoch: 9 step: 3, loss is 0.0008178063435479999\n",
      "epoch: 9 step: 4, loss is 0.0002244965435238555\n",
      "epoch: 9 step: 5, loss is 0.04933135583996773\n",
      "epoch: 9 step: 6, loss is 0.0008536921232007444\n",
      "epoch: 9 step: 7, loss is 0.010591540485620499\n",
      "epoch: 9 step: 8, loss is 0.0007767914794385433\n",
      "epoch: 9 step: 9, loss is 0.008099772036075592\n",
      "epoch: 9 step: 10, loss is 8.140956197166815e-05\n",
      "epoch: 9 step: 11, loss is 0.0002766503894235939\n",
      "epoch: 9 step: 12, loss is 0.006403053179383278\n",
      "epoch: 9 step: 13, loss is 0.012753786519169807\n",
      "epoch: 9 step: 14, loss is 0.0007204918656498194\n",
      "epoch: 9 step: 15, loss is 0.011002297513186932\n",
      "epoch: 9 step: 16, loss is 0.02147672325372696\n",
      "epoch: 9 step: 17, loss is 0.0814894437789917\n",
      "epoch: 9 step: 18, loss is 0.0010822879849001765\n",
      "epoch: 9 step: 19, loss is 0.00010312924132449552\n",
      "epoch: 9 step: 20, loss is 0.027320725843310356\n",
      "epoch: 9 step: 21, loss is 0.00023717666044831276\n",
      "epoch: 9 step: 22, loss is 0.07557699829339981\n",
      "epoch: 9 step: 23, loss is 0.0013509890995919704\n",
      "epoch: 9 step: 24, loss is 0.06252114474773407\n",
      "epoch: 9 step: 25, loss is 0.01856066845357418\n",
      "epoch: 9 step: 26, loss is 0.005253583192825317\n",
      "epoch: 9 step: 27, loss is 0.02643411234021187\n",
      "epoch: 9 step: 28, loss is 0.0024483015295118093\n",
      "epoch: 9 step: 29, loss is 0.0008000459056347609\n",
      "epoch: 9 step: 30, loss is 0.00047641657874919474\n",
      "epoch: 9 step: 31, loss is 0.011378873139619827\n",
      "epoch: 9 step: 32, loss is 0.0732811838388443\n",
      "epoch: 9 step: 33, loss is 0.0004232876526657492\n",
      "epoch: 9 step: 34, loss is 0.0007931253639981151\n",
      "epoch: 9 step: 35, loss is 0.007344183977693319\n",
      "epoch: 9 step: 36, loss is 0.0006741512333974242\n",
      "epoch: 9 step: 37, loss is 0.012974383309483528\n",
      "epoch: 9 step: 38, loss is 0.0007810675306245685\n",
      "epoch: 9 step: 39, loss is 0.007034990005195141\n",
      "epoch: 9 step: 40, loss is 0.0009086435893550515\n",
      "epoch: 9 step: 41, loss is 0.017628738656640053\n",
      "epoch: 9 step: 42, loss is 0.005269449669867754\n",
      "epoch: 9 step: 43, loss is 0.001628527301363647\n",
      "epoch: 9 step: 44, loss is 0.008960522711277008\n",
      "epoch: 9 step: 45, loss is 0.006548922508955002\n",
      "epoch: 9 step: 46, loss is 0.0015196445165202022\n",
      "epoch: 9 step: 47, loss is 0.014595347456634045\n",
      "epoch: 9 step: 48, loss is 0.0004353315744083375\n",
      "epoch: 9 step: 49, loss is 0.003675058949738741\n",
      "epoch: 9 step: 50, loss is 0.008777938783168793\n",
      "epoch: 9 step: 51, loss is 4.5848613808630034e-05\n",
      "epoch: 9 step: 52, loss is 0.0003553190326783806\n",
      "epoch: 9 step: 53, loss is 5.257929296931252e-05\n",
      "epoch: 9 step: 54, loss is 0.1013159304857254\n",
      "epoch: 9 step: 55, loss is 0.00022633741900790483\n",
      "epoch: 9 step: 56, loss is 0.00954810157418251\n",
      "epoch: 9 step: 57, loss is 0.00021761984680779278\n",
      "epoch: 9 step: 58, loss is 0.007865486666560173\n",
      "epoch: 9 step: 59, loss is 0.00014123294386081398\n",
      "epoch: 9 step: 60, loss is 0.16821619868278503\n",
      "epoch: 9 step: 61, loss is 0.00035589587059803307\n",
      "epoch: 9 step: 62, loss is 0.017649933695793152\n",
      "epoch: 9 step: 63, loss is 0.024234727025032043\n",
      "epoch: 9 step: 64, loss is 0.001007798477075994\n",
      "epoch: 9 step: 65, loss is 0.00031728233443573117\n",
      "epoch: 9 step: 66, loss is 0.015829451382160187\n",
      "epoch: 9 step: 67, loss is 0.049370989203453064\n",
      "epoch: 9 step: 68, loss is 5.783123924629763e-05\n",
      "epoch: 9 step: 69, loss is 0.0002238695160485804\n",
      "epoch: 9 step: 70, loss is 0.0001934299652930349\n",
      "epoch: 9 step: 71, loss is 6.744978600181639e-05\n",
      "epoch: 9 step: 72, loss is 0.0014930578181520104\n",
      "epoch: 9 step: 73, loss is 5.700205292669125e-05\n",
      "epoch: 9 step: 74, loss is 0.00014300613838713616\n",
      "epoch: 9 step: 75, loss is 0.0029630321078002453\n",
      "epoch: 9 step: 76, loss is 0.003329968312755227\n",
      "epoch: 9 step: 77, loss is 0.015182779170572758\n",
      "epoch: 9 step: 78, loss is 7.28922022972256e-05\n",
      "epoch: 9 step: 79, loss is 0.002634854521602392\n",
      "epoch: 9 step: 80, loss is 0.02282864600419998\n",
      "epoch: 9 step: 81, loss is 0.016154801473021507\n",
      "epoch: 9 step: 82, loss is 0.004834021907299757\n",
      "epoch: 9 step: 83, loss is 0.024725599214434624\n",
      "epoch: 9 step: 84, loss is 0.0011781726498156786\n",
      "epoch: 9 step: 85, loss is 0.0002196346176788211\n",
      "epoch: 9 step: 86, loss is 0.0002998037089128047\n",
      "epoch: 9 step: 87, loss is 0.00012374177458696067\n",
      "epoch: 9 step: 88, loss is 2.263888018205762e-05\n",
      "epoch: 9 step: 89, loss is 0.0011171747464686632\n",
      "epoch: 9 step: 90, loss is 0.014617766253650188\n",
      "epoch: 9 step: 91, loss is 0.00037788250483572483\n",
      "epoch: 9 step: 92, loss is 0.043128617107868195\n",
      "epoch: 9 step: 93, loss is 0.014926372095942497\n",
      "epoch: 9 step: 94, loss is 0.0013804472982883453\n",
      "epoch: 9 step: 95, loss is 0.0016394649865105748\n",
      "epoch: 9 step: 96, loss is 0.002955944510176778\n",
      "epoch: 9 step: 97, loss is 0.0004174869100097567\n",
      "epoch: 9 step: 98, loss is 0.0007794497068971395\n",
      "epoch: 9 step: 99, loss is 0.002215768676251173\n",
      "epoch: 9 step: 100, loss is 0.004126559942960739\n",
      "epoch: 9 step: 101, loss is 0.0008875604253262281\n",
      "epoch: 9 step: 102, loss is 0.0030457666143774986\n",
      "epoch: 9 step: 103, loss is 0.01928447000682354\n",
      "epoch: 9 step: 104, loss is 3.1821284210309386e-05\n",
      "epoch: 9 step: 105, loss is 0.006396788638085127\n",
      "epoch: 9 step: 106, loss is 0.0007985227275639772\n",
      "epoch: 9 step: 107, loss is 0.0705864354968071\n",
      "epoch: 9 step: 108, loss is 0.029010677710175514\n",
      "epoch: 9 step: 109, loss is 0.03619351238012314\n",
      "epoch: 9 step: 110, loss is 0.016102008521556854\n",
      "epoch: 9 step: 111, loss is 3.717005893122405e-05\n",
      "epoch: 9 step: 112, loss is 0.00042710278648883104\n",
      "epoch: 9 step: 113, loss is 0.004627829883247614\n",
      "epoch: 9 step: 114, loss is 0.15075550973415375\n",
      "epoch: 9 step: 115, loss is 0.0030471812933683395\n",
      "epoch: 9 step: 116, loss is 2.6635994800017215e-05\n",
      "epoch: 9 step: 117, loss is 0.0001857125316746533\n",
      "epoch: 9 step: 118, loss is 0.00018323239055462182\n",
      "epoch: 9 step: 119, loss is 0.0023786667734384537\n",
      "epoch: 9 step: 120, loss is 0.0005191719974391162\n",
      "epoch: 9 step: 121, loss is 0.1586872637271881\n",
      "epoch: 9 step: 122, loss is 4.7758287109900266e-05\n",
      "epoch: 9 step: 123, loss is 0.0015139367897063494\n",
      "epoch: 9 step: 124, loss is 0.0002004709531320259\n",
      "epoch: 9 step: 125, loss is 0.020690998062491417\n",
      "epoch: 9 step: 126, loss is 0.0016622929833829403\n",
      "epoch: 9 step: 127, loss is 0.00012623217480722815\n",
      "epoch: 9 step: 128, loss is 0.005455019883811474\n",
      "epoch: 9 step: 129, loss is 6.865736213512719e-05\n",
      "epoch: 9 step: 130, loss is 1.099377368518617e-05\n",
      "epoch: 9 step: 131, loss is 0.012181207537651062\n",
      "epoch: 9 step: 132, loss is 0.0006757110240869224\n",
      "epoch: 9 step: 133, loss is 0.0004479220078792423\n",
      "epoch: 9 step: 134, loss is 0.008282070979475975\n",
      "epoch: 9 step: 135, loss is 0.01312774233520031\n",
      "epoch: 9 step: 136, loss is 0.00019532772421371192\n",
      "epoch: 9 step: 137, loss is 0.06987569481134415\n",
      "epoch: 9 step: 138, loss is 0.021523747593164444\n",
      "epoch: 9 step: 139, loss is 0.00013836224388796836\n",
      "epoch: 9 step: 140, loss is 0.00019269136828370392\n",
      "epoch: 9 step: 141, loss is 0.00036260634078644216\n",
      "epoch: 9 step: 142, loss is 0.0005116729880683124\n",
      "epoch: 9 step: 143, loss is 7.19822637620382e-05\n",
      "epoch: 9 step: 144, loss is 0.09309885650873184\n",
      "epoch: 9 step: 145, loss is 0.0012020071735605597\n",
      "epoch: 9 step: 146, loss is 0.034060247242450714\n",
      "epoch: 9 step: 147, loss is 0.0001735742698656395\n",
      "epoch: 9 step: 148, loss is 0.0001310124935116619\n",
      "epoch: 9 step: 149, loss is 0.0003309464664198458\n",
      "epoch: 9 step: 150, loss is 0.01187279261648655\n",
      "epoch: 9 step: 151, loss is 2.5879840904963203e-05\n",
      "epoch: 9 step: 152, loss is 0.09396238625049591\n",
      "epoch: 9 step: 153, loss is 0.0005133732920512557\n",
      "epoch: 9 step: 154, loss is 0.006729212589561939\n",
      "epoch: 9 step: 155, loss is 0.0017983439611271024\n",
      "epoch: 9 step: 156, loss is 0.00040113728027790785\n",
      "epoch: 9 step: 157, loss is 0.026986325159668922\n",
      "epoch: 9 step: 158, loss is 0.00966758094727993\n",
      "epoch: 9 step: 159, loss is 0.0063085095025599\n",
      "epoch: 9 step: 160, loss is 0.006962175946682692\n",
      "epoch: 9 step: 161, loss is 0.005456737242639065\n",
      "epoch: 9 step: 162, loss is 0.23794624209403992\n",
      "epoch: 9 step: 163, loss is 0.00197694287635386\n",
      "epoch: 9 step: 164, loss is 0.0007822099723853171\n",
      "epoch: 9 step: 165, loss is 0.00025197825743816793\n",
      "epoch: 9 step: 166, loss is 5.52698147657793e-05\n",
      "epoch: 9 step: 167, loss is 0.001245438470505178\n",
      "epoch: 9 step: 168, loss is 0.00023993634385988116\n",
      "epoch: 9 step: 169, loss is 0.014973661862313747\n",
      "epoch: 9 step: 170, loss is 0.0005894542555324733\n",
      "epoch: 9 step: 171, loss is 0.004413953050971031\n",
      "epoch: 9 step: 172, loss is 0.0009030852816067636\n",
      "epoch: 9 step: 173, loss is 0.0011614050017669797\n",
      "epoch: 9 step: 174, loss is 1.441838776372606e-05\n",
      "epoch: 9 step: 175, loss is 0.017892539501190186\n",
      "epoch: 9 step: 176, loss is 0.0003608404367696494\n",
      "epoch: 9 step: 177, loss is 0.0013461766066029668\n",
      "epoch: 9 step: 178, loss is 0.00011323922808514908\n",
      "epoch: 9 step: 179, loss is 0.00018050483777187765\n",
      "epoch: 9 step: 180, loss is 0.022097542881965637\n",
      "epoch: 9 step: 181, loss is 0.00010073844896396622\n",
      "epoch: 9 step: 182, loss is 0.01101345382630825\n",
      "epoch: 9 step: 183, loss is 0.11270222067832947\n",
      "epoch: 9 step: 184, loss is 0.03168845176696777\n",
      "epoch: 9 step: 185, loss is 0.0013014909345656633\n",
      "epoch: 9 step: 186, loss is 0.005480661056935787\n",
      "epoch: 9 step: 187, loss is 0.0032154724467545748\n",
      "epoch: 9 step: 188, loss is 2.558838968980126e-05\n",
      "epoch: 9 step: 189, loss is 8.39498607092537e-05\n",
      "epoch: 9 step: 190, loss is 0.0005559893907047808\n",
      "epoch: 9 step: 191, loss is 0.005653649102896452\n",
      "epoch: 9 step: 192, loss is 0.00613992428407073\n",
      "epoch: 9 step: 193, loss is 0.0004340113082434982\n",
      "epoch: 9 step: 194, loss is 0.005645208526402712\n",
      "epoch: 9 step: 195, loss is 0.001743473345413804\n",
      "epoch: 9 step: 196, loss is 0.0007535631884820759\n",
      "epoch: 9 step: 197, loss is 4.760986485052854e-06\n",
      "epoch: 9 step: 198, loss is 0.009726767428219318\n",
      "epoch: 9 step: 199, loss is 0.09547386318445206\n",
      "epoch: 9 step: 200, loss is 3.6370485759107396e-05\n",
      "epoch: 9 step: 201, loss is 0.00020671653328463435\n",
      "epoch: 9 step: 202, loss is 0.016051925718784332\n",
      "epoch: 9 step: 203, loss is 0.0010618376545608044\n",
      "epoch: 9 step: 204, loss is 7.546056440332904e-05\n",
      "epoch: 9 step: 205, loss is 0.0022540155332535505\n",
      "epoch: 9 step: 206, loss is 0.002836469793692231\n",
      "epoch: 9 step: 207, loss is 0.022893041372299194\n",
      "epoch: 9 step: 208, loss is 3.2858966733329e-05\n",
      "epoch: 9 step: 209, loss is 0.014235593378543854\n",
      "epoch: 9 step: 210, loss is 0.004496091045439243\n",
      "epoch: 9 step: 211, loss is 0.0002936889068223536\n",
      "epoch: 9 step: 212, loss is 0.23214727640151978\n",
      "epoch: 9 step: 213, loss is 0.0006343735731206834\n",
      "epoch: 9 step: 214, loss is 0.00012102506298106164\n",
      "epoch: 9 step: 215, loss is 0.00021270973957143724\n",
      "epoch: 9 step: 216, loss is 0.0010902450885623693\n",
      "epoch: 9 step: 217, loss is 1.6614143532933667e-05\n",
      "epoch: 9 step: 218, loss is 0.021546026691794395\n",
      "epoch: 9 step: 219, loss is 1.3747849152423441e-05\n",
      "epoch: 9 step: 220, loss is 0.002453726250678301\n",
      "epoch: 9 step: 221, loss is 0.02763349562883377\n",
      "epoch: 9 step: 222, loss is 0.0017394361784681678\n",
      "epoch: 9 step: 223, loss is 0.00017810803547035903\n",
      "epoch: 9 step: 224, loss is 0.00029362240456975996\n",
      "epoch: 9 step: 225, loss is 0.00410818587988615\n",
      "epoch: 9 step: 226, loss is 0.001290563610382378\n",
      "epoch: 9 step: 227, loss is 0.001650892780162394\n",
      "epoch: 9 step: 228, loss is 3.7284618883859366e-05\n",
      "epoch: 9 step: 229, loss is 0.009163221344351768\n",
      "epoch: 9 step: 230, loss is 0.00027602395857684314\n",
      "epoch: 9 step: 231, loss is 0.019411319866776466\n",
      "epoch: 9 step: 232, loss is 0.09300211817026138\n",
      "epoch: 9 step: 233, loss is 0.002665226347744465\n",
      "epoch: 9 step: 234, loss is 0.201494038105011\n",
      "epoch: 9 step: 235, loss is 0.008077718317508698\n",
      "epoch: 9 step: 236, loss is 0.023173853754997253\n",
      "epoch: 9 step: 237, loss is 0.0005806171684525907\n",
      "epoch: 9 step: 238, loss is 0.0011224336922168732\n",
      "epoch: 9 step: 239, loss is 0.008935097604990005\n",
      "epoch: 9 step: 240, loss is 0.0086206691339612\n",
      "epoch: 9 step: 241, loss is 0.004631494637578726\n",
      "epoch: 9 step: 242, loss is 0.02156125009059906\n",
      "epoch: 9 step: 243, loss is 0.0001762571046128869\n",
      "epoch: 9 step: 244, loss is 0.019073989242315292\n",
      "epoch: 9 step: 245, loss is 0.00024005495652090758\n",
      "epoch: 9 step: 246, loss is 0.009121805429458618\n",
      "epoch: 9 step: 247, loss is 0.007010344881564379\n",
      "epoch: 9 step: 248, loss is 0.0038232277147471905\n",
      "epoch: 9 step: 249, loss is 0.00889703631401062\n",
      "epoch: 9 step: 250, loss is 0.010701851919293404\n",
      "epoch: 9 step: 251, loss is 0.00020383208175189793\n",
      "epoch: 9 step: 252, loss is 0.0002289153344463557\n",
      "epoch: 9 step: 253, loss is 0.004418572410941124\n",
      "epoch: 9 step: 254, loss is 0.001524713821709156\n",
      "epoch: 9 step: 255, loss is 0.013475994579494\n",
      "epoch: 9 step: 256, loss is 0.003999545704573393\n",
      "epoch: 9 step: 257, loss is 0.005945811979472637\n",
      "epoch: 9 step: 258, loss is 0.002151045249775052\n",
      "epoch: 9 step: 259, loss is 0.001643418101593852\n",
      "epoch: 9 step: 260, loss is 0.0032469183206558228\n",
      "epoch: 9 step: 261, loss is 2.2148642528918572e-05\n",
      "epoch: 9 step: 262, loss is 0.0029899198561906815\n",
      "epoch: 9 step: 263, loss is 0.0001172971969936043\n",
      "epoch: 9 step: 264, loss is 0.01481383852660656\n",
      "epoch: 9 step: 265, loss is 0.00036252909922041\n",
      "epoch: 9 step: 266, loss is 0.0010208250023424625\n",
      "epoch: 9 step: 267, loss is 0.008938750252127647\n",
      "epoch: 9 step: 268, loss is 0.0016881690826267004\n",
      "epoch: 9 step: 269, loss is 0.0015028641792014241\n",
      "epoch: 9 step: 270, loss is 0.0014011714374646544\n",
      "epoch: 9 step: 271, loss is 0.010767554864287376\n",
      "epoch: 9 step: 272, loss is 0.00612052483484149\n",
      "epoch: 9 step: 273, loss is 0.0023570838384330273\n",
      "epoch: 9 step: 274, loss is 0.025786785408854485\n",
      "epoch: 9 step: 275, loss is 0.03603225201368332\n",
      "epoch: 9 step: 276, loss is 0.0022960833739489317\n",
      "epoch: 9 step: 277, loss is 0.02034778520464897\n",
      "epoch: 9 step: 278, loss is 6.865190516691655e-05\n",
      "epoch: 9 step: 279, loss is 0.0008071466581895947\n",
      "epoch: 9 step: 280, loss is 7.752727105980739e-05\n",
      "epoch: 9 step: 281, loss is 0.0034642897080630064\n",
      "epoch: 9 step: 282, loss is 0.0005060058319941163\n",
      "epoch: 9 step: 283, loss is 9.774979844223708e-05\n",
      "epoch: 9 step: 284, loss is 0.06967885047197342\n",
      "epoch: 9 step: 285, loss is 4.284793249098584e-05\n",
      "epoch: 9 step: 286, loss is 0.00042082477011717856\n",
      "epoch: 9 step: 287, loss is 0.0007388307130895555\n",
      "epoch: 9 step: 288, loss is 0.00017763712094165385\n",
      "epoch: 9 step: 289, loss is 0.003501744708046317\n",
      "epoch: 9 step: 290, loss is 5.6155684433178976e-05\n",
      "epoch: 9 step: 291, loss is 0.070336252450943\n",
      "epoch: 9 step: 292, loss is 0.00022759257990401238\n",
      "epoch: 9 step: 293, loss is 0.035358574241399765\n",
      "epoch: 9 step: 294, loss is 5.676900764228776e-05\n",
      "epoch: 9 step: 295, loss is 0.0007308493368327618\n",
      "epoch: 9 step: 296, loss is 0.031858187168836594\n",
      "epoch: 9 step: 297, loss is 0.0004814538115169853\n",
      "epoch: 9 step: 298, loss is 0.001536227879114449\n",
      "epoch: 9 step: 299, loss is 0.0003366558230482042\n",
      "epoch: 9 step: 300, loss is 0.0005090255290269852\n",
      "epoch: 9 step: 301, loss is 0.0033262053038924932\n",
      "epoch: 9 step: 302, loss is 5.589387001236901e-05\n",
      "epoch: 9 step: 303, loss is 0.02758031338453293\n",
      "epoch: 9 step: 304, loss is 0.028185252100229263\n",
      "epoch: 9 step: 305, loss is 0.00136120046954602\n",
      "epoch: 9 step: 306, loss is 0.08935122191905975\n",
      "epoch: 9 step: 307, loss is 0.013471955433487892\n",
      "epoch: 9 step: 308, loss is 0.00037374350358732045\n",
      "epoch: 9 step: 309, loss is 0.0053674522787332535\n",
      "epoch: 9 step: 310, loss is 0.00031520280754193664\n",
      "epoch: 9 step: 311, loss is 0.011644624173641205\n",
      "epoch: 9 step: 312, loss is 0.002231566933915019\n",
      "epoch: 9 step: 313, loss is 0.00020223113824613392\n",
      "epoch: 9 step: 314, loss is 0.0005118371336720884\n",
      "epoch: 9 step: 315, loss is 0.010363850742578506\n",
      "epoch: 9 step: 316, loss is 0.0009239610517397523\n",
      "epoch: 9 step: 317, loss is 0.005652535706758499\n",
      "epoch: 9 step: 318, loss is 0.010538906790316105\n",
      "epoch: 9 step: 319, loss is 0.057312313467264175\n",
      "epoch: 9 step: 320, loss is 0.00017515900253783911\n",
      "epoch: 9 step: 321, loss is 0.04390260577201843\n",
      "epoch: 9 step: 322, loss is 8.088711183518171e-05\n",
      "epoch: 9 step: 323, loss is 7.190860196715221e-05\n",
      "epoch: 9 step: 324, loss is 0.0001523113896837458\n",
      "epoch: 9 step: 325, loss is 2.2131407604319975e-05\n",
      "epoch: 9 step: 326, loss is 0.0023289336822927\n",
      "epoch: 9 step: 327, loss is 0.000414425041526556\n",
      "epoch: 9 step: 328, loss is 0.004969083238393068\n",
      "epoch: 9 step: 329, loss is 0.007735485676676035\n",
      "epoch: 9 step: 330, loss is 3.386347816558555e-05\n",
      "epoch: 9 step: 331, loss is 0.10493407398462296\n",
      "epoch: 9 step: 332, loss is 8.14750965218991e-05\n",
      "epoch: 9 step: 333, loss is 0.0011127192992717028\n",
      "epoch: 9 step: 334, loss is 2.4207600290537812e-05\n",
      "epoch: 9 step: 335, loss is 0.0008037402294576168\n",
      "epoch: 9 step: 336, loss is 0.09552575647830963\n",
      "epoch: 9 step: 337, loss is 4.515289037954062e-05\n",
      "epoch: 9 step: 338, loss is 0.0008550492348149419\n",
      "epoch: 9 step: 339, loss is 0.053672730922698975\n",
      "epoch: 9 step: 340, loss is 0.001460596569813788\n",
      "epoch: 9 step: 341, loss is 0.002552952151745558\n",
      "epoch: 9 step: 342, loss is 0.006786619778722525\n",
      "epoch: 9 step: 343, loss is 0.061098549515008926\n",
      "epoch: 9 step: 344, loss is 0.003680458292365074\n",
      "epoch: 9 step: 345, loss is 0.0004132509639021009\n",
      "epoch: 9 step: 346, loss is 0.00139068893622607\n",
      "epoch: 9 step: 347, loss is 0.010445104911923409\n",
      "epoch: 9 step: 348, loss is 0.004347013775259256\n",
      "epoch: 9 step: 349, loss is 0.0002087228640448302\n",
      "epoch: 9 step: 350, loss is 0.0007260172860696912\n",
      "epoch: 9 step: 351, loss is 0.13939881324768066\n",
      "epoch: 9 step: 352, loss is 0.00015789212193340063\n",
      "epoch: 9 step: 353, loss is 0.005030046217143536\n",
      "epoch: 9 step: 354, loss is 0.00011090498446719721\n",
      "epoch: 9 step: 355, loss is 0.003794109681621194\n",
      "epoch: 9 step: 356, loss is 0.001475668977946043\n",
      "epoch: 9 step: 357, loss is 5.954037624178454e-05\n",
      "epoch: 9 step: 358, loss is 0.00010345192276872694\n",
      "epoch: 9 step: 359, loss is 0.00030363170662894845\n",
      "epoch: 9 step: 360, loss is 0.2172730416059494\n",
      "epoch: 9 step: 361, loss is 0.11911415308713913\n",
      "epoch: 9 step: 362, loss is 0.00056271388893947\n",
      "epoch: 9 step: 363, loss is 0.001847625244408846\n",
      "epoch: 9 step: 364, loss is 0.0018488757777959108\n",
      "epoch: 9 step: 365, loss is 0.0010841566836461425\n",
      "epoch: 9 step: 366, loss is 0.00621166592463851\n",
      "epoch: 9 step: 367, loss is 0.0024809043388813734\n",
      "epoch: 9 step: 368, loss is 0.0454317145049572\n",
      "epoch: 9 step: 369, loss is 0.21376864612102509\n",
      "epoch: 9 step: 370, loss is 0.04264940321445465\n",
      "epoch: 9 step: 371, loss is 0.048865269869565964\n",
      "epoch: 9 step: 372, loss is 0.0017017818754538894\n",
      "epoch: 9 step: 373, loss is 0.057840101420879364\n",
      "epoch: 9 step: 374, loss is 0.050345808267593384\n",
      "epoch: 9 step: 375, loss is 0.10532637685537338\n",
      "epoch: 9 step: 376, loss is 0.020127205178141594\n",
      "epoch: 9 step: 377, loss is 0.04098052531480789\n",
      "epoch: 9 step: 378, loss is 0.135708287358284\n",
      "epoch: 9 step: 379, loss is 0.014720882289111614\n",
      "epoch: 9 step: 380, loss is 0.014619988389313221\n",
      "epoch: 9 step: 381, loss is 0.04990486055612564\n",
      "epoch: 9 step: 382, loss is 0.006568150594830513\n",
      "epoch: 9 step: 383, loss is 0.0019215503707528114\n",
      "epoch: 9 step: 384, loss is 0.0008812026935629547\n",
      "epoch: 9 step: 385, loss is 0.0002349492278881371\n",
      "epoch: 9 step: 386, loss is 0.00964015256613493\n",
      "epoch: 9 step: 387, loss is 0.0011797499610111117\n",
      "epoch: 9 step: 388, loss is 0.0030644533690065145\n",
      "epoch: 9 step: 389, loss is 0.0007001790218055248\n",
      "epoch: 9 step: 390, loss is 0.0007731609512120485\n",
      "epoch: 9 step: 391, loss is 0.0010695989476516843\n",
      "epoch: 9 step: 392, loss is 0.027442527934908867\n",
      "epoch: 9 step: 393, loss is 0.0066733891144394875\n",
      "epoch: 9 step: 394, loss is 0.0034520041663199663\n",
      "epoch: 9 step: 395, loss is 0.000348242640029639\n",
      "epoch: 9 step: 396, loss is 0.00012601639900822192\n",
      "epoch: 9 step: 397, loss is 0.004722074139863253\n",
      "epoch: 9 step: 398, loss is 0.002548083197325468\n",
      "epoch: 9 step: 399, loss is 0.0017025918932631612\n",
      "epoch: 9 step: 400, loss is 0.0029028714634478092\n",
      "epoch: 9 step: 401, loss is 0.0011192384408786893\n",
      "epoch: 9 step: 402, loss is 2.7659763873089105e-05\n",
      "epoch: 9 step: 403, loss is 0.0004649966140277684\n",
      "epoch: 9 step: 404, loss is 0.0006450836663134396\n",
      "epoch: 9 step: 405, loss is 0.0004141040553804487\n",
      "epoch: 9 step: 406, loss is 0.0005775438039563596\n",
      "epoch: 9 step: 407, loss is 0.0007392422994598746\n",
      "epoch: 9 step: 408, loss is 0.0003935548011213541\n",
      "epoch: 9 step: 409, loss is 0.010552844032645226\n",
      "epoch: 9 step: 410, loss is 0.0002455282083246857\n",
      "epoch: 9 step: 411, loss is 0.00040927506051957607\n",
      "epoch: 9 step: 412, loss is 0.003998545929789543\n",
      "epoch: 9 step: 413, loss is 0.02737356908619404\n",
      "epoch: 9 step: 414, loss is 5.6107001000782475e-05\n",
      "epoch: 9 step: 415, loss is 4.2542968003544956e-05\n",
      "epoch: 9 step: 416, loss is 6.750128522980958e-05\n",
      "epoch: 9 step: 417, loss is 0.23054435849189758\n",
      "epoch: 9 step: 418, loss is 0.0010911518475040793\n",
      "epoch: 9 step: 419, loss is 0.0012513789115473628\n",
      "epoch: 9 step: 420, loss is 0.0011077361414209008\n",
      "epoch: 9 step: 421, loss is 0.00021955021657049656\n",
      "epoch: 9 step: 422, loss is 0.025890737771987915\n",
      "epoch: 9 step: 423, loss is 0.0004901430220343173\n",
      "epoch: 9 step: 424, loss is 0.0006630585412494838\n",
      "epoch: 9 step: 425, loss is 0.0006682269158773124\n",
      "epoch: 9 step: 426, loss is 2.4593928173999302e-05\n",
      "epoch: 9 step: 427, loss is 0.025372736155986786\n",
      "epoch: 9 step: 428, loss is 0.0006318832747638226\n",
      "epoch: 9 step: 429, loss is 4.916260877507739e-05\n",
      "epoch: 9 step: 430, loss is 0.011474020779132843\n",
      "epoch: 9 step: 431, loss is 0.0029970291070640087\n",
      "epoch: 9 step: 432, loss is 0.007813914678990841\n",
      "epoch: 9 step: 433, loss is 0.00043353583896532655\n",
      "epoch: 9 step: 434, loss is 0.007078925147652626\n",
      "epoch: 9 step: 435, loss is 0.08471005409955978\n",
      "epoch: 9 step: 436, loss is 0.00010880306217586622\n",
      "epoch: 9 step: 437, loss is 0.004212950821965933\n",
      "epoch: 9 step: 438, loss is 0.01641528494656086\n",
      "epoch: 9 step: 439, loss is 0.03858853131532669\n",
      "epoch: 9 step: 440, loss is 0.0018466279143467546\n",
      "epoch: 9 step: 441, loss is 0.00024843832943588495\n",
      "epoch: 9 step: 442, loss is 0.0004066898545715958\n",
      "epoch: 9 step: 443, loss is 0.012385580688714981\n",
      "epoch: 9 step: 444, loss is 0.0007287086918950081\n",
      "epoch: 9 step: 445, loss is 0.0005312705179676414\n",
      "epoch: 9 step: 446, loss is 0.00133050000295043\n",
      "epoch: 9 step: 447, loss is 0.19429074227809906\n",
      "epoch: 9 step: 448, loss is 0.010049257427453995\n",
      "epoch: 9 step: 449, loss is 5.256466829450801e-05\n",
      "epoch: 9 step: 450, loss is 0.10524678975343704\n",
      "epoch: 9 step: 451, loss is 0.0040165032260119915\n",
      "epoch: 9 step: 452, loss is 0.0021850732155144215\n",
      "epoch: 9 step: 453, loss is 0.08644285798072815\n",
      "epoch: 9 step: 454, loss is 0.34289833903312683\n",
      "epoch: 9 step: 455, loss is 0.00020119399414397776\n",
      "epoch: 9 step: 456, loss is 0.048810750246047974\n",
      "epoch: 9 step: 457, loss is 0.0006675944314338267\n",
      "epoch: 9 step: 458, loss is 0.012022298760712147\n",
      "epoch: 9 step: 459, loss is 0.0004042420769110322\n",
      "epoch: 9 step: 460, loss is 2.7737292839447036e-05\n",
      "epoch: 9 step: 461, loss is 0.0004861570487264544\n",
      "epoch: 9 step: 462, loss is 0.03212519362568855\n",
      "epoch: 9 step: 463, loss is 0.0002507777535356581\n",
      "epoch: 9 step: 464, loss is 0.01243499480187893\n",
      "epoch: 9 step: 465, loss is 0.08129911869764328\n",
      "epoch: 9 step: 466, loss is 0.0008388496353290975\n",
      "epoch: 9 step: 467, loss is 0.039877183735370636\n",
      "epoch: 9 step: 468, loss is 0.002639776561409235\n",
      "epoch: 9 step: 469, loss is 0.001557209761813283\n",
      "epoch: 9 step: 470, loss is 0.006870325189083815\n",
      "epoch: 9 step: 471, loss is 0.0035968362353742123\n",
      "epoch: 9 step: 472, loss is 0.0021775509230792522\n",
      "epoch: 9 step: 473, loss is 0.00056998006766662\n",
      "epoch: 9 step: 474, loss is 0.00246996502391994\n",
      "epoch: 9 step: 475, loss is 0.00024592599947936833\n",
      "epoch: 9 step: 476, loss is 0.015169341117143631\n",
      "epoch: 9 step: 477, loss is 0.0017468439182266593\n",
      "epoch: 9 step: 478, loss is 0.011320146732032299\n",
      "epoch: 9 step: 479, loss is 0.022783851251006126\n",
      "epoch: 9 step: 480, loss is 0.023891396820545197\n",
      "epoch: 9 step: 481, loss is 0.000717844464816153\n",
      "epoch: 9 step: 482, loss is 0.009999125264585018\n",
      "epoch: 9 step: 483, loss is 0.0012249358696863055\n",
      "epoch: 9 step: 484, loss is 0.010862321592867374\n",
      "epoch: 9 step: 485, loss is 0.0003535801079124212\n",
      "epoch: 9 step: 486, loss is 0.0013956178445369005\n",
      "epoch: 9 step: 487, loss is 0.0007950214203447104\n",
      "epoch: 9 step: 488, loss is 0.048682790249586105\n",
      "epoch: 9 step: 489, loss is 0.0003360675473231822\n",
      "epoch: 9 step: 490, loss is 0.002096715848892927\n",
      "epoch: 9 step: 491, loss is 0.0029871731530874968\n",
      "epoch: 9 step: 492, loss is 0.0008620318258181214\n",
      "epoch: 9 step: 493, loss is 0.0001747318747220561\n",
      "epoch: 9 step: 494, loss is 0.04806428402662277\n",
      "epoch: 9 step: 495, loss is 0.0015186525415629148\n",
      "epoch: 9 step: 496, loss is 7.096840272424743e-05\n",
      "epoch: 9 step: 497, loss is 0.006853350438177586\n",
      "epoch: 9 step: 498, loss is 0.0013188507873564959\n",
      "epoch: 9 step: 499, loss is 0.00296960910782218\n",
      "epoch: 9 step: 500, loss is 0.00024353989283554256\n",
      "epoch: 9 step: 501, loss is 0.0017654697876423597\n",
      "epoch: 9 step: 502, loss is 0.01697154901921749\n",
      "epoch: 9 step: 503, loss is 0.0012100671883672476\n",
      "epoch: 9 step: 504, loss is 0.0026636836118996143\n",
      "epoch: 9 step: 505, loss is 0.01404696423560381\n",
      "epoch: 9 step: 506, loss is 0.0009378611575812101\n",
      "epoch: 9 step: 507, loss is 0.012217340059578419\n",
      "epoch: 9 step: 508, loss is 0.0004431484849192202\n",
      "epoch: 9 step: 509, loss is 0.00277019664645195\n",
      "epoch: 9 step: 510, loss is 0.003434466663748026\n",
      "epoch: 9 step: 511, loss is 0.007106141187250614\n",
      "epoch: 9 step: 512, loss is 0.0020231539383530617\n",
      "epoch: 9 step: 513, loss is 0.0019588610157370567\n",
      "epoch: 9 step: 514, loss is 0.010609466582536697\n",
      "epoch: 9 step: 515, loss is 0.013840715400874615\n",
      "epoch: 9 step: 516, loss is 6.908576324349269e-05\n",
      "epoch: 9 step: 517, loss is 0.005981614347547293\n",
      "epoch: 9 step: 518, loss is 0.040514007210731506\n",
      "epoch: 9 step: 519, loss is 0.05426771193742752\n",
      "epoch: 9 step: 520, loss is 0.037742406129837036\n",
      "epoch: 9 step: 521, loss is 0.008825782686471939\n",
      "epoch: 9 step: 522, loss is 0.03637280315160751\n",
      "epoch: 9 step: 523, loss is 0.010289153084158897\n",
      "epoch: 9 step: 524, loss is 0.00041845193482004106\n",
      "epoch: 9 step: 525, loss is 0.00010392160766059533\n",
      "epoch: 9 step: 526, loss is 0.001748046139255166\n",
      "epoch: 9 step: 527, loss is 0.000999665935523808\n",
      "epoch: 9 step: 528, loss is 0.0029297596774995327\n",
      "epoch: 9 step: 529, loss is 0.0011991411447525024\n",
      "epoch: 9 step: 530, loss is 0.017492331564426422\n",
      "epoch: 9 step: 531, loss is 0.005696567706763744\n",
      "epoch: 9 step: 532, loss is 0.0019445596262812614\n",
      "epoch: 9 step: 533, loss is 0.004196690395474434\n",
      "epoch: 9 step: 534, loss is 0.0004859166801907122\n",
      "epoch: 9 step: 535, loss is 0.002540060319006443\n",
      "epoch: 9 step: 536, loss is 0.045857999473810196\n",
      "epoch: 9 step: 537, loss is 0.00045887735905125737\n",
      "epoch: 9 step: 538, loss is 6.655835022684187e-05\n",
      "epoch: 9 step: 539, loss is 4.198546594125219e-05\n",
      "epoch: 9 step: 540, loss is 0.0006939973100088537\n",
      "epoch: 9 step: 541, loss is 0.002016007900238037\n",
      "epoch: 9 step: 542, loss is 0.00036916331737302244\n",
      "epoch: 9 step: 543, loss is 0.0036000297404825687\n",
      "epoch: 9 step: 544, loss is 0.00035083721741102636\n",
      "epoch: 9 step: 545, loss is 0.024487823247909546\n",
      "epoch: 9 step: 546, loss is 3.152221324853599e-05\n",
      "epoch: 9 step: 547, loss is 0.0005170515505596995\n",
      "epoch: 9 step: 548, loss is 8.795378380455077e-05\n",
      "epoch: 9 step: 549, loss is 5.08094162796624e-05\n",
      "epoch: 9 step: 550, loss is 0.0003727005096152425\n",
      "epoch: 9 step: 551, loss is 0.002002477878704667\n",
      "epoch: 9 step: 552, loss is 0.03403313830494881\n",
      "epoch: 9 step: 553, loss is 0.023694956675171852\n",
      "epoch: 9 step: 554, loss is 0.00393223762512207\n",
      "epoch: 9 step: 555, loss is 0.009857828728854656\n",
      "epoch: 9 step: 556, loss is 0.000119063064630609\n",
      "epoch: 9 step: 557, loss is 0.007625858299434185\n",
      "epoch: 9 step: 558, loss is 0.00025174306938424706\n",
      "epoch: 9 step: 559, loss is 0.0014295644359663129\n",
      "epoch: 9 step: 560, loss is 0.0008201519376598299\n",
      "epoch: 9 step: 561, loss is 0.004162763245403767\n",
      "epoch: 9 step: 562, loss is 0.0002991164510603994\n",
      "epoch: 9 step: 563, loss is 0.0016960280481725931\n",
      "epoch: 9 step: 564, loss is 0.00029202341102063656\n",
      "epoch: 9 step: 565, loss is 7.261561404448003e-05\n",
      "epoch: 9 step: 566, loss is 0.0004113231843803078\n",
      "epoch: 9 step: 567, loss is 0.006122614722698927\n",
      "epoch: 9 step: 568, loss is 0.0007691741921007633\n",
      "epoch: 9 step: 569, loss is 0.00040898105362430215\n",
      "epoch: 9 step: 570, loss is 0.0019037306774407625\n",
      "epoch: 9 step: 571, loss is 0.0009426094475202262\n",
      "epoch: 9 step: 572, loss is 0.001235831412486732\n",
      "epoch: 9 step: 573, loss is 0.009920642711222172\n",
      "epoch: 9 step: 574, loss is 0.0016464554937556386\n",
      "epoch: 9 step: 575, loss is 0.011556452140212059\n",
      "epoch: 9 step: 576, loss is 1.5893554518697783e-05\n",
      "epoch: 9 step: 577, loss is 0.000710712221916765\n",
      "epoch: 9 step: 578, loss is 6.17608820903115e-05\n",
      "epoch: 9 step: 579, loss is 0.0038662957958877087\n",
      "epoch: 9 step: 580, loss is 0.0023364457301795483\n",
      "epoch: 9 step: 581, loss is 0.012532639317214489\n",
      "epoch: 9 step: 582, loss is 0.0016280828276649117\n",
      "epoch: 9 step: 583, loss is 0.0044776322320103645\n",
      "epoch: 9 step: 584, loss is 0.0795569121837616\n",
      "epoch: 9 step: 585, loss is 0.007648457307368517\n",
      "epoch: 9 step: 586, loss is 0.02001146785914898\n",
      "epoch: 9 step: 587, loss is 0.0129037881270051\n",
      "epoch: 9 step: 588, loss is 0.03103392943739891\n",
      "epoch: 9 step: 589, loss is 0.0009934123372659087\n",
      "epoch: 9 step: 590, loss is 0.000613731681369245\n",
      "epoch: 9 step: 591, loss is 0.00016764082829467952\n",
      "epoch: 9 step: 592, loss is 0.0533561185002327\n",
      "epoch: 9 step: 593, loss is 0.016117950901389122\n",
      "epoch: 9 step: 594, loss is 0.0021033312659710646\n",
      "epoch: 9 step: 595, loss is 0.0006804150762036443\n",
      "epoch: 9 step: 596, loss is 0.08915293961763382\n",
      "epoch: 9 step: 597, loss is 0.00032154127256944776\n",
      "epoch: 9 step: 598, loss is 0.00052824174053967\n",
      "epoch: 9 step: 599, loss is 0.013790477998554707\n",
      "epoch: 9 step: 600, loss is 0.0003606806858442724\n",
      "epoch: 9 step: 601, loss is 0.00988895632326603\n",
      "epoch: 9 step: 602, loss is 0.0020235285628587008\n",
      "epoch: 9 step: 603, loss is 2.935690281447023e-05\n",
      "epoch: 9 step: 604, loss is 0.005261977203190327\n",
      "epoch: 9 step: 605, loss is 0.02615366131067276\n",
      "epoch: 9 step: 606, loss is 0.12696045637130737\n",
      "epoch: 9 step: 607, loss is 0.0006217144546099007\n",
      "epoch: 9 step: 608, loss is 0.010948465205729008\n",
      "epoch: 9 step: 609, loss is 0.0036757076159119606\n",
      "epoch: 9 step: 610, loss is 0.1250532567501068\n",
      "epoch: 9 step: 611, loss is 0.0019825135823339224\n",
      "epoch: 9 step: 612, loss is 0.011118685826659203\n",
      "epoch: 9 step: 613, loss is 0.0003446276532486081\n",
      "epoch: 9 step: 614, loss is 0.0026658771093934774\n",
      "epoch: 9 step: 615, loss is 0.0712093859910965\n",
      "epoch: 9 step: 616, loss is 0.002474820241332054\n",
      "epoch: 9 step: 617, loss is 0.00013261022104416043\n",
      "epoch: 9 step: 618, loss is 0.010736895725131035\n",
      "epoch: 9 step: 619, loss is 3.3640681067481637e-06\n",
      "epoch: 9 step: 620, loss is 0.0003112411068286747\n",
      "epoch: 9 step: 621, loss is 0.012278684414923191\n",
      "epoch: 9 step: 622, loss is 0.004703100770711899\n",
      "epoch: 9 step: 623, loss is 0.015323170460760593\n",
      "epoch: 9 step: 624, loss is 0.0007020813645794988\n",
      "epoch: 9 step: 625, loss is 0.001644038362428546\n",
      "epoch: 9 step: 626, loss is 0.032567452639341354\n",
      "epoch: 9 step: 627, loss is 0.0010203344281762838\n",
      "epoch: 9 step: 628, loss is 0.025542909279465675\n",
      "epoch: 9 step: 629, loss is 0.00040477351285517216\n",
      "epoch: 9 step: 630, loss is 0.0018143015913665295\n",
      "epoch: 9 step: 631, loss is 0.040256794542074203\n",
      "epoch: 9 step: 632, loss is 1.3320684956852347e-05\n",
      "epoch: 9 step: 633, loss is 0.013238231651484966\n",
      "epoch: 9 step: 634, loss is 0.0002694172435440123\n",
      "epoch: 9 step: 635, loss is 0.03424537926912308\n",
      "epoch: 9 step: 636, loss is 0.0020874610636383295\n",
      "epoch: 9 step: 637, loss is 0.0009085683268494904\n",
      "epoch: 9 step: 638, loss is 0.01037386991083622\n",
      "epoch: 9 step: 639, loss is 0.00301711936481297\n",
      "epoch: 9 step: 640, loss is 4.396065924083814e-05\n",
      "epoch: 9 step: 641, loss is 0.008585960604250431\n",
      "epoch: 9 step: 642, loss is 4.738214920507744e-05\n",
      "epoch: 9 step: 643, loss is 0.009274856187403202\n",
      "epoch: 9 step: 644, loss is 0.07980351895093918\n",
      "epoch: 9 step: 645, loss is 0.010061396285891533\n",
      "epoch: 9 step: 646, loss is 0.008913559839129448\n",
      "epoch: 9 step: 647, loss is 0.004209067672491074\n",
      "epoch: 9 step: 648, loss is 0.006820545997470617\n",
      "epoch: 9 step: 649, loss is 0.0004037368926219642\n",
      "epoch: 9 step: 650, loss is 0.00014923771959729493\n",
      "epoch: 9 step: 651, loss is 0.0023670357186347246\n",
      "epoch: 9 step: 652, loss is 0.00011502796405693516\n",
      "epoch: 9 step: 653, loss is 0.0008706560474820435\n",
      "epoch: 9 step: 654, loss is 0.0009281316306442022\n",
      "epoch: 9 step: 655, loss is 0.003462615655735135\n",
      "epoch: 9 step: 656, loss is 0.01800837740302086\n",
      "epoch: 9 step: 657, loss is 0.005043906159698963\n",
      "epoch: 9 step: 658, loss is 0.0011619700817391276\n",
      "epoch: 9 step: 659, loss is 6.52531161904335e-05\n",
      "epoch: 9 step: 660, loss is 0.0027764018159359694\n",
      "epoch: 9 step: 661, loss is 0.048710621893405914\n",
      "epoch: 9 step: 662, loss is 0.004616680555045605\n",
      "epoch: 9 step: 663, loss is 0.010065697133541107\n",
      "epoch: 9 step: 664, loss is 0.012153612449765205\n",
      "epoch: 9 step: 665, loss is 0.025490235537290573\n",
      "epoch: 9 step: 666, loss is 0.00038018537452444434\n",
      "epoch: 9 step: 667, loss is 0.0018873100634664297\n",
      "epoch: 9 step: 668, loss is 0.011205858550965786\n",
      "epoch: 9 step: 669, loss is 0.0034391386434435844\n",
      "epoch: 9 step: 670, loss is 0.004068383015692234\n",
      "epoch: 9 step: 671, loss is 0.000277194834779948\n",
      "epoch: 9 step: 672, loss is 9.867624612525105e-05\n",
      "epoch: 9 step: 673, loss is 0.0005858545773662627\n",
      "epoch: 9 step: 674, loss is 0.0021953654941171408\n",
      "epoch: 9 step: 675, loss is 0.019531480967998505\n",
      "epoch: 9 step: 676, loss is 0.0007717947592027485\n",
      "epoch: 9 step: 677, loss is 0.0004908315022476017\n",
      "epoch: 9 step: 678, loss is 0.02976740524172783\n",
      "epoch: 9 step: 679, loss is 0.00014303326315712184\n",
      "epoch: 9 step: 680, loss is 0.006782794836908579\n",
      "epoch: 9 step: 681, loss is 0.0005297607276588678\n",
      "epoch: 9 step: 682, loss is 0.000703085504937917\n",
      "epoch: 9 step: 683, loss is 0.0003977190353907645\n",
      "epoch: 9 step: 684, loss is 0.0031369205098599195\n",
      "epoch: 9 step: 685, loss is 0.0001531547895865515\n",
      "epoch: 9 step: 686, loss is 0.0018879867857322097\n",
      "epoch: 9 step: 687, loss is 0.00022551683650817722\n",
      "epoch: 9 step: 688, loss is 8.214516856241971e-05\n",
      "epoch: 9 step: 689, loss is 5.96751669945661e-05\n",
      "epoch: 9 step: 690, loss is 0.0003445255570113659\n",
      "epoch: 9 step: 691, loss is 0.02981729619204998\n",
      "epoch: 9 step: 692, loss is 3.374611696926877e-05\n",
      "epoch: 9 step: 693, loss is 0.00031742185819894075\n",
      "epoch: 9 step: 694, loss is 0.06159593164920807\n",
      "epoch: 9 step: 695, loss is 2.143574965884909e-05\n",
      "epoch: 9 step: 696, loss is 0.0026318789459764957\n",
      "epoch: 9 step: 697, loss is 0.006483856588602066\n",
      "epoch: 9 step: 698, loss is 0.11210299283266068\n",
      "epoch: 9 step: 699, loss is 0.00022669704048894346\n",
      "epoch: 9 step: 700, loss is 0.0006506426725536585\n",
      "epoch: 9 step: 701, loss is 6.265952833928168e-05\n",
      "epoch: 9 step: 702, loss is 0.0003008709754794836\n",
      "epoch: 9 step: 703, loss is 0.0061265332624316216\n",
      "epoch: 9 step: 704, loss is 0.0003360045957379043\n",
      "epoch: 9 step: 705, loss is 0.0016792967217043042\n",
      "epoch: 9 step: 706, loss is 9.586798842065036e-05\n",
      "epoch: 9 step: 707, loss is 7.554906915174797e-05\n",
      "epoch: 9 step: 708, loss is 0.00038868747651576996\n",
      "epoch: 9 step: 709, loss is 0.011763831600546837\n",
      "epoch: 9 step: 710, loss is 0.08491428941488266\n",
      "epoch: 9 step: 711, loss is 0.3809526860713959\n",
      "epoch: 9 step: 712, loss is 0.0011084300931543112\n",
      "epoch: 9 step: 713, loss is 5.8388795878272504e-05\n",
      "epoch: 9 step: 714, loss is 0.0012182826176285744\n",
      "epoch: 9 step: 715, loss is 0.001468092086724937\n",
      "epoch: 9 step: 716, loss is 0.0009306059218943119\n",
      "epoch: 9 step: 717, loss is 0.08428814262151718\n",
      "epoch: 9 step: 718, loss is 0.0013880983460694551\n",
      "epoch: 9 step: 719, loss is 0.03852861374616623\n",
      "epoch: 9 step: 720, loss is 0.0004615947254933417\n",
      "epoch: 9 step: 721, loss is 0.000279789965134114\n",
      "epoch: 9 step: 722, loss is 0.0039004564750939608\n",
      "epoch: 9 step: 723, loss is 0.0002664188505150378\n",
      "epoch: 9 step: 724, loss is 0.00020561474957503378\n",
      "epoch: 9 step: 725, loss is 0.01029692031443119\n",
      "epoch: 9 step: 726, loss is 0.005080427974462509\n",
      "epoch: 9 step: 727, loss is 0.0015909947687759995\n",
      "epoch: 9 step: 728, loss is 0.0003436352126300335\n",
      "epoch: 9 step: 729, loss is 0.025938348844647408\n",
      "epoch: 9 step: 730, loss is 7.524415559601039e-05\n",
      "epoch: 9 step: 731, loss is 0.0006597346509806812\n",
      "epoch: 9 step: 732, loss is 0.019953306764364243\n",
      "epoch: 9 step: 733, loss is 0.0029471581801772118\n",
      "epoch: 9 step: 734, loss is 0.0075867557898163795\n",
      "epoch: 9 step: 735, loss is 0.0011684210039675236\n",
      "epoch: 9 step: 736, loss is 0.003943522926419973\n",
      "epoch: 9 step: 737, loss is 9.421673894394189e-05\n",
      "epoch: 9 step: 738, loss is 0.05180586874485016\n",
      "epoch: 9 step: 739, loss is 0.00034999873605556786\n",
      "epoch: 9 step: 740, loss is 0.02634679153561592\n",
      "epoch: 9 step: 741, loss is 0.006865586154162884\n",
      "epoch: 9 step: 742, loss is 0.07849560678005219\n",
      "epoch: 9 step: 743, loss is 0.0012088706716895103\n",
      "epoch: 9 step: 744, loss is 0.0007474892772734165\n",
      "epoch: 9 step: 745, loss is 0.004053075332194567\n",
      "epoch: 9 step: 746, loss is 0.05846170336008072\n",
      "epoch: 9 step: 747, loss is 0.00882923323661089\n",
      "epoch: 9 step: 748, loss is 0.014715711586177349\n",
      "epoch: 9 step: 749, loss is 0.03279924392700195\n",
      "epoch: 9 step: 750, loss is 0.00016473045980092138\n",
      "epoch: 9 step: 751, loss is 0.0025974158197641373\n",
      "epoch: 9 step: 752, loss is 0.0005566565087065101\n",
      "epoch: 9 step: 753, loss is 0.0030101945158094168\n",
      "epoch: 9 step: 754, loss is 0.000310216419165954\n",
      "epoch: 9 step: 755, loss is 0.005469813942909241\n",
      "epoch: 9 step: 756, loss is 0.001505611464381218\n",
      "epoch: 9 step: 757, loss is 4.183305645710789e-05\n",
      "epoch: 9 step: 758, loss is 0.0005746785318478942\n",
      "epoch: 9 step: 759, loss is 0.004524068906903267\n",
      "epoch: 9 step: 760, loss is 0.0004316349804867059\n",
      "epoch: 9 step: 761, loss is 0.00011117552639916539\n",
      "epoch: 9 step: 762, loss is 0.0010805134661495686\n",
      "epoch: 9 step: 763, loss is 0.005067304242402315\n",
      "epoch: 9 step: 764, loss is 0.000846761860884726\n",
      "epoch: 9 step: 765, loss is 0.0005244529456831515\n",
      "epoch: 9 step: 766, loss is 0.005612686742097139\n",
      "epoch: 9 step: 767, loss is 0.002329159528017044\n",
      "epoch: 9 step: 768, loss is 0.00020235781266819686\n",
      "epoch: 9 step: 769, loss is 0.003921846859157085\n",
      "epoch: 9 step: 770, loss is 0.03822998329997063\n",
      "epoch: 9 step: 771, loss is 0.00012882759619969875\n",
      "epoch: 9 step: 772, loss is 0.0064583816565573215\n",
      "epoch: 9 step: 773, loss is 0.005916496738791466\n",
      "epoch: 9 step: 774, loss is 0.0009996574372053146\n",
      "epoch: 9 step: 775, loss is 0.002579299034550786\n",
      "epoch: 9 step: 776, loss is 0.0014490322209894657\n",
      "epoch: 9 step: 777, loss is 0.00017214885156136006\n",
      "epoch: 9 step: 778, loss is 0.0003496892168186605\n",
      "epoch: 9 step: 779, loss is 1.564150988997426e-05\n",
      "epoch: 9 step: 780, loss is 0.006788522936403751\n",
      "epoch: 9 step: 781, loss is 0.19636116921901703\n",
      "epoch: 9 step: 782, loss is 9.739197412272915e-05\n",
      "epoch: 9 step: 783, loss is 0.0037832828238606453\n",
      "epoch: 9 step: 784, loss is 0.00031180845689959824\n",
      "epoch: 9 step: 785, loss is 0.0016347675118595362\n",
      "epoch: 9 step: 786, loss is 0.010089032351970673\n",
      "epoch: 9 step: 787, loss is 0.00015312092727981508\n",
      "epoch: 9 step: 788, loss is 0.0008636889397166669\n",
      "epoch: 9 step: 789, loss is 0.02091638743877411\n",
      "epoch: 9 step: 790, loss is 0.04775030165910721\n",
      "epoch: 9 step: 791, loss is 0.00018561555771157146\n",
      "epoch: 9 step: 792, loss is 0.03034079261124134\n",
      "epoch: 9 step: 793, loss is 0.0006020940490998328\n",
      "epoch: 9 step: 794, loss is 0.00044545368291437626\n",
      "epoch: 9 step: 795, loss is 0.002130998531356454\n",
      "epoch: 9 step: 796, loss is 0.0053288377821445465\n",
      "epoch: 9 step: 797, loss is 0.0006617370527237654\n",
      "epoch: 9 step: 798, loss is 0.00046039815060794353\n",
      "epoch: 9 step: 799, loss is 0.001870034378953278\n",
      "epoch: 9 step: 800, loss is 0.043266985565423965\n",
      "epoch: 9 step: 801, loss is 3.550520705175586e-05\n",
      "epoch: 9 step: 802, loss is 0.08677665889263153\n",
      "epoch: 9 step: 803, loss is 0.002528550336137414\n",
      "epoch: 9 step: 804, loss is 0.025516917929053307\n",
      "epoch: 9 step: 805, loss is 0.013153331354260445\n",
      "epoch: 9 step: 806, loss is 0.011414945125579834\n",
      "epoch: 9 step: 807, loss is 0.018112702295184135\n",
      "epoch: 9 step: 808, loss is 0.00173321389593184\n",
      "epoch: 9 step: 809, loss is 0.004014786332845688\n",
      "epoch: 9 step: 810, loss is 0.1127304956316948\n",
      "epoch: 9 step: 811, loss is 4.019356856588274e-05\n",
      "epoch: 9 step: 812, loss is 0.0006500769522972405\n",
      "epoch: 9 step: 813, loss is 0.0048995716497302055\n",
      "epoch: 9 step: 814, loss is 0.012860863469541073\n",
      "epoch: 9 step: 815, loss is 8.7886699475348e-05\n",
      "epoch: 9 step: 816, loss is 0.0028527649119496346\n",
      "epoch: 9 step: 817, loss is 0.08133530616760254\n",
      "epoch: 9 step: 818, loss is 0.0028176233172416687\n",
      "epoch: 9 step: 819, loss is 0.030032185837626457\n",
      "epoch: 9 step: 820, loss is 0.001981842564418912\n",
      "epoch: 9 step: 821, loss is 0.0003318459785077721\n",
      "epoch: 9 step: 822, loss is 0.0050967419520020485\n",
      "epoch: 9 step: 823, loss is 0.0001415961014572531\n",
      "epoch: 9 step: 824, loss is 0.00974368117749691\n",
      "epoch: 9 step: 825, loss is 0.0003141848719678819\n",
      "epoch: 9 step: 826, loss is 0.028125138953328133\n",
      "epoch: 9 step: 827, loss is 0.002088276669383049\n",
      "epoch: 9 step: 828, loss is 0.06436213105916977\n",
      "epoch: 9 step: 829, loss is 0.01929921843111515\n",
      "epoch: 9 step: 830, loss is 0.009857810102403164\n",
      "epoch: 9 step: 831, loss is 5.658448935719207e-05\n",
      "epoch: 9 step: 832, loss is 0.0016799995210021734\n",
      "epoch: 9 step: 833, loss is 0.002051781164482236\n",
      "epoch: 9 step: 834, loss is 0.03310215473175049\n",
      "epoch: 9 step: 835, loss is 2.265499824716244e-05\n",
      "epoch: 9 step: 836, loss is 0.006090571638196707\n",
      "epoch: 9 step: 837, loss is 0.007696019485592842\n",
      "epoch: 9 step: 838, loss is 0.003457942046225071\n",
      "epoch: 9 step: 839, loss is 0.01752486266195774\n",
      "epoch: 9 step: 840, loss is 0.00019785799668170512\n",
      "epoch: 9 step: 841, loss is 0.0012560415780171752\n",
      "epoch: 9 step: 842, loss is 0.001251967973075807\n",
      "epoch: 9 step: 843, loss is 0.014864244498312473\n",
      "epoch: 9 step: 844, loss is 0.3013278841972351\n",
      "epoch: 9 step: 845, loss is 0.003607007209211588\n",
      "epoch: 9 step: 846, loss is 0.0018388574244454503\n",
      "epoch: 9 step: 847, loss is 0.0011320817284286022\n",
      "epoch: 9 step: 848, loss is 0.0057004461996257305\n",
      "epoch: 9 step: 849, loss is 0.030049432069063187\n",
      "epoch: 9 step: 850, loss is 0.00014964959700591862\n",
      "epoch: 9 step: 851, loss is 0.0005446248105727136\n",
      "epoch: 9 step: 852, loss is 0.02057875320315361\n",
      "epoch: 9 step: 853, loss is 0.00033918762346729636\n",
      "epoch: 9 step: 854, loss is 0.00022966557298786938\n",
      "epoch: 9 step: 855, loss is 0.05388734117150307\n",
      "epoch: 9 step: 856, loss is 0.005282948724925518\n",
      "epoch: 9 step: 857, loss is 0.0062021962366998196\n",
      "epoch: 9 step: 858, loss is 0.015310346148908138\n",
      "epoch: 9 step: 859, loss is 0.0001622687268536538\n",
      "epoch: 9 step: 860, loss is 0.00016540446085855365\n",
      "epoch: 9 step: 861, loss is 0.00021154843852855265\n",
      "epoch: 9 step: 862, loss is 0.00010339185246266425\n",
      "epoch: 9 step: 863, loss is 0.1402738392353058\n",
      "epoch: 9 step: 864, loss is 0.003178310114890337\n",
      "epoch: 9 step: 865, loss is 0.14452937245368958\n",
      "epoch: 9 step: 866, loss is 0.0001916121254907921\n",
      "epoch: 9 step: 867, loss is 0.005020381882786751\n",
      "epoch: 9 step: 868, loss is 0.015461907722055912\n",
      "epoch: 9 step: 869, loss is 0.012366422452032566\n",
      "epoch: 9 step: 870, loss is 0.0006958008743822575\n",
      "epoch: 9 step: 871, loss is 0.14934401214122772\n",
      "epoch: 9 step: 872, loss is 4.760966021422064e-06\n",
      "epoch: 9 step: 873, loss is 6.881518493173644e-05\n",
      "epoch: 9 step: 874, loss is 0.002624582964926958\n",
      "epoch: 9 step: 875, loss is 9.477953426539898e-05\n",
      "epoch: 9 step: 876, loss is 0.0010961911175400019\n",
      "epoch: 9 step: 877, loss is 0.0017403525998815894\n",
      "epoch: 9 step: 878, loss is 0.05004649609327316\n",
      "epoch: 9 step: 879, loss is 0.0007706319447606802\n",
      "epoch: 9 step: 880, loss is 0.003687840886414051\n",
      "epoch: 9 step: 881, loss is 0.0017677699215710163\n",
      "epoch: 9 step: 882, loss is 0.0022934209555387497\n",
      "epoch: 9 step: 883, loss is 0.005421397276222706\n",
      "epoch: 9 step: 884, loss is 0.024419443681836128\n",
      "epoch: 9 step: 885, loss is 0.000186709949048236\n",
      "epoch: 9 step: 886, loss is 0.008069143630564213\n",
      "epoch: 9 step: 887, loss is 0.1028214767575264\n",
      "epoch: 9 step: 888, loss is 0.001564801437780261\n",
      "epoch: 9 step: 889, loss is 7.655657100258395e-05\n",
      "epoch: 9 step: 890, loss is 0.0007860995829105377\n",
      "epoch: 9 step: 891, loss is 0.04277164861559868\n",
      "epoch: 9 step: 892, loss is 0.03956674784421921\n",
      "epoch: 9 step: 893, loss is 9.909623258863576e-06\n",
      "epoch: 9 step: 894, loss is 6.325889262370765e-05\n",
      "epoch: 9 step: 895, loss is 0.0009795756777748466\n",
      "epoch: 9 step: 896, loss is 0.00749277975410223\n",
      "epoch: 9 step: 897, loss is 0.0002402855607215315\n",
      "epoch: 9 step: 898, loss is 0.00027470223722048104\n",
      "epoch: 9 step: 899, loss is 0.0006854971288703382\n",
      "epoch: 9 step: 900, loss is 0.004551532678306103\n",
      "epoch: 9 step: 901, loss is 0.00038563497946597636\n",
      "epoch: 9 step: 902, loss is 0.0012807321036234498\n",
      "epoch: 9 step: 903, loss is 0.009560436941683292\n",
      "epoch: 9 step: 904, loss is 0.0005416848580352962\n",
      "epoch: 9 step: 905, loss is 0.00041822108323685825\n",
      "epoch: 9 step: 906, loss is 0.012620493769645691\n",
      "epoch: 9 step: 907, loss is 0.014201948419213295\n",
      "epoch: 9 step: 908, loss is 0.007954210974276066\n",
      "epoch: 9 step: 909, loss is 0.09519390016794205\n",
      "epoch: 9 step: 910, loss is 0.007544375490397215\n",
      "epoch: 9 step: 911, loss is 0.0008358004270121455\n",
      "epoch: 9 step: 912, loss is 0.00013727978512179106\n",
      "epoch: 9 step: 913, loss is 0.005289954133331776\n",
      "epoch: 9 step: 914, loss is 0.044667262583971024\n",
      "epoch: 9 step: 915, loss is 0.042912375181913376\n",
      "epoch: 9 step: 916, loss is 0.0004516600747592747\n",
      "epoch: 9 step: 917, loss is 0.010319123044610023\n",
      "epoch: 9 step: 918, loss is 0.0038087412249296904\n",
      "epoch: 9 step: 919, loss is 0.0037579727359116077\n",
      "epoch: 9 step: 920, loss is 0.0009093040134757757\n",
      "epoch: 9 step: 921, loss is 0.003541684942319989\n",
      "epoch: 9 step: 922, loss is 0.012862900272011757\n",
      "epoch: 9 step: 923, loss is 0.0267537422478199\n",
      "epoch: 9 step: 924, loss is 0.004931367468088865\n",
      "epoch: 9 step: 925, loss is 0.013042770326137543\n",
      "epoch: 9 step: 926, loss is 0.007123623508960009\n",
      "epoch: 9 step: 927, loss is 0.00037012918619439006\n",
      "epoch: 9 step: 928, loss is 0.00013453001156449318\n",
      "epoch: 9 step: 929, loss is 0.12958896160125732\n",
      "epoch: 9 step: 930, loss is 0.017697051167488098\n",
      "epoch: 9 step: 931, loss is 0.03056309185922146\n",
      "epoch: 9 step: 932, loss is 0.00010771803499665111\n",
      "epoch: 9 step: 933, loss is 0.0027361377142369747\n",
      "epoch: 9 step: 934, loss is 0.0010930330026894808\n",
      "epoch: 9 step: 935, loss is 0.0002009490126511082\n",
      "epoch: 9 step: 936, loss is 0.0006360780680552125\n",
      "epoch: 9 step: 937, loss is 0.0009869878413155675\n",
      "epoch: 9 step: 938, loss is 0.00048138338024728\n",
      "epoch: 9 step: 939, loss is 0.003478554543107748\n",
      "epoch: 9 step: 940, loss is 0.010872739367187023\n",
      "epoch: 9 step: 941, loss is 0.00290379929356277\n",
      "epoch: 9 step: 942, loss is 0.005754785146564245\n",
      "epoch: 9 step: 943, loss is 0.0003264061233494431\n",
      "epoch: 9 step: 944, loss is 0.00022850290406495333\n",
      "epoch: 9 step: 945, loss is 0.003948127385228872\n",
      "epoch: 9 step: 946, loss is 0.001408267067745328\n",
      "epoch: 9 step: 947, loss is 0.0006270820158533752\n",
      "epoch: 9 step: 948, loss is 0.07642791420221329\n",
      "epoch: 9 step: 949, loss is 0.0871119350194931\n",
      "epoch: 9 step: 950, loss is 1.698259984550532e-05\n",
      "epoch: 9 step: 951, loss is 0.0001723891618894413\n",
      "epoch: 9 step: 952, loss is 0.001890922081656754\n",
      "epoch: 9 step: 953, loss is 0.0006122731138020754\n",
      "epoch: 9 step: 954, loss is 0.03203585743904114\n",
      "epoch: 9 step: 955, loss is 0.004124138504266739\n",
      "epoch: 9 step: 956, loss is 0.01270424947142601\n",
      "epoch: 9 step: 957, loss is 0.0017064748099073768\n",
      "epoch: 9 step: 958, loss is 0.00019825970230158418\n",
      "epoch: 9 step: 959, loss is 5.510677146958187e-05\n",
      "epoch: 9 step: 960, loss is 0.02581476792693138\n",
      "epoch: 9 step: 961, loss is 0.24666886031627655\n",
      "epoch: 9 step: 962, loss is 0.002200586488470435\n",
      "epoch: 9 step: 963, loss is 0.006828185170888901\n",
      "epoch: 9 step: 964, loss is 0.009648553095757961\n",
      "epoch: 9 step: 965, loss is 0.02449887990951538\n",
      "epoch: 9 step: 966, loss is 0.0015774774365127087\n",
      "epoch: 9 step: 967, loss is 0.0003311023465357721\n",
      "epoch: 9 step: 968, loss is 0.016021449118852615\n",
      "epoch: 9 step: 969, loss is 0.001115676830522716\n",
      "epoch: 9 step: 970, loss is 0.0006890170043334365\n",
      "epoch: 9 step: 971, loss is 1.8054732208838686e-05\n",
      "epoch: 9 step: 972, loss is 0.02745319902896881\n",
      "epoch: 9 step: 973, loss is 0.15819257497787476\n",
      "epoch: 9 step: 974, loss is 0.0008300119079649448\n",
      "epoch: 9 step: 975, loss is 0.0002437479852233082\n",
      "epoch: 9 step: 976, loss is 0.0007675873348489404\n",
      "epoch: 9 step: 977, loss is 0.0001709625794319436\n",
      "epoch: 9 step: 978, loss is 0.0018992372788488865\n",
      "epoch: 9 step: 979, loss is 5.793101536255563e-06\n",
      "epoch: 9 step: 980, loss is 0.002413363428786397\n",
      "epoch: 9 step: 981, loss is 0.019833818078041077\n",
      "epoch: 9 step: 982, loss is 0.17320865392684937\n",
      "epoch: 9 step: 983, loss is 0.0004607242008205503\n",
      "epoch: 9 step: 984, loss is 0.0034508223179727793\n",
      "epoch: 9 step: 985, loss is 0.00017355829186271876\n",
      "epoch: 9 step: 986, loss is 0.0005279545439407229\n",
      "epoch: 9 step: 987, loss is 3.7910060200374573e-05\n",
      "epoch: 9 step: 988, loss is 8.313667785841972e-05\n",
      "epoch: 9 step: 989, loss is 0.00018413938232697546\n",
      "epoch: 9 step: 990, loss is 0.0006080844323150814\n",
      "epoch: 9 step: 991, loss is 0.0014209020882844925\n",
      "epoch: 9 step: 992, loss is 0.22101913392543793\n",
      "epoch: 9 step: 993, loss is 0.0008895184728316963\n",
      "epoch: 9 step: 994, loss is 0.0034811801742762327\n",
      "epoch: 9 step: 995, loss is 0.014320764690637589\n",
      "epoch: 9 step: 996, loss is 0.0003358011308591813\n",
      "epoch: 9 step: 997, loss is 0.0002191918611060828\n",
      "epoch: 9 step: 998, loss is 0.0023826598189771175\n",
      "epoch: 9 step: 999, loss is 0.0007466458482667804\n",
      "epoch: 9 step: 1000, loss is 1.6292629879899323e-05\n",
      "epoch: 9 step: 1001, loss is 0.056561026722192764\n",
      "epoch: 9 step: 1002, loss is 0.007092411629855633\n",
      "epoch: 9 step: 1003, loss is 0.10111311078071594\n",
      "epoch: 9 step: 1004, loss is 0.00993415154516697\n",
      "epoch: 9 step: 1005, loss is 0.0013171244645491242\n",
      "epoch: 9 step: 1006, loss is 0.00037659265217371285\n",
      "epoch: 9 step: 1007, loss is 0.013613599352538586\n",
      "epoch: 9 step: 1008, loss is 0.0025596192572265863\n",
      "epoch: 9 step: 1009, loss is 0.001159433275461197\n",
      "epoch: 9 step: 1010, loss is 0.02222784049808979\n",
      "epoch: 9 step: 1011, loss is 0.09833983331918716\n",
      "epoch: 9 step: 1012, loss is 0.001252188696525991\n",
      "epoch: 9 step: 1013, loss is 0.00047077113413251936\n",
      "epoch: 9 step: 1014, loss is 0.0219705943018198\n",
      "epoch: 9 step: 1015, loss is 0.027162572368979454\n",
      "epoch: 9 step: 1016, loss is 0.005914780311286449\n",
      "epoch: 9 step: 1017, loss is 0.00010621307592373341\n",
      "epoch: 9 step: 1018, loss is 0.0006754789501428604\n",
      "epoch: 9 step: 1019, loss is 0.0288094375282526\n",
      "epoch: 9 step: 1020, loss is 0.00026744496426545084\n",
      "epoch: 9 step: 1021, loss is 0.0008661348838359118\n",
      "epoch: 9 step: 1022, loss is 0.05322050303220749\n",
      "epoch: 9 step: 1023, loss is 0.0007332950481213629\n",
      "epoch: 9 step: 1024, loss is 0.022853584960103035\n",
      "epoch: 9 step: 1025, loss is 0.010670931078493595\n",
      "epoch: 9 step: 1026, loss is 0.0007792599499225616\n",
      "epoch: 9 step: 1027, loss is 0.034340959042310715\n",
      "epoch: 9 step: 1028, loss is 0.002581376116722822\n",
      "epoch: 9 step: 1029, loss is 0.0006200234638527036\n",
      "epoch: 9 step: 1030, loss is 1.910625178425107e-05\n",
      "epoch: 9 step: 1031, loss is 1.4089676369621884e-05\n",
      "epoch: 9 step: 1032, loss is 0.0001897039619507268\n",
      "epoch: 9 step: 1033, loss is 0.0003924804332200438\n",
      "epoch: 9 step: 1034, loss is 0.000766265788115561\n",
      "epoch: 9 step: 1035, loss is 0.0003269800217822194\n",
      "epoch: 9 step: 1036, loss is 0.03970593214035034\n",
      "epoch: 9 step: 1037, loss is 7.73711726651527e-05\n",
      "epoch: 9 step: 1038, loss is 0.0032062623649835587\n",
      "epoch: 9 step: 1039, loss is 0.039401501417160034\n",
      "epoch: 9 step: 1040, loss is 0.004705388098955154\n",
      "epoch: 9 step: 1041, loss is 0.0014732865383848548\n",
      "epoch: 9 step: 1042, loss is 3.839697455987334e-05\n",
      "epoch: 9 step: 1043, loss is 0.00013767427299171686\n",
      "epoch: 9 step: 1044, loss is 0.0019531843718141317\n",
      "epoch: 9 step: 1045, loss is 0.03944738209247589\n",
      "epoch: 9 step: 1046, loss is 0.006543527822941542\n",
      "epoch: 9 step: 1047, loss is 0.004745680373162031\n",
      "epoch: 9 step: 1048, loss is 0.0021139243617653847\n",
      "epoch: 9 step: 1049, loss is 0.014954963698983192\n",
      "epoch: 9 step: 1050, loss is 0.019137876108288765\n",
      "epoch: 9 step: 1051, loss is 1.829552638810128e-05\n",
      "epoch: 9 step: 1052, loss is 0.0011065793223679066\n",
      "epoch: 9 step: 1053, loss is 0.012654058635234833\n",
      "epoch: 9 step: 1054, loss is 0.0002302124339621514\n",
      "epoch: 9 step: 1055, loss is 0.02759568952023983\n",
      "epoch: 9 step: 1056, loss is 0.0261926781386137\n",
      "epoch: 9 step: 1057, loss is 0.007738181389868259\n",
      "epoch: 9 step: 1058, loss is 0.0022368761710822582\n",
      "epoch: 9 step: 1059, loss is 0.00023341504856944084\n",
      "epoch: 9 step: 1060, loss is 0.0005664616473950446\n",
      "epoch: 9 step: 1061, loss is 7.888483378337696e-05\n",
      "epoch: 9 step: 1062, loss is 0.00044091662857681513\n",
      "epoch: 9 step: 1063, loss is 0.08309735357761383\n",
      "epoch: 9 step: 1064, loss is 0.0005239956662990153\n",
      "epoch: 9 step: 1065, loss is 0.00561798270791769\n",
      "epoch: 9 step: 1066, loss is 0.0002252730482723564\n",
      "epoch: 9 step: 1067, loss is 1.274680471397005e-05\n",
      "epoch: 9 step: 1068, loss is 7.573239417979494e-05\n",
      "epoch: 9 step: 1069, loss is 6.607192335650325e-05\n",
      "epoch: 9 step: 1070, loss is 0.0009538268786855042\n",
      "epoch: 9 step: 1071, loss is 7.91983911767602e-05\n",
      "epoch: 9 step: 1072, loss is 0.0029995448421686888\n",
      "epoch: 9 step: 1073, loss is 0.06638659536838531\n",
      "epoch: 9 step: 1074, loss is 0.015096100978553295\n",
      "epoch: 9 step: 1075, loss is 0.000171972656971775\n",
      "epoch: 9 step: 1076, loss is 0.00130801135674119\n",
      "epoch: 9 step: 1077, loss is 0.004824853036552668\n",
      "epoch: 9 step: 1078, loss is 0.00014112806820776314\n",
      "epoch: 9 step: 1079, loss is 0.09420356154441833\n",
      "epoch: 9 step: 1080, loss is 0.0017931335605680943\n",
      "epoch: 9 step: 1081, loss is 0.01991548016667366\n",
      "epoch: 9 step: 1082, loss is 4.663393701775931e-05\n",
      "epoch: 9 step: 1083, loss is 0.0012230768334120512\n",
      "epoch: 9 step: 1084, loss is 0.004309488460421562\n",
      "epoch: 9 step: 1085, loss is 0.011490627191960812\n",
      "epoch: 9 step: 1086, loss is 0.005520332138985395\n",
      "epoch: 9 step: 1087, loss is 7.894151167420205e-06\n",
      "epoch: 9 step: 1088, loss is 0.018905963748693466\n",
      "epoch: 9 step: 1089, loss is 0.022116120904684067\n",
      "epoch: 9 step: 1090, loss is 0.0010609582532197237\n",
      "epoch: 9 step: 1091, loss is 0.007987380027770996\n",
      "epoch: 9 step: 1092, loss is 0.01046046707779169\n",
      "epoch: 9 step: 1093, loss is 0.15590734779834747\n",
      "epoch: 9 step: 1094, loss is 0.008937513455748558\n",
      "epoch: 9 step: 1095, loss is 0.026248250156641006\n",
      "epoch: 9 step: 1096, loss is 0.0018232495058327913\n",
      "epoch: 9 step: 1097, loss is 0.029000606387853622\n",
      "epoch: 9 step: 1098, loss is 0.0013560723746195436\n",
      "epoch: 9 step: 1099, loss is 1.9517743567121215e-05\n",
      "epoch: 9 step: 1100, loss is 0.007203234825283289\n",
      "epoch: 9 step: 1101, loss is 0.0002989535278175026\n",
      "epoch: 9 step: 1102, loss is 0.004475742112845182\n",
      "epoch: 9 step: 1103, loss is 0.00032058899523690343\n",
      "epoch: 9 step: 1104, loss is 0.0975944772362709\n",
      "epoch: 9 step: 1105, loss is 0.008104628883302212\n",
      "epoch: 9 step: 1106, loss is 0.0004944388056173921\n",
      "epoch: 9 step: 1107, loss is 0.001431189477443695\n",
      "epoch: 9 step: 1108, loss is 5.318957846611738e-05\n",
      "epoch: 9 step: 1109, loss is 0.03572043031454086\n",
      "epoch: 9 step: 1110, loss is 0.0005069974577054381\n",
      "epoch: 9 step: 1111, loss is 0.00040923175401985645\n",
      "epoch: 9 step: 1112, loss is 0.13030877709388733\n",
      "epoch: 9 step: 1113, loss is 0.001377997687086463\n",
      "epoch: 9 step: 1114, loss is 0.0008356136968359351\n",
      "epoch: 9 step: 1115, loss is 6.110766844358295e-05\n",
      "epoch: 9 step: 1116, loss is 0.0011635591508820653\n",
      "epoch: 9 step: 1117, loss is 0.010671812109649181\n",
      "epoch: 9 step: 1118, loss is 0.00011961520795011893\n",
      "epoch: 9 step: 1119, loss is 0.029197098687291145\n",
      "epoch: 9 step: 1120, loss is 0.0011953178327530622\n",
      "epoch: 9 step: 1121, loss is 0.00042542029405012727\n",
      "epoch: 9 step: 1122, loss is 0.0017174755921587348\n",
      "epoch: 9 step: 1123, loss is 0.0011942227138206363\n",
      "epoch: 9 step: 1124, loss is 0.009179506450891495\n",
      "epoch: 9 step: 1125, loss is 0.0017583116423338652\n",
      "epoch: 9 step: 1126, loss is 0.0021396782249212265\n",
      "epoch: 9 step: 1127, loss is 0.000946085317991674\n",
      "epoch: 9 step: 1128, loss is 0.019495172426104546\n",
      "epoch: 9 step: 1129, loss is 0.0003592772991396487\n",
      "epoch: 9 step: 1130, loss is 0.0010422260966151953\n",
      "epoch: 9 step: 1131, loss is 9.101638715947047e-05\n",
      "epoch: 9 step: 1132, loss is 0.00034419482108205557\n",
      "epoch: 9 step: 1133, loss is 0.002031513024121523\n",
      "epoch: 9 step: 1134, loss is 0.0036409245803952217\n",
      "epoch: 9 step: 1135, loss is 0.08425992727279663\n",
      "epoch: 9 step: 1136, loss is 0.005227080546319485\n",
      "epoch: 9 step: 1137, loss is 0.00018729516887106001\n",
      "epoch: 9 step: 1138, loss is 0.00010041074710898101\n",
      "epoch: 9 step: 1139, loss is 7.034995360299945e-05\n",
      "epoch: 9 step: 1140, loss is 0.0009566621156409383\n",
      "epoch: 9 step: 1141, loss is 7.705646567046642e-05\n",
      "epoch: 9 step: 1142, loss is 0.0036210762336850166\n",
      "epoch: 9 step: 1143, loss is 0.0005129588535055518\n",
      "epoch: 9 step: 1144, loss is 0.03499550744891167\n",
      "epoch: 9 step: 1145, loss is 0.0006394552765414119\n",
      "epoch: 9 step: 1146, loss is 0.006542616989463568\n",
      "epoch: 9 step: 1147, loss is 0.0044603799469769\n",
      "epoch: 9 step: 1148, loss is 0.008241153322160244\n",
      "epoch: 9 step: 1149, loss is 0.010567444376647472\n",
      "epoch: 9 step: 1150, loss is 0.0027430725749582052\n",
      "epoch: 9 step: 1151, loss is 0.056433986872434616\n",
      "epoch: 9 step: 1152, loss is 7.188293238868937e-05\n",
      "epoch: 9 step: 1153, loss is 0.0002247960219392553\n",
      "epoch: 9 step: 1154, loss is 0.001596000394783914\n",
      "epoch: 9 step: 1155, loss is 1.1897581316588912e-05\n",
      "epoch: 9 step: 1156, loss is 0.09827218949794769\n",
      "epoch: 9 step: 1157, loss is 0.012228707782924175\n",
      "epoch: 9 step: 1158, loss is 0.0001788075314834714\n",
      "epoch: 9 step: 1159, loss is 0.0024674157612025738\n",
      "epoch: 9 step: 1160, loss is 0.004083134699612856\n",
      "epoch: 9 step: 1161, loss is 0.00029746966902166605\n",
      "epoch: 9 step: 1162, loss is 0.028878360986709595\n",
      "epoch: 9 step: 1163, loss is 0.06271146237850189\n",
      "epoch: 9 step: 1164, loss is 0.10911941528320312\n",
      "epoch: 9 step: 1165, loss is 0.000597310543525964\n",
      "epoch: 9 step: 1166, loss is 0.0004904181696474552\n",
      "epoch: 9 step: 1167, loss is 0.0008447239524684846\n",
      "epoch: 9 step: 1168, loss is 3.7006684578955173e-05\n",
      "epoch: 9 step: 1169, loss is 0.0015914227114990354\n",
      "epoch: 9 step: 1170, loss is 0.0010505622485652566\n",
      "epoch: 9 step: 1171, loss is 0.0005675196298398077\n",
      "epoch: 9 step: 1172, loss is 0.003368167206645012\n",
      "epoch: 9 step: 1173, loss is 0.0001951711456058547\n",
      "epoch: 9 step: 1174, loss is 0.014134802855551243\n",
      "epoch: 9 step: 1175, loss is 8.379556675208732e-05\n",
      "epoch: 9 step: 1176, loss is 0.00027035936363972723\n",
      "epoch: 9 step: 1177, loss is 0.004742049146443605\n",
      "epoch: 9 step: 1178, loss is 0.004721623845398426\n",
      "epoch: 9 step: 1179, loss is 0.000608290545642376\n",
      "epoch: 9 step: 1180, loss is 0.0017617688281461596\n",
      "epoch: 9 step: 1181, loss is 6.600010965485126e-05\n",
      "epoch: 9 step: 1182, loss is 0.00033154181437566876\n",
      "epoch: 9 step: 1183, loss is 0.00034908001543954015\n",
      "epoch: 9 step: 1184, loss is 0.0007502174703404307\n",
      "epoch: 9 step: 1185, loss is 0.00010505385580472648\n",
      "epoch: 9 step: 1186, loss is 0.04337795078754425\n",
      "epoch: 9 step: 1187, loss is 0.00048082403372973204\n",
      "epoch: 9 step: 1188, loss is 0.0004919896018691361\n",
      "epoch: 9 step: 1189, loss is 0.0008219853625632823\n",
      "epoch: 9 step: 1190, loss is 0.009318491443991661\n",
      "epoch: 9 step: 1191, loss is 0.0038897194899618626\n",
      "epoch: 9 step: 1192, loss is 0.37639981508255005\n",
      "epoch: 9 step: 1193, loss is 8.796142356004566e-05\n",
      "epoch: 9 step: 1194, loss is 0.12097356468439102\n",
      "epoch: 9 step: 1195, loss is 0.0002953510847873986\n",
      "epoch: 9 step: 1196, loss is 0.0009029424982145429\n",
      "epoch: 9 step: 1197, loss is 0.010885398834943771\n",
      "epoch: 9 step: 1198, loss is 0.0010388842783868313\n",
      "epoch: 9 step: 1199, loss is 0.006551199126988649\n",
      "epoch: 9 step: 1200, loss is 0.0024426975287497044\n",
      "epoch: 9 step: 1201, loss is 0.00035936382482759655\n",
      "epoch: 9 step: 1202, loss is 0.01549069955945015\n",
      "epoch: 9 step: 1203, loss is 0.00042903341818600893\n",
      "epoch: 9 step: 1204, loss is 0.0024987468495965004\n",
      "epoch: 9 step: 1205, loss is 0.010413160547614098\n",
      "epoch: 9 step: 1206, loss is 0.0001008184117381461\n",
      "epoch: 9 step: 1207, loss is 0.0020290266256779432\n",
      "epoch: 9 step: 1208, loss is 0.008623254485428333\n",
      "epoch: 9 step: 1209, loss is 0.0007974888430908322\n",
      "epoch: 9 step: 1210, loss is 0.004015758167952299\n",
      "epoch: 9 step: 1211, loss is 0.0015525822527706623\n",
      "epoch: 9 step: 1212, loss is 0.03505932539701462\n",
      "epoch: 9 step: 1213, loss is 0.016471873968839645\n",
      "epoch: 9 step: 1214, loss is 0.0004203791031613946\n",
      "epoch: 9 step: 1215, loss is 0.0019100577337667346\n",
      "epoch: 9 step: 1216, loss is 0.0009168210672214627\n",
      "epoch: 9 step: 1217, loss is 0.06752247363328934\n",
      "epoch: 9 step: 1218, loss is 0.014219922013580799\n",
      "epoch: 9 step: 1219, loss is 0.000450815015938133\n",
      "epoch: 9 step: 1220, loss is 0.00014673963596578687\n",
      "epoch: 9 step: 1221, loss is 0.0007255017990246415\n",
      "epoch: 9 step: 1222, loss is 0.020514044910669327\n",
      "epoch: 9 step: 1223, loss is 0.0009096822468563914\n",
      "epoch: 9 step: 1224, loss is 0.0007355223642662168\n",
      "epoch: 9 step: 1225, loss is 0.0636061504483223\n",
      "epoch: 9 step: 1226, loss is 0.0003981244517490268\n",
      "epoch: 9 step: 1227, loss is 0.000259686989011243\n",
      "epoch: 9 step: 1228, loss is 0.000136700487928465\n",
      "epoch: 9 step: 1229, loss is 0.00023409479763358831\n",
      "epoch: 9 step: 1230, loss is 0.08987604081630707\n",
      "epoch: 9 step: 1231, loss is 0.042768172919750214\n",
      "epoch: 9 step: 1232, loss is 0.0004000004264526069\n",
      "epoch: 9 step: 1233, loss is 0.0005683065392076969\n",
      "epoch: 9 step: 1234, loss is 0.010754230432212353\n",
      "epoch: 9 step: 1235, loss is 0.0005330684944055974\n",
      "epoch: 9 step: 1236, loss is 0.00039252705755643547\n",
      "epoch: 9 step: 1237, loss is 0.0025534348096698523\n",
      "epoch: 9 step: 1238, loss is 7.183655543485656e-05\n",
      "epoch: 9 step: 1239, loss is 1.6182004401343875e-05\n",
      "epoch: 9 step: 1240, loss is 0.0055558281019330025\n",
      "epoch: 9 step: 1241, loss is 1.0727678272814956e-05\n",
      "epoch: 9 step: 1242, loss is 0.00011040169192710891\n",
      "epoch: 9 step: 1243, loss is 0.0036954956594854593\n",
      "epoch: 9 step: 1244, loss is 0.005746627226471901\n",
      "epoch: 9 step: 1245, loss is 0.005540008191019297\n",
      "epoch: 9 step: 1246, loss is 3.7428675568662584e-05\n",
      "epoch: 9 step: 1247, loss is 0.074659563601017\n",
      "epoch: 9 step: 1248, loss is 0.00363514618948102\n",
      "epoch: 9 step: 1249, loss is 0.2264982908964157\n",
      "epoch: 9 step: 1250, loss is 0.0021072132512927055\n",
      "epoch: 9 step: 1251, loss is 0.00010103698150487617\n",
      "epoch: 9 step: 1252, loss is 0.0016507189720869064\n",
      "epoch: 9 step: 1253, loss is 0.0004040702187921852\n",
      "epoch: 9 step: 1254, loss is 0.0290643610060215\n",
      "epoch: 9 step: 1255, loss is 0.0018827038584277034\n",
      "epoch: 9 step: 1256, loss is 0.0012295456836000085\n",
      "epoch: 9 step: 1257, loss is 0.007868552580475807\n",
      "epoch: 9 step: 1258, loss is 0.020751090720295906\n",
      "epoch: 9 step: 1259, loss is 0.016827918589115143\n",
      "epoch: 9 step: 1260, loss is 0.005667140707373619\n",
      "epoch: 9 step: 1261, loss is 0.0044687786139547825\n",
      "epoch: 9 step: 1262, loss is 0.07715768367052078\n",
      "epoch: 9 step: 1263, loss is 0.0075346012599766254\n",
      "epoch: 9 step: 1264, loss is 0.0046644266694784164\n",
      "epoch: 9 step: 1265, loss is 0.00045005453284829855\n",
      "epoch: 9 step: 1266, loss is 0.0396030992269516\n",
      "epoch: 9 step: 1267, loss is 0.04903525859117508\n",
      "epoch: 9 step: 1268, loss is 0.0010640890104696155\n",
      "epoch: 9 step: 1269, loss is 0.0024564361665397882\n",
      "epoch: 9 step: 1270, loss is 0.0004535728949122131\n",
      "epoch: 9 step: 1271, loss is 0.05881990119814873\n",
      "epoch: 9 step: 1272, loss is 0.06876165419816971\n",
      "epoch: 9 step: 1273, loss is 0.003907611593604088\n",
      "epoch: 9 step: 1274, loss is 0.0008631143718957901\n",
      "epoch: 9 step: 1275, loss is 0.0005886732251383364\n",
      "epoch: 9 step: 1276, loss is 0.0003425852919463068\n",
      "epoch: 9 step: 1277, loss is 0.019268469884991646\n",
      "epoch: 9 step: 1278, loss is 0.0017872486496344209\n",
      "epoch: 9 step: 1279, loss is 0.00040249040466733277\n",
      "epoch: 9 step: 1280, loss is 0.010536475107073784\n",
      "epoch: 9 step: 1281, loss is 0.0011172890663146973\n",
      "epoch: 9 step: 1282, loss is 0.0015500709414482117\n",
      "epoch: 9 step: 1283, loss is 0.0014753860887140036\n",
      "epoch: 9 step: 1284, loss is 0.00042549840873107314\n",
      "epoch: 9 step: 1285, loss is 0.020865023136138916\n",
      "epoch: 9 step: 1286, loss is 0.00037845748011022806\n",
      "epoch: 9 step: 1287, loss is 0.0012546897633001208\n",
      "epoch: 9 step: 1288, loss is 0.002135364105924964\n",
      "epoch: 9 step: 1289, loss is 0.003469698131084442\n",
      "epoch: 9 step: 1290, loss is 0.027688682079315186\n",
      "epoch: 9 step: 1291, loss is 0.07232049107551575\n",
      "epoch: 9 step: 1292, loss is 0.026831990107893944\n",
      "epoch: 9 step: 1293, loss is 0.004190848208963871\n",
      "epoch: 9 step: 1294, loss is 0.0077428510412573814\n",
      "epoch: 9 step: 1295, loss is 7.474295125575736e-05\n",
      "epoch: 9 step: 1296, loss is 0.012497592717409134\n",
      "epoch: 9 step: 1297, loss is 0.003223258536309004\n",
      "epoch: 9 step: 1298, loss is 0.011344565078616142\n",
      "epoch: 9 step: 1299, loss is 0.0007183972629718482\n",
      "epoch: 9 step: 1300, loss is 0.024979880079627037\n",
      "epoch: 9 step: 1301, loss is 0.0008103448781184852\n",
      "epoch: 9 step: 1302, loss is 8.731604611966759e-05\n",
      "epoch: 9 step: 1303, loss is 0.27848246693611145\n",
      "epoch: 9 step: 1304, loss is 0.000833486788906157\n",
      "epoch: 9 step: 1305, loss is 0.00029686279594898224\n",
      "epoch: 9 step: 1306, loss is 0.06029660254716873\n",
      "epoch: 9 step: 1307, loss is 0.00016140314983204007\n",
      "epoch: 9 step: 1308, loss is 0.001772135030478239\n",
      "epoch: 9 step: 1309, loss is 0.0001649511104915291\n",
      "epoch: 9 step: 1310, loss is 0.06206250935792923\n",
      "epoch: 9 step: 1311, loss is 0.00012243707897141576\n",
      "epoch: 9 step: 1312, loss is 0.0007864971412345767\n",
      "epoch: 9 step: 1313, loss is 0.0006167835672385991\n",
      "epoch: 9 step: 1314, loss is 0.0013419397873803973\n",
      "epoch: 9 step: 1315, loss is 0.00024956517154350877\n",
      "epoch: 9 step: 1316, loss is 0.00022264133440330625\n",
      "epoch: 9 step: 1317, loss is 0.00012509358930401504\n",
      "epoch: 9 step: 1318, loss is 0.0004770226660184562\n",
      "epoch: 9 step: 1319, loss is 0.03891335800290108\n",
      "epoch: 9 step: 1320, loss is 0.004215484019368887\n",
      "epoch: 9 step: 1321, loss is 7.119321526261047e-05\n",
      "epoch: 9 step: 1322, loss is 0.0030034317169338465\n",
      "epoch: 9 step: 1323, loss is 0.013072578236460686\n",
      "epoch: 9 step: 1324, loss is 0.17379066348075867\n",
      "epoch: 9 step: 1325, loss is 0.03187277540564537\n",
      "epoch: 9 step: 1326, loss is 0.008742566220462322\n",
      "epoch: 9 step: 1327, loss is 0.0003958563902415335\n",
      "epoch: 9 step: 1328, loss is 0.0004170908941887319\n",
      "epoch: 9 step: 1329, loss is 0.00041514699114486575\n",
      "epoch: 9 step: 1330, loss is 0.005166773684322834\n",
      "epoch: 9 step: 1331, loss is 0.00021305072004906833\n",
      "epoch: 9 step: 1332, loss is 0.00011585088941501454\n",
      "epoch: 9 step: 1333, loss is 0.004548699129372835\n",
      "epoch: 9 step: 1334, loss is 0.003233099589124322\n",
      "epoch: 9 step: 1335, loss is 0.008543710224330425\n",
      "epoch: 9 step: 1336, loss is 0.002215092768892646\n",
      "epoch: 9 step: 1337, loss is 0.0016115023754537106\n",
      "epoch: 9 step: 1338, loss is 0.00011967786122113466\n",
      "epoch: 9 step: 1339, loss is 3.8713726098649204e-05\n",
      "epoch: 9 step: 1340, loss is 0.05572963505983353\n",
      "epoch: 9 step: 1341, loss is 0.0007522506639361382\n",
      "epoch: 9 step: 1342, loss is 0.006549029611051083\n",
      "epoch: 9 step: 1343, loss is 0.01701115444302559\n",
      "epoch: 9 step: 1344, loss is 0.25453153252601624\n",
      "epoch: 9 step: 1345, loss is 0.014668116346001625\n",
      "epoch: 9 step: 1346, loss is 0.13137505948543549\n",
      "epoch: 9 step: 1347, loss is 8.663417247589678e-05\n",
      "epoch: 9 step: 1348, loss is 0.006131658796221018\n",
      "epoch: 9 step: 1349, loss is 0.10729580372571945\n",
      "epoch: 9 step: 1350, loss is 0.0070943511091172695\n",
      "epoch: 9 step: 1351, loss is 0.00032549267052672803\n",
      "epoch: 9 step: 1352, loss is 0.045246671885252\n",
      "epoch: 9 step: 1353, loss is 0.033106379210948944\n",
      "epoch: 9 step: 1354, loss is 0.0223141610622406\n",
      "epoch: 9 step: 1355, loss is 0.006325162015855312\n",
      "epoch: 9 step: 1356, loss is 0.0024515714030712843\n",
      "epoch: 9 step: 1357, loss is 0.009310347959399223\n",
      "epoch: 9 step: 1358, loss is 0.0005920945550315082\n",
      "epoch: 9 step: 1359, loss is 0.0009434332605451345\n",
      "epoch: 9 step: 1360, loss is 0.03668324649333954\n",
      "epoch: 9 step: 1361, loss is 0.00015654484741389751\n",
      "epoch: 9 step: 1362, loss is 0.001527154236100614\n",
      "epoch: 9 step: 1363, loss is 0.0008919804822653532\n",
      "epoch: 9 step: 1364, loss is 0.005813901778310537\n",
      "epoch: 9 step: 1365, loss is 0.0030648892279714346\n",
      "epoch: 9 step: 1366, loss is 0.044964708387851715\n",
      "epoch: 9 step: 1367, loss is 7.43982382118702e-05\n",
      "epoch: 9 step: 1368, loss is 0.0046051014214754105\n",
      "epoch: 9 step: 1369, loss is 0.00032476315391249955\n",
      "epoch: 9 step: 1370, loss is 0.0008450824534520507\n",
      "epoch: 9 step: 1371, loss is 0.021573228761553764\n",
      "epoch: 9 step: 1372, loss is 0.0068566324189305305\n",
      "epoch: 9 step: 1373, loss is 0.0021693012677133083\n",
      "epoch: 9 step: 1374, loss is 0.005067578516900539\n",
      "epoch: 9 step: 1375, loss is 0.034610550850629807\n",
      "epoch: 9 step: 1376, loss is 0.005049142520874739\n",
      "epoch: 9 step: 1377, loss is 0.012632723897695541\n",
      "epoch: 9 step: 1378, loss is 0.005540220066905022\n",
      "epoch: 9 step: 1379, loss is 0.001343497890047729\n",
      "epoch: 9 step: 1380, loss is 0.000974032562226057\n",
      "epoch: 9 step: 1381, loss is 0.0008156666299328208\n",
      "epoch: 9 step: 1382, loss is 0.06768153607845306\n",
      "epoch: 9 step: 1383, loss is 0.12734472751617432\n",
      "epoch: 9 step: 1384, loss is 0.003838007338345051\n",
      "epoch: 9 step: 1385, loss is 0.0016212814953178167\n",
      "epoch: 9 step: 1386, loss is 0.009364696219563484\n",
      "epoch: 9 step: 1387, loss is 0.0029721050523221493\n",
      "epoch: 9 step: 1388, loss is 0.0005694559658877552\n",
      "epoch: 9 step: 1389, loss is 0.00036554387770593166\n",
      "epoch: 9 step: 1390, loss is 0.0004786797217093408\n",
      "epoch: 9 step: 1391, loss is 0.0015029293717816472\n",
      "epoch: 9 step: 1392, loss is 0.0003094879211857915\n",
      "epoch: 9 step: 1393, loss is 0.004192089196294546\n",
      "epoch: 9 step: 1394, loss is 0.016513273119926453\n",
      "epoch: 9 step: 1395, loss is 0.006417371798306704\n",
      "epoch: 9 step: 1396, loss is 0.0016086350660771132\n",
      "epoch: 9 step: 1397, loss is 0.030782274901866913\n",
      "epoch: 9 step: 1398, loss is 0.0011263228952884674\n",
      "epoch: 9 step: 1399, loss is 0.2015945017337799\n",
      "epoch: 9 step: 1400, loss is 0.005056906025856733\n",
      "epoch: 9 step: 1401, loss is 0.007766791619360447\n",
      "epoch: 9 step: 1402, loss is 0.004504268057644367\n",
      "epoch: 9 step: 1403, loss is 0.0013541682856157422\n",
      "epoch: 9 step: 1404, loss is 0.0004905601381324232\n",
      "epoch: 9 step: 1405, loss is 0.02033897116780281\n",
      "epoch: 9 step: 1406, loss is 0.0005846080603078008\n",
      "epoch: 9 step: 1407, loss is 0.0048713902942836285\n",
      "epoch: 9 step: 1408, loss is 0.009931433014571667\n",
      "epoch: 9 step: 1409, loss is 0.0012608258984982967\n",
      "epoch: 9 step: 1410, loss is 0.0050313412211835384\n",
      "epoch: 9 step: 1411, loss is 0.02750370278954506\n",
      "epoch: 9 step: 1412, loss is 0.003163457615301013\n",
      "epoch: 9 step: 1413, loss is 0.0013032141141593456\n",
      "epoch: 9 step: 1414, loss is 0.00015426235040649772\n",
      "epoch: 9 step: 1415, loss is 0.0762716755270958\n",
      "epoch: 9 step: 1416, loss is 0.0007427777745760977\n",
      "epoch: 9 step: 1417, loss is 0.006818231660872698\n",
      "epoch: 9 step: 1418, loss is 0.003998772706836462\n",
      "epoch: 9 step: 1419, loss is 0.06395496428012848\n",
      "epoch: 9 step: 1420, loss is 0.001531807822175324\n",
      "epoch: 9 step: 1421, loss is 0.014342281967401505\n",
      "epoch: 9 step: 1422, loss is 0.001749144634231925\n",
      "epoch: 9 step: 1423, loss is 0.0008651733514852822\n",
      "epoch: 9 step: 1424, loss is 0.00018583243945613503\n",
      "epoch: 9 step: 1425, loss is 0.00014615108375437558\n",
      "epoch: 9 step: 1426, loss is 0.0059592584148049355\n",
      "epoch: 9 step: 1427, loss is 0.054169267416000366\n",
      "epoch: 9 step: 1428, loss is 0.00522408215329051\n",
      "epoch: 9 step: 1429, loss is 0.00013262829452287406\n",
      "epoch: 9 step: 1430, loss is 0.10848966985940933\n",
      "epoch: 9 step: 1431, loss is 0.0013510837452486157\n",
      "epoch: 9 step: 1432, loss is 0.04077035188674927\n",
      "epoch: 9 step: 1433, loss is 0.0018804635619744658\n",
      "epoch: 9 step: 1434, loss is 0.0007258977275341749\n",
      "epoch: 9 step: 1435, loss is 0.00479896878823638\n",
      "epoch: 9 step: 1436, loss is 0.0024687182158231735\n",
      "epoch: 9 step: 1437, loss is 0.15258292853832245\n",
      "epoch: 9 step: 1438, loss is 0.02637435309588909\n",
      "epoch: 9 step: 1439, loss is 0.04746532067656517\n",
      "epoch: 9 step: 1440, loss is 0.0461796410381794\n",
      "epoch: 9 step: 1441, loss is 0.0024241339415311813\n",
      "epoch: 9 step: 1442, loss is 8.059261745074764e-05\n",
      "epoch: 9 step: 1443, loss is 0.00024411942285951227\n",
      "epoch: 9 step: 1444, loss is 4.481637733988464e-05\n",
      "epoch: 9 step: 1445, loss is 0.004271499812602997\n",
      "epoch: 9 step: 1446, loss is 0.015167011879384518\n",
      "epoch: 9 step: 1447, loss is 0.0015824491856619716\n",
      "epoch: 9 step: 1448, loss is 0.02891365811228752\n",
      "epoch: 9 step: 1449, loss is 0.001860276679508388\n",
      "epoch: 9 step: 1450, loss is 0.08607646822929382\n",
      "epoch: 9 step: 1451, loss is 0.00047683672164566815\n",
      "epoch: 9 step: 1452, loss is 0.01859075017273426\n",
      "epoch: 9 step: 1453, loss is 0.0809376984834671\n",
      "epoch: 9 step: 1454, loss is 0.0015544891357421875\n",
      "epoch: 9 step: 1455, loss is 0.0007446068339049816\n",
      "epoch: 9 step: 1456, loss is 0.061408039182424545\n",
      "epoch: 9 step: 1457, loss is 0.01274105440825224\n",
      "epoch: 9 step: 1458, loss is 0.012369083240628242\n",
      "epoch: 9 step: 1459, loss is 0.00433438690379262\n",
      "epoch: 9 step: 1460, loss is 0.06313628703355789\n",
      "epoch: 9 step: 1461, loss is 0.007349736988544464\n",
      "epoch: 9 step: 1462, loss is 5.867716026841663e-05\n",
      "epoch: 9 step: 1463, loss is 0.0015610631089657545\n",
      "epoch: 9 step: 1464, loss is 0.07200561463832855\n",
      "epoch: 9 step: 1465, loss is 0.001677880180068314\n",
      "epoch: 9 step: 1466, loss is 1.9190014427294955e-05\n",
      "epoch: 9 step: 1467, loss is 0.004585821181535721\n",
      "epoch: 9 step: 1468, loss is 3.2717998692533e-05\n",
      "epoch: 9 step: 1469, loss is 0.00991901196539402\n",
      "epoch: 9 step: 1470, loss is 0.031186169013381004\n",
      "epoch: 9 step: 1471, loss is 0.00017452474276069552\n",
      "epoch: 9 step: 1472, loss is 0.006780819967389107\n",
      "epoch: 9 step: 1473, loss is 0.0013618192169815302\n",
      "epoch: 9 step: 1474, loss is 0.040336351841688156\n",
      "epoch: 9 step: 1475, loss is 0.001882100128568709\n",
      "epoch: 9 step: 1476, loss is 0.0030300745274871588\n",
      "epoch: 9 step: 1477, loss is 0.005245675798505545\n",
      "epoch: 9 step: 1478, loss is 0.0007280302816070616\n",
      "epoch: 9 step: 1479, loss is 0.1427944302558899\n",
      "epoch: 9 step: 1480, loss is 0.015845369547605515\n",
      "epoch: 9 step: 1481, loss is 0.008188745938241482\n",
      "epoch: 9 step: 1482, loss is 0.013467608951032162\n",
      "epoch: 9 step: 1483, loss is 0.00019936631724704057\n",
      "epoch: 9 step: 1484, loss is 0.0018349329475313425\n",
      "epoch: 9 step: 1485, loss is 0.010823930613696575\n",
      "epoch: 9 step: 1486, loss is 0.025022070854902267\n",
      "epoch: 9 step: 1487, loss is 8.169514330802485e-05\n",
      "epoch: 9 step: 1488, loss is 0.0036331762094050646\n",
      "epoch: 9 step: 1489, loss is 2.8236338039278053e-05\n",
      "epoch: 9 step: 1490, loss is 0.004256772343069315\n",
      "epoch: 9 step: 1491, loss is 0.0016139388317242265\n",
      "epoch: 9 step: 1492, loss is 0.000322102103382349\n",
      "epoch: 9 step: 1493, loss is 0.00176433811429888\n",
      "epoch: 9 step: 1494, loss is 0.009481243789196014\n",
      "epoch: 9 step: 1495, loss is 0.00016981754743028432\n",
      "epoch: 9 step: 1496, loss is 0.0024850512854754925\n",
      "epoch: 9 step: 1497, loss is 0.02469283901154995\n",
      "epoch: 9 step: 1498, loss is 0.04803001508116722\n",
      "epoch: 9 step: 1499, loss is 0.00040270117460750043\n",
      "epoch: 9 step: 1500, loss is 0.06837920099496841\n",
      "epoch: 9 step: 1501, loss is 5.200621217227308e-06\n",
      "epoch: 9 step: 1502, loss is 8.505726145813242e-05\n",
      "epoch: 9 step: 1503, loss is 7.164809358073398e-05\n",
      "epoch: 9 step: 1504, loss is 0.03616577014327049\n",
      "epoch: 9 step: 1505, loss is 0.03484801948070526\n",
      "epoch: 9 step: 1506, loss is 0.024558160454034805\n",
      "epoch: 9 step: 1507, loss is 0.00459908926859498\n",
      "epoch: 9 step: 1508, loss is 0.012012109160423279\n",
      "epoch: 9 step: 1509, loss is 0.000410205393563956\n",
      "epoch: 9 step: 1510, loss is 0.0020597060211002827\n",
      "epoch: 9 step: 1511, loss is 0.11801806837320328\n",
      "epoch: 9 step: 1512, loss is 0.00014958897372707725\n",
      "epoch: 9 step: 1513, loss is 9.288686123909429e-05\n",
      "epoch: 9 step: 1514, loss is 0.00119096040725708\n",
      "epoch: 9 step: 1515, loss is 0.00011807830742327496\n",
      "epoch: 9 step: 1516, loss is 0.00026336987502872944\n",
      "epoch: 9 step: 1517, loss is 0.0002956468379124999\n",
      "epoch: 9 step: 1518, loss is 0.0030722948722541332\n",
      "epoch: 9 step: 1519, loss is 0.000216013373574242\n",
      "epoch: 9 step: 1520, loss is 8.496972441207618e-05\n",
      "epoch: 9 step: 1521, loss is 0.1035129576921463\n",
      "epoch: 9 step: 1522, loss is 0.14305399358272552\n",
      "epoch: 9 step: 1523, loss is 0.0010637342929840088\n",
      "epoch: 9 step: 1524, loss is 0.0062946719117462635\n",
      "epoch: 9 step: 1525, loss is 0.019686270505189896\n",
      "epoch: 9 step: 1526, loss is 0.002831080462783575\n",
      "epoch: 9 step: 1527, loss is 0.0017308202804997563\n",
      "epoch: 9 step: 1528, loss is 0.00023087234876584262\n",
      "epoch: 9 step: 1529, loss is 0.014721600338816643\n",
      "epoch: 9 step: 1530, loss is 0.00027958161081187427\n",
      "epoch: 9 step: 1531, loss is 4.261293724994175e-05\n",
      "epoch: 9 step: 1532, loss is 0.00024137295258697122\n",
      "epoch: 9 step: 1533, loss is 0.00016751463408581913\n",
      "epoch: 9 step: 1534, loss is 0.0017473672050982714\n",
      "epoch: 9 step: 1535, loss is 0.008158378303050995\n",
      "epoch: 9 step: 1536, loss is 0.057768695056438446\n",
      "epoch: 9 step: 1537, loss is 0.0025801402516663074\n",
      "epoch: 9 step: 1538, loss is 0.08719498664140701\n",
      "epoch: 9 step: 1539, loss is 0.11334463953971863\n",
      "epoch: 9 step: 1540, loss is 0.00024852246860973537\n",
      "epoch: 9 step: 1541, loss is 0.001842804136686027\n",
      "epoch: 9 step: 1542, loss is 0.030226917937397957\n",
      "epoch: 9 step: 1543, loss is 0.00040437455754727125\n",
      "epoch: 9 step: 1544, loss is 0.00013772526290267706\n",
      "epoch: 9 step: 1545, loss is 0.0003613074659369886\n",
      "epoch: 9 step: 1546, loss is 0.0017072769114747643\n",
      "epoch: 9 step: 1547, loss is 0.018049081787467003\n",
      "epoch: 9 step: 1548, loss is 0.010179447941482067\n",
      "epoch: 9 step: 1549, loss is 0.0012677356135100126\n",
      "epoch: 9 step: 1550, loss is 0.014592919498682022\n",
      "epoch: 9 step: 1551, loss is 0.09751205146312714\n",
      "epoch: 9 step: 1552, loss is 0.007564895786345005\n",
      "epoch: 9 step: 1553, loss is 0.03338593244552612\n",
      "epoch: 9 step: 1554, loss is 0.11139098554849625\n",
      "epoch: 9 step: 1555, loss is 0.0003394050872884691\n",
      "epoch: 9 step: 1556, loss is 0.003151676384732127\n",
      "epoch: 9 step: 1557, loss is 0.04510482773184776\n",
      "epoch: 9 step: 1558, loss is 0.0005385018885135651\n",
      "epoch: 9 step: 1559, loss is 0.030349330976605415\n",
      "epoch: 9 step: 1560, loss is 4.472869841265492e-05\n",
      "epoch: 9 step: 1561, loss is 0.06088456138968468\n",
      "epoch: 9 step: 1562, loss is 0.006007925141602755\n",
      "epoch: 9 step: 1563, loss is 1.4988091606937815e-05\n",
      "epoch: 9 step: 1564, loss is 0.057993341237306595\n",
      "epoch: 9 step: 1565, loss is 0.01708114519715309\n",
      "epoch: 9 step: 1566, loss is 0.00537063367664814\n",
      "epoch: 9 step: 1567, loss is 0.009741651825606823\n",
      "epoch: 9 step: 1568, loss is 4.55532172054518e-05\n",
      "epoch: 9 step: 1569, loss is 0.00042978583951480687\n",
      "epoch: 9 step: 1570, loss is 0.0005807462148368359\n",
      "epoch: 9 step: 1571, loss is 0.00030201932531781495\n",
      "epoch: 9 step: 1572, loss is 0.0037789391353726387\n",
      "epoch: 9 step: 1573, loss is 0.013192897662520409\n",
      "epoch: 9 step: 1574, loss is 0.008100869134068489\n",
      "epoch: 9 step: 1575, loss is 9.656384463596623e-06\n",
      "epoch: 9 step: 1576, loss is 0.00010874340659938753\n",
      "epoch: 9 step: 1577, loss is 0.0071963961236178875\n",
      "epoch: 9 step: 1578, loss is 5.8360135881230235e-05\n",
      "epoch: 9 step: 1579, loss is 0.00020094052888453007\n",
      "epoch: 9 step: 1580, loss is 0.006903780624270439\n",
      "epoch: 9 step: 1581, loss is 0.00016234320355579257\n",
      "epoch: 9 step: 1582, loss is 0.006480836309492588\n",
      "epoch: 9 step: 1583, loss is 0.0009066120255738497\n",
      "epoch: 9 step: 1584, loss is 0.00026516109937801957\n",
      "epoch: 9 step: 1585, loss is 0.12290968745946884\n",
      "epoch: 9 step: 1586, loss is 0.006584644317626953\n",
      "epoch: 9 step: 1587, loss is 0.09949008375406265\n",
      "epoch: 9 step: 1588, loss is 0.01714724488556385\n",
      "epoch: 9 step: 1589, loss is 0.003941173665225506\n",
      "epoch: 9 step: 1590, loss is 0.014052818529307842\n",
      "epoch: 9 step: 1591, loss is 0.00018309529696125537\n",
      "epoch: 9 step: 1592, loss is 9.716145541460719e-06\n",
      "epoch: 9 step: 1593, loss is 0.0002444430429022759\n",
      "epoch: 9 step: 1594, loss is 0.00029355662991292775\n",
      "epoch: 9 step: 1595, loss is 0.002814951818436384\n",
      "epoch: 9 step: 1596, loss is 0.000571404816582799\n",
      "epoch: 9 step: 1597, loss is 7.903417281340808e-05\n",
      "epoch: 9 step: 1598, loss is 0.0847504660487175\n",
      "epoch: 9 step: 1599, loss is 0.008849766105413437\n",
      "epoch: 9 step: 1600, loss is 9.89673935691826e-05\n",
      "epoch: 9 step: 1601, loss is 0.00021438795374706388\n",
      "epoch: 9 step: 1602, loss is 0.00808709766715765\n",
      "epoch: 9 step: 1603, loss is 0.0003648973652161658\n",
      "epoch: 9 step: 1604, loss is 0.005480734631419182\n",
      "epoch: 9 step: 1605, loss is 0.0002584907924756408\n",
      "epoch: 9 step: 1606, loss is 0.0006382738938555121\n",
      "epoch: 9 step: 1607, loss is 0.00027379568200558424\n",
      "epoch: 9 step: 1608, loss is 0.05627281218767166\n",
      "epoch: 9 step: 1609, loss is 0.001355198910459876\n",
      "epoch: 9 step: 1610, loss is 0.010677888989448547\n",
      "epoch: 9 step: 1611, loss is 0.0025646695867180824\n",
      "epoch: 9 step: 1612, loss is 0.0007370321545749903\n",
      "epoch: 9 step: 1613, loss is 0.00013588654110208154\n",
      "epoch: 9 step: 1614, loss is 0.00012611209240276366\n",
      "epoch: 9 step: 1615, loss is 0.00205064145848155\n",
      "epoch: 9 step: 1616, loss is 0.0005816692137159407\n",
      "epoch: 9 step: 1617, loss is 0.00013467248936649412\n",
      "epoch: 9 step: 1618, loss is 0.19501589238643646\n",
      "epoch: 9 step: 1619, loss is 0.015192938037216663\n",
      "epoch: 9 step: 1620, loss is 9.685321128927171e-05\n",
      "epoch: 9 step: 1621, loss is 0.0006392826908268034\n",
      "epoch: 9 step: 1622, loss is 0.0023449158761650324\n",
      "epoch: 9 step: 1623, loss is 0.07683263719081879\n",
      "epoch: 9 step: 1624, loss is 0.0001624462311156094\n",
      "epoch: 9 step: 1625, loss is 0.0032898690551519394\n",
      "epoch: 9 step: 1626, loss is 0.0003827131586149335\n",
      "epoch: 9 step: 1627, loss is 0.004524686373770237\n",
      "epoch: 9 step: 1628, loss is 4.37217386206612e-05\n",
      "epoch: 9 step: 1629, loss is 0.0003528655506670475\n",
      "epoch: 9 step: 1630, loss is 0.00036311266012489796\n",
      "epoch: 9 step: 1631, loss is 5.0564547564135864e-05\n",
      "epoch: 9 step: 1632, loss is 0.039037611335515976\n",
      "epoch: 9 step: 1633, loss is 0.016887115314602852\n",
      "epoch: 9 step: 1634, loss is 0.0004130122542846948\n",
      "epoch: 9 step: 1635, loss is 0.0007314044050872326\n",
      "epoch: 9 step: 1636, loss is 0.1077125295996666\n",
      "epoch: 9 step: 1637, loss is 0.0002039988321484998\n",
      "epoch: 9 step: 1638, loss is 4.354579505161382e-05\n",
      "epoch: 9 step: 1639, loss is 0.008448516018688679\n",
      "epoch: 9 step: 1640, loss is 0.1082235649228096\n",
      "epoch: 9 step: 1641, loss is 0.0062127485871315\n",
      "epoch: 9 step: 1642, loss is 0.00012912061356473714\n",
      "epoch: 9 step: 1643, loss is 0.0022154664620757103\n",
      "epoch: 9 step: 1644, loss is 0.022524239495396614\n",
      "epoch: 9 step: 1645, loss is 3.0218152460292913e-05\n",
      "epoch: 9 step: 1646, loss is 0.000668843335006386\n",
      "epoch: 9 step: 1647, loss is 0.02667652815580368\n",
      "epoch: 9 step: 1648, loss is 0.10458708554506302\n",
      "epoch: 9 step: 1649, loss is 0.004738002084195614\n",
      "epoch: 9 step: 1650, loss is 0.01273159310221672\n",
      "epoch: 9 step: 1651, loss is 0.1102077066898346\n",
      "epoch: 9 step: 1652, loss is 0.0010966702830046415\n",
      "epoch: 9 step: 1653, loss is 0.0002811785670928657\n",
      "epoch: 9 step: 1654, loss is 0.007101790979504585\n",
      "epoch: 9 step: 1655, loss is 0.0005320989293977618\n",
      "epoch: 9 step: 1656, loss is 0.06168212369084358\n",
      "epoch: 9 step: 1657, loss is 0.00015488006465602666\n",
      "epoch: 9 step: 1658, loss is 0.0004307727504055947\n",
      "epoch: 9 step: 1659, loss is 0.000554493279196322\n",
      "epoch: 9 step: 1660, loss is 0.1748969852924347\n",
      "epoch: 9 step: 1661, loss is 0.0009554935386404395\n",
      "epoch: 9 step: 1662, loss is 0.016998399049043655\n",
      "epoch: 9 step: 1663, loss is 0.0009371071355417371\n",
      "epoch: 9 step: 1664, loss is 0.08755217492580414\n",
      "epoch: 9 step: 1665, loss is 0.0004967661225236952\n",
      "epoch: 9 step: 1666, loss is 0.0013345321640372276\n",
      "epoch: 9 step: 1667, loss is 0.00091703818179667\n",
      "epoch: 9 step: 1668, loss is 0.004727429244667292\n",
      "epoch: 9 step: 1669, loss is 0.00029143787105567753\n",
      "epoch: 9 step: 1670, loss is 0.021568192169070244\n",
      "epoch: 9 step: 1671, loss is 3.9318772905971855e-05\n",
      "epoch: 9 step: 1672, loss is 0.0005265951040200889\n",
      "epoch: 9 step: 1673, loss is 0.0390797033905983\n",
      "epoch: 9 step: 1674, loss is 0.038642916828393936\n",
      "epoch: 9 step: 1675, loss is 0.0013156378408893943\n",
      "epoch: 9 step: 1676, loss is 0.0010222704149782658\n",
      "epoch: 9 step: 1677, loss is 0.001395543571561575\n",
      "epoch: 9 step: 1678, loss is 0.02790522761642933\n",
      "epoch: 9 step: 1679, loss is 0.00032726459903642535\n",
      "epoch: 9 step: 1680, loss is 0.06003138795495033\n",
      "epoch: 9 step: 1681, loss is 0.059625495225191116\n",
      "epoch: 9 step: 1682, loss is 0.00025351953809149563\n",
      "epoch: 9 step: 1683, loss is 0.021893814206123352\n",
      "epoch: 9 step: 1684, loss is 0.0036794680636376143\n",
      "epoch: 9 step: 1685, loss is 0.00455512385815382\n",
      "epoch: 9 step: 1686, loss is 4.8219731979770586e-05\n",
      "epoch: 9 step: 1687, loss is 0.009445288218557835\n",
      "epoch: 9 step: 1688, loss is 7.123727846192196e-05\n",
      "epoch: 9 step: 1689, loss is 0.018405454233288765\n",
      "epoch: 9 step: 1690, loss is 0.0017465738346800208\n",
      "epoch: 9 step: 1691, loss is 6.287291762419045e-05\n",
      "epoch: 9 step: 1692, loss is 0.0806330218911171\n",
      "epoch: 9 step: 1693, loss is 0.07948125153779984\n",
      "epoch: 9 step: 1694, loss is 0.0004921105573885143\n",
      "epoch: 9 step: 1695, loss is 0.002549296710640192\n",
      "epoch: 9 step: 1696, loss is 0.0004815916472580284\n",
      "epoch: 9 step: 1697, loss is 0.004140345845371485\n",
      "epoch: 9 step: 1698, loss is 0.016848187893629074\n",
      "epoch: 9 step: 1699, loss is 0.011394825764000416\n",
      "epoch: 9 step: 1700, loss is 0.10634885728359222\n",
      "epoch: 9 step: 1701, loss is 0.017600854858756065\n",
      "epoch: 9 step: 1702, loss is 4.772278043674305e-05\n",
      "epoch: 9 step: 1703, loss is 0.0036978041753172874\n",
      "epoch: 9 step: 1704, loss is 0.0028099489863961935\n",
      "epoch: 9 step: 1705, loss is 0.025790857151150703\n",
      "epoch: 9 step: 1706, loss is 0.002490733051672578\n",
      "epoch: 9 step: 1707, loss is 0.041918039321899414\n",
      "epoch: 9 step: 1708, loss is 0.010526065714657307\n",
      "epoch: 9 step: 1709, loss is 0.0005109476041980088\n",
      "epoch: 9 step: 1710, loss is 0.0020772668067365885\n",
      "epoch: 9 step: 1711, loss is 0.006044204346835613\n",
      "epoch: 9 step: 1712, loss is 0.240107923746109\n",
      "epoch: 9 step: 1713, loss is 0.03769471496343613\n",
      "epoch: 9 step: 1714, loss is 0.0004854078288190067\n",
      "epoch: 9 step: 1715, loss is 0.0009674328612163663\n",
      "epoch: 9 step: 1716, loss is 0.0005559974815696478\n",
      "epoch: 9 step: 1717, loss is 0.014563985168933868\n",
      "epoch: 9 step: 1718, loss is 0.0004965962143614888\n",
      "epoch: 9 step: 1719, loss is 0.008370205760002136\n",
      "epoch: 9 step: 1720, loss is 0.002274390310049057\n",
      "epoch: 9 step: 1721, loss is 0.00034637728822417557\n",
      "epoch: 9 step: 1722, loss is 0.002850221237167716\n",
      "epoch: 9 step: 1723, loss is 0.00633638771250844\n",
      "epoch: 9 step: 1724, loss is 0.006571484263986349\n",
      "epoch: 9 step: 1725, loss is 0.011294515803456306\n",
      "epoch: 9 step: 1726, loss is 0.06110117584466934\n",
      "epoch: 9 step: 1727, loss is 0.004982177168130875\n",
      "epoch: 9 step: 1728, loss is 0.00021967203065287322\n",
      "epoch: 9 step: 1729, loss is 0.02046905644237995\n",
      "epoch: 9 step: 1730, loss is 4.660241029341705e-05\n",
      "epoch: 9 step: 1731, loss is 0.0002618317084852606\n",
      "epoch: 9 step: 1732, loss is 0.010177920572459698\n",
      "epoch: 9 step: 1733, loss is 0.001969304634258151\n",
      "epoch: 9 step: 1734, loss is 0.01768459565937519\n",
      "epoch: 9 step: 1735, loss is 0.0011522648856043816\n",
      "epoch: 9 step: 1736, loss is 0.10151300579309464\n",
      "epoch: 9 step: 1737, loss is 0.000840009655803442\n",
      "epoch: 9 step: 1738, loss is 0.1653771847486496\n",
      "epoch: 9 step: 1739, loss is 0.007455817889422178\n",
      "epoch: 9 step: 1740, loss is 0.11326384544372559\n",
      "epoch: 9 step: 1741, loss is 0.04341484606266022\n",
      "epoch: 9 step: 1742, loss is 0.03547742962837219\n",
      "epoch: 9 step: 1743, loss is 0.012949973344802856\n",
      "epoch: 9 step: 1744, loss is 0.0017482270486652851\n",
      "epoch: 9 step: 1745, loss is 0.001232394017279148\n",
      "epoch: 9 step: 1746, loss is 0.03127271309494972\n",
      "epoch: 9 step: 1747, loss is 0.00023185351165011525\n",
      "epoch: 9 step: 1748, loss is 0.006395880598574877\n",
      "epoch: 9 step: 1749, loss is 0.14899082481861115\n",
      "epoch: 9 step: 1750, loss is 0.00018158979946747422\n",
      "epoch: 9 step: 1751, loss is 0.021356938406825066\n",
      "epoch: 9 step: 1752, loss is 0.0065405662171542645\n",
      "epoch: 9 step: 1753, loss is 0.027583779767155647\n",
      "epoch: 9 step: 1754, loss is 0.00015577327576465905\n",
      "epoch: 9 step: 1755, loss is 0.00017493392806500196\n",
      "epoch: 9 step: 1756, loss is 0.014673219993710518\n",
      "epoch: 9 step: 1757, loss is 0.002594694262370467\n",
      "epoch: 9 step: 1758, loss is 0.0042884168215096\n",
      "epoch: 9 step: 1759, loss is 0.005120178684592247\n",
      "epoch: 9 step: 1760, loss is 0.00013123163080308586\n",
      "epoch: 9 step: 1761, loss is 0.0020852854941040277\n",
      "epoch: 9 step: 1762, loss is 9.228182898368686e-05\n",
      "epoch: 9 step: 1763, loss is 0.00070762395625934\n",
      "epoch: 9 step: 1764, loss is 0.0035290494561195374\n",
      "epoch: 9 step: 1765, loss is 0.01784369722008705\n",
      "epoch: 9 step: 1766, loss is 0.0013348936336115003\n",
      "epoch: 9 step: 1767, loss is 0.02627701684832573\n",
      "epoch: 9 step: 1768, loss is 0.0006146834348328412\n",
      "epoch: 9 step: 1769, loss is 6.537209992529824e-05\n",
      "epoch: 9 step: 1770, loss is 0.002175041940063238\n",
      "epoch: 9 step: 1771, loss is 0.04681162163615227\n",
      "epoch: 9 step: 1772, loss is 0.03464550897479057\n",
      "epoch: 9 step: 1773, loss is 0.001082074362784624\n",
      "epoch: 9 step: 1774, loss is 0.0018021789146587253\n",
      "epoch: 9 step: 1775, loss is 0.02468215674161911\n",
      "epoch: 9 step: 1776, loss is 0.001264698221348226\n",
      "epoch: 9 step: 1777, loss is 0.009935549460351467\n",
      "epoch: 9 step: 1778, loss is 0.010647597722709179\n",
      "epoch: 9 step: 1779, loss is 0.00024938074056990445\n",
      "epoch: 9 step: 1780, loss is 0.014861909672617912\n",
      "epoch: 9 step: 1781, loss is 0.0001565745478728786\n",
      "epoch: 9 step: 1782, loss is 0.0016657280502840877\n",
      "epoch: 9 step: 1783, loss is 0.08737824857234955\n",
      "epoch: 9 step: 1784, loss is 0.0018875617533922195\n",
      "epoch: 9 step: 1785, loss is 0.015487398952245712\n",
      "epoch: 9 step: 1786, loss is 0.021601341664791107\n",
      "epoch: 9 step: 1787, loss is 0.0004953256575390697\n",
      "epoch: 9 step: 1788, loss is 0.03784998506307602\n",
      "epoch: 9 step: 1789, loss is 0.00010733365343185142\n",
      "epoch: 9 step: 1790, loss is 0.0009989903774112463\n",
      "epoch: 9 step: 1791, loss is 0.0001370937388855964\n",
      "epoch: 9 step: 1792, loss is 3.0249168048612773e-05\n",
      "epoch: 9 step: 1793, loss is 0.000603427819442004\n",
      "epoch: 9 step: 1794, loss is 0.03723438084125519\n",
      "epoch: 9 step: 1795, loss is 0.00821490678936243\n",
      "epoch: 9 step: 1796, loss is 0.0008057957165874541\n",
      "epoch: 9 step: 1797, loss is 0.004758072085678577\n",
      "epoch: 9 step: 1798, loss is 0.0006159353069961071\n",
      "epoch: 9 step: 1799, loss is 8.50507422001101e-06\n",
      "epoch: 9 step: 1800, loss is 0.1403684765100479\n",
      "epoch: 9 step: 1801, loss is 0.005731184966862202\n",
      "epoch: 9 step: 1802, loss is 0.0012528271181508899\n",
      "epoch: 9 step: 1803, loss is 0.04103149101138115\n",
      "epoch: 9 step: 1804, loss is 0.006412633694708347\n",
      "epoch: 9 step: 1805, loss is 0.0003297984367236495\n",
      "epoch: 9 step: 1806, loss is 0.004526602569967508\n",
      "epoch: 9 step: 1807, loss is 0.0017827192787081003\n",
      "epoch: 9 step: 1808, loss is 0.001304897479712963\n",
      "epoch: 9 step: 1809, loss is 0.00022953566804062575\n",
      "epoch: 9 step: 1810, loss is 0.004636242054402828\n",
      "epoch: 9 step: 1811, loss is 0.0033164932392537594\n",
      "epoch: 9 step: 1812, loss is 0.005133349914103746\n",
      "epoch: 9 step: 1813, loss is 0.005968221928924322\n",
      "epoch: 9 step: 1814, loss is 0.001568849547766149\n",
      "epoch: 9 step: 1815, loss is 0.00024975347332656384\n",
      "epoch: 9 step: 1816, loss is 6.021290028002113e-05\n",
      "epoch: 9 step: 1817, loss is 0.18887801468372345\n",
      "epoch: 9 step: 1818, loss is 0.033072154968976974\n",
      "epoch: 9 step: 1819, loss is 0.12424512952566147\n",
      "epoch: 9 step: 1820, loss is 0.0003942307084798813\n",
      "epoch: 9 step: 1821, loss is 0.006388572044670582\n",
      "epoch: 9 step: 1822, loss is 0.010876552201807499\n",
      "epoch: 9 step: 1823, loss is 0.00720769539475441\n",
      "epoch: 9 step: 1824, loss is 0.004572245758026838\n",
      "epoch: 9 step: 1825, loss is 0.0005173092358745635\n",
      "epoch: 9 step: 1826, loss is 0.0005574547685682774\n",
      "epoch: 9 step: 1827, loss is 0.0014282045885920525\n",
      "epoch: 9 step: 1828, loss is 1.1245479981880635e-05\n",
      "epoch: 9 step: 1829, loss is 0.0021371154580265284\n",
      "epoch: 9 step: 1830, loss is 0.008533697575330734\n",
      "epoch: 9 step: 1831, loss is 0.0013207512674853206\n",
      "epoch: 9 step: 1832, loss is 0.004696574527770281\n",
      "epoch: 9 step: 1833, loss is 0.0004257209657225758\n",
      "epoch: 9 step: 1834, loss is 0.03259662911295891\n",
      "epoch: 9 step: 1835, loss is 0.00353821087628603\n",
      "epoch: 9 step: 1836, loss is 5.297437382978387e-06\n",
      "epoch: 9 step: 1837, loss is 0.004670446738600731\n",
      "epoch: 9 step: 1838, loss is 0.005803171545267105\n",
      "epoch: 9 step: 1839, loss is 0.0005866893334314227\n",
      "epoch: 9 step: 1840, loss is 0.13184812664985657\n",
      "epoch: 9 step: 1841, loss is 0.13375557959079742\n",
      "epoch: 9 step: 1842, loss is 0.01342645101249218\n",
      "epoch: 9 step: 1843, loss is 0.0006316489889286458\n",
      "epoch: 9 step: 1844, loss is 0.028032002970576286\n",
      "epoch: 9 step: 1845, loss is 0.00033782696118578315\n",
      "epoch: 9 step: 1846, loss is 3.7838632124476135e-05\n",
      "epoch: 9 step: 1847, loss is 0.0001331649109488353\n",
      "epoch: 9 step: 1848, loss is 0.046984512358903885\n",
      "epoch: 9 step: 1849, loss is 0.07367251068353653\n",
      "epoch: 9 step: 1850, loss is 0.06864600628614426\n",
      "epoch: 9 step: 1851, loss is 0.0006446092156693339\n",
      "epoch: 9 step: 1852, loss is 0.010950688272714615\n",
      "epoch: 9 step: 1853, loss is 0.002285634633153677\n",
      "epoch: 9 step: 1854, loss is 5.9824764321092516e-05\n",
      "epoch: 9 step: 1855, loss is 0.01564909890294075\n",
      "epoch: 9 step: 1856, loss is 0.01511380635201931\n",
      "epoch: 9 step: 1857, loss is 0.00014048966113477945\n",
      "epoch: 9 step: 1858, loss is 0.0033690123818814754\n",
      "epoch: 9 step: 1859, loss is 0.04943469911813736\n",
      "epoch: 9 step: 1860, loss is 0.14174014329910278\n",
      "epoch: 9 step: 1861, loss is 0.0015298058278858662\n",
      "epoch: 9 step: 1862, loss is 0.0204494409263134\n",
      "epoch: 9 step: 1863, loss is 0.00015629961853846908\n",
      "epoch: 9 step: 1864, loss is 0.13432370126247406\n",
      "epoch: 9 step: 1865, loss is 2.4286895495606586e-05\n",
      "epoch: 9 step: 1866, loss is 0.0007526376866735518\n",
      "epoch: 9 step: 1867, loss is 0.007609589956700802\n",
      "epoch: 9 step: 1868, loss is 0.00018561536853667349\n",
      "epoch: 9 step: 1869, loss is 0.00022919273760635406\n",
      "epoch: 9 step: 1870, loss is 0.001960435649380088\n",
      "epoch: 9 step: 1871, loss is 0.00012784928549081087\n",
      "epoch: 9 step: 1872, loss is 0.05381843075156212\n",
      "epoch: 9 step: 1873, loss is 0.0028665263671427965\n",
      "epoch: 9 step: 1874, loss is 0.0005968220066279173\n",
      "epoch: 9 step: 1875, loss is 0.0035092805046588182\n",
      "Train epoch time: 13480.805 ms, per step time: 7.190 ms\n",
      "epoch: 10 step: 1, loss is 0.018602317199110985\n",
      "epoch: 10 step: 2, loss is 0.0014561867574229836\n",
      "epoch: 10 step: 3, loss is 0.013204095885157585\n",
      "epoch: 10 step: 4, loss is 0.014707433991134167\n",
      "epoch: 10 step: 5, loss is 0.0352366603910923\n",
      "epoch: 10 step: 6, loss is 0.00011091497435700148\n",
      "epoch: 10 step: 7, loss is 0.03162572905421257\n",
      "epoch: 10 step: 8, loss is 0.0005577572155743837\n",
      "epoch: 10 step: 9, loss is 0.0355423204600811\n",
      "epoch: 10 step: 10, loss is 3.592373104766011e-05\n",
      "epoch: 10 step: 11, loss is 0.00010181046673096716\n",
      "epoch: 10 step: 12, loss is 7.250930502777919e-05\n",
      "epoch: 10 step: 13, loss is 0.01641078107059002\n",
      "epoch: 10 step: 14, loss is 0.00016636699729133397\n",
      "epoch: 10 step: 15, loss is 0.0003413172671571374\n",
      "epoch: 10 step: 16, loss is 0.05263839662075043\n",
      "epoch: 10 step: 17, loss is 0.0024546959903091192\n",
      "epoch: 10 step: 18, loss is 7.511374133173376e-05\n",
      "epoch: 10 step: 19, loss is 0.011211572214961052\n",
      "epoch: 10 step: 20, loss is 0.0009417045512236655\n",
      "epoch: 10 step: 21, loss is 0.00012887023331131786\n",
      "epoch: 10 step: 22, loss is 0.10039190202951431\n",
      "epoch: 10 step: 23, loss is 0.062005415558815\n",
      "epoch: 10 step: 24, loss is 0.00019735677051357925\n",
      "epoch: 10 step: 25, loss is 0.0013675232185050845\n",
      "epoch: 10 step: 26, loss is 0.0813540443778038\n",
      "epoch: 10 step: 27, loss is 0.0009679733775556087\n",
      "epoch: 10 step: 28, loss is 0.014257624745368958\n",
      "epoch: 10 step: 29, loss is 0.00019859379972331226\n",
      "epoch: 10 step: 30, loss is 0.0005202583270147443\n",
      "epoch: 10 step: 31, loss is 0.0002645630738697946\n",
      "epoch: 10 step: 32, loss is 0.00016631584730930626\n",
      "epoch: 10 step: 33, loss is 0.019797950983047485\n",
      "epoch: 10 step: 34, loss is 0.036869876086711884\n",
      "epoch: 10 step: 35, loss is 0.000286003079963848\n",
      "epoch: 10 step: 36, loss is 0.009573301300406456\n",
      "epoch: 10 step: 37, loss is 0.04053786024451256\n",
      "epoch: 10 step: 38, loss is 0.00016756307741161436\n",
      "epoch: 10 step: 39, loss is 0.005475699435919523\n",
      "epoch: 10 step: 40, loss is 0.02616824582219124\n",
      "epoch: 10 step: 41, loss is 0.0326198972761631\n",
      "epoch: 10 step: 42, loss is 0.0004953740281052887\n",
      "epoch: 10 step: 43, loss is 0.14165981113910675\n",
      "epoch: 10 step: 44, loss is 0.003631587140262127\n",
      "epoch: 10 step: 45, loss is 0.016187511384487152\n",
      "epoch: 10 step: 46, loss is 0.0009765403810888529\n",
      "epoch: 10 step: 47, loss is 0.00022881723998580128\n",
      "epoch: 10 step: 48, loss is 0.001042235060594976\n",
      "epoch: 10 step: 49, loss is 0.0019785622134804726\n",
      "epoch: 10 step: 50, loss is 0.06635426729917526\n",
      "epoch: 10 step: 51, loss is 0.14017900824546814\n",
      "epoch: 10 step: 52, loss is 0.000509094272274524\n",
      "epoch: 10 step: 53, loss is 8.610206714365631e-05\n",
      "epoch: 10 step: 54, loss is 0.0005205245106481016\n",
      "epoch: 10 step: 55, loss is 0.12150012701749802\n",
      "epoch: 10 step: 56, loss is 0.0021103625185787678\n",
      "epoch: 10 step: 57, loss is 0.015750138089060783\n",
      "epoch: 10 step: 58, loss is 0.000493744621053338\n",
      "epoch: 10 step: 59, loss is 0.0007259688572958112\n",
      "epoch: 10 step: 60, loss is 0.000522483023814857\n",
      "epoch: 10 step: 61, loss is 6.529992970172316e-05\n",
      "epoch: 10 step: 62, loss is 0.00011443554831203073\n",
      "epoch: 10 step: 63, loss is 0.004039091058075428\n",
      "epoch: 10 step: 64, loss is 0.03906571865081787\n",
      "epoch: 10 step: 65, loss is 0.0012809503823518753\n",
      "epoch: 10 step: 66, loss is 0.013124817982316017\n",
      "epoch: 10 step: 67, loss is 0.003988622687757015\n",
      "epoch: 10 step: 68, loss is 0.00011724195064743981\n",
      "epoch: 10 step: 69, loss is 0.02665688470005989\n",
      "epoch: 10 step: 70, loss is 0.0029247638303786516\n",
      "epoch: 10 step: 71, loss is 0.0020524903666228056\n",
      "epoch: 10 step: 72, loss is 0.005102943163365126\n",
      "epoch: 10 step: 73, loss is 0.0012357134837657213\n",
      "epoch: 10 step: 74, loss is 0.02188940718770027\n",
      "epoch: 10 step: 75, loss is 0.006842636037617922\n",
      "epoch: 10 step: 76, loss is 0.00042483521974645555\n",
      "epoch: 10 step: 77, loss is 0.013575982302427292\n",
      "epoch: 10 step: 78, loss is 0.00014123774599283934\n",
      "epoch: 10 step: 79, loss is 0.006173441186547279\n",
      "epoch: 10 step: 80, loss is 0.019044334068894386\n",
      "epoch: 10 step: 81, loss is 0.0012645125389099121\n",
      "epoch: 10 step: 82, loss is 0.00021285004913806915\n",
      "epoch: 10 step: 83, loss is 0.00010228259634459391\n",
      "epoch: 10 step: 84, loss is 8.672201511217281e-05\n",
      "epoch: 10 step: 85, loss is 0.00019308911578264087\n",
      "epoch: 10 step: 86, loss is 0.0018176654120907187\n",
      "epoch: 10 step: 87, loss is 0.00020632774976547807\n",
      "epoch: 10 step: 88, loss is 0.0005112354992888868\n",
      "epoch: 10 step: 89, loss is 0.00013708647747989744\n",
      "epoch: 10 step: 90, loss is 0.003976963460445404\n",
      "epoch: 10 step: 91, loss is 6.07507536187768e-05\n",
      "epoch: 10 step: 92, loss is 0.0016263587167486548\n",
      "epoch: 10 step: 93, loss is 2.151240551029332e-05\n",
      "epoch: 10 step: 94, loss is 0.03700517863035202\n",
      "epoch: 10 step: 95, loss is 0.000435987749369815\n",
      "epoch: 10 step: 96, loss is 0.0051319655030965805\n",
      "epoch: 10 step: 97, loss is 0.00048221449833363295\n",
      "epoch: 10 step: 98, loss is 0.0014336691237986088\n",
      "epoch: 10 step: 99, loss is 0.007095107343047857\n",
      "epoch: 10 step: 100, loss is 7.961025403346866e-05\n",
      "epoch: 10 step: 101, loss is 0.0006080204620957375\n",
      "epoch: 10 step: 102, loss is 0.006101960316300392\n",
      "epoch: 10 step: 103, loss is 0.00011945488950004801\n",
      "epoch: 10 step: 104, loss is 0.0005132884252816439\n",
      "epoch: 10 step: 105, loss is 0.0011819800129160285\n",
      "epoch: 10 step: 106, loss is 0.0025069427210837603\n",
      "epoch: 10 step: 107, loss is 0.0003573638678062707\n",
      "epoch: 10 step: 108, loss is 0.0009062631870619953\n",
      "epoch: 10 step: 109, loss is 0.0005192944663576782\n",
      "epoch: 10 step: 110, loss is 3.315618232591078e-05\n",
      "epoch: 10 step: 111, loss is 0.0022089348640292883\n",
      "epoch: 10 step: 112, loss is 0.01884845457971096\n",
      "epoch: 10 step: 113, loss is 1.662810791458469e-05\n",
      "epoch: 10 step: 114, loss is 6.022809975547716e-05\n",
      "epoch: 10 step: 115, loss is 6.121395563241094e-05\n",
      "epoch: 10 step: 116, loss is 0.002471056068316102\n",
      "epoch: 10 step: 117, loss is 0.00027718409546650946\n",
      "epoch: 10 step: 118, loss is 0.0008160618599504232\n",
      "epoch: 10 step: 119, loss is 0.0005933491047471762\n",
      "epoch: 10 step: 120, loss is 4.4225555029697716e-05\n",
      "epoch: 10 step: 121, loss is 2.8378786737448536e-05\n",
      "epoch: 10 step: 122, loss is 0.005769999697804451\n",
      "epoch: 10 step: 123, loss is 0.003569302149116993\n",
      "epoch: 10 step: 124, loss is 0.0017097003292292356\n",
      "epoch: 10 step: 125, loss is 8.567544864490628e-05\n",
      "epoch: 10 step: 126, loss is 0.0013426417717710137\n",
      "epoch: 10 step: 127, loss is 0.002816526684910059\n",
      "epoch: 10 step: 128, loss is 0.0003624323580879718\n",
      "epoch: 10 step: 129, loss is 0.011781738139688969\n",
      "epoch: 10 step: 130, loss is 0.027061661705374718\n",
      "epoch: 10 step: 131, loss is 0.0005020503886044025\n",
      "epoch: 10 step: 132, loss is 0.0005978296976536512\n",
      "epoch: 10 step: 133, loss is 0.004597833845764399\n",
      "epoch: 10 step: 134, loss is 0.0004578503721859306\n",
      "epoch: 10 step: 135, loss is 0.00021783431293442845\n",
      "epoch: 10 step: 136, loss is 0.028344694525003433\n",
      "epoch: 10 step: 137, loss is 0.08383291214704514\n",
      "epoch: 10 step: 138, loss is 0.000509018951561302\n",
      "epoch: 10 step: 139, loss is 0.0009966448415070772\n",
      "epoch: 10 step: 140, loss is 0.030107274651527405\n",
      "epoch: 10 step: 141, loss is 6.752656918251887e-05\n",
      "epoch: 10 step: 142, loss is 0.0011925679864361882\n",
      "epoch: 10 step: 143, loss is 3.202362131560221e-05\n",
      "epoch: 10 step: 144, loss is 3.5112105251755565e-05\n",
      "epoch: 10 step: 145, loss is 0.00026160344714298844\n",
      "epoch: 10 step: 146, loss is 0.0010254103690385818\n",
      "epoch: 10 step: 147, loss is 0.0038220270071178675\n",
      "epoch: 10 step: 148, loss is 4.110390364076011e-05\n",
      "epoch: 10 step: 149, loss is 0.0014445226406678557\n",
      "epoch: 10 step: 150, loss is 0.0007323143072426319\n",
      "epoch: 10 step: 151, loss is 0.0134804155677557\n",
      "epoch: 10 step: 152, loss is 0.0004591415636241436\n",
      "epoch: 10 step: 153, loss is 0.0009435320389457047\n",
      "epoch: 10 step: 154, loss is 0.07766880095005035\n",
      "epoch: 10 step: 155, loss is 0.0002601429878268391\n",
      "epoch: 10 step: 156, loss is 0.0009559150203131139\n",
      "epoch: 10 step: 157, loss is 0.03722342103719711\n",
      "epoch: 10 step: 158, loss is 0.0022181798703968525\n",
      "epoch: 10 step: 159, loss is 0.00012164298823336139\n",
      "epoch: 10 step: 160, loss is 0.050127916038036346\n",
      "epoch: 10 step: 161, loss is 0.012996917590498924\n",
      "epoch: 10 step: 162, loss is 0.00026892608730122447\n",
      "epoch: 10 step: 163, loss is 0.003231718670576811\n",
      "epoch: 10 step: 164, loss is 0.002367029432207346\n",
      "epoch: 10 step: 165, loss is 3.8273330574156716e-05\n",
      "epoch: 10 step: 166, loss is 0.0007844229112379253\n",
      "epoch: 10 step: 167, loss is 0.0010603773407638073\n",
      "epoch: 10 step: 168, loss is 0.045937519520521164\n",
      "epoch: 10 step: 169, loss is 3.8861806388013065e-05\n",
      "epoch: 10 step: 170, loss is 7.044773519737646e-05\n",
      "epoch: 10 step: 171, loss is 0.001411742647178471\n",
      "epoch: 10 step: 172, loss is 0.00471954932436347\n",
      "epoch: 10 step: 173, loss is 0.02071468159556389\n",
      "epoch: 10 step: 174, loss is 0.010843822732567787\n",
      "epoch: 10 step: 175, loss is 0.028382964432239532\n",
      "epoch: 10 step: 176, loss is 0.0008192087989300489\n",
      "epoch: 10 step: 177, loss is 0.0040430170483887196\n",
      "epoch: 10 step: 178, loss is 0.00032524875132367015\n",
      "epoch: 10 step: 179, loss is 0.001027011196129024\n",
      "epoch: 10 step: 180, loss is 0.0005024726269766688\n",
      "epoch: 10 step: 181, loss is 0.01722341775894165\n",
      "epoch: 10 step: 182, loss is 3.030275911442004e-05\n",
      "epoch: 10 step: 183, loss is 0.0006388318724930286\n",
      "epoch: 10 step: 184, loss is 0.0006323790294118226\n",
      "epoch: 10 step: 185, loss is 0.0002312946307938546\n",
      "epoch: 10 step: 186, loss is 0.02459164336323738\n",
      "epoch: 10 step: 187, loss is 0.0001948176504811272\n",
      "epoch: 10 step: 188, loss is 0.0007128090364858508\n",
      "epoch: 10 step: 189, loss is 0.007671547122299671\n",
      "epoch: 10 step: 190, loss is 0.07894173264503479\n",
      "epoch: 10 step: 191, loss is 0.009708798490464687\n",
      "epoch: 10 step: 192, loss is 0.0005717849126085639\n",
      "epoch: 10 step: 193, loss is 0.03671475872397423\n",
      "epoch: 10 step: 194, loss is 0.00046797352842986584\n",
      "epoch: 10 step: 195, loss is 0.007341030985116959\n",
      "epoch: 10 step: 196, loss is 2.4430635676253587e-05\n",
      "epoch: 10 step: 197, loss is 0.012991882860660553\n",
      "epoch: 10 step: 198, loss is 0.034724291414022446\n",
      "epoch: 10 step: 199, loss is 0.00024549078079871833\n",
      "epoch: 10 step: 200, loss is 0.00303470971994102\n",
      "epoch: 10 step: 201, loss is 0.0727594643831253\n",
      "epoch: 10 step: 202, loss is 0.00033738435013219714\n",
      "epoch: 10 step: 203, loss is 0.05973878130316734\n",
      "epoch: 10 step: 204, loss is 0.00045919488184154034\n",
      "epoch: 10 step: 205, loss is 0.0008934072102420032\n",
      "epoch: 10 step: 206, loss is 0.007834956049919128\n",
      "epoch: 10 step: 207, loss is 0.008940190076828003\n",
      "epoch: 10 step: 208, loss is 0.00021718538482673466\n",
      "epoch: 10 step: 209, loss is 0.00012196390161989257\n",
      "epoch: 10 step: 210, loss is 0.004659946076571941\n",
      "epoch: 10 step: 211, loss is 0.005658164620399475\n",
      "epoch: 10 step: 212, loss is 0.02569929137825966\n",
      "epoch: 10 step: 213, loss is 9.862984006758779e-05\n",
      "epoch: 10 step: 214, loss is 0.004126585554331541\n",
      "epoch: 10 step: 215, loss is 0.00048031166079454124\n",
      "epoch: 10 step: 216, loss is 0.0004496978363022208\n",
      "epoch: 10 step: 217, loss is 3.4780703572323546e-05\n",
      "epoch: 10 step: 218, loss is 0.00035853570443578064\n",
      "epoch: 10 step: 219, loss is 0.00017747028323356062\n",
      "epoch: 10 step: 220, loss is 0.03672526776790619\n",
      "epoch: 10 step: 221, loss is 0.00019869710376951844\n",
      "epoch: 10 step: 222, loss is 0.0001916452602017671\n",
      "epoch: 10 step: 223, loss is 1.942520248121582e-05\n",
      "epoch: 10 step: 224, loss is 0.016292423009872437\n",
      "epoch: 10 step: 225, loss is 0.002452073385939002\n",
      "epoch: 10 step: 226, loss is 0.0004707006155513227\n",
      "epoch: 10 step: 227, loss is 0.008078698068857193\n",
      "epoch: 10 step: 228, loss is 0.04312499612569809\n",
      "epoch: 10 step: 229, loss is 0.00048260873882099986\n",
      "epoch: 10 step: 230, loss is 0.0008657328435219824\n",
      "epoch: 10 step: 231, loss is 2.0785391825484112e-05\n",
      "epoch: 10 step: 232, loss is 0.00043589246342889965\n",
      "epoch: 10 step: 233, loss is 0.00014698025188408792\n",
      "epoch: 10 step: 234, loss is 0.0014243641635403037\n",
      "epoch: 10 step: 235, loss is 0.008252464234828949\n",
      "epoch: 10 step: 236, loss is 0.06140654906630516\n",
      "epoch: 10 step: 237, loss is 0.12198741734027863\n",
      "epoch: 10 step: 238, loss is 0.00014442240353673697\n",
      "epoch: 10 step: 239, loss is 0.0002537073160056025\n",
      "epoch: 10 step: 240, loss is 0.0018048196798190475\n",
      "epoch: 10 step: 241, loss is 0.0008765918319113553\n",
      "epoch: 10 step: 242, loss is 0.044280800968408585\n",
      "epoch: 10 step: 243, loss is 0.0116966487839818\n",
      "epoch: 10 step: 244, loss is 0.00025770938373170793\n",
      "epoch: 10 step: 245, loss is 0.0033790275920182467\n",
      "epoch: 10 step: 246, loss is 0.006472959648817778\n",
      "epoch: 10 step: 247, loss is 0.0008054979844018817\n",
      "epoch: 10 step: 248, loss is 0.12515032291412354\n",
      "epoch: 10 step: 249, loss is 0.012363757938146591\n",
      "epoch: 10 step: 250, loss is 0.0001516940537840128\n",
      "epoch: 10 step: 251, loss is 0.00057166104670614\n",
      "epoch: 10 step: 252, loss is 3.0808450901531614e-06\n",
      "epoch: 10 step: 253, loss is 0.10240967571735382\n",
      "epoch: 10 step: 254, loss is 0.0012959553860127926\n",
      "epoch: 10 step: 255, loss is 0.0007369727827608585\n",
      "epoch: 10 step: 256, loss is 0.05086592584848404\n",
      "epoch: 10 step: 257, loss is 3.20130966429133e-05\n",
      "epoch: 10 step: 258, loss is 0.004130075685679913\n",
      "epoch: 10 step: 259, loss is 0.00015308587171602994\n",
      "epoch: 10 step: 260, loss is 0.006518945097923279\n",
      "epoch: 10 step: 261, loss is 0.053689420223236084\n",
      "epoch: 10 step: 262, loss is 0.0012274844339117408\n",
      "epoch: 10 step: 263, loss is 0.000548800453543663\n",
      "epoch: 10 step: 264, loss is 8.349677227670327e-05\n",
      "epoch: 10 step: 265, loss is 0.0010074771707877517\n",
      "epoch: 10 step: 266, loss is 9.111179679166526e-05\n",
      "epoch: 10 step: 267, loss is 0.02645278535783291\n",
      "epoch: 10 step: 268, loss is 0.005071238614618778\n",
      "epoch: 10 step: 269, loss is 0.0016451996052637696\n",
      "epoch: 10 step: 270, loss is 0.00013485152157954872\n",
      "epoch: 10 step: 271, loss is 0.03385660797357559\n",
      "epoch: 10 step: 272, loss is 0.0019093247829005122\n",
      "epoch: 10 step: 273, loss is 0.0016056547174230218\n",
      "epoch: 10 step: 274, loss is 0.05674286559224129\n",
      "epoch: 10 step: 275, loss is 0.00040293450001627207\n",
      "epoch: 10 step: 276, loss is 0.1099764034152031\n",
      "epoch: 10 step: 277, loss is 0.04003577306866646\n",
      "epoch: 10 step: 278, loss is 0.17213192582130432\n",
      "epoch: 10 step: 279, loss is 3.750235555344261e-05\n",
      "epoch: 10 step: 280, loss is 5.3011644922662526e-05\n",
      "epoch: 10 step: 281, loss is 0.021407868713140488\n",
      "epoch: 10 step: 282, loss is 0.0009070818778127432\n",
      "epoch: 10 step: 283, loss is 0.00017436769849155098\n",
      "epoch: 10 step: 284, loss is 0.22281759977340698\n",
      "epoch: 10 step: 285, loss is 0.017886627465486526\n",
      "epoch: 10 step: 286, loss is 0.08967023342847824\n",
      "epoch: 10 step: 287, loss is 0.017255188897252083\n",
      "epoch: 10 step: 288, loss is 0.0003123179485555738\n",
      "epoch: 10 step: 289, loss is 0.01672794297337532\n",
      "epoch: 10 step: 290, loss is 0.005331430584192276\n",
      "epoch: 10 step: 291, loss is 0.00027160163153894246\n",
      "epoch: 10 step: 292, loss is 0.00021456317335832864\n",
      "epoch: 10 step: 293, loss is 0.0008487346349284053\n",
      "epoch: 10 step: 294, loss is 0.001119139138609171\n",
      "epoch: 10 step: 295, loss is 5.591908120550215e-05\n",
      "epoch: 10 step: 296, loss is 0.00037304425495676696\n",
      "epoch: 10 step: 297, loss is 0.004384865518659353\n",
      "epoch: 10 step: 298, loss is 0.002165938029065728\n",
      "epoch: 10 step: 299, loss is 0.0004981237580068409\n",
      "epoch: 10 step: 300, loss is 0.12060611695051193\n",
      "epoch: 10 step: 301, loss is 0.0005996986292302608\n",
      "epoch: 10 step: 302, loss is 0.0029664989560842514\n",
      "epoch: 10 step: 303, loss is 0.007825505919754505\n",
      "epoch: 10 step: 304, loss is 0.007039688061922789\n",
      "epoch: 10 step: 305, loss is 0.40094834566116333\n",
      "epoch: 10 step: 306, loss is 0.06700678169727325\n",
      "epoch: 10 step: 307, loss is 0.04678747057914734\n",
      "epoch: 10 step: 308, loss is 0.00036012931377626956\n",
      "epoch: 10 step: 309, loss is 0.03237392380833626\n",
      "epoch: 10 step: 310, loss is 0.003955569583922625\n",
      "epoch: 10 step: 311, loss is 0.0019382962491363287\n",
      "epoch: 10 step: 312, loss is 7.541556260548532e-05\n",
      "epoch: 10 step: 313, loss is 0.017036546021699905\n",
      "epoch: 10 step: 314, loss is 0.0018293563043698668\n",
      "epoch: 10 step: 315, loss is 0.005456860177218914\n",
      "epoch: 10 step: 316, loss is 0.040857065469026566\n",
      "epoch: 10 step: 317, loss is 0.0038528249133378267\n",
      "epoch: 10 step: 318, loss is 0.05375530570745468\n",
      "epoch: 10 step: 319, loss is 0.000676196301355958\n",
      "epoch: 10 step: 320, loss is 0.00047442916547879577\n",
      "epoch: 10 step: 321, loss is 0.011369612067937851\n",
      "epoch: 10 step: 322, loss is 0.001376088708639145\n",
      "epoch: 10 step: 323, loss is 0.014874742366373539\n",
      "epoch: 10 step: 324, loss is 0.0018559172749519348\n",
      "epoch: 10 step: 325, loss is 0.0010911730350926518\n",
      "epoch: 10 step: 326, loss is 0.032772909849882126\n",
      "epoch: 10 step: 327, loss is 0.0010967164998874068\n",
      "epoch: 10 step: 328, loss is 3.441526860115118e-05\n",
      "epoch: 10 step: 329, loss is 0.0013671976048499346\n",
      "epoch: 10 step: 330, loss is 0.0068243094719946384\n",
      "epoch: 10 step: 331, loss is 0.002274826169013977\n",
      "epoch: 10 step: 332, loss is 0.11728394776582718\n",
      "epoch: 10 step: 333, loss is 0.00025596932391636074\n",
      "epoch: 10 step: 334, loss is 0.002526052063331008\n",
      "epoch: 10 step: 335, loss is 0.00055946089560166\n",
      "epoch: 10 step: 336, loss is 0.0013580548111349344\n",
      "epoch: 10 step: 337, loss is 0.00048699439503252506\n",
      "epoch: 10 step: 338, loss is 0.01620160974562168\n",
      "epoch: 10 step: 339, loss is 0.00016240481636486948\n",
      "epoch: 10 step: 340, loss is 0.003255910938605666\n",
      "epoch: 10 step: 341, loss is 0.003035366302356124\n",
      "epoch: 10 step: 342, loss is 0.0002948838518932462\n",
      "epoch: 10 step: 343, loss is 0.006179580930620432\n",
      "epoch: 10 step: 344, loss is 0.2374519556760788\n",
      "epoch: 10 step: 345, loss is 0.009200148284435272\n",
      "epoch: 10 step: 346, loss is 0.0018979483284056187\n",
      "epoch: 10 step: 347, loss is 0.0003349992912262678\n",
      "epoch: 10 step: 348, loss is 0.00038334610871970654\n",
      "epoch: 10 step: 349, loss is 0.0014642418827861547\n",
      "epoch: 10 step: 350, loss is 0.002455047331750393\n",
      "epoch: 10 step: 351, loss is 0.00014919982641004026\n",
      "epoch: 10 step: 352, loss is 0.0022238530218601227\n",
      "epoch: 10 step: 353, loss is 0.012015704065561295\n",
      "epoch: 10 step: 354, loss is 0.004544048570096493\n",
      "epoch: 10 step: 355, loss is 0.022145677357912064\n",
      "epoch: 10 step: 356, loss is 0.0006483917240984738\n",
      "epoch: 10 step: 357, loss is 0.1104540228843689\n",
      "epoch: 10 step: 358, loss is 0.00029259143047966063\n",
      "epoch: 10 step: 359, loss is 0.0005467807641252875\n",
      "epoch: 10 step: 360, loss is 0.019477782770991325\n",
      "epoch: 10 step: 361, loss is 0.062176160514354706\n",
      "epoch: 10 step: 362, loss is 0.0017924692947417498\n",
      "epoch: 10 step: 363, loss is 0.0005746964598074555\n",
      "epoch: 10 step: 364, loss is 0.009975370950996876\n",
      "epoch: 10 step: 365, loss is 0.006723064463585615\n",
      "epoch: 10 step: 366, loss is 0.0011027903528884053\n",
      "epoch: 10 step: 367, loss is 0.00034791382495313883\n",
      "epoch: 10 step: 368, loss is 0.0018467155750840902\n",
      "epoch: 10 step: 369, loss is 0.02446911670267582\n",
      "epoch: 10 step: 370, loss is 0.02051454409956932\n",
      "epoch: 10 step: 371, loss is 0.0023293111007660627\n",
      "epoch: 10 step: 372, loss is 0.0023943609558045864\n",
      "epoch: 10 step: 373, loss is 0.00010822929471032694\n",
      "epoch: 10 step: 374, loss is 0.004920159466564655\n",
      "epoch: 10 step: 375, loss is 6.462690362241119e-05\n",
      "epoch: 10 step: 376, loss is 0.0001843463978730142\n",
      "epoch: 10 step: 377, loss is 0.047801535576581955\n",
      "epoch: 10 step: 378, loss is 0.0036796447820961475\n",
      "epoch: 10 step: 379, loss is 0.0008846099954098463\n",
      "epoch: 10 step: 380, loss is 0.0016659290995448828\n",
      "epoch: 10 step: 381, loss is 7.581047248095274e-05\n",
      "epoch: 10 step: 382, loss is 0.010889625176787376\n",
      "epoch: 10 step: 383, loss is 0.00022989381977822632\n",
      "epoch: 10 step: 384, loss is 0.001705974922515452\n",
      "epoch: 10 step: 385, loss is 0.0015266137197613716\n",
      "epoch: 10 step: 386, loss is 0.02749001421034336\n",
      "epoch: 10 step: 387, loss is 0.0003776653320528567\n",
      "epoch: 10 step: 388, loss is 8.409049769397825e-05\n",
      "epoch: 10 step: 389, loss is 0.003848019987344742\n",
      "epoch: 10 step: 390, loss is 0.0016160996165126562\n",
      "epoch: 10 step: 391, loss is 0.0004394047718960792\n",
      "epoch: 10 step: 392, loss is 0.0019914035219699144\n",
      "epoch: 10 step: 393, loss is 0.09597045183181763\n",
      "epoch: 10 step: 394, loss is 0.000557645340450108\n",
      "epoch: 10 step: 395, loss is 0.00016417862207163125\n",
      "epoch: 10 step: 396, loss is 0.0013568941503763199\n",
      "epoch: 10 step: 397, loss is 0.002720095682889223\n",
      "epoch: 10 step: 398, loss is 0.0006356541998684406\n",
      "epoch: 10 step: 399, loss is 0.07983317971229553\n",
      "epoch: 10 step: 400, loss is 0.0012192660942673683\n",
      "epoch: 10 step: 401, loss is 0.012261157855391502\n",
      "epoch: 10 step: 402, loss is 0.00024628962273709476\n",
      "epoch: 10 step: 403, loss is 0.00037426367634907365\n",
      "epoch: 10 step: 404, loss is 0.0020283025223761797\n",
      "epoch: 10 step: 405, loss is 0.0016966431867331266\n",
      "epoch: 10 step: 406, loss is 0.017936866730451584\n",
      "epoch: 10 step: 407, loss is 0.0010566313285380602\n",
      "epoch: 10 step: 408, loss is 0.00048114877426996827\n",
      "epoch: 10 step: 409, loss is 0.016574060544371605\n",
      "epoch: 10 step: 410, loss is 0.00400538882240653\n",
      "epoch: 10 step: 411, loss is 0.02472393959760666\n",
      "epoch: 10 step: 412, loss is 0.013643987476825714\n",
      "epoch: 10 step: 413, loss is 0.011670281179249287\n",
      "epoch: 10 step: 414, loss is 0.001370759098790586\n",
      "epoch: 10 step: 415, loss is 0.0018393888603895903\n",
      "epoch: 10 step: 416, loss is 0.003455091267824173\n",
      "epoch: 10 step: 417, loss is 0.00039328227285295725\n",
      "epoch: 10 step: 418, loss is 0.0005969412159174681\n",
      "epoch: 10 step: 419, loss is 0.0005225574714131653\n",
      "epoch: 10 step: 420, loss is 0.0009388611069880426\n",
      "epoch: 10 step: 421, loss is 5.827787390444428e-05\n",
      "epoch: 10 step: 422, loss is 4.237056054989807e-05\n",
      "epoch: 10 step: 423, loss is 4.2142219172092155e-05\n",
      "epoch: 10 step: 424, loss is 0.010520990006625652\n",
      "epoch: 10 step: 425, loss is 0.24680310487747192\n",
      "epoch: 10 step: 426, loss is 0.0003616935573518276\n",
      "epoch: 10 step: 427, loss is 0.03660718724131584\n",
      "epoch: 10 step: 428, loss is 0.00036561378510668874\n",
      "epoch: 10 step: 429, loss is 0.001852653338573873\n",
      "epoch: 10 step: 430, loss is 0.0001736076665110886\n",
      "epoch: 10 step: 431, loss is 0.004163522273302078\n",
      "epoch: 10 step: 432, loss is 0.0015772992046549916\n",
      "epoch: 10 step: 433, loss is 0.0040811169892549515\n",
      "epoch: 10 step: 434, loss is 0.0005236223805695772\n",
      "epoch: 10 step: 435, loss is 0.0021846434101462364\n",
      "epoch: 10 step: 436, loss is 3.052913234569132e-05\n",
      "epoch: 10 step: 437, loss is 0.049804214388132095\n",
      "epoch: 10 step: 438, loss is 0.007076284382492304\n",
      "epoch: 10 step: 439, loss is 0.001961458008736372\n",
      "epoch: 10 step: 440, loss is 0.00039953290252014995\n",
      "epoch: 10 step: 441, loss is 0.00022190708841662854\n",
      "epoch: 10 step: 442, loss is 0.0013744186144322157\n",
      "epoch: 10 step: 443, loss is 0.000539857370313257\n",
      "epoch: 10 step: 444, loss is 0.0022694289218634367\n",
      "epoch: 10 step: 445, loss is 5.761483407695778e-05\n",
      "epoch: 10 step: 446, loss is 0.0011078999377787113\n",
      "epoch: 10 step: 447, loss is 0.0003132836427539587\n",
      "epoch: 10 step: 448, loss is 0.0009799851104617119\n",
      "epoch: 10 step: 449, loss is 0.024572283029556274\n",
      "epoch: 10 step: 450, loss is 0.016369415447115898\n",
      "epoch: 10 step: 451, loss is 0.00013275498349685222\n",
      "epoch: 10 step: 452, loss is 0.005055603571236134\n",
      "epoch: 10 step: 453, loss is 0.003258988494053483\n",
      "epoch: 10 step: 454, loss is 0.028621641919016838\n",
      "epoch: 10 step: 455, loss is 0.0007257579709403217\n",
      "epoch: 10 step: 456, loss is 0.0038112427573651075\n",
      "epoch: 10 step: 457, loss is 0.0018050388898700476\n",
      "epoch: 10 step: 458, loss is 0.10087306797504425\n",
      "epoch: 10 step: 459, loss is 0.0005873921909369528\n",
      "epoch: 10 step: 460, loss is 0.058505866676568985\n",
      "epoch: 10 step: 461, loss is 0.0016952946316450834\n",
      "epoch: 10 step: 462, loss is 0.002553110010921955\n",
      "epoch: 10 step: 463, loss is 0.008245899342000484\n",
      "epoch: 10 step: 464, loss is 0.00022056314628571272\n",
      "epoch: 10 step: 465, loss is 0.0003484543412923813\n",
      "epoch: 10 step: 466, loss is 0.0024775175843387842\n",
      "epoch: 10 step: 467, loss is 0.07781460136175156\n",
      "epoch: 10 step: 468, loss is 4.4495351176010445e-05\n",
      "epoch: 10 step: 469, loss is 5.5169286497402936e-05\n",
      "epoch: 10 step: 470, loss is 0.0013703459408134222\n",
      "epoch: 10 step: 471, loss is 0.031184645369648933\n",
      "epoch: 10 step: 472, loss is 0.0005984092131257057\n",
      "epoch: 10 step: 473, loss is 0.015089686028659344\n",
      "epoch: 10 step: 474, loss is 0.00044160938705317676\n",
      "epoch: 10 step: 475, loss is 0.020523814484477043\n",
      "epoch: 10 step: 476, loss is 0.02825678512454033\n",
      "epoch: 10 step: 477, loss is 0.0008247338701039553\n",
      "epoch: 10 step: 478, loss is 0.0005267222295515239\n",
      "epoch: 10 step: 479, loss is 0.0035070490557700396\n",
      "epoch: 10 step: 480, loss is 0.00022228501620702446\n",
      "epoch: 10 step: 481, loss is 0.00035532747278921306\n",
      "epoch: 10 step: 482, loss is 0.11247305572032928\n",
      "epoch: 10 step: 483, loss is 0.08174276351928711\n",
      "epoch: 10 step: 484, loss is 0.000924833700992167\n",
      "epoch: 10 step: 485, loss is 0.13247378170490265\n",
      "epoch: 10 step: 486, loss is 0.012530189007520676\n",
      "epoch: 10 step: 487, loss is 0.006815002299845219\n",
      "epoch: 10 step: 488, loss is 0.0009842077270150185\n",
      "epoch: 10 step: 489, loss is 7.233575161080807e-05\n",
      "epoch: 10 step: 490, loss is 7.723598537268117e-05\n",
      "epoch: 10 step: 491, loss is 0.0009308894514106214\n",
      "epoch: 10 step: 492, loss is 0.007079812698066235\n",
      "epoch: 10 step: 493, loss is 0.0011644798796623945\n",
      "epoch: 10 step: 494, loss is 0.0013291977811604738\n",
      "epoch: 10 step: 495, loss is 0.34269052743911743\n",
      "epoch: 10 step: 496, loss is 0.0016522395890206099\n",
      "epoch: 10 step: 497, loss is 0.0004857952881138772\n",
      "epoch: 10 step: 498, loss is 0.00034017558209598064\n",
      "epoch: 10 step: 499, loss is 0.1155712679028511\n",
      "epoch: 10 step: 500, loss is 0.03986946865916252\n",
      "epoch: 10 step: 501, loss is 0.0003650489670690149\n",
      "epoch: 10 step: 502, loss is 0.0001280710130231455\n",
      "epoch: 10 step: 503, loss is 0.00038345696520991623\n",
      "epoch: 10 step: 504, loss is 0.03496671840548515\n",
      "epoch: 10 step: 505, loss is 6.889540236443281e-05\n",
      "epoch: 10 step: 506, loss is 0.11642954498529434\n",
      "epoch: 10 step: 507, loss is 0.0013291651848703623\n",
      "epoch: 10 step: 508, loss is 0.0006446362822316587\n",
      "epoch: 10 step: 509, loss is 0.00021189599647186697\n",
      "epoch: 10 step: 510, loss is 0.11931920796632767\n",
      "epoch: 10 step: 511, loss is 0.010183693841099739\n",
      "epoch: 10 step: 512, loss is 0.011288652196526527\n",
      "epoch: 10 step: 513, loss is 0.028397545218467712\n",
      "epoch: 10 step: 514, loss is 0.00355600006878376\n",
      "epoch: 10 step: 515, loss is 0.0007139568915590644\n",
      "epoch: 10 step: 516, loss is 0.031533509492874146\n",
      "epoch: 10 step: 517, loss is 0.03273655101656914\n",
      "epoch: 10 step: 518, loss is 0.0008943828870542347\n",
      "epoch: 10 step: 519, loss is 0.0005583137972280383\n",
      "epoch: 10 step: 520, loss is 0.00220252457074821\n",
      "epoch: 10 step: 521, loss is 0.17328041791915894\n",
      "epoch: 10 step: 522, loss is 0.027959804981946945\n",
      "epoch: 10 step: 523, loss is 0.0036670577246695757\n",
      "epoch: 10 step: 524, loss is 0.008445434272289276\n",
      "epoch: 10 step: 525, loss is 0.005057308357208967\n",
      "epoch: 10 step: 526, loss is 0.00012832922220695764\n",
      "epoch: 10 step: 527, loss is 0.0006395198870450258\n",
      "epoch: 10 step: 528, loss is 0.002701310208067298\n",
      "epoch: 10 step: 529, loss is 0.037869010120630264\n",
      "epoch: 10 step: 530, loss is 0.00010430550901219249\n",
      "epoch: 10 step: 531, loss is 0.0035188905894756317\n",
      "epoch: 10 step: 532, loss is 0.005975759122520685\n",
      "epoch: 10 step: 533, loss is 0.00023370864801108837\n",
      "epoch: 10 step: 534, loss is 0.00022452071425504982\n",
      "epoch: 10 step: 535, loss is 0.00015260845248121768\n",
      "epoch: 10 step: 536, loss is 0.0013615285279229283\n",
      "epoch: 10 step: 537, loss is 9.897609561448917e-05\n",
      "epoch: 10 step: 538, loss is 0.03206657990813255\n",
      "epoch: 10 step: 539, loss is 0.0019455243600532413\n",
      "epoch: 10 step: 540, loss is 0.003981644753366709\n",
      "epoch: 10 step: 541, loss is 0.018694108352065086\n",
      "epoch: 10 step: 542, loss is 0.0025174005422741175\n",
      "epoch: 10 step: 543, loss is 0.00031867268262431026\n",
      "epoch: 10 step: 544, loss is 0.017296161502599716\n",
      "epoch: 10 step: 545, loss is 0.0190139040350914\n",
      "epoch: 10 step: 546, loss is 6.034601392457262e-05\n",
      "epoch: 10 step: 547, loss is 0.03812006860971451\n",
      "epoch: 10 step: 548, loss is 0.0026858854107558727\n",
      "epoch: 10 step: 549, loss is 0.016757328063249588\n",
      "epoch: 10 step: 550, loss is 0.0012791585177183151\n",
      "epoch: 10 step: 551, loss is 0.00019769954087678343\n",
      "epoch: 10 step: 552, loss is 0.002814267762005329\n",
      "epoch: 10 step: 553, loss is 0.0015478910645470023\n",
      "epoch: 10 step: 554, loss is 0.014185716398060322\n",
      "epoch: 10 step: 555, loss is 0.0016916926251724362\n",
      "epoch: 10 step: 556, loss is 0.0018576527945697308\n",
      "epoch: 10 step: 557, loss is 0.0035422281362116337\n",
      "epoch: 10 step: 558, loss is 0.08645657449960709\n",
      "epoch: 10 step: 559, loss is 0.003183674765750766\n",
      "epoch: 10 step: 560, loss is 0.002712873974815011\n",
      "epoch: 10 step: 561, loss is 0.00017795962048694491\n",
      "epoch: 10 step: 562, loss is 0.00027091350057162344\n",
      "epoch: 10 step: 563, loss is 0.0031928836833685637\n",
      "epoch: 10 step: 564, loss is 0.0017299571773037314\n",
      "epoch: 10 step: 565, loss is 0.0019079053308814764\n",
      "epoch: 10 step: 566, loss is 0.00020475094788707793\n",
      "epoch: 10 step: 567, loss is 0.012756694108247757\n",
      "epoch: 10 step: 568, loss is 0.0005402479437179863\n",
      "epoch: 10 step: 569, loss is 0.00018684001406654716\n",
      "epoch: 10 step: 570, loss is 0.00031036039581522346\n",
      "epoch: 10 step: 571, loss is 0.0031467864755541086\n",
      "epoch: 10 step: 572, loss is 0.052981797605752945\n",
      "epoch: 10 step: 573, loss is 0.010873571038246155\n",
      "epoch: 10 step: 574, loss is 0.004023353569209576\n",
      "epoch: 10 step: 575, loss is 0.011896897107362747\n",
      "epoch: 10 step: 576, loss is 2.908847454818897e-05\n",
      "epoch: 10 step: 577, loss is 0.008369768038392067\n",
      "epoch: 10 step: 578, loss is 1.9634608179330826e-05\n",
      "epoch: 10 step: 579, loss is 0.03940170630812645\n",
      "epoch: 10 step: 580, loss is 0.001355339540168643\n",
      "epoch: 10 step: 581, loss is 0.00044709607027471066\n",
      "epoch: 10 step: 582, loss is 3.0089035135461017e-05\n",
      "epoch: 10 step: 583, loss is 0.031782276928424835\n",
      "epoch: 10 step: 584, loss is 0.0010413371492177248\n",
      "epoch: 10 step: 585, loss is 0.056002505123615265\n",
      "epoch: 10 step: 586, loss is 0.000772617815528065\n",
      "epoch: 10 step: 587, loss is 0.002490813145413995\n",
      "epoch: 10 step: 588, loss is 0.001196820056065917\n",
      "epoch: 10 step: 589, loss is 0.00031358154956251383\n",
      "epoch: 10 step: 590, loss is 0.011097408831119537\n",
      "epoch: 10 step: 591, loss is 0.008859440684318542\n",
      "epoch: 10 step: 592, loss is 0.007631354033946991\n",
      "epoch: 10 step: 593, loss is 0.05245281755924225\n",
      "epoch: 10 step: 594, loss is 0.00024403532734140754\n",
      "epoch: 10 step: 595, loss is 0.0004265889001544565\n",
      "epoch: 10 step: 596, loss is 3.562412166502327e-05\n",
      "epoch: 10 step: 597, loss is 0.00010923179797828197\n",
      "epoch: 10 step: 598, loss is 0.14320935308933258\n",
      "epoch: 10 step: 599, loss is 0.005194068420678377\n",
      "epoch: 10 step: 600, loss is 0.0015315520577132702\n",
      "epoch: 10 step: 601, loss is 0.015711108222603798\n",
      "epoch: 10 step: 602, loss is 0.08068013191223145\n",
      "epoch: 10 step: 603, loss is 0.005546097178012133\n",
      "epoch: 10 step: 604, loss is 0.008151372894644737\n",
      "epoch: 10 step: 605, loss is 0.022131042554974556\n",
      "epoch: 10 step: 606, loss is 0.001273327274248004\n",
      "epoch: 10 step: 607, loss is 0.008504710160195827\n",
      "epoch: 10 step: 608, loss is 0.1263210028409958\n",
      "epoch: 10 step: 609, loss is 0.0003305184072814882\n",
      "epoch: 10 step: 610, loss is 0.0013169690500944853\n",
      "epoch: 10 step: 611, loss is 0.0709385871887207\n",
      "epoch: 10 step: 612, loss is 0.00033137641730718315\n",
      "epoch: 10 step: 613, loss is 0.02280261553823948\n",
      "epoch: 10 step: 614, loss is 0.0007999951485544443\n",
      "epoch: 10 step: 615, loss is 0.07756265252828598\n",
      "epoch: 10 step: 616, loss is 0.00033958000130951405\n",
      "epoch: 10 step: 617, loss is 9.68363310676068e-05\n",
      "epoch: 10 step: 618, loss is 3.052446845686063e-05\n",
      "epoch: 10 step: 619, loss is 0.002077089622616768\n",
      "epoch: 10 step: 620, loss is 0.039794981479644775\n",
      "epoch: 10 step: 621, loss is 0.047224219888448715\n",
      "epoch: 10 step: 622, loss is 0.0022291431669145823\n",
      "epoch: 10 step: 623, loss is 0.00291380169801414\n",
      "epoch: 10 step: 624, loss is 0.0003897442074958235\n",
      "epoch: 10 step: 625, loss is 0.0050398861058056355\n",
      "epoch: 10 step: 626, loss is 0.0023589152842760086\n",
      "epoch: 10 step: 627, loss is 0.07076399773359299\n",
      "epoch: 10 step: 628, loss is 0.010443580336868763\n",
      "epoch: 10 step: 629, loss is 0.019630441442131996\n",
      "epoch: 10 step: 630, loss is 0.009739868342876434\n",
      "epoch: 10 step: 631, loss is 0.1335602104663849\n",
      "epoch: 10 step: 632, loss is 0.05726529657840729\n",
      "epoch: 10 step: 633, loss is 0.0029200802091509104\n",
      "epoch: 10 step: 634, loss is 0.008904502727091312\n",
      "epoch: 10 step: 635, loss is 0.0003389346820767969\n",
      "epoch: 10 step: 636, loss is 0.0481894426047802\n",
      "epoch: 10 step: 637, loss is 0.004511116538196802\n",
      "epoch: 10 step: 638, loss is 0.017510907724499702\n",
      "epoch: 10 step: 639, loss is 0.0002678322489373386\n",
      "epoch: 10 step: 640, loss is 0.08887480199337006\n",
      "epoch: 10 step: 641, loss is 0.0034362401347607374\n",
      "epoch: 10 step: 642, loss is 3.753948476514779e-05\n",
      "epoch: 10 step: 643, loss is 0.0010123623069375753\n",
      "epoch: 10 step: 644, loss is 0.050367146730422974\n",
      "epoch: 10 step: 645, loss is 0.001521926955319941\n",
      "epoch: 10 step: 646, loss is 0.0077155171893537045\n",
      "epoch: 10 step: 647, loss is 0.008390218950808048\n",
      "epoch: 10 step: 648, loss is 0.047575052827596664\n",
      "epoch: 10 step: 649, loss is 0.0007749242940917611\n",
      "epoch: 10 step: 650, loss is 0.007092698477208614\n",
      "epoch: 10 step: 651, loss is 0.04920319467782974\n",
      "epoch: 10 step: 652, loss is 0.0011756105814129114\n",
      "epoch: 10 step: 653, loss is 0.005687435157597065\n",
      "epoch: 10 step: 654, loss is 0.003995596431195736\n",
      "epoch: 10 step: 655, loss is 0.012390973046422005\n",
      "epoch: 10 step: 656, loss is 0.35989996790885925\n",
      "epoch: 10 step: 657, loss is 0.15252646803855896\n",
      "epoch: 10 step: 658, loss is 0.006341168191283941\n",
      "epoch: 10 step: 659, loss is 0.003998534753918648\n",
      "epoch: 10 step: 660, loss is 0.08732304722070694\n",
      "epoch: 10 step: 661, loss is 0.0860995501279831\n",
      "epoch: 10 step: 662, loss is 0.03401276469230652\n",
      "epoch: 10 step: 663, loss is 0.02106570638716221\n",
      "epoch: 10 step: 664, loss is 0.2058095484972\n",
      "epoch: 10 step: 665, loss is 8.160917786881328e-05\n",
      "epoch: 10 step: 666, loss is 0.17972464859485626\n",
      "epoch: 10 step: 667, loss is 0.012428605929017067\n",
      "epoch: 10 step: 668, loss is 8.329404226969928e-05\n",
      "epoch: 10 step: 669, loss is 0.0005869214073754847\n",
      "epoch: 10 step: 670, loss is 0.0022692556958645582\n",
      "epoch: 10 step: 671, loss is 0.016258973628282547\n",
      "epoch: 10 step: 672, loss is 0.0038377572782337666\n",
      "epoch: 10 step: 673, loss is 0.046269893646240234\n",
      "epoch: 10 step: 674, loss is 0.014297650195658207\n",
      "epoch: 10 step: 675, loss is 0.03823532909154892\n",
      "epoch: 10 step: 676, loss is 0.011474799364805222\n",
      "epoch: 10 step: 677, loss is 0.12474619597196579\n",
      "epoch: 10 step: 678, loss is 0.07114983350038528\n",
      "epoch: 10 step: 679, loss is 0.01655820943415165\n",
      "epoch: 10 step: 680, loss is 0.11986212432384491\n",
      "epoch: 10 step: 681, loss is 0.03781602531671524\n",
      "epoch: 10 step: 682, loss is 0.004839939996600151\n",
      "epoch: 10 step: 683, loss is 0.002678381511941552\n",
      "epoch: 10 step: 684, loss is 0.0004305947804823518\n",
      "epoch: 10 step: 685, loss is 0.001774960313923657\n",
      "epoch: 10 step: 686, loss is 0.0012019457062706351\n",
      "epoch: 10 step: 687, loss is 0.003768220078200102\n",
      "epoch: 10 step: 688, loss is 0.0002797227934934199\n",
      "epoch: 10 step: 689, loss is 0.0008080814150162041\n",
      "epoch: 10 step: 690, loss is 0.016909386962652206\n",
      "epoch: 10 step: 691, loss is 0.07143165171146393\n",
      "epoch: 10 step: 692, loss is 0.0012482923921197653\n",
      "epoch: 10 step: 693, loss is 0.0014255072455853224\n",
      "epoch: 10 step: 694, loss is 0.0007476450991816819\n",
      "epoch: 10 step: 695, loss is 7.192289922386408e-05\n",
      "epoch: 10 step: 696, loss is 0.16322913765907288\n",
      "epoch: 10 step: 697, loss is 0.0009985488140955567\n",
      "epoch: 10 step: 698, loss is 0.0004238061374053359\n",
      "epoch: 10 step: 699, loss is 0.05884349346160889\n",
      "epoch: 10 step: 700, loss is 0.03086286410689354\n",
      "epoch: 10 step: 701, loss is 0.018210947513580322\n",
      "epoch: 10 step: 702, loss is 0.003915850073099136\n",
      "epoch: 10 step: 703, loss is 0.00035558041417971253\n",
      "epoch: 10 step: 704, loss is 0.0014951230259612203\n",
      "epoch: 10 step: 705, loss is 0.02035074308514595\n",
      "epoch: 10 step: 706, loss is 0.002754656597971916\n",
      "epoch: 10 step: 707, loss is 0.005257792770862579\n",
      "epoch: 10 step: 708, loss is 0.0010798618895933032\n",
      "epoch: 10 step: 709, loss is 0.08301728218793869\n",
      "epoch: 10 step: 710, loss is 0.004835293162614107\n",
      "epoch: 10 step: 711, loss is 0.00032809266122058034\n",
      "epoch: 10 step: 712, loss is 0.014147858135402203\n",
      "epoch: 10 step: 713, loss is 0.001635457039810717\n",
      "epoch: 10 step: 714, loss is 6.011910591041669e-05\n",
      "epoch: 10 step: 715, loss is 5.055041765444912e-05\n",
      "epoch: 10 step: 716, loss is 0.0006428855704143643\n",
      "epoch: 10 step: 717, loss is 0.004681273363530636\n",
      "epoch: 10 step: 718, loss is 0.017092706635594368\n",
      "epoch: 10 step: 719, loss is 0.0835622102022171\n",
      "epoch: 10 step: 720, loss is 0.0022048731334507465\n",
      "epoch: 10 step: 721, loss is 0.07084117829799652\n",
      "epoch: 10 step: 722, loss is 0.0021133311092853546\n",
      "epoch: 10 step: 723, loss is 0.09580270946025848\n",
      "epoch: 10 step: 724, loss is 0.12911511957645416\n",
      "epoch: 10 step: 725, loss is 0.0009103676420636475\n",
      "epoch: 10 step: 726, loss is 0.00017702959303278476\n",
      "epoch: 10 step: 727, loss is 0.0007670063059777021\n",
      "epoch: 10 step: 728, loss is 0.0008055692887865007\n",
      "epoch: 10 step: 729, loss is 0.009326044470071793\n",
      "epoch: 10 step: 730, loss is 0.0008815082255750895\n",
      "epoch: 10 step: 731, loss is 0.004554074723273516\n",
      "epoch: 10 step: 732, loss is 0.000884507317095995\n",
      "epoch: 10 step: 733, loss is 0.0009499277221038938\n",
      "epoch: 10 step: 734, loss is 0.010232637636363506\n",
      "epoch: 10 step: 735, loss is 0.04753757268190384\n",
      "epoch: 10 step: 736, loss is 0.003030893625691533\n",
      "epoch: 10 step: 737, loss is 0.0005405994015745819\n",
      "epoch: 10 step: 738, loss is 0.0029804990626871586\n",
      "epoch: 10 step: 739, loss is 0.22948512434959412\n",
      "epoch: 10 step: 740, loss is 0.010968619026243687\n",
      "epoch: 10 step: 741, loss is 0.00848279520869255\n",
      "epoch: 10 step: 742, loss is 0.0001562232937430963\n",
      "epoch: 10 step: 743, loss is 0.022958163172006607\n",
      "epoch: 10 step: 744, loss is 0.07065993547439575\n",
      "epoch: 10 step: 745, loss is 0.00022191484458744526\n",
      "epoch: 10 step: 746, loss is 0.17366303503513336\n",
      "epoch: 10 step: 747, loss is 0.0015042098239064217\n",
      "epoch: 10 step: 748, loss is 0.015201928094029427\n",
      "epoch: 10 step: 749, loss is 0.026990428566932678\n",
      "epoch: 10 step: 750, loss is 0.04325878247618675\n",
      "epoch: 10 step: 751, loss is 0.00020313385175541043\n",
      "epoch: 10 step: 752, loss is 0.013498306274414062\n",
      "epoch: 10 step: 753, loss is 0.0016604344127699733\n",
      "epoch: 10 step: 754, loss is 0.0001724993489915505\n",
      "epoch: 10 step: 755, loss is 8.535588131053373e-05\n",
      "epoch: 10 step: 756, loss is 4.280131543055177e-05\n",
      "epoch: 10 step: 757, loss is 0.0020083889830857515\n",
      "epoch: 10 step: 758, loss is 0.00029450381407514215\n",
      "epoch: 10 step: 759, loss is 0.00041221335413865745\n",
      "epoch: 10 step: 760, loss is 0.055602073669433594\n",
      "epoch: 10 step: 761, loss is 0.0003941985487472266\n",
      "epoch: 10 step: 762, loss is 0.007123786490410566\n",
      "epoch: 10 step: 763, loss is 0.00015567931404802948\n",
      "epoch: 10 step: 764, loss is 0.002424138830974698\n",
      "epoch: 10 step: 765, loss is 0.0017860697116702795\n",
      "epoch: 10 step: 766, loss is 0.016261376440525055\n",
      "epoch: 10 step: 767, loss is 0.000317034573527053\n",
      "epoch: 10 step: 768, loss is 0.002661007223650813\n",
      "epoch: 10 step: 769, loss is 0.0006835748208686709\n",
      "epoch: 10 step: 770, loss is 0.0007728523924015462\n",
      "epoch: 10 step: 771, loss is 0.0003820023266598582\n",
      "epoch: 10 step: 772, loss is 0.0001894541783258319\n",
      "epoch: 10 step: 773, loss is 0.0014576849061995745\n",
      "epoch: 10 step: 774, loss is 0.03961028903722763\n",
      "epoch: 10 step: 775, loss is 0.011692924425005913\n",
      "epoch: 10 step: 776, loss is 0.004977810196578503\n",
      "epoch: 10 step: 777, loss is 0.0025176454801112413\n",
      "epoch: 10 step: 778, loss is 0.02187635935842991\n",
      "epoch: 10 step: 779, loss is 0.07602420449256897\n",
      "epoch: 10 step: 780, loss is 0.0031991591677069664\n",
      "epoch: 10 step: 781, loss is 0.003156217047944665\n",
      "epoch: 10 step: 782, loss is 0.09644878655672073\n",
      "epoch: 10 step: 783, loss is 1.8529221051721834e-05\n",
      "epoch: 10 step: 784, loss is 0.00010319692228222266\n",
      "epoch: 10 step: 785, loss is 0.0004158715019002557\n",
      "epoch: 10 step: 786, loss is 0.3723442852497101\n",
      "epoch: 10 step: 787, loss is 0.00017896885401569307\n",
      "epoch: 10 step: 788, loss is 0.0023414152674376965\n",
      "epoch: 10 step: 789, loss is 8.389783033635467e-05\n",
      "epoch: 10 step: 790, loss is 0.005900230258703232\n",
      "epoch: 10 step: 791, loss is 0.0783836841583252\n",
      "epoch: 10 step: 792, loss is 0.000450878927949816\n",
      "epoch: 10 step: 793, loss is 0.0005439029191620648\n",
      "epoch: 10 step: 794, loss is 0.013335715979337692\n",
      "epoch: 10 step: 795, loss is 0.001107652555219829\n",
      "epoch: 10 step: 796, loss is 0.001920972834341228\n",
      "epoch: 10 step: 797, loss is 6.27163753961213e-05\n",
      "epoch: 10 step: 798, loss is 0.01203413400799036\n",
      "epoch: 10 step: 799, loss is 0.02877948246896267\n",
      "epoch: 10 step: 800, loss is 0.006517472211271524\n",
      "epoch: 10 step: 801, loss is 0.0027864028234034777\n",
      "epoch: 10 step: 802, loss is 0.0007075900794006884\n",
      "epoch: 10 step: 803, loss is 0.021633824333548546\n",
      "epoch: 10 step: 804, loss is 0.0003561984922271222\n",
      "epoch: 10 step: 805, loss is 0.00023483700351789594\n",
      "epoch: 10 step: 806, loss is 0.012686207890510559\n",
      "epoch: 10 step: 807, loss is 0.0011930664768442512\n",
      "epoch: 10 step: 808, loss is 0.04729132354259491\n",
      "epoch: 10 step: 809, loss is 0.005847567692399025\n",
      "epoch: 10 step: 810, loss is 0.03662881627678871\n",
      "epoch: 10 step: 811, loss is 0.001548652769997716\n",
      "epoch: 10 step: 812, loss is 0.0007142559043131769\n",
      "epoch: 10 step: 813, loss is 0.0014330192934721708\n",
      "epoch: 10 step: 814, loss is 0.00010762068268377334\n",
      "epoch: 10 step: 815, loss is 0.00012068670184817165\n",
      "epoch: 10 step: 816, loss is 0.00036014910438098013\n",
      "epoch: 10 step: 817, loss is 0.0008553840452805161\n",
      "epoch: 10 step: 818, loss is 0.09468664228916168\n",
      "epoch: 10 step: 819, loss is 0.0006594618316739798\n",
      "epoch: 10 step: 820, loss is 0.0017600383143872023\n",
      "epoch: 10 step: 821, loss is 0.000705067184753716\n",
      "epoch: 10 step: 822, loss is 0.0010199318639934063\n",
      "epoch: 10 step: 823, loss is 0.00023806560784578323\n",
      "epoch: 10 step: 824, loss is 0.005498334765434265\n",
      "epoch: 10 step: 825, loss is 0.0013076703762635589\n",
      "epoch: 10 step: 826, loss is 0.006668559741228819\n",
      "epoch: 10 step: 827, loss is 0.001010879292152822\n",
      "epoch: 10 step: 828, loss is 0.05869237706065178\n",
      "epoch: 10 step: 829, loss is 0.02514650672674179\n",
      "epoch: 10 step: 830, loss is 0.0011991674546152353\n",
      "epoch: 10 step: 831, loss is 9.432671504328027e-05\n",
      "epoch: 10 step: 832, loss is 0.0005996327963657677\n",
      "epoch: 10 step: 833, loss is 0.00846446305513382\n",
      "epoch: 10 step: 834, loss is 0.005940278526395559\n",
      "epoch: 10 step: 835, loss is 0.00318717653863132\n",
      "epoch: 10 step: 836, loss is 0.000732478394638747\n",
      "epoch: 10 step: 837, loss is 0.00033275395981036127\n",
      "epoch: 10 step: 838, loss is 0.17572329938411713\n",
      "epoch: 10 step: 839, loss is 0.0013244174188002944\n",
      "epoch: 10 step: 840, loss is 0.012879082933068275\n",
      "epoch: 10 step: 841, loss is 0.008053367026150227\n",
      "epoch: 10 step: 842, loss is 0.0001579087838763371\n",
      "epoch: 10 step: 843, loss is 0.00010060894419439137\n",
      "epoch: 10 step: 844, loss is 0.00029349152464419603\n",
      "epoch: 10 step: 845, loss is 0.0009958220180124044\n",
      "epoch: 10 step: 846, loss is 0.0007773501565679908\n",
      "epoch: 10 step: 847, loss is 0.0073604402132332325\n",
      "epoch: 10 step: 848, loss is 0.00012248492566868663\n",
      "epoch: 10 step: 849, loss is 0.0005254268762655556\n",
      "epoch: 10 step: 850, loss is 9.657293412601575e-05\n",
      "epoch: 10 step: 851, loss is 0.0005627713399007916\n",
      "epoch: 10 step: 852, loss is 9.462692105444148e-05\n",
      "epoch: 10 step: 853, loss is 0.0039491006173193455\n",
      "epoch: 10 step: 854, loss is 0.0012929511722177267\n",
      "epoch: 10 step: 855, loss is 0.010379267856478691\n",
      "epoch: 10 step: 856, loss is 0.0016318554989993572\n",
      "epoch: 10 step: 857, loss is 0.0004241972928866744\n",
      "epoch: 10 step: 858, loss is 0.002399383345618844\n",
      "epoch: 10 step: 859, loss is 0.00013621652033179998\n",
      "epoch: 10 step: 860, loss is 0.0003266272251494229\n",
      "epoch: 10 step: 861, loss is 0.016308508813381195\n",
      "epoch: 10 step: 862, loss is 0.006897445768117905\n",
      "epoch: 10 step: 863, loss is 0.0009042223682627082\n",
      "epoch: 10 step: 864, loss is 0.04518209770321846\n",
      "epoch: 10 step: 865, loss is 0.0007296816911548376\n",
      "epoch: 10 step: 866, loss is 5.6933156884042546e-05\n",
      "epoch: 10 step: 867, loss is 3.655744149000384e-05\n",
      "epoch: 10 step: 868, loss is 0.0022420850582420826\n",
      "epoch: 10 step: 869, loss is 0.00018622436618898064\n",
      "epoch: 10 step: 870, loss is 0.0025497442111372948\n",
      "epoch: 10 step: 871, loss is 0.0630737841129303\n",
      "epoch: 10 step: 872, loss is 0.0010923882946372032\n",
      "epoch: 10 step: 873, loss is 0.03381182625889778\n",
      "epoch: 10 step: 874, loss is 0.00020343755022622645\n",
      "epoch: 10 step: 875, loss is 0.10016026347875595\n",
      "epoch: 10 step: 876, loss is 0.00024153880076482892\n",
      "epoch: 10 step: 877, loss is 0.026452602818608284\n",
      "epoch: 10 step: 878, loss is 0.05580616742372513\n",
      "epoch: 10 step: 879, loss is 0.0006887261988595128\n",
      "epoch: 10 step: 880, loss is 0.0008587925112806261\n",
      "epoch: 10 step: 881, loss is 0.0007223149877972901\n",
      "epoch: 10 step: 882, loss is 0.001768333138898015\n",
      "epoch: 10 step: 883, loss is 0.0004371856339275837\n",
      "epoch: 10 step: 884, loss is 9.092013351619244e-05\n",
      "epoch: 10 step: 885, loss is 0.03291720524430275\n",
      "epoch: 10 step: 886, loss is 0.0025024947244673967\n",
      "epoch: 10 step: 887, loss is 7.092285522958264e-05\n",
      "epoch: 10 step: 888, loss is 0.0003196740581188351\n",
      "epoch: 10 step: 889, loss is 0.01088558230549097\n",
      "epoch: 10 step: 890, loss is 0.001734709134325385\n",
      "epoch: 10 step: 891, loss is 0.001729366835206747\n",
      "epoch: 10 step: 892, loss is 1.1176314728800207e-05\n",
      "epoch: 10 step: 893, loss is 0.048861440271139145\n",
      "epoch: 10 step: 894, loss is 0.016658447682857513\n",
      "epoch: 10 step: 895, loss is 0.00010904273221967742\n",
      "epoch: 10 step: 896, loss is 0.08016679435968399\n",
      "epoch: 10 step: 897, loss is 0.004865036811679602\n",
      "epoch: 10 step: 898, loss is 0.008019862696528435\n",
      "epoch: 10 step: 899, loss is 0.0002802018716465682\n",
      "epoch: 10 step: 900, loss is 0.0005377211491577327\n",
      "epoch: 10 step: 901, loss is 0.0006543160998262465\n",
      "epoch: 10 step: 902, loss is 0.004330817144364119\n",
      "epoch: 10 step: 903, loss is 0.0026554421056061983\n",
      "epoch: 10 step: 904, loss is 0.004043140914291143\n",
      "epoch: 10 step: 905, loss is 0.01486499048769474\n",
      "epoch: 10 step: 906, loss is 0.00040585617534816265\n",
      "epoch: 10 step: 907, loss is 0.0009150841506198049\n",
      "epoch: 10 step: 908, loss is 0.005029281135648489\n",
      "epoch: 10 step: 909, loss is 0.00015121905016712844\n",
      "epoch: 10 step: 910, loss is 5.133374725119211e-05\n",
      "epoch: 10 step: 911, loss is 0.0002266989031340927\n",
      "epoch: 10 step: 912, loss is 0.0028721624985337257\n",
      "epoch: 10 step: 913, loss is 0.00036304534296505153\n",
      "epoch: 10 step: 914, loss is 0.0022458091843873262\n",
      "epoch: 10 step: 915, loss is 0.002416084986180067\n",
      "epoch: 10 step: 916, loss is 0.026074489578604698\n",
      "epoch: 10 step: 917, loss is 0.0007860761834308505\n",
      "epoch: 10 step: 918, loss is 0.0011950586922466755\n",
      "epoch: 10 step: 919, loss is 0.002227615565061569\n",
      "epoch: 10 step: 920, loss is 0.022919580340385437\n",
      "epoch: 10 step: 921, loss is 0.00022838432050775737\n",
      "epoch: 10 step: 922, loss is 0.0007497004698961973\n",
      "epoch: 10 step: 923, loss is 0.0008941715350374579\n",
      "epoch: 10 step: 924, loss is 0.0041808076202869415\n",
      "epoch: 10 step: 925, loss is 0.03977268189191818\n",
      "epoch: 10 step: 926, loss is 0.002499908674508333\n",
      "epoch: 10 step: 927, loss is 0.0002792901068460196\n",
      "epoch: 10 step: 928, loss is 0.01497466117143631\n",
      "epoch: 10 step: 929, loss is 3.395597013877705e-05\n",
      "epoch: 10 step: 930, loss is 0.0030203917995095253\n",
      "epoch: 10 step: 931, loss is 0.005505900364369154\n",
      "epoch: 10 step: 932, loss is 0.0010544167598709464\n",
      "epoch: 10 step: 933, loss is 0.08734223991632462\n",
      "epoch: 10 step: 934, loss is 0.006718216463923454\n",
      "epoch: 10 step: 935, loss is 0.06797562539577484\n",
      "epoch: 10 step: 936, loss is 0.12634822726249695\n",
      "epoch: 10 step: 937, loss is 0.019934510812163353\n",
      "epoch: 10 step: 938, loss is 0.0981353223323822\n",
      "epoch: 10 step: 939, loss is 0.029299499467015266\n",
      "epoch: 10 step: 940, loss is 0.17705288529396057\n",
      "epoch: 10 step: 941, loss is 0.00029181528952904046\n",
      "epoch: 10 step: 942, loss is 0.005393105559051037\n",
      "epoch: 10 step: 943, loss is 0.003831104375422001\n",
      "epoch: 10 step: 944, loss is 0.020547088235616684\n",
      "epoch: 10 step: 945, loss is 0.000601826817728579\n",
      "epoch: 10 step: 946, loss is 0.00025942575302906334\n",
      "epoch: 10 step: 947, loss is 0.0037595280446112156\n",
      "epoch: 10 step: 948, loss is 0.00046691796160303056\n",
      "epoch: 10 step: 949, loss is 0.03887413442134857\n",
      "epoch: 10 step: 950, loss is 0.0006155334413051605\n",
      "epoch: 10 step: 951, loss is 0.06184305250644684\n",
      "epoch: 10 step: 952, loss is 0.0005859325756318867\n",
      "epoch: 10 step: 953, loss is 7.830768299754709e-05\n",
      "epoch: 10 step: 954, loss is 0.0002860861422959715\n",
      "epoch: 10 step: 955, loss is 0.009345171973109245\n",
      "epoch: 10 step: 956, loss is 0.029732901602983475\n",
      "epoch: 10 step: 957, loss is 0.016542546451091766\n",
      "epoch: 10 step: 958, loss is 0.0007725141476839781\n",
      "epoch: 10 step: 959, loss is 0.007860221900045872\n",
      "epoch: 10 step: 960, loss is 0.00650047417730093\n",
      "epoch: 10 step: 961, loss is 0.0009341053664684296\n",
      "epoch: 10 step: 962, loss is 0.0005058787646703422\n",
      "epoch: 10 step: 963, loss is 0.003491294104605913\n",
      "epoch: 10 step: 964, loss is 0.10901526361703873\n",
      "epoch: 10 step: 965, loss is 0.09274991601705551\n",
      "epoch: 10 step: 966, loss is 0.0605255551636219\n",
      "epoch: 10 step: 967, loss is 0.0032521802932024\n",
      "epoch: 10 step: 968, loss is 0.0010475683957338333\n",
      "epoch: 10 step: 969, loss is 0.008453807793557644\n",
      "epoch: 10 step: 970, loss is 0.019320372492074966\n",
      "epoch: 10 step: 971, loss is 9.86800660029985e-05\n",
      "epoch: 10 step: 972, loss is 0.00162847142200917\n",
      "epoch: 10 step: 973, loss is 0.05494605749845505\n",
      "epoch: 10 step: 974, loss is 0.00712021766230464\n",
      "epoch: 10 step: 975, loss is 0.0017716126749292016\n",
      "epoch: 10 step: 976, loss is 0.03885014355182648\n",
      "epoch: 10 step: 977, loss is 0.0007436658488586545\n",
      "epoch: 10 step: 978, loss is 0.005674843210726976\n",
      "epoch: 10 step: 979, loss is 0.04951377585530281\n",
      "epoch: 10 step: 980, loss is 0.010364926420152187\n",
      "epoch: 10 step: 981, loss is 0.04421110823750496\n",
      "epoch: 10 step: 982, loss is 0.0002918296668212861\n",
      "epoch: 10 step: 983, loss is 0.0018796169897541404\n",
      "epoch: 10 step: 984, loss is 0.0006129171815700829\n",
      "epoch: 10 step: 985, loss is 0.0015051858499646187\n",
      "epoch: 10 step: 986, loss is 4.243837975081988e-05\n",
      "epoch: 10 step: 987, loss is 0.02642495185136795\n",
      "epoch: 10 step: 988, loss is 0.0005094167427159846\n",
      "epoch: 10 step: 989, loss is 0.0006513621774502099\n",
      "epoch: 10 step: 990, loss is 0.0626324713230133\n",
      "epoch: 10 step: 991, loss is 0.0014544755686074495\n",
      "epoch: 10 step: 992, loss is 0.01470954716205597\n",
      "epoch: 10 step: 993, loss is 0.005690719932317734\n",
      "epoch: 10 step: 994, loss is 0.0012307872530072927\n",
      "epoch: 10 step: 995, loss is 0.0032275544945150614\n",
      "epoch: 10 step: 996, loss is 0.008470865897834301\n",
      "epoch: 10 step: 997, loss is 0.023096006363630295\n",
      "epoch: 10 step: 998, loss is 0.006533669773489237\n",
      "epoch: 10 step: 999, loss is 0.0455540269613266\n",
      "epoch: 10 step: 1000, loss is 0.16270779073238373\n",
      "epoch: 10 step: 1001, loss is 0.00018959994486067444\n",
      "epoch: 10 step: 1002, loss is 0.0015487720957025886\n",
      "epoch: 10 step: 1003, loss is 0.0004629524191841483\n",
      "epoch: 10 step: 1004, loss is 0.025951717048883438\n",
      "epoch: 10 step: 1005, loss is 0.034455396234989166\n",
      "epoch: 10 step: 1006, loss is 0.0007642649579793215\n",
      "epoch: 10 step: 1007, loss is 0.035116590559482574\n",
      "epoch: 10 step: 1008, loss is 0.05655321478843689\n",
      "epoch: 10 step: 1009, loss is 0.0006418318953365088\n",
      "epoch: 10 step: 1010, loss is 0.0003898973227478564\n",
      "epoch: 10 step: 1011, loss is 0.0008841761155053973\n",
      "epoch: 10 step: 1012, loss is 0.0050912522710859776\n",
      "epoch: 10 step: 1013, loss is 0.0006065097404643893\n",
      "epoch: 10 step: 1014, loss is 0.004708501975983381\n",
      "epoch: 10 step: 1015, loss is 0.0012481356970965862\n",
      "epoch: 10 step: 1016, loss is 0.00020571090863086283\n",
      "epoch: 10 step: 1017, loss is 0.022810248658061028\n",
      "epoch: 10 step: 1018, loss is 0.0006612180732190609\n",
      "epoch: 10 step: 1019, loss is 0.0003397157706785947\n",
      "epoch: 10 step: 1020, loss is 0.03203140199184418\n",
      "epoch: 10 step: 1021, loss is 0.002952328883111477\n",
      "epoch: 10 step: 1022, loss is 0.026844773441553116\n",
      "epoch: 10 step: 1023, loss is 0.00027184488135389984\n",
      "epoch: 10 step: 1024, loss is 0.00015697948401793838\n",
      "epoch: 10 step: 1025, loss is 0.0011339260963723063\n",
      "epoch: 10 step: 1026, loss is 0.0058928742073476315\n",
      "epoch: 10 step: 1027, loss is 0.040867503732442856\n",
      "epoch: 10 step: 1028, loss is 0.00033704409725032747\n",
      "epoch: 10 step: 1029, loss is 0.006161253433674574\n",
      "epoch: 10 step: 1030, loss is 0.0007510624127462506\n",
      "epoch: 10 step: 1031, loss is 0.0036062579602003098\n",
      "epoch: 10 step: 1032, loss is 0.0007107266574166715\n",
      "epoch: 10 step: 1033, loss is 0.06636504828929901\n",
      "epoch: 10 step: 1034, loss is 7.634567737113684e-05\n",
      "epoch: 10 step: 1035, loss is 0.00029910821467638016\n",
      "epoch: 10 step: 1036, loss is 0.009081020019948483\n",
      "epoch: 10 step: 1037, loss is 0.00020268000662326813\n",
      "epoch: 10 step: 1038, loss is 0.0006248203571885824\n",
      "epoch: 10 step: 1039, loss is 0.0470907986164093\n",
      "epoch: 10 step: 1040, loss is 0.009923117235302925\n",
      "epoch: 10 step: 1041, loss is 0.0006016920087859035\n",
      "epoch: 10 step: 1042, loss is 8.428622095379978e-05\n",
      "epoch: 10 step: 1043, loss is 0.0020748290698975325\n",
      "epoch: 10 step: 1044, loss is 0.02100277692079544\n",
      "epoch: 10 step: 1045, loss is 0.0004156410286668688\n",
      "epoch: 10 step: 1046, loss is 5.9154033806407824e-05\n",
      "epoch: 10 step: 1047, loss is 3.736513463081792e-05\n",
      "epoch: 10 step: 1048, loss is 0.003807990113273263\n",
      "epoch: 10 step: 1049, loss is 0.0009756942745298147\n",
      "epoch: 10 step: 1050, loss is 0.00013507780386134982\n",
      "epoch: 10 step: 1051, loss is 5.188710929360241e-05\n",
      "epoch: 10 step: 1052, loss is 7.130888116080314e-05\n",
      "epoch: 10 step: 1053, loss is 2.564584974606987e-05\n",
      "epoch: 10 step: 1054, loss is 0.0001903479715110734\n",
      "epoch: 10 step: 1055, loss is 0.001462025218643248\n",
      "epoch: 10 step: 1056, loss is 0.01943090185523033\n",
      "epoch: 10 step: 1057, loss is 0.01129196584224701\n",
      "epoch: 10 step: 1058, loss is 0.0008775345049798489\n",
      "epoch: 10 step: 1059, loss is 0.018077153712511063\n",
      "epoch: 10 step: 1060, loss is 0.00030976685229688883\n",
      "epoch: 10 step: 1061, loss is 0.03549406677484512\n",
      "epoch: 10 step: 1062, loss is 3.421615838306025e-05\n",
      "epoch: 10 step: 1063, loss is 0.01648597978055477\n",
      "epoch: 10 step: 1064, loss is 0.00012545351637527347\n",
      "epoch: 10 step: 1065, loss is 0.0021687676198780537\n",
      "epoch: 10 step: 1066, loss is 2.3049449737300165e-05\n",
      "epoch: 10 step: 1067, loss is 0.00032910940353758633\n",
      "epoch: 10 step: 1068, loss is 0.033874306827783585\n",
      "epoch: 10 step: 1069, loss is 0.03425263985991478\n",
      "epoch: 10 step: 1070, loss is 0.002071816474199295\n",
      "epoch: 10 step: 1071, loss is 0.0001490604190621525\n",
      "epoch: 10 step: 1072, loss is 0.004079144913703203\n",
      "epoch: 10 step: 1073, loss is 0.028086544945836067\n",
      "epoch: 10 step: 1074, loss is 0.0013735061511397362\n",
      "epoch: 10 step: 1075, loss is 8.546197204850614e-05\n",
      "epoch: 10 step: 1076, loss is 0.037213198840618134\n",
      "epoch: 10 step: 1077, loss is 0.005529121961444616\n",
      "epoch: 10 step: 1078, loss is 0.0051224809139966965\n",
      "epoch: 10 step: 1079, loss is 0.005178768187761307\n",
      "epoch: 10 step: 1080, loss is 0.003890146967023611\n",
      "epoch: 10 step: 1081, loss is 0.0006644242093898356\n",
      "epoch: 10 step: 1082, loss is 0.07496371120214462\n",
      "epoch: 10 step: 1083, loss is 7.661403651582077e-05\n",
      "epoch: 10 step: 1084, loss is 0.00020260346354916692\n",
      "epoch: 10 step: 1085, loss is 0.010307100601494312\n",
      "epoch: 10 step: 1086, loss is 0.014429325237870216\n",
      "epoch: 10 step: 1087, loss is 0.022059332579374313\n",
      "epoch: 10 step: 1088, loss is 0.0008277521119453013\n",
      "epoch: 10 step: 1089, loss is 0.024850551038980484\n",
      "epoch: 10 step: 1090, loss is 9.696692723082379e-05\n",
      "epoch: 10 step: 1091, loss is 0.01070126611739397\n",
      "epoch: 10 step: 1092, loss is 0.013535569421947002\n",
      "epoch: 10 step: 1093, loss is 0.00013443437637761235\n",
      "epoch: 10 step: 1094, loss is 2.7415919248596765e-05\n",
      "epoch: 10 step: 1095, loss is 1.0799989468068816e-05\n",
      "epoch: 10 step: 1096, loss is 0.0003812245558947325\n",
      "epoch: 10 step: 1097, loss is 4.4070984586142e-06\n",
      "epoch: 10 step: 1098, loss is 0.0006361278938129544\n",
      "epoch: 10 step: 1099, loss is 0.15462149679660797\n",
      "epoch: 10 step: 1100, loss is 0.00025728592299856246\n",
      "epoch: 10 step: 1101, loss is 0.0529605969786644\n",
      "epoch: 10 step: 1102, loss is 0.00019745237659662962\n",
      "epoch: 10 step: 1103, loss is 0.0008251931867562234\n",
      "epoch: 10 step: 1104, loss is 0.0021069305948913097\n",
      "epoch: 10 step: 1105, loss is 0.0033634966239333153\n",
      "epoch: 10 step: 1106, loss is 0.00039518639096058905\n",
      "epoch: 10 step: 1107, loss is 2.099375706166029e-05\n",
      "epoch: 10 step: 1108, loss is 0.00013407829101197422\n",
      "epoch: 10 step: 1109, loss is 0.002975418698042631\n",
      "epoch: 10 step: 1110, loss is 0.0002691950066946447\n",
      "epoch: 10 step: 1111, loss is 0.086431585252285\n",
      "epoch: 10 step: 1112, loss is 0.007739149034023285\n",
      "epoch: 10 step: 1113, loss is 0.0005875450442545116\n",
      "epoch: 10 step: 1114, loss is 0.0003389330522622913\n",
      "epoch: 10 step: 1115, loss is 0.0014190498040989041\n",
      "epoch: 10 step: 1116, loss is 0.0019200906390324235\n",
      "epoch: 10 step: 1117, loss is 2.5470451873843558e-05\n",
      "epoch: 10 step: 1118, loss is 0.0012096884893253446\n",
      "epoch: 10 step: 1119, loss is 0.0012427462497726083\n",
      "epoch: 10 step: 1120, loss is 0.01801847666501999\n",
      "epoch: 10 step: 1121, loss is 0.0002263173519168049\n",
      "epoch: 10 step: 1122, loss is 0.0036831179168075323\n",
      "epoch: 10 step: 1123, loss is 0.026675749570131302\n",
      "epoch: 10 step: 1124, loss is 0.004059425555169582\n",
      "epoch: 10 step: 1125, loss is 0.00015321638784371316\n",
      "epoch: 10 step: 1126, loss is 0.00018531813111621886\n",
      "epoch: 10 step: 1127, loss is 0.00014456348435487598\n",
      "epoch: 10 step: 1128, loss is 0.0001395777362631634\n",
      "epoch: 10 step: 1129, loss is 0.01887485198676586\n",
      "epoch: 10 step: 1130, loss is 0.0005027443985454738\n",
      "epoch: 10 step: 1131, loss is 0.021918559446930885\n",
      "epoch: 10 step: 1132, loss is 0.00389542942866683\n",
      "epoch: 10 step: 1133, loss is 0.000207478558877483\n",
      "epoch: 10 step: 1134, loss is 0.03854154422879219\n",
      "epoch: 10 step: 1135, loss is 0.0007867477252148092\n",
      "epoch: 10 step: 1136, loss is 0.0033466664608567953\n",
      "epoch: 10 step: 1137, loss is 3.61781676474493e-05\n",
      "epoch: 10 step: 1138, loss is 0.0001401656772941351\n",
      "epoch: 10 step: 1139, loss is 0.007166765630245209\n",
      "epoch: 10 step: 1140, loss is 0.00020495234639383852\n",
      "epoch: 10 step: 1141, loss is 6.492476677522063e-05\n",
      "epoch: 10 step: 1142, loss is 0.0007277295808307827\n",
      "epoch: 10 step: 1143, loss is 0.005602174438536167\n",
      "epoch: 10 step: 1144, loss is 7.209925388451666e-05\n",
      "epoch: 10 step: 1145, loss is 2.821344787662383e-05\n",
      "epoch: 10 step: 1146, loss is 0.0024911370128393173\n",
      "epoch: 10 step: 1147, loss is 2.4776147256488912e-05\n",
      "epoch: 10 step: 1148, loss is 6.918206054251641e-05\n",
      "epoch: 10 step: 1149, loss is 0.0003341981500852853\n",
      "epoch: 10 step: 1150, loss is 0.00865979865193367\n",
      "epoch: 10 step: 1151, loss is 0.0014599707210436463\n",
      "epoch: 10 step: 1152, loss is 0.0004820641770493239\n",
      "epoch: 10 step: 1153, loss is 0.018915778025984764\n",
      "epoch: 10 step: 1154, loss is 0.00039420975372195244\n",
      "epoch: 10 step: 1155, loss is 0.00028201364330016077\n",
      "epoch: 10 step: 1156, loss is 0.0016329202335327864\n",
      "epoch: 10 step: 1157, loss is 0.048044729977846146\n",
      "epoch: 10 step: 1158, loss is 7.835296855773777e-05\n",
      "epoch: 10 step: 1159, loss is 0.0802510678768158\n",
      "epoch: 10 step: 1160, loss is 0.006722600664943457\n",
      "epoch: 10 step: 1161, loss is 9.876536205410957e-05\n",
      "epoch: 10 step: 1162, loss is 0.0006838603876531124\n",
      "epoch: 10 step: 1163, loss is 0.00047863207873888314\n",
      "epoch: 10 step: 1164, loss is 0.0009054878610186279\n",
      "epoch: 10 step: 1165, loss is 0.13219639658927917\n",
      "epoch: 10 step: 1166, loss is 9.655084431869909e-05\n",
      "epoch: 10 step: 1167, loss is 0.005819146521389484\n",
      "epoch: 10 step: 1168, loss is 0.001232362468726933\n",
      "epoch: 10 step: 1169, loss is 6.622033106395975e-05\n",
      "epoch: 10 step: 1170, loss is 0.03463180363178253\n",
      "epoch: 10 step: 1171, loss is 0.002102837897837162\n",
      "epoch: 10 step: 1172, loss is 0.0016760709695518017\n",
      "epoch: 10 step: 1173, loss is 1.1921500117750838e-05\n",
      "epoch: 10 step: 1174, loss is 0.0049903374165296555\n",
      "epoch: 10 step: 1175, loss is 0.02341054379940033\n",
      "epoch: 10 step: 1176, loss is 0.0007125320262275636\n",
      "epoch: 10 step: 1177, loss is 0.00396890752017498\n",
      "epoch: 10 step: 1178, loss is 0.00022443146735895425\n",
      "epoch: 10 step: 1179, loss is 0.09792554378509521\n",
      "epoch: 10 step: 1180, loss is 0.00036661108606494963\n",
      "epoch: 10 step: 1181, loss is 0.012915335595607758\n",
      "epoch: 10 step: 1182, loss is 0.0012483035679906607\n",
      "epoch: 10 step: 1183, loss is 0.023395109921693802\n",
      "epoch: 10 step: 1184, loss is 0.010193475522100925\n",
      "epoch: 10 step: 1185, loss is 0.0023304466158151627\n",
      "epoch: 10 step: 1186, loss is 0.0014795962488278747\n",
      "epoch: 10 step: 1187, loss is 0.03746801242232323\n",
      "epoch: 10 step: 1188, loss is 0.000173185981111601\n",
      "epoch: 10 step: 1189, loss is 7.716083200648427e-05\n",
      "epoch: 10 step: 1190, loss is 7.633243512827903e-05\n",
      "epoch: 10 step: 1191, loss is 0.00010804540215758607\n",
      "epoch: 10 step: 1192, loss is 2.0272351321182214e-05\n",
      "epoch: 10 step: 1193, loss is 0.00012629726552404463\n",
      "epoch: 10 step: 1194, loss is 0.0003212063165847212\n",
      "epoch: 10 step: 1195, loss is 0.03698735684156418\n",
      "epoch: 10 step: 1196, loss is 0.0009578084573149681\n",
      "epoch: 10 step: 1197, loss is 0.12311234325170517\n",
      "epoch: 10 step: 1198, loss is 0.000751570682041347\n",
      "epoch: 10 step: 1199, loss is 0.0001288418279727921\n",
      "epoch: 10 step: 1200, loss is 0.00023338472237810493\n",
      "epoch: 10 step: 1201, loss is 0.021402006968855858\n",
      "epoch: 10 step: 1202, loss is 0.00044344193884171546\n",
      "epoch: 10 step: 1203, loss is 0.0035231064539402723\n",
      "epoch: 10 step: 1204, loss is 0.030538739636540413\n",
      "epoch: 10 step: 1205, loss is 0.00013494343147613108\n",
      "epoch: 10 step: 1206, loss is 0.005446750205010176\n",
      "epoch: 10 step: 1207, loss is 0.00019052563584409654\n",
      "epoch: 10 step: 1208, loss is 0.008003808557987213\n",
      "epoch: 10 step: 1209, loss is 0.047666676342487335\n",
      "epoch: 10 step: 1210, loss is 0.018349092453718185\n",
      "epoch: 10 step: 1211, loss is 0.003749373834580183\n",
      "epoch: 10 step: 1212, loss is 0.06309323012828827\n",
      "epoch: 10 step: 1213, loss is 5.922521813772619e-05\n",
      "epoch: 10 step: 1214, loss is 0.0003920390736311674\n",
      "epoch: 10 step: 1215, loss is 0.013922052457928658\n",
      "epoch: 10 step: 1216, loss is 0.0003569567925296724\n",
      "epoch: 10 step: 1217, loss is 7.42527554393746e-05\n",
      "epoch: 10 step: 1218, loss is 0.002017449587583542\n",
      "epoch: 10 step: 1219, loss is 0.0004911950090900064\n",
      "epoch: 10 step: 1220, loss is 0.0011111960047855973\n",
      "epoch: 10 step: 1221, loss is 0.00012200750643387437\n",
      "epoch: 10 step: 1222, loss is 6.373101496137679e-05\n",
      "epoch: 10 step: 1223, loss is 5.9644124121405184e-05\n",
      "epoch: 10 step: 1224, loss is 0.030448881909251213\n",
      "epoch: 10 step: 1225, loss is 0.021318916231393814\n",
      "epoch: 10 step: 1226, loss is 0.004282668232917786\n",
      "epoch: 10 step: 1227, loss is 0.00731672765687108\n",
      "epoch: 10 step: 1228, loss is 0.000311097624944523\n",
      "epoch: 10 step: 1229, loss is 0.0007442506030201912\n",
      "epoch: 10 step: 1230, loss is 0.00012801430420950055\n",
      "epoch: 10 step: 1231, loss is 0.1737617701292038\n",
      "epoch: 10 step: 1232, loss is 0.0063062808476388454\n",
      "epoch: 10 step: 1233, loss is 0.0001753019168972969\n",
      "epoch: 10 step: 1234, loss is 0.0004247776232659817\n",
      "epoch: 10 step: 1235, loss is 0.15865099430084229\n",
      "epoch: 10 step: 1236, loss is 0.0010078044142574072\n",
      "epoch: 10 step: 1237, loss is 0.005008663050830364\n",
      "epoch: 10 step: 1238, loss is 0.004360052291303873\n",
      "epoch: 10 step: 1239, loss is 0.001933868508785963\n",
      "epoch: 10 step: 1240, loss is 0.020985864102840424\n",
      "epoch: 10 step: 1241, loss is 0.00018833225476555526\n",
      "epoch: 10 step: 1242, loss is 0.00019635989156085998\n",
      "epoch: 10 step: 1243, loss is 7.301515142899007e-05\n",
      "epoch: 10 step: 1244, loss is 0.00039408105658367276\n",
      "epoch: 10 step: 1245, loss is 7.889261905802414e-05\n",
      "epoch: 10 step: 1246, loss is 0.0005089500918984413\n",
      "epoch: 10 step: 1247, loss is 0.0007582941907458007\n",
      "epoch: 10 step: 1248, loss is 0.00011926311708521098\n",
      "epoch: 10 step: 1249, loss is 0.0002912805648520589\n",
      "epoch: 10 step: 1250, loss is 0.03934714198112488\n",
      "epoch: 10 step: 1251, loss is 0.0008551233331672847\n",
      "epoch: 10 step: 1252, loss is 0.1018507108092308\n",
      "epoch: 10 step: 1253, loss is 0.0003168937982991338\n",
      "epoch: 10 step: 1254, loss is 0.1558726727962494\n",
      "epoch: 10 step: 1255, loss is 0.008052116259932518\n",
      "epoch: 10 step: 1256, loss is 0.0025428207591176033\n",
      "epoch: 10 step: 1257, loss is 0.023709503933787346\n",
      "epoch: 10 step: 1258, loss is 0.00855946447700262\n",
      "epoch: 10 step: 1259, loss is 0.0021455748938024044\n",
      "epoch: 10 step: 1260, loss is 7.961375558807049e-06\n",
      "epoch: 10 step: 1261, loss is 0.00011361759243300185\n",
      "epoch: 10 step: 1262, loss is 0.00015079972217790782\n",
      "epoch: 10 step: 1263, loss is 0.000581588305067271\n",
      "epoch: 10 step: 1264, loss is 6.442968151532114e-05\n",
      "epoch: 10 step: 1265, loss is 0.0056791347451508045\n",
      "epoch: 10 step: 1266, loss is 0.005436339415609837\n",
      "epoch: 10 step: 1267, loss is 0.042516231536865234\n",
      "epoch: 10 step: 1268, loss is 0.00037490666727535427\n",
      "epoch: 10 step: 1269, loss is 2.4640230549266562e-05\n",
      "epoch: 10 step: 1270, loss is 0.0008170445798896253\n",
      "epoch: 10 step: 1271, loss is 0.0024752786848694086\n",
      "epoch: 10 step: 1272, loss is 0.062096379697322845\n",
      "epoch: 10 step: 1273, loss is 0.00023359054466709495\n",
      "epoch: 10 step: 1274, loss is 7.055783498799428e-05\n",
      "epoch: 10 step: 1275, loss is 0.007500768173485994\n",
      "epoch: 10 step: 1276, loss is 0.0030979057773947716\n",
      "epoch: 10 step: 1277, loss is 0.0007751636439934373\n",
      "epoch: 10 step: 1278, loss is 0.0005855847266502678\n",
      "epoch: 10 step: 1279, loss is 0.10845078527927399\n",
      "epoch: 10 step: 1280, loss is 0.04800626263022423\n",
      "epoch: 10 step: 1281, loss is 0.00022287933097686619\n",
      "epoch: 10 step: 1282, loss is 0.0007007801323197782\n",
      "epoch: 10 step: 1283, loss is 0.003942052833735943\n",
      "epoch: 10 step: 1284, loss is 0.0036858217790722847\n",
      "epoch: 10 step: 1285, loss is 0.0007045582751743495\n",
      "epoch: 10 step: 1286, loss is 0.0028698062524199486\n",
      "epoch: 10 step: 1287, loss is 0.12267624586820602\n",
      "epoch: 10 step: 1288, loss is 0.006478899158537388\n",
      "epoch: 10 step: 1289, loss is 0.0264586191624403\n",
      "epoch: 10 step: 1290, loss is 1.9080902347923256e-05\n",
      "epoch: 10 step: 1291, loss is 0.00045906842569820583\n",
      "epoch: 10 step: 1292, loss is 0.00553525285795331\n",
      "epoch: 10 step: 1293, loss is 7.698284753132612e-05\n",
      "epoch: 10 step: 1294, loss is 0.04954466596245766\n",
      "epoch: 10 step: 1295, loss is 0.006226846016943455\n",
      "epoch: 10 step: 1296, loss is 0.0017076840158551931\n",
      "epoch: 10 step: 1297, loss is 0.09581556171178818\n",
      "epoch: 10 step: 1298, loss is 0.15629853308200836\n",
      "epoch: 10 step: 1299, loss is 6.129532994236797e-05\n",
      "epoch: 10 step: 1300, loss is 2.3703913029748946e-05\n",
      "epoch: 10 step: 1301, loss is 5.4658506996929646e-05\n",
      "epoch: 10 step: 1302, loss is 0.00207850756123662\n",
      "epoch: 10 step: 1303, loss is 0.004465173929929733\n",
      "epoch: 10 step: 1304, loss is 0.07106800377368927\n",
      "epoch: 10 step: 1305, loss is 0.005685784388333559\n",
      "epoch: 10 step: 1306, loss is 0.18644921481609344\n",
      "epoch: 10 step: 1307, loss is 0.013795274309813976\n",
      "epoch: 10 step: 1308, loss is 0.00016795571718830615\n",
      "epoch: 10 step: 1309, loss is 0.00021620217012241483\n",
      "epoch: 10 step: 1310, loss is 0.0007425806252285838\n",
      "epoch: 10 step: 1311, loss is 0.000637119694147259\n",
      "epoch: 10 step: 1312, loss is 0.010953133925795555\n",
      "epoch: 10 step: 1313, loss is 0.002540921326726675\n",
      "epoch: 10 step: 1314, loss is 2.4789893359411508e-05\n",
      "epoch: 10 step: 1315, loss is 0.0001461135398130864\n",
      "epoch: 10 step: 1316, loss is 0.00012216600589454174\n",
      "epoch: 10 step: 1317, loss is 0.0008863054681569338\n",
      "epoch: 10 step: 1318, loss is 0.0007661353447474539\n",
      "epoch: 10 step: 1319, loss is 0.00027968198992311954\n",
      "epoch: 10 step: 1320, loss is 5.282037818687968e-05\n",
      "epoch: 10 step: 1321, loss is 6.187885446706787e-05\n",
      "epoch: 10 step: 1322, loss is 0.0008351111318916082\n",
      "epoch: 10 step: 1323, loss is 0.028064733371138573\n",
      "epoch: 10 step: 1324, loss is 0.00023655331460759044\n",
      "epoch: 10 step: 1325, loss is 0.0018054794054478407\n",
      "epoch: 10 step: 1326, loss is 0.010162457823753357\n",
      "epoch: 10 step: 1327, loss is 0.00017365528037771583\n",
      "epoch: 10 step: 1328, loss is 0.0003174150479026139\n",
      "epoch: 10 step: 1329, loss is 0.009644314646720886\n",
      "epoch: 10 step: 1330, loss is 0.0008299516630358994\n",
      "epoch: 10 step: 1331, loss is 0.006736866198480129\n",
      "epoch: 10 step: 1332, loss is 0.00011890099267475307\n",
      "epoch: 10 step: 1333, loss is 0.006285442970693111\n",
      "epoch: 10 step: 1334, loss is 0.010565943084657192\n",
      "epoch: 10 step: 1335, loss is 0.021612809970974922\n",
      "epoch: 10 step: 1336, loss is 0.0033018863759934902\n",
      "epoch: 10 step: 1337, loss is 0.0011689105303958058\n",
      "epoch: 10 step: 1338, loss is 0.0005642366595566273\n",
      "epoch: 10 step: 1339, loss is 0.002194950357079506\n",
      "epoch: 10 step: 1340, loss is 0.0012403378495946527\n",
      "epoch: 10 step: 1341, loss is 0.0007179921958595514\n",
      "epoch: 10 step: 1342, loss is 0.005001266486942768\n",
      "epoch: 10 step: 1343, loss is 0.00042304405360482633\n",
      "epoch: 10 step: 1344, loss is 0.005729857832193375\n",
      "epoch: 10 step: 1345, loss is 0.0026096440851688385\n",
      "epoch: 10 step: 1346, loss is 6.921569001860917e-05\n",
      "epoch: 10 step: 1347, loss is 0.01931513287127018\n",
      "epoch: 10 step: 1348, loss is 0.0012923527974635363\n",
      "epoch: 10 step: 1349, loss is 0.0005145471077412367\n",
      "epoch: 10 step: 1350, loss is 0.11802367866039276\n",
      "epoch: 10 step: 1351, loss is 0.00033351604361087084\n",
      "epoch: 10 step: 1352, loss is 0.0001549634471302852\n",
      "epoch: 10 step: 1353, loss is 0.0015732931206002831\n",
      "epoch: 10 step: 1354, loss is 0.0009802674176171422\n",
      "epoch: 10 step: 1355, loss is 0.0066949897445738316\n",
      "epoch: 10 step: 1356, loss is 0.001236367505043745\n",
      "epoch: 10 step: 1357, loss is 0.0071427603252232075\n",
      "epoch: 10 step: 1358, loss is 0.00022316341346595436\n",
      "epoch: 10 step: 1359, loss is 2.033621603914071e-05\n",
      "epoch: 10 step: 1360, loss is 1.9744213659578236e-06\n",
      "epoch: 10 step: 1361, loss is 3.041106538148597e-05\n",
      "epoch: 10 step: 1362, loss is 0.0009732699254527688\n",
      "epoch: 10 step: 1363, loss is 0.0005401226808317006\n",
      "epoch: 10 step: 1364, loss is 0.4073568880558014\n",
      "epoch: 10 step: 1365, loss is 5.625342964776792e-06\n",
      "epoch: 10 step: 1366, loss is 0.008277584798634052\n",
      "epoch: 10 step: 1367, loss is 0.006926106754690409\n",
      "epoch: 10 step: 1368, loss is 0.000497779983561486\n",
      "epoch: 10 step: 1369, loss is 4.2904492147499695e-05\n",
      "epoch: 10 step: 1370, loss is 0.006709430832415819\n",
      "epoch: 10 step: 1371, loss is 0.0001308111532125622\n",
      "epoch: 10 step: 1372, loss is 0.0003356492379680276\n",
      "epoch: 10 step: 1373, loss is 5.232529656495899e-05\n",
      "epoch: 10 step: 1374, loss is 9.125459473580122e-05\n",
      "epoch: 10 step: 1375, loss is 0.12505477666854858\n",
      "epoch: 10 step: 1376, loss is 0.0017208332428708673\n",
      "epoch: 10 step: 1377, loss is 0.001065860502421856\n",
      "epoch: 10 step: 1378, loss is 0.0018645020900294185\n",
      "epoch: 10 step: 1379, loss is 8.38221749290824e-05\n",
      "epoch: 10 step: 1380, loss is 0.0002471347397658974\n",
      "epoch: 10 step: 1381, loss is 4.311087468522601e-05\n",
      "epoch: 10 step: 1382, loss is 0.0011338199255988002\n",
      "epoch: 10 step: 1383, loss is 5.609045547316782e-05\n",
      "epoch: 10 step: 1384, loss is 0.0008353309240192175\n",
      "epoch: 10 step: 1385, loss is 3.2532909244764596e-05\n",
      "epoch: 10 step: 1386, loss is 0.0007171799661591649\n",
      "epoch: 10 step: 1387, loss is 0.004008845891803503\n",
      "epoch: 10 step: 1388, loss is 0.00026391790015622973\n",
      "epoch: 10 step: 1389, loss is 0.18095611035823822\n",
      "epoch: 10 step: 1390, loss is 0.009700902737677097\n",
      "epoch: 10 step: 1391, loss is 0.027824297547340393\n",
      "epoch: 10 step: 1392, loss is 0.00038654959644190967\n",
      "epoch: 10 step: 1393, loss is 0.004427645821124315\n",
      "epoch: 10 step: 1394, loss is 0.00034253302146680653\n",
      "epoch: 10 step: 1395, loss is 0.32821333408355713\n",
      "epoch: 10 step: 1396, loss is 0.05989888310432434\n",
      "epoch: 10 step: 1397, loss is 0.0007709458004683256\n",
      "epoch: 10 step: 1398, loss is 4.980538506060839e-05\n",
      "epoch: 10 step: 1399, loss is 0.010996022261679173\n",
      "epoch: 10 step: 1400, loss is 0.0006777518428862095\n",
      "epoch: 10 step: 1401, loss is 0.004834447521716356\n",
      "epoch: 10 step: 1402, loss is 0.004807308316230774\n",
      "epoch: 10 step: 1403, loss is 0.06515238434076309\n",
      "epoch: 10 step: 1404, loss is 0.04219328984618187\n",
      "epoch: 10 step: 1405, loss is 0.046536434441804886\n",
      "epoch: 10 step: 1406, loss is 0.0073946937918663025\n",
      "epoch: 10 step: 1407, loss is 0.009020335040986538\n",
      "epoch: 10 step: 1408, loss is 0.07850860059261322\n",
      "epoch: 10 step: 1409, loss is 0.0007155000930652022\n",
      "epoch: 10 step: 1410, loss is 0.0016440056497231126\n",
      "epoch: 10 step: 1411, loss is 0.0015590719413012266\n",
      "epoch: 10 step: 1412, loss is 0.0008978972327895463\n",
      "epoch: 10 step: 1413, loss is 0.00044587216689251363\n",
      "epoch: 10 step: 1414, loss is 0.0038108660373836756\n",
      "epoch: 10 step: 1415, loss is 0.006555804517120123\n",
      "epoch: 10 step: 1416, loss is 0.03650650382041931\n",
      "epoch: 10 step: 1417, loss is 0.0025866671930998564\n",
      "epoch: 10 step: 1418, loss is 0.0012404053704813123\n",
      "epoch: 10 step: 1419, loss is 0.018179358914494514\n",
      "epoch: 10 step: 1420, loss is 0.006008259952068329\n",
      "epoch: 10 step: 1421, loss is 0.005381509196013212\n",
      "epoch: 10 step: 1422, loss is 0.007293858099728823\n",
      "epoch: 10 step: 1423, loss is 0.00011625753541011363\n",
      "epoch: 10 step: 1424, loss is 0.0008625759510323405\n",
      "epoch: 10 step: 1425, loss is 0.00024290168948937207\n",
      "epoch: 10 step: 1426, loss is 0.0007971274899318814\n",
      "epoch: 10 step: 1427, loss is 0.00036709767300635576\n",
      "epoch: 10 step: 1428, loss is 0.0013892629649490118\n",
      "epoch: 10 step: 1429, loss is 0.005713988095521927\n",
      "epoch: 10 step: 1430, loss is 0.00020260829478502274\n",
      "epoch: 10 step: 1431, loss is 0.01402259711176157\n",
      "epoch: 10 step: 1432, loss is 0.0004170324536971748\n",
      "epoch: 10 step: 1433, loss is 0.002531087724491954\n",
      "epoch: 10 step: 1434, loss is 0.001033323584124446\n",
      "epoch: 10 step: 1435, loss is 0.0011462761322036386\n",
      "epoch: 10 step: 1436, loss is 0.000119804433779791\n",
      "epoch: 10 step: 1437, loss is 0.00684206560254097\n",
      "epoch: 10 step: 1438, loss is 0.0004010943230241537\n",
      "epoch: 10 step: 1439, loss is 4.052941221743822e-05\n",
      "epoch: 10 step: 1440, loss is 0.0039838263764977455\n",
      "epoch: 10 step: 1441, loss is 2.2341040676110424e-05\n",
      "epoch: 10 step: 1442, loss is 0.0013361574383452535\n",
      "epoch: 10 step: 1443, loss is 0.00191069848369807\n",
      "epoch: 10 step: 1444, loss is 0.013498254120349884\n",
      "epoch: 10 step: 1445, loss is 0.0003514589334372431\n",
      "epoch: 10 step: 1446, loss is 4.8641060857335106e-05\n",
      "epoch: 10 step: 1447, loss is 0.0032299025915563107\n",
      "epoch: 10 step: 1448, loss is 0.00038784215576015413\n",
      "epoch: 10 step: 1449, loss is 0.004196800757199526\n",
      "epoch: 10 step: 1450, loss is 0.003941723611205816\n",
      "epoch: 10 step: 1451, loss is 0.00024500221479684114\n",
      "epoch: 10 step: 1452, loss is 0.024214154109358788\n",
      "epoch: 10 step: 1453, loss is 0.07160402089357376\n",
      "epoch: 10 step: 1454, loss is 0.0001135070197051391\n",
      "epoch: 10 step: 1455, loss is 0.0013130289735272527\n",
      "epoch: 10 step: 1456, loss is 0.0037515743169933558\n",
      "epoch: 10 step: 1457, loss is 0.003942581824958324\n",
      "epoch: 10 step: 1458, loss is 0.0008775605820119381\n",
      "epoch: 10 step: 1459, loss is 0.06263066828250885\n",
      "epoch: 10 step: 1460, loss is 0.0001597526133991778\n",
      "epoch: 10 step: 1461, loss is 0.00396822951734066\n",
      "epoch: 10 step: 1462, loss is 0.0001662198337726295\n",
      "epoch: 10 step: 1463, loss is 4.0216993511421606e-05\n",
      "epoch: 10 step: 1464, loss is 0.00030945430626161397\n",
      "epoch: 10 step: 1465, loss is 0.0012085101334378123\n",
      "epoch: 10 step: 1466, loss is 0.0007059222552925348\n",
      "epoch: 10 step: 1467, loss is 0.0005971966893412173\n",
      "epoch: 10 step: 1468, loss is 0.000524994102306664\n",
      "epoch: 10 step: 1469, loss is 4.218249523546547e-05\n",
      "epoch: 10 step: 1470, loss is 0.0004471857682801783\n",
      "epoch: 10 step: 1471, loss is 3.6553181416820735e-05\n",
      "epoch: 10 step: 1472, loss is 2.893752753152512e-05\n",
      "epoch: 10 step: 1473, loss is 0.005836441181600094\n",
      "epoch: 10 step: 1474, loss is 0.35196059942245483\n",
      "epoch: 10 step: 1475, loss is 0.03332182392477989\n",
      "epoch: 10 step: 1476, loss is 0.000964265433140099\n",
      "epoch: 10 step: 1477, loss is 0.0007214925135485828\n",
      "epoch: 10 step: 1478, loss is 0.0010489692213013768\n",
      "epoch: 10 step: 1479, loss is 0.00031346260220743716\n",
      "epoch: 10 step: 1480, loss is 0.00012863568554166704\n",
      "epoch: 10 step: 1481, loss is 0.02200951986014843\n",
      "epoch: 10 step: 1482, loss is 0.008012599311769009\n",
      "epoch: 10 step: 1483, loss is 0.05793216824531555\n",
      "epoch: 10 step: 1484, loss is 4.3176582948945e-06\n",
      "epoch: 10 step: 1485, loss is 0.0008369091083295643\n",
      "epoch: 10 step: 1486, loss is 0.011672290973365307\n",
      "epoch: 10 step: 1487, loss is 0.0004185789148323238\n",
      "epoch: 10 step: 1488, loss is 0.001142362249083817\n",
      "epoch: 10 step: 1489, loss is 0.00015950256783980876\n",
      "epoch: 10 step: 1490, loss is 0.0022700040135532618\n",
      "epoch: 10 step: 1491, loss is 0.0006181598291732371\n",
      "epoch: 10 step: 1492, loss is 0.08423013240098953\n",
      "epoch: 10 step: 1493, loss is 0.00032568120514042675\n",
      "epoch: 10 step: 1494, loss is 0.019835175946354866\n",
      "epoch: 10 step: 1495, loss is 0.000895922479685396\n",
      "epoch: 10 step: 1496, loss is 0.017261821776628494\n",
      "epoch: 10 step: 1497, loss is 0.009922610595822334\n",
      "epoch: 10 step: 1498, loss is 0.010718288831412792\n",
      "epoch: 10 step: 1499, loss is 0.03159742057323456\n",
      "epoch: 10 step: 1500, loss is 0.00028589193243533373\n",
      "epoch: 10 step: 1501, loss is 0.00021858543914277107\n",
      "epoch: 10 step: 1502, loss is 0.0036464212462306023\n",
      "epoch: 10 step: 1503, loss is 0.01589927449822426\n",
      "epoch: 10 step: 1504, loss is 0.009276867844164371\n",
      "epoch: 10 step: 1505, loss is 0.010228736326098442\n",
      "epoch: 10 step: 1506, loss is 0.001758134108968079\n",
      "epoch: 10 step: 1507, loss is 0.09039958566427231\n",
      "epoch: 10 step: 1508, loss is 7.623815326951444e-05\n",
      "epoch: 10 step: 1509, loss is 0.0039167203940451145\n",
      "epoch: 10 step: 1510, loss is 0.00032037566415965557\n",
      "epoch: 10 step: 1511, loss is 4.457756949705072e-05\n",
      "epoch: 10 step: 1512, loss is 1.859801159298513e-05\n",
      "epoch: 10 step: 1513, loss is 0.000605753215495497\n",
      "epoch: 10 step: 1514, loss is 0.0001949247671291232\n",
      "epoch: 10 step: 1515, loss is 0.0011910025496035814\n",
      "epoch: 10 step: 1516, loss is 0.23239542543888092\n",
      "epoch: 10 step: 1517, loss is 0.05071846395730972\n",
      "epoch: 10 step: 1518, loss is 0.0009049522923305631\n",
      "epoch: 10 step: 1519, loss is 0.0028215665370225906\n",
      "epoch: 10 step: 1520, loss is 0.003176626283675432\n",
      "epoch: 10 step: 1521, loss is 0.0007145313429646194\n",
      "epoch: 10 step: 1522, loss is 0.0010452663991600275\n",
      "epoch: 10 step: 1523, loss is 0.14052258431911469\n",
      "epoch: 10 step: 1524, loss is 0.0034607897978276014\n",
      "epoch: 10 step: 1525, loss is 8.240813622251153e-05\n",
      "epoch: 10 step: 1526, loss is 0.0019452903652563691\n",
      "epoch: 10 step: 1527, loss is 0.01120744552463293\n",
      "epoch: 10 step: 1528, loss is 0.09139124304056168\n",
      "epoch: 10 step: 1529, loss is 0.1504133641719818\n",
      "epoch: 10 step: 1530, loss is 0.003259260207414627\n",
      "epoch: 10 step: 1531, loss is 0.0010955226607620716\n",
      "epoch: 10 step: 1532, loss is 0.00032241950975731015\n",
      "epoch: 10 step: 1533, loss is 0.06742803007364273\n",
      "epoch: 10 step: 1534, loss is 0.003696044674143195\n",
      "epoch: 10 step: 1535, loss is 0.0010690921917557716\n",
      "epoch: 10 step: 1536, loss is 0.00019364907348062843\n",
      "epoch: 10 step: 1537, loss is 4.4750409870175645e-05\n",
      "epoch: 10 step: 1538, loss is 0.003813104936853051\n",
      "epoch: 10 step: 1539, loss is 0.0006666480330750346\n",
      "epoch: 10 step: 1540, loss is 0.0001814669230952859\n",
      "epoch: 10 step: 1541, loss is 0.004965429659932852\n",
      "epoch: 10 step: 1542, loss is 0.0014936529332771897\n",
      "epoch: 10 step: 1543, loss is 0.0012878852430731058\n",
      "epoch: 10 step: 1544, loss is 0.007113651838153601\n",
      "epoch: 10 step: 1545, loss is 0.08571745455265045\n",
      "epoch: 10 step: 1546, loss is 0.0006022023153491318\n",
      "epoch: 10 step: 1547, loss is 0.004622523207217455\n",
      "epoch: 10 step: 1548, loss is 0.005541513673961163\n",
      "epoch: 10 step: 1549, loss is 0.0014514824142679572\n",
      "epoch: 10 step: 1550, loss is 0.17342393100261688\n",
      "epoch: 10 step: 1551, loss is 0.01912062242627144\n",
      "epoch: 10 step: 1552, loss is 0.06460648030042648\n",
      "epoch: 10 step: 1553, loss is 0.012255149893462658\n",
      "epoch: 10 step: 1554, loss is 0.06809849292039871\n",
      "epoch: 10 step: 1555, loss is 0.006643711589276791\n",
      "epoch: 10 step: 1556, loss is 0.09818434715270996\n",
      "epoch: 10 step: 1557, loss is 0.018380865454673767\n",
      "epoch: 10 step: 1558, loss is 0.018922535702586174\n",
      "epoch: 10 step: 1559, loss is 0.10721142590045929\n",
      "epoch: 10 step: 1560, loss is 0.000709746265783906\n",
      "epoch: 10 step: 1561, loss is 0.0033821971155703068\n",
      "epoch: 10 step: 1562, loss is 0.00022432189143728465\n",
      "epoch: 10 step: 1563, loss is 0.0025102985091507435\n",
      "epoch: 10 step: 1564, loss is 0.005223571322858334\n",
      "epoch: 10 step: 1565, loss is 0.028746401891112328\n",
      "epoch: 10 step: 1566, loss is 0.0018089843215420842\n",
      "epoch: 10 step: 1567, loss is 0.0024187504313886166\n",
      "epoch: 10 step: 1568, loss is 0.007197511848062277\n",
      "epoch: 10 step: 1569, loss is 0.061571117490530014\n",
      "epoch: 10 step: 1570, loss is 0.0008478302625007927\n",
      "epoch: 10 step: 1571, loss is 0.04015887528657913\n",
      "epoch: 10 step: 1572, loss is 0.018313663080334663\n",
      "epoch: 10 step: 1573, loss is 0.05610669031739235\n",
      "epoch: 10 step: 1574, loss is 0.0005322895594872534\n",
      "epoch: 10 step: 1575, loss is 0.000173579144757241\n",
      "epoch: 10 step: 1576, loss is 0.0033039271365851164\n",
      "epoch: 10 step: 1577, loss is 0.0019881129264831543\n",
      "epoch: 10 step: 1578, loss is 0.17076966166496277\n",
      "epoch: 10 step: 1579, loss is 0.012477812357246876\n",
      "epoch: 10 step: 1580, loss is 0.00044994332711212337\n",
      "epoch: 10 step: 1581, loss is 0.003037956776097417\n",
      "epoch: 10 step: 1582, loss is 0.001906819292344153\n",
      "epoch: 10 step: 1583, loss is 0.0001347781071672216\n",
      "epoch: 10 step: 1584, loss is 0.00030321895610541105\n",
      "epoch: 10 step: 1585, loss is 0.0015871330397203565\n",
      "epoch: 10 step: 1586, loss is 0.00025019244640134275\n",
      "epoch: 10 step: 1587, loss is 0.012216154485940933\n",
      "epoch: 10 step: 1588, loss is 8.751024142839015e-05\n",
      "epoch: 10 step: 1589, loss is 0.020527295768260956\n",
      "epoch: 10 step: 1590, loss is 0.00021657800243701786\n",
      "epoch: 10 step: 1591, loss is 0.003937241155654192\n",
      "epoch: 10 step: 1592, loss is 0.00019178226648364216\n",
      "epoch: 10 step: 1593, loss is 0.0002419179945718497\n",
      "epoch: 10 step: 1594, loss is 0.000668165332172066\n",
      "epoch: 10 step: 1595, loss is 6.436517287511379e-05\n",
      "epoch: 10 step: 1596, loss is 0.037233974784612656\n",
      "epoch: 10 step: 1597, loss is 0.031875014305114746\n",
      "epoch: 10 step: 1598, loss is 0.0022698002867400646\n",
      "epoch: 10 step: 1599, loss is 0.004573247395455837\n",
      "epoch: 10 step: 1600, loss is 0.000804154493380338\n",
      "epoch: 10 step: 1601, loss is 0.0009884999599307775\n",
      "epoch: 10 step: 1602, loss is 0.04683316498994827\n",
      "epoch: 10 step: 1603, loss is 5.2077863074373454e-05\n",
      "epoch: 10 step: 1604, loss is 0.003598844865337014\n",
      "epoch: 10 step: 1605, loss is 0.0007295555551536381\n",
      "epoch: 10 step: 1606, loss is 0.02919132448732853\n",
      "epoch: 10 step: 1607, loss is 0.00033584696939215064\n",
      "epoch: 10 step: 1608, loss is 0.00022844300838187337\n",
      "epoch: 10 step: 1609, loss is 0.007871503010392189\n",
      "epoch: 10 step: 1610, loss is 0.0005834146868437529\n",
      "epoch: 10 step: 1611, loss is 0.0017970636254176497\n",
      "epoch: 10 step: 1612, loss is 0.10739727318286896\n",
      "epoch: 10 step: 1613, loss is 0.00021247459517326206\n",
      "epoch: 10 step: 1614, loss is 0.0018686798866838217\n",
      "epoch: 10 step: 1615, loss is 0.001704931491985917\n",
      "epoch: 10 step: 1616, loss is 0.0002888640738092363\n",
      "epoch: 10 step: 1617, loss is 0.0003294950001873076\n",
      "epoch: 10 step: 1618, loss is 0.0003946491633541882\n",
      "epoch: 10 step: 1619, loss is 0.001428660354577005\n",
      "epoch: 10 step: 1620, loss is 0.012596370652318\n",
      "epoch: 10 step: 1621, loss is 0.005972180049866438\n",
      "epoch: 10 step: 1622, loss is 0.0007112614111974835\n",
      "epoch: 10 step: 1623, loss is 0.10337569564580917\n",
      "epoch: 10 step: 1624, loss is 0.00424157315865159\n",
      "epoch: 10 step: 1625, loss is 0.012795472517609596\n",
      "epoch: 10 step: 1626, loss is 5.849897570442408e-05\n",
      "epoch: 10 step: 1627, loss is 0.014136786572635174\n",
      "epoch: 10 step: 1628, loss is 0.06886462867259979\n",
      "epoch: 10 step: 1629, loss is 0.0008643711917102337\n",
      "epoch: 10 step: 1630, loss is 4.641776740754722e-06\n",
      "epoch: 10 step: 1631, loss is 8.94087424967438e-05\n",
      "epoch: 10 step: 1632, loss is 0.09164388477802277\n",
      "epoch: 10 step: 1633, loss is 0.010791716165840626\n",
      "epoch: 10 step: 1634, loss is 0.0005986324395053089\n",
      "epoch: 10 step: 1635, loss is 0.04025351256132126\n",
      "epoch: 10 step: 1636, loss is 3.0346960556926206e-05\n",
      "epoch: 10 step: 1637, loss is 0.00024651482817716897\n",
      "epoch: 10 step: 1638, loss is 0.007101318798959255\n",
      "epoch: 10 step: 1639, loss is 0.09739893674850464\n",
      "epoch: 10 step: 1640, loss is 0.0021716677583754063\n",
      "epoch: 10 step: 1641, loss is 5.802571104140952e-05\n",
      "epoch: 10 step: 1642, loss is 0.0006477048736996949\n",
      "epoch: 10 step: 1643, loss is 0.009643509984016418\n",
      "epoch: 10 step: 1644, loss is 0.11368662863969803\n",
      "epoch: 10 step: 1645, loss is 7.672223728150129e-05\n",
      "epoch: 10 step: 1646, loss is 0.0010608654702082276\n",
      "epoch: 10 step: 1647, loss is 0.001459059421904385\n",
      "epoch: 10 step: 1648, loss is 0.0013810602249577641\n",
      "epoch: 10 step: 1649, loss is 2.627778121677693e-05\n",
      "epoch: 10 step: 1650, loss is 0.019847700372338295\n",
      "epoch: 10 step: 1651, loss is 0.0015250417636707425\n",
      "epoch: 10 step: 1652, loss is 0.00043565238593146205\n",
      "epoch: 10 step: 1653, loss is 0.1507287174463272\n",
      "epoch: 10 step: 1654, loss is 4.880221968051046e-05\n",
      "epoch: 10 step: 1655, loss is 6.471912638517097e-05\n",
      "epoch: 10 step: 1656, loss is 0.0006995921139605343\n",
      "epoch: 10 step: 1657, loss is 0.010152320377528667\n",
      "epoch: 10 step: 1658, loss is 3.262434256612323e-05\n",
      "epoch: 10 step: 1659, loss is 4.957793862558901e-05\n",
      "epoch: 10 step: 1660, loss is 0.006491676904261112\n",
      "epoch: 10 step: 1661, loss is 0.0006208744016475976\n",
      "epoch: 10 step: 1662, loss is 0.0039022404234856367\n",
      "epoch: 10 step: 1663, loss is 0.00025340530555695295\n",
      "epoch: 10 step: 1664, loss is 8.508563041687012e-05\n",
      "epoch: 10 step: 1665, loss is 0.03489373251795769\n",
      "epoch: 10 step: 1666, loss is 0.04284808784723282\n",
      "epoch: 10 step: 1667, loss is 0.00024040075368247926\n",
      "epoch: 10 step: 1668, loss is 0.0006895536207593977\n",
      "epoch: 10 step: 1669, loss is 0.002259783213958144\n",
      "epoch: 10 step: 1670, loss is 0.0037223880644887686\n",
      "epoch: 10 step: 1671, loss is 2.7299392968416214e-05\n",
      "epoch: 10 step: 1672, loss is 0.0003333972708787769\n",
      "epoch: 10 step: 1673, loss is 0.06012178957462311\n",
      "epoch: 10 step: 1674, loss is 0.008557848632335663\n",
      "epoch: 10 step: 1675, loss is 4.330388765083626e-05\n",
      "epoch: 10 step: 1676, loss is 0.0007549297297373414\n",
      "epoch: 10 step: 1677, loss is 0.13995882868766785\n",
      "epoch: 10 step: 1678, loss is 0.002505512675270438\n",
      "epoch: 10 step: 1679, loss is 0.00039802343235351145\n",
      "epoch: 10 step: 1680, loss is 0.005847780499607325\n",
      "epoch: 10 step: 1681, loss is 0.014955548569560051\n",
      "epoch: 10 step: 1682, loss is 0.019773492589592934\n",
      "epoch: 10 step: 1683, loss is 0.007208314724266529\n",
      "epoch: 10 step: 1684, loss is 0.02632889710366726\n",
      "epoch: 10 step: 1685, loss is 0.00015620767953805625\n",
      "epoch: 10 step: 1686, loss is 0.0003934241540264338\n",
      "epoch: 10 step: 1687, loss is 0.0008531195926479995\n",
      "epoch: 10 step: 1688, loss is 0.000891603238414973\n",
      "epoch: 10 step: 1689, loss is 0.0004621547705028206\n",
      "epoch: 10 step: 1690, loss is 0.011137682013213634\n",
      "epoch: 10 step: 1691, loss is 0.001837787451222539\n",
      "epoch: 10 step: 1692, loss is 0.0006209429702721536\n",
      "epoch: 10 step: 1693, loss is 0.01751680113375187\n",
      "epoch: 10 step: 1694, loss is 0.10646762698888779\n",
      "epoch: 10 step: 1695, loss is 0.0005415815976448357\n",
      "epoch: 10 step: 1696, loss is 0.022070249542593956\n",
      "epoch: 10 step: 1697, loss is 0.0004624660068657249\n",
      "epoch: 10 step: 1698, loss is 0.00038376139127649367\n",
      "epoch: 10 step: 1699, loss is 0.0006670609000138938\n",
      "epoch: 10 step: 1700, loss is 0.0001974404585780576\n",
      "epoch: 10 step: 1701, loss is 0.00017410755390301347\n",
      "epoch: 10 step: 1702, loss is 0.001411797828041017\n",
      "epoch: 10 step: 1703, loss is 0.010435485281050205\n",
      "epoch: 10 step: 1704, loss is 0.0011422717943787575\n",
      "epoch: 10 step: 1705, loss is 0.00537505280226469\n",
      "epoch: 10 step: 1706, loss is 0.006967406719923019\n",
      "epoch: 10 step: 1707, loss is 0.0009909216314554214\n",
      "epoch: 10 step: 1708, loss is 0.014109373092651367\n",
      "epoch: 10 step: 1709, loss is 0.005684811156243086\n",
      "epoch: 10 step: 1710, loss is 0.00591910257935524\n",
      "epoch: 10 step: 1711, loss is 0.0021106197964400053\n",
      "epoch: 10 step: 1712, loss is 0.00015214380982797593\n",
      "epoch: 10 step: 1713, loss is 5.643410258926451e-05\n",
      "epoch: 10 step: 1714, loss is 0.033404942601919174\n",
      "epoch: 10 step: 1715, loss is 0.0009878610726445913\n",
      "epoch: 10 step: 1716, loss is 0.009591877460479736\n",
      "epoch: 10 step: 1717, loss is 0.010896318592131138\n",
      "epoch: 10 step: 1718, loss is 0.001981192035600543\n",
      "epoch: 10 step: 1719, loss is 0.13688477873802185\n",
      "epoch: 10 step: 1720, loss is 0.0027699703350663185\n",
      "epoch: 10 step: 1721, loss is 0.035559821873903275\n",
      "epoch: 10 step: 1722, loss is 0.00025146533153019845\n",
      "epoch: 10 step: 1723, loss is 0.0005030512111261487\n",
      "epoch: 10 step: 1724, loss is 0.0034604715183377266\n",
      "epoch: 10 step: 1725, loss is 0.011330735869705677\n",
      "epoch: 10 step: 1726, loss is 0.006201190873980522\n",
      "epoch: 10 step: 1727, loss is 3.786654633586295e-05\n",
      "epoch: 10 step: 1728, loss is 0.006778503768146038\n",
      "epoch: 10 step: 1729, loss is 0.010437259450554848\n",
      "epoch: 10 step: 1730, loss is 0.0009019480203278363\n",
      "epoch: 10 step: 1731, loss is 0.00604739785194397\n",
      "epoch: 10 step: 1732, loss is 0.0004254844971001148\n",
      "epoch: 10 step: 1733, loss is 0.002937992801889777\n",
      "epoch: 10 step: 1734, loss is 0.00342946988530457\n",
      "epoch: 10 step: 1735, loss is 0.0002304466615896672\n",
      "epoch: 10 step: 1736, loss is 0.023269973695278168\n",
      "epoch: 10 step: 1737, loss is 0.015994703397154808\n",
      "epoch: 10 step: 1738, loss is 0.00028683041455224156\n",
      "epoch: 10 step: 1739, loss is 0.0013474770821630955\n",
      "epoch: 10 step: 1740, loss is 0.010486581362783909\n",
      "epoch: 10 step: 1741, loss is 0.001889299019239843\n",
      "epoch: 10 step: 1742, loss is 0.0003788884205278009\n",
      "epoch: 10 step: 1743, loss is 0.08049260824918747\n",
      "epoch: 10 step: 1744, loss is 0.00048704384244047105\n",
      "epoch: 10 step: 1745, loss is 7.508944690926e-05\n",
      "epoch: 10 step: 1746, loss is 0.06258457899093628\n",
      "epoch: 10 step: 1747, loss is 3.618429764173925e-05\n",
      "epoch: 10 step: 1748, loss is 0.0009678989299573004\n",
      "epoch: 10 step: 1749, loss is 0.0010264808079227805\n",
      "epoch: 10 step: 1750, loss is 0.0027706779073923826\n",
      "epoch: 10 step: 1751, loss is 0.0031895979773253202\n",
      "epoch: 10 step: 1752, loss is 0.0023945632856339216\n",
      "epoch: 10 step: 1753, loss is 0.0008133782539516687\n",
      "epoch: 10 step: 1754, loss is 0.008392448537051678\n",
      "epoch: 10 step: 1755, loss is 0.0033230187837034464\n",
      "epoch: 10 step: 1756, loss is 0.0013394127599895\n",
      "epoch: 10 step: 1757, loss is 8.04234150564298e-05\n",
      "epoch: 10 step: 1758, loss is 6.994609429966658e-05\n",
      "epoch: 10 step: 1759, loss is 0.00445994408801198\n",
      "epoch: 10 step: 1760, loss is 0.0663587674498558\n",
      "epoch: 10 step: 1761, loss is 0.0004117001371923834\n",
      "epoch: 10 step: 1762, loss is 0.0024897633120417595\n",
      "epoch: 10 step: 1763, loss is 0.007777283899486065\n",
      "epoch: 10 step: 1764, loss is 0.005972931627184153\n",
      "epoch: 10 step: 1765, loss is 0.0016124212415888906\n",
      "epoch: 10 step: 1766, loss is 0.004903122317045927\n",
      "epoch: 10 step: 1767, loss is 0.0008176395203918219\n",
      "epoch: 10 step: 1768, loss is 0.050634656101465225\n",
      "epoch: 10 step: 1769, loss is 8.677589357830584e-05\n",
      "epoch: 10 step: 1770, loss is 0.0004745290498249233\n",
      "epoch: 10 step: 1771, loss is 0.0002830452867783606\n",
      "epoch: 10 step: 1772, loss is 0.0010351240634918213\n",
      "epoch: 10 step: 1773, loss is 0.08626295626163483\n",
      "epoch: 10 step: 1774, loss is 5.422125832410529e-05\n",
      "epoch: 10 step: 1775, loss is 0.00040568236727267504\n",
      "epoch: 10 step: 1776, loss is 0.014601977542042732\n",
      "epoch: 10 step: 1777, loss is 0.003942765761166811\n",
      "epoch: 10 step: 1778, loss is 6.0616486734943464e-05\n",
      "epoch: 10 step: 1779, loss is 0.004325907677412033\n",
      "epoch: 10 step: 1780, loss is 0.0025386223569512367\n",
      "epoch: 10 step: 1781, loss is 0.001437480328604579\n",
      "epoch: 10 step: 1782, loss is 0.0015225510578602552\n",
      "epoch: 10 step: 1783, loss is 0.0006790452753193676\n",
      "epoch: 10 step: 1784, loss is 0.0005181385786272585\n",
      "epoch: 10 step: 1785, loss is 0.019761214032769203\n",
      "epoch: 10 step: 1786, loss is 0.00010105514229508117\n",
      "epoch: 10 step: 1787, loss is 0.00023019892978481948\n",
      "epoch: 10 step: 1788, loss is 0.00027893594233319163\n",
      "epoch: 10 step: 1789, loss is 0.0025915110018104315\n",
      "epoch: 10 step: 1790, loss is 0.0007831631228327751\n",
      "epoch: 10 step: 1791, loss is 0.20115387439727783\n",
      "epoch: 10 step: 1792, loss is 0.0037081583868712187\n",
      "epoch: 10 step: 1793, loss is 0.0001223643630510196\n",
      "epoch: 10 step: 1794, loss is 0.0009537126170471311\n",
      "epoch: 10 step: 1795, loss is 0.0002680506731849164\n",
      "epoch: 10 step: 1796, loss is 0.0007448497344739735\n",
      "epoch: 10 step: 1797, loss is 6.871601362945512e-05\n",
      "epoch: 10 step: 1798, loss is 0.008147431537508965\n",
      "epoch: 10 step: 1799, loss is 0.0002011786273214966\n",
      "epoch: 10 step: 1800, loss is 0.041376594454050064\n",
      "epoch: 10 step: 1801, loss is 6.696448690490797e-05\n",
      "epoch: 10 step: 1802, loss is 0.003906857687979937\n",
      "epoch: 10 step: 1803, loss is 0.00574745237827301\n",
      "epoch: 10 step: 1804, loss is 0.00022719762637279928\n",
      "epoch: 10 step: 1805, loss is 0.0002889129682444036\n",
      "epoch: 10 step: 1806, loss is 0.00014627931523136795\n",
      "epoch: 10 step: 1807, loss is 1.0276887223881204e-05\n",
      "epoch: 10 step: 1808, loss is 0.019787749275565147\n",
      "epoch: 10 step: 1809, loss is 0.015287060290575027\n",
      "epoch: 10 step: 1810, loss is 2.5843390176305547e-05\n",
      "epoch: 10 step: 1811, loss is 0.00022902560885995626\n",
      "epoch: 10 step: 1812, loss is 0.0006559373578056693\n",
      "epoch: 10 step: 1813, loss is 0.0015238896012306213\n",
      "epoch: 10 step: 1814, loss is 0.008459359407424927\n",
      "epoch: 10 step: 1815, loss is 0.0003221084480173886\n",
      "epoch: 10 step: 1816, loss is 0.11752661317586899\n",
      "epoch: 10 step: 1817, loss is 0.0032260853331536055\n",
      "epoch: 10 step: 1818, loss is 0.00019937168690375984\n",
      "epoch: 10 step: 1819, loss is 0.00011731604899978265\n",
      "epoch: 10 step: 1820, loss is 0.00045512287761084735\n",
      "epoch: 10 step: 1821, loss is 0.0005203830078244209\n",
      "epoch: 10 step: 1822, loss is 0.016165820881724358\n",
      "epoch: 10 step: 1823, loss is 6.929475057404488e-05\n",
      "epoch: 10 step: 1824, loss is 0.008177755400538445\n",
      "epoch: 10 step: 1825, loss is 0.009904615581035614\n",
      "epoch: 10 step: 1826, loss is 5.5371659982483834e-05\n",
      "epoch: 10 step: 1827, loss is 0.001183413085527718\n",
      "epoch: 10 step: 1828, loss is 0.00018011030624620616\n",
      "epoch: 10 step: 1829, loss is 0.0005288728279992938\n",
      "epoch: 10 step: 1830, loss is 0.0004728257190436125\n",
      "epoch: 10 step: 1831, loss is 6.450610817410052e-05\n",
      "epoch: 10 step: 1832, loss is 0.0005132158985361457\n",
      "epoch: 10 step: 1833, loss is 0.009084134362637997\n",
      "epoch: 10 step: 1834, loss is 3.172497599734925e-05\n",
      "epoch: 10 step: 1835, loss is 6.650981231359765e-05\n",
      "epoch: 10 step: 1836, loss is 3.0044328013900667e-05\n",
      "epoch: 10 step: 1837, loss is 0.0012559087481349707\n",
      "epoch: 10 step: 1838, loss is 8.251552935689688e-05\n",
      "epoch: 10 step: 1839, loss is 0.00046550907427445054\n",
      "epoch: 10 step: 1840, loss is 0.0002311848074896261\n",
      "epoch: 10 step: 1841, loss is 0.12158818542957306\n",
      "epoch: 10 step: 1842, loss is 0.0003407233743928373\n",
      "epoch: 10 step: 1843, loss is 0.0015207179822027683\n",
      "epoch: 10 step: 1844, loss is 0.005571209825575352\n",
      "epoch: 10 step: 1845, loss is 5.504788350663148e-05\n",
      "epoch: 10 step: 1846, loss is 0.0005107467295601964\n",
      "epoch: 10 step: 1847, loss is 0.004744993522763252\n",
      "epoch: 10 step: 1848, loss is 9.39966703299433e-05\n",
      "epoch: 10 step: 1849, loss is 0.0026601615827530622\n",
      "epoch: 10 step: 1850, loss is 8.722616621525958e-05\n",
      "epoch: 10 step: 1851, loss is 0.000498661829624325\n",
      "epoch: 10 step: 1852, loss is 0.04285816848278046\n",
      "epoch: 10 step: 1853, loss is 0.00047042962978594005\n",
      "epoch: 10 step: 1854, loss is 3.982690395787358e-05\n",
      "epoch: 10 step: 1855, loss is 0.004677872639149427\n",
      "epoch: 10 step: 1856, loss is 0.004221986047923565\n",
      "epoch: 10 step: 1857, loss is 0.00011440969683462754\n",
      "epoch: 10 step: 1858, loss is 0.0033174902200698853\n",
      "epoch: 10 step: 1859, loss is 0.0012159452307969332\n",
      "epoch: 10 step: 1860, loss is 0.00562951760366559\n",
      "epoch: 10 step: 1861, loss is 0.00017122429562732577\n",
      "epoch: 10 step: 1862, loss is 0.00015822990098968148\n",
      "epoch: 10 step: 1863, loss is 0.0008855073247104883\n",
      "epoch: 10 step: 1864, loss is 0.03730325028300285\n",
      "epoch: 10 step: 1865, loss is 0.01163429580628872\n",
      "epoch: 10 step: 1866, loss is 0.002015674253925681\n",
      "epoch: 10 step: 1867, loss is 5.485718065756373e-05\n",
      "epoch: 10 step: 1868, loss is 0.1274900734424591\n",
      "epoch: 10 step: 1869, loss is 0.0027209792751818895\n",
      "epoch: 10 step: 1870, loss is 0.003113867249339819\n",
      "epoch: 10 step: 1871, loss is 0.0015269749565050006\n",
      "epoch: 10 step: 1872, loss is 0.03534582629799843\n",
      "epoch: 10 step: 1873, loss is 0.00020644326286856085\n",
      "epoch: 10 step: 1874, loss is 0.0002719102776609361\n",
      "epoch: 10 step: 1875, loss is 8.621726010460407e-05\n",
      "Train epoch time: 13303.603 ms, per step time: 7.095 ms\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    context.set_context(mode=context.GRAPH_MODE, device_target='CPU')\n",
    "    # dataset\n",
    "    ds_train = create_dataset(os.path.join('.\\MNIST_DATA', \"train\"),\n",
    "                            cfg.batch_size)\n",
    "    # network\n",
    "    network = LeNet5(cfg.num_classes)\n",
    "    net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction=\"mean\")\n",
    "    net_opt = nn.Momentum(network.trainable_params(), cfg.lr, cfg.momentum)\n",
    "    time_cb = TimeMonitor(data_size=ds_train.get_dataset_size())\n",
    "    \n",
    "    # save checkpoint\n",
    "    config_ck = CheckpointConfig(save_checkpoint_steps=cfg.save_checkpoint_steps,\n",
    "                                 keep_checkpoint_max=cfg.keep_checkpoint_max)\n",
    "    ckpoint_cb = ModelCheckpoint(prefix=\"checkpoint_lenet\", directory='./ckpt', config=config_ck)\n",
    "    \n",
    "    # model\n",
    "    model = Model(network, net_loss, net_opt, metrics={\"Accuracy\": Accuracy()})\n",
    "    \n",
    "    # train\n",
    "    print(\"============== Starting Training ==============\")\n",
    "    model.train(cfg['epoch_size'], ds_train, callbacks=[time_cb, ckpoint_cb, LossMonitor()],\n",
    "                dataset_sink_mode=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindspore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
